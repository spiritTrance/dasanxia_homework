{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.dataset as ds\n",
    "\n",
    "# 定义超参数\n",
    "num_classes = 10 # 分类的类别数\n",
    "num_features = 784 # 输入的特征数（28x28的图片）\n",
    "hidden_size = 256 # 隐藏层的神经元数\n",
    "batch_size = 64 # 批次大小\n",
    "learning_rate = 0.01 # 学习率\n",
    "num_epochs = 10 # 训练的轮数\n",
    "\n",
    "# 创建数据集，这里使用MNIST数据集作为示例\n",
    "train_dataset = ds.MnistDataset(\"mnist/train\")\n",
    "test_dataset = ds.MnistDataset(\"mnist/test\")\n",
    "\n",
    "# 定义数据转换和增强操作\n",
    "transform = [\n",
    "ds.ToTensor(), # 将图片转换为张量\n",
    "ds.Normalize((0.1307,), (0.3081,)) # 将图片归一化\n",
    "]\n",
    "\n",
    "# 应用数据转换和增强操作\n",
    "train_dataset = train_dataset.map(operations=transform, input_columns=\"image\")\n",
    "test_dataset = test_dataset.map(operations=transform, input_columns=\"image\")\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = ds.GeneratorDataset(train_dataset, [\"image\", \"label\"], batch_size=batch_size, shuffle=True)\n",
    "test_loader = ds.GeneratorDataset(test_dataset, [\"image\", \"label\"], batch_size=batch_size)\n",
    "\n",
    "# 定义全连接神经网络模型\n",
    "class FCN(nn.Cell):\n",
    "    def __init__(self, num_features, hidden_size, num_classes):\n",
    "        super(FCN, self).__init__()\n",
    "        self.fc1 = nn.Dense(num_features, hidden_size) # 第一层全连接层，输入特征数为num_features，输出特征数为hidden_size\n",
    "        self.relu = nn.ReLU() # 激活函数，使用ReLU函数\n",
    "        self.fc2 = nn.Dense(hidden_size, num_classes) # 第二层全连接层，输入特征数为hidden_size，输出特征数为num_classes\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.fc1(x) # 将输入x通过第一层全连接层得到输出x\n",
    "        x = self.relu(x) # 将输出x通过激活函数得到输出x\n",
    "        x = self.fc2(x) # 将输出x通过第二层全连接层得到输出x\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = FCN(num_features, hidden_size, num_classes)\n",
    "\n",
    "# 定义优化器，使用随机梯度下降（SGD）算法\n",
    "optimizer = nn.SGD(model.trainable_params(), learning_rate=learning_rate)\n",
    "\n",
    "# 定义损失函数，使用交叉熵损失函数\n",
    "loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "\n",
    "# 定义评估指标，使用准确率（accuracy）\n",
    "metric = mindspore.nn.Accuracy()\n",
    "\n",
    "# 定义训练循环函数\n",
    "def train(model, train_loader, loss_fn, optimizer, metric):\n",
    "    model.train() # 将模型设置为训练模式\n",
    "    for images, labels in train_loader: # 遍历训练数据集中的每个批次\n",
    "        images = images.reshape(-1, num_features) # 将图片张量展平为一维向量，方便输入全连接层\n",
    "        output = model(images) # 将输入图片通过模型得到输出预测值\n",
    "        loss = loss_fn(output, labels) # 计算预测值和真实标签之间的损失值\n",
    "        optimizer.clear_grad() # 清除梯度缓存\n",
    "        loss.backward() # 反向传播计算梯度值\n",
    "        optimizer.step() # 更新模型参数\n",
    "        metric.update(output, labels) # 更新评估指标的值\n",
    "\n",
    "# 定义测试循环函数\n",
    "def test(model, test_loader, metric):\n",
    "    model.eval() # 将模型设置为评估模式\n",
    "    for images, labels in test_loader: # 遍历测试数据集中的每个批次\n",
    "        images = images.reshape(-1, num_features) # 将图片张量展平为一维向量，方便输入全连接层\n",
    "        output = model(images) # 将输入图片通过模型得到输出预测值\n",
    "        metric.update(output, labels) # 更新评估指标的值\n",
    "\n",
    "# 开始训练和测试模型\n",
    "for epoch in range(num_epochs): # 遍历每个训练轮数\n",
    "    train(model, train_loader, loss_fn, optimizer, metric) # 调用训练循环函数进行训练\n",
    "    train_acc = metric.eval() # 计算训练集上的准确率值\n",
    "    metric.clear() # 清除评估指标的缓存值\n",
    "    test(model, test_loader, metric) # 调用测试循环函数进行测试\n",
    "    test_acc = metric.eval() # 计算测试集上的准确率值\n",
    "    metric.clear() # 清除评估指标的缓存值\n",
    "    print(f\"Epoch {epoch+1}, Train Accuracy: {train_acc}, Test Accuracy: {test_acc}\") # 打印每个轮数的训练集和测试集上的准确率值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mindspore' has no attribute 'grad_all_with_sens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[39mreturn\u001b[39;00m net\n\u001b[0;32m     53\u001b[0m \u001b[39m# 训练模型\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m net \u001b[39m=\u001b[39m train(net, x, y)\n\u001b[0;32m     56\u001b[0m \u001b[39m# 打印模型参数\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mw:\u001b[39m\u001b[39m\"\u001b[39m, net\u001b[39m.\u001b[39mw\u001b[39m.\u001b[39masnumpy())\n",
      "Cell \u001b[1;32mIn[3], line 48\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, x, y)\u001b[0m\n\u001b[0;32m     46\u001b[0m y_pred, reg_loss \u001b[39m=\u001b[39m net(x) \u001b[39m# 前向传播\u001b[39;00m\n\u001b[0;32m     47\u001b[0m loss \u001b[39m=\u001b[39m mse_loss(y, y_pred) \u001b[39m+\u001b[39m reg_loss \u001b[39m# 总损失\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m grads \u001b[39m=\u001b[39m mindspore\u001b[39m.\u001b[39;49mgrad_all_with_sens(net)(x, loss) \u001b[39m# 计算梯度\u001b[39;00m\n\u001b[0;32m     49\u001b[0m optimizer(grads) \u001b[39m# 更新参数\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Loss \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39masnumpy()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'mindspore' has no attribute 'grad_all_with_sens'"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "import numpy as np\n",
    "\n",
    "# 定义超参数\n",
    "scale = 0.01 # L1正则化因子\n",
    "learning_rate = 0.1 # 学习率\n",
    "epochs = 100 # 迭代次数\n",
    "\n",
    "# 定义数据集\n",
    "x = np.array([[0.2, 0.7], [0.3, 0.9], [0.4, 0.8], [0.6, 0.4], [0.7, 0.5], [0.9, 0.3]])\n",
    "y = np.array([[2.4], [3.2], [3.6], [2.8], [3.4], [3.8]])\n",
    "x = mindspore.Tensor(x, mindspore.float32)\n",
    "y = mindspore.Tensor(y, mindspore.float32)\n",
    "\n",
    "# 定义模型\n",
    "class LinearRegression(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.w = mindspore.Parameter(mindspore.Tensor(np.random.randn(2, 1), mindspore.float32), name=\"w\")\n",
    "        self.b = mindspore.Parameter(mindspore.Tensor(np.random.randn(1), mindspore.float32), name=\"b\")\n",
    "        self.matmul = ops.MatMul()\n",
    "        self.add = ops.Add()\n",
    "        self.l1_regularizer = nn.L1Regularizer(scale) # L1正则化函数\n",
    "\n",
    "    def construct(self, x):\n",
    "        y_pred = self.add(self.matmul(x, self.w), self.b) # 预测值\n",
    "        reg_loss = self.l1_regularizer(self.w) # 正则化损失\n",
    "        return y_pred, reg_loss\n",
    "\n",
    "# 定义损失函数\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ops.ReduceMean()(ops.Square()(y_true - y_pred)) # 均方误差损失\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "net = LinearRegression()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = nn.SGD(params=net.trainable_params(), learning_rate=learning_rate)\n",
    "\n",
    "# 定义训练过程\n",
    "def train(net, x, y):\n",
    "    for epoch in range(epochs):\n",
    "        y_pred, reg_loss = net(x) # 前向传播\n",
    "        loss = mse_loss(y, y_pred) + reg_loss # 总损失\n",
    "        grads = mindspore.grad_all_with_sens(net)(x, loss) # 计算梯度\n",
    "        optimizer(grads) # 更新参数\n",
    "        print(f\"Epoch {epoch}, Loss {loss.asnumpy()}\")\n",
    "        return net\n",
    "\n",
    "# 训练模型\n",
    "net = train(net, x, y)\n",
    "\n",
    "# 打印模型参数\n",
    "print(\"w:\", net.w.asnumpy())\n",
    "print(\"b:\", net.b.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "from mindspore.train.callback import Callback\n",
    "\n",
    "# 定义一个自定义的回调函数，继承自Callback类\n",
    "class LossAccCallback(Callback):\n",
    "    def __init__(self, model, train_dataset, test_dataset):\n",
    "        super(LossAccCallback, self).__init__()\n",
    "        self.model = model # 模型实例\n",
    "        self.train_dataset = train_dataset # 训练数据集\n",
    "        self.test_dataset = test_dataset # 测试数据集\n",
    "        self.train_loss = [] # 记录每个epoch的训练集上的loss\n",
    "        self.train_acc = [] # 记录每个epoch的训练集上的accuracy\n",
    "        self.test_loss = [] # 记录每个epoch的测试集上的loss\n",
    "        self.test_acc = [] # 记录每个epoch的测试集上的accuracy\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "    # 在每个epoch结束时，调用该方法\n",
    "        cb_params = run_context.original_args() # 获取回调参数\n",
    "        epoch_num = cb_params.cur_epoch_num # 获取当前的epoch数\n",
    "        train_loss = self.model.eval(self.train_dataset, dataset_sink_mode=False)[\"loss\"] # 获取训练集上的loss\n",
    "        train_acc = self.model.eval(self.train_dataset, dataset_sink_mode=False)[\"acc\"] # 获取训练集上的accuracy\n",
    "        test_loss = self.model.eval(self.test_dataset, dataset_sink_mode=False)[\"loss\"] # 获取测试集上的loss\n",
    "        test_acc = self.model.eval(self.test_dataset, dataset_sink_mode=False)[\"acc\"] # 获取测试集上的accuracy\n",
    "        self.train_loss.append(train_loss) # 将训练集上的loss添加到列表中\n",
    "        self.train_acc.append(train_acc) # 将训练集上的accuracy添加到列表中\n",
    "        self.test_loss.append(test_loss) # 将测试集上的loss添加到列表中\n",
    "        self.test_acc.append(test_acc) # 将测试集上的accuracy添加到列表中\n",
    "        print(f\"Epoch {epoch_num}, train loss: {train_loss}, train acc: {train_acc}, test loss: {test_loss}, test acc: {test_acc}\") # 打印每个epoch的结果\n",
    "\n",
    "# 创建模型实例，这里使用一个全连接神经网络作为示例\n",
    "model = nn.Dense(784, 10)\n",
    "\n",
    "# 创建损失函数实例，这里使用交叉熵损失函数作为示例\n",
    "loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "\n",
    "# 创建优化器实例，这里使用Adam优化器作为示例\n",
    "optimizer = nn.Adam(model.trainable_params())\n",
    "\n",
    "# 创建训练数据集和测试数据集，这里使用MNIST数据集作为示例\n",
    "train_dataset = mindspore.dataset.MnistDataset(\"mnist_data\", usage=\"train\")\n",
    "test_dataset = mindspore.dataset.MnistDataset(\"mnist_data\", usage=\"test\")\n",
    "\n",
    "# 创建自定义的回调函数实例，传入模型，训练数据集和测试数据集参数\n",
    "callback = LossAccCallback(model, train_dataset, test_dataset)\n",
    "\n",
    "# 调用model.train方法，开始训练模型，传入callback参数为自定义的回调函数实例\n",
    "model.train(epoch=10, train_dataset=train_dataset, callbacks=callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
