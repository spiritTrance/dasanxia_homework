{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入实验环境"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://bbs.huaweicloud.com/blogs/344369"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import sys\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context\n",
    "from mindspore.nn.metrics import Accuracy, Loss\n",
    "from mindspore.train import Model\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore import Tensor\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target='CPU') \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = edict({\n",
    "    'train_size': 60000,  # 训练集大小\n",
    "    'test_size': 10000,  # 测试集大小\n",
    "    # 'train_size': 600,  # 训练集大小\n",
    "    # 'test_size': 100,  # 测试集大小\n",
    "    'channel': 1,  # 图片通道数\n",
    "    'image_height': 28,  # 图片高度\n",
    "    'image_width': 28,  # 图片宽度\n",
    "    'batch_size': 256,\n",
    "    'num_classes': 10,  # 分类类别\n",
    "    'lr': 0.001,  # 学习率\n",
    "    'epoch_size': 20,  # 训练次数\n",
    "    'data_dir_train': os.path.join('fashion-mnist', 'train'),\n",
    "    'data_dir_test': os.path.join('fashion-mnist', 'test'),\n",
    "}) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取和预处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义函数用于读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file_name):\n",
    "    '''\n",
    "    :param file_name: 文件路径\n",
    "    :return:  训练或者测试数据\n",
    "    如下是训练的图片的二进制格式\n",
    "    [offset] [type]          [value]          [description]\n",
    "    0000     32 bit integer  0x00000803(2051) magic number\n",
    "    0004     32 bit integer  60000            number of images\n",
    "    0008     32 bit integer  28               number of rows\n",
    "    0012     32 bit integer  28               number of columns\n",
    "    0016     unsigned byte   ??               pixel\n",
    "    0017     unsigned byte   ??               pixel\n",
    "    ........\n",
    "    xxxx     unsigned byte   ??               pixel\n",
    "    '''\n",
    "    file_handle = open(file_name, \"rb\")  # 以二进制打开文档\n",
    "    file_content = file_handle.read()  # 读取到缓冲区中\n",
    "    head = struct.unpack_from('>IIII', file_content, 0)  # 取前4个整数，返回一个元组\n",
    "    offset = struct.calcsize('>IIII')\n",
    "    imgNum = head[1]  # 图片数\n",
    "    width = head[2]  # 宽度\n",
    "    height = head[3]  # 高度\n",
    "    bits = imgNum * width * height  # data一共有60000*28*28个像素值\n",
    "    bitsString = '>' + str(bits) + 'B'  # fmt格式：'>47040000B'\n",
    "    imgs = struct.unpack_from(bitsString, file_content, offset)  # 取data数据，返回一个元组\n",
    "    imgs_array = np.array(imgs, np.float32).reshape((imgNum, width * height))  # 最后将读取的数据reshape成 【图片数，图片像素】二维数组\n",
    "    return imgs_array\n",
    "\n",
    "\n",
    "def read_label(file_name):\n",
    "    '''\n",
    "    :param file_name:\n",
    "    :return:\n",
    "    标签的格式如下：\n",
    "    [offset] [type]          [value]          [description]\n",
    "    0000     32 bit integer  0x00000801(2049) magic number (MSB first)\n",
    "    0004     32 bit integer  60000            number of items\n",
    "    0008     unsigned byte   ??               label\n",
    "    0009     unsigned byte   ??               label\n",
    "    ........\n",
    "    xxxx     unsigned byte   ??               label\n",
    "    The labels values are 0 to 9.\n",
    "    '''\n",
    "    file_handle = open(file_name, \"rb\")  # 以二进制打开文档\n",
    "    file_content = file_handle.read()  # 读取到缓冲区中\n",
    "    head = struct.unpack_from('>II', file_content, 0)  # 取前2个整数，返回一个元组\n",
    "    offset = struct.calcsize('>II')\n",
    "    labelNum = head[1]  # label数\n",
    "    bitsString = '>' + str(labelNum) + 'B'  # fmt格式：'>47040000B'\n",
    "    label = struct.unpack_from(bitsString, file_content, offset)  # 取data数据，返回一个元组\n",
    "    return np.array(label, np.int32)\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    # 文件获取\n",
    "    train_image = os.path.join(cfg.data_dir_train, 'train-images-idx3-ubyte')\n",
    "    test_image = os.path.join(cfg.data_dir_test, \"t10k-images-idx3-ubyte\")\n",
    "    train_label = os.path.join(cfg.data_dir_train, \"train-labels-idx1-ubyte\")\n",
    "    test_label = os.path.join(cfg.data_dir_test, \"t10k-labels-idx1-ubyte\")\n",
    "    # 读取数据\n",
    "    train_x = read_image(train_image)\n",
    "    test_x = read_image(test_image)\n",
    "    train_y = read_label(train_label)\n",
    "    test_y = read_label(test_label)\n",
    "    return train_x, train_y, test_x, test_y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = get_data()\n",
    "train_x = train_x.reshape(-1, 1, cfg.image_height, cfg.image_width)\n",
    "test_x = test_x.reshape(-1, 1, cfg.image_height, cfg.image_width)\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "print('训练数据集样本数：', train_x.shape[0])\n",
    "print('测试数据集样本数：', test_y.shape[0])\n",
    "print('通道数/图像长/宽：', train_x.shape[1:])\n",
    "print('一张图像的标签样式：', train_y[0])  # 一共10类，用0-9的数字表达类别。\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(train_x[0,0,...])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换数据类型为Dataset\n",
    "def create_dataset():\n",
    "    XY_train = list(zip(train_x, train_y))\n",
    "    ds_train = ds.GeneratorDataset(XY_train, ['x', 'y'])\n",
    "    ds_train = ds_train.shuffle(buffer_size=1000).batch(cfg.batch_size, drop_remainder=True)\n",
    "    XY_test = list(zip(test_x, test_y))\n",
    "    ds_test = ds.GeneratorDataset(XY_test, ['x', 'y'])\n",
    "    ds_test = ds_test.shuffle(buffer_size=1000).batch(cfg.batch_size, drop_remainder=True)\n",
    "    return ds_train, ds_test \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回调函数声明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个自定义的回调函数，继承自Callback类\n",
    "from mindspore.train.callback import Callback\n",
    "class EvalCallBack(Callback):\n",
    "    def __init__(self, model, train_dataset, test_dataset, dataset_sink_mode = False, epochs_to_eval = 1):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        # epochs_to_eval是一个int数字，代表着：每隔多少个epoch进行一次验证\n",
    "        self.epochs_to_eval = epochs_to_eval\n",
    "        self.per_eval = {\"epoch\": [], \"test_acc\": [], \"train_acc\": [], \"test_loss\": [], \"train_loss\": []}\n",
    "        self.dataset_sink_mode = dataset_sink_mode\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        # 获取到现在的epoch数\n",
    "        cb_param = run_context.original_args()\n",
    "        cur_epoch = cb_param.cur_epoch_num\n",
    "        # 如果达到进行验证的epoch数，则进行以下验证操作\n",
    "        if cur_epoch % self.epochs_to_eval == 0:\n",
    "            # 此处model设定的metrics是准确率Accuracy\n",
    "            train_acc = self.model.eval(self.train_dataset, dataset_sink_mode=self.dataset_sink_mode)[\"train_acc\"]\n",
    "            test_acc = self.model.eval(self.test_dataset, dataset_sink_mode=self.dataset_sink_mode)[\"test_acc\"]\n",
    "            train_loss = self.model.eval(self.train_dataset, dataset_sink_mode=self.dataset_sink_mode)[\"train_loss\"]\n",
    "            test_loss = self.model.eval(self.test_dataset, dataset_sink_mode=self.dataset_sink_mode)[\"test_loss\"]\n",
    "            self.per_eval[\"epoch\"].append(cur_epoch)\n",
    "            self.per_eval[\"test_acc\"].append(test_acc)\n",
    "            self.per_eval[\"train_acc\"].append(train_acc)\n",
    "            self.per_eval[\"train_loss\"].append(train_loss)\n",
    "            self.per_eval[\"test_loss\"].append(test_loss)\n",
    "            print(\"Epoch{}: train_acc: {}, test_acc: {}, train_loss: {}, test_loss{}\".format(cur_epoch, train_acc, test_acc, train_loss, test_loss))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetWork"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseLine NetWork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义卷积神经网络，BaseLine\n",
    "class ForwardFashion_baseline(nn.Cell):\n",
    "    def __init__(self, num_class=10):  # 一共分十类，图片通道数是1\n",
    "        super(ForwardFashion_baseline, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.conv1 = nn.Conv2d(1, 32,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv2 = nn.Conv2d(32, 64,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv3 = nn.Conv2d(64, 128,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(128 * 11 * 11, 128)\n",
    "        self.fc2 = nn.Dense(128, self.num_class)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network With BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义卷积神经网络，批量归一化\n",
    "class ForwardFashionWithBatchNorm(nn.Cell):\n",
    "    def __init__(self, num_class=10):  # 一共分十类，图片通道数是1\n",
    "        super(ForwardFashionWithBatchNorm, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.conv1 = nn.Conv2d(1, 32,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv2 = nn.Conv2d(32, 64,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv3 = nn.Conv2d(64, 128,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(3200, 128)\n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Dense(128, self.num_class)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetWork with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义卷积神经网络，有Dropout正则化\n",
    "class ForwardFashionWithDropout(nn.Cell):\n",
    "    def __init__(self, num_class=10):  # 一共分十类，图片通道数是1\n",
    "        super(ForwardFashionWithDropout, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.conv1 = nn.Conv2d(1, 32,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv2 = nn.Conv2d(32, 64,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv3 = nn.Conv2d(64, 128,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(3200, 128)\n",
    "        self.fc2 = nn.Dense(128, self.num_class)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network With BatchNorm And Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义卷积神经网络，批量归一化\n",
    "class ForwardFashionWithBatchNormAndDropout(nn.Cell):\n",
    "    def __init__(self, num_class=10):  # 一共分十类，图片通道数是1\n",
    "        super(ForwardFashionWithBatchNormAndDropout, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.conv1 = nn.Conv2d(1, 32,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv2 = nn.Conv2d(32, 64,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv3 = nn.Conv2d(64, 128,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(3200, 128)\n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Dense(128, self.num_class)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1正则 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import ops\n",
    "# 定义具有L1正则化的交叉熵损失函数\n",
    "class L1CrossEntropyLoss(nn.Cell):\n",
    "    def __init__(self, params, l1_weight = 1e-4, reduction = \"mean\"):\n",
    "        super(L1CrossEntropyLoss, self).__init__(reduction)\n",
    "        self.params = params\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.l1 = nn.L1Regularizer(l1_weight)\n",
    "        self.reduce_sum = ops.ReduceSum()\n",
    "        self.concat = ops.Concat(axis=0)\n",
    "        self.reshape = ops.Reshape()\n",
    "\n",
    "    def construct(self, pred, label):\n",
    "        flatten_params = self.concat([self.reshape(p, (-1,)) for p in self.params])\n",
    "        ce_loss = self.ce(pred, label)\n",
    "        l1_loss = self.reduce_sum(self.l1(flatten_params))      # 计算L1范数惩罚项\n",
    "        total_loss = ce_loss + l1_loss\n",
    "        return total_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2正则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import ops\n",
    "# 定义具有L2正则化的交叉熵损失函数\n",
    "class L2CrossEntropyLoss(nn.Cell):\n",
    "    def __init__(self, params, l2_weight = 1e-4, reduction = \"mean\"):\n",
    "        super(L2CrossEntropyLoss, self).__init__(reduction)\n",
    "        self.params = params\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.l1 = nn.L1Regularizer(1)\n",
    "        self.l2_weight = l2_weight\n",
    "        self.reduce_sum = ops.ReduceSum()\n",
    "        self.concat = ops.Concat(axis=0)\n",
    "        self.reshape = ops.Reshape()\n",
    "\n",
    "    def construct(self, pred, label):\n",
    "        flatten_params = self.concat([self.reshape(p, (-1,)) for p in self.params])\n",
    "        ce_loss = self.ce(pred, label)\n",
    "        l1_loss = self.reduce_sum(self.l1(flatten_params)) * self.reduce_sum(self.l1(flatten_params)) * self.l2_weight      # 计算L1范数惩罚项\n",
    "        total_loss = ce_loss + l1_loss\n",
    "        return total_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, loss_fn):\n",
    "    ds_train, ds_test = create_dataset()\n",
    "    # 定义模型的损失函数，优化器    \n",
    "    net_opt = nn.Adam(network.trainable_params(), cfg.lr)\n",
    "    # 训练模型\n",
    "    model = Model(network, loss_fn=loss_fn, optimizer=net_opt, metrics={'test_acc': Accuracy(), 'train_acc': Accuracy(), 'test_loss': Loss(),  'train_loss': Loss()})\n",
    "    loss_cb = EvalCallBack(model=model, train_dataset=ds_train, test_dataset=ds_test)\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    model.train(cfg.epoch_size, ds_train, callbacks=[loss_cb], dataset_sink_mode=True)\n",
    "    print(loss_cb.per_eval)\n",
    "    metricDic = loss_cb.per_eval\n",
    "    return model, metricDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveMetric(path, metric):\n",
    "    with open(path, encoding=\"utf8\", mode=\"w\") as f:\n",
    "        f.write(str(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(path, model):\n",
    "    mindspore.save_checkpoint(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def drawAndSaveFig(path, metricList, title):\n",
    "    print(metricList)\n",
    "    epochNum = len(metricList[\"train_acc\"])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot([i for i in range(epochNum)], metricList[\"train_acc\"], metricList[\"test_acc\"])\n",
    "    plt.legend([\"train_acc\", \"test_acc\"])\n",
    "    plt.title(title + \"_acc\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot([i for i in range(epochNum)], metricList[\"train_loss\"], metricList[\"test_loss\"])\n",
    "    plt.legend([\"train_loss\", \"test_loss\"])\n",
    "    plt.title(title + \"_loss\")\n",
    "    plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "network = ForwardFashion_baseline(cfg.num_classes)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model, metricDic = train(network, loss_fn)\n",
    "saveMetric(\"./metric/baseline_metric.txt\", metricDic)\n",
    "saveModel(\"./model/baseline_model.ckpt\", network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawAndSaveFig(\"./img/metric_baseLine\", metricDic, \"BaseLine\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1\n",
    "network_L1 = ForwardFashion_baseline(cfg.num_classes)\n",
    "loss_fn_L1 = L1CrossEntropyLoss(network_L1.trainable_params(), l1_weight=1e-8)\n",
    "model_L1, metricDic_L1 = train(network_L1, loss_fn_L1)\n",
    "saveMetric(\"./metric/baseline_metric_L1.txt\", metricDic_L1)\n",
    "saveModel(\"./model/baseline_model_L1.ckpt\", network_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawAndSaveFig(\"./img/metric_L1\", metricDic_L1, \"metric_L1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2\n",
    "network_L2 = ForwardFashion_baseline(cfg.num_classes)\n",
    "loss_fn_L2 = L2CrossEntropyLoss(network_L2.trainable_params(), l2_weight=1e-15)\n",
    "model_L2, metricDic_L2 = train(network_L2, loss_fn_L2)\n",
    "saveMetric(\"./metric/baseline_metric_L2.txt\", metricDic_L2)\n",
    "saveModel(\"./model/baseline_model_L2.ckpt\", network_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawAndSaveFig(\"./img/metric_L2\", metricDic_L2, \"metric_L2\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BN\n",
    "network_BN = ForwardFashionWithBatchNorm(cfg.num_classes)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model_BN, metricDic_BN = train(network_BN, loss_fn)\n",
    "saveMetric(\"./metric/baseline_metric_BN.txt\", metricDic_BN)\n",
    "saveModel(\"./model/baseline_model_BN.ckpt\", network_BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawAndSaveFig(\"./img/metric_BN\", metricDic_BN, \"metric_BN\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "network_Drop = ForwardFashionWithDropout(cfg.num_classes)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model_Drop, metricDic_Drop = train(network_Drop, loss_fn)\n",
    "saveMetric(\"./metric/baseline_metric_Drop.txt\", metricDic_Drop)\n",
    "saveModel(\"./model/baseline_model_Drop.ckpt\", network_Drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawAndSaveFig(\"./img/metric_Dropout\", metricDic_Drop, \"metric_Dropout\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BN + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout + BN\n",
    "network_Drop_BN = ForwardFashionWithBatchNormAndDropout(cfg.num_classes)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model_Drop_BN, metricDic_Drop_BN = train(network_Drop_BN, loss_fn)\n",
    "saveMetric(\"./metric/baseline_metric_Drop_BN.txt\", metricDic_Drop_BN)\n",
    "saveModel(\"./model/baseline_model_Drop_BN.ckpt\", network_Drop_BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawAndSaveFig(\"./img/metric_Dropout_BN\", metricDic_Drop_BN, \"metric_Dropout_BN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train.callback import Callback\n",
    "class EvalCallBackWithEarlyStop(Callback):\n",
    "    def __init__(self, model, train_dataset, test_dataset, dataset_sink_mode = False, epochs_to_eval = 1, acc_delta_limit = 0.01, patience = 0):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        # epochs_to_eval是一个int数字，代表着：每隔多少个epoch进行一次验证\n",
    "        self.epochs_to_eval = epochs_to_eval\n",
    "        self.per_eval = {\"epoch\": [], \"test_acc\": [], \"train_acc\": [], \"test_loss\": [], \"train_loss\": []}\n",
    "        self.dataset_sink_mode = dataset_sink_mode\n",
    "        self.accDelta = acc_delta_limit\n",
    "        self.patience = patience\n",
    "        self.failEpoch = 0\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        # 获取到现在的epoch数\n",
    "        cb_param = run_context.original_args()\n",
    "        cur_epoch = cb_param.cur_epoch_num\n",
    "        # 如果达到进行验证的epoch数，则进行以下验证操作\n",
    "        if cur_epoch % self.epochs_to_eval == 0:\n",
    "            # 此处model设定的metrics是准确率Accuracy\n",
    "            train_acc = self.model.eval(self.train_dataset, dataset_sink_mode=self.dataset_sink_mode)[\"train_acc\"]\n",
    "            test_acc = self.model.eval(self.test_dataset, dataset_sink_mode=self.dataset_sink_mode)[\"test_acc\"]\n",
    "            train_loss = self.model.eval(self.train_dataset, dataset_sink_mode=self.dataset_sink_mode)[\"train_loss\"]\n",
    "            test_loss = self.model.eval(self.test_dataset, dataset_sink_mode=self.dataset_sink_mode)[\"test_loss\"]\n",
    "            self.per_eval[\"epoch\"].append(cur_epoch)\n",
    "            self.per_eval[\"test_acc\"].append(test_acc)\n",
    "            self.per_eval[\"train_acc\"].append(train_acc)\n",
    "            self.per_eval[\"train_loss\"].append(train_loss)\n",
    "            self.per_eval[\"test_loss\"].append(test_loss)\n",
    "            print(\"Epoch{}: train_acc: {}, test_acc: {}, train_loss: {}, test_loss{}\".format(cur_epoch, train_acc, test_acc, train_loss, test_loss))\n",
    "            delta = test_acc\n",
    "            if len(self.per_eval[\"test_acc\"]) != 1:\n",
    "                delta -= self.per_eval[\"test_acc\"][-2]\n",
    "            print(delta, self.failEpoch, self. patience)\n",
    "            if delta < self.accDelta:\n",
    "                if self.failEpoch >= self.patience:\n",
    "                    print(\"Stop.\")\n",
    "                    run_context.request_stop()\n",
    "                else:\n",
    "                    self.failEpoch += 1\n",
    "                    print(\"本次更新未达标，相关参数: \", self.failEpoch)\n",
    "            else:\n",
    "                self.failEpoch = 0      # 达标，清空\n",
    "                    \n",
    "                    \n",
    "\n",
    "# early stop\n",
    "cfg.epoch_size = 100\n",
    "def train_earlyStop(network, loss_fn):\n",
    "    ds_train, ds_test = create_dataset()\n",
    "    net_opt = nn.Adam(network.trainable_params(), cfg.lr)\n",
    "    model = Model(network, loss_fn=loss_fn, optimizer=net_opt, metrics={'test_acc': Accuracy(), 'train_acc': Accuracy(), 'test_loss': Loss(),  'train_loss': Loss()})\n",
    "    loss_cb = EvalCallBackWithEarlyStop(model=model, train_dataset=ds_train, test_dataset=ds_test, acc_delta_limit = 0.01, patience = 2)\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    model.train(cfg.epoch_size, ds_train, callbacks=[loss_cb], dataset_sink_mode=True)\n",
    "    print(loss_cb.per_eval)\n",
    "    metricDic = loss_cb.per_eval\n",
    "    return model, metricDic\n",
    "\n",
    "\n",
    "network_earlyStop = ForwardFashion_baseline(cfg.num_classes)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model_earlyStop, metricDic_earlyStop = train_earlyStop(network_earlyStop, loss_fn)\n",
    "saveMetric(\"./metric/baseline_metric_EarlyStop.txt\", metricDic_earlyStop)\n",
    "saveModel(\"./model/baseline_model_EarlyStop.ckpt\", network_earlyStop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘图\n",
    "drawAndSaveFig(\"./img/metric_EarlyStop\", metricDic_earlyStop, \"metric_EarlyStop\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------定义可视化函数--------------------------------\n",
    "# 输入预测结果序列，真实标签序列，以及图片序列\n",
    "# 目标是根据预测值对错，让其标签显示为红色或者蓝色。对：标签为红色；错：标签为蓝色\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "def plot_image(predictions_array, true_label, img):\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    # 显示对应图片\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    # 显示预测结果的颜色，如果对上了是蓝色，否则为红色\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    # 显示对应标签的格式，样式\n",
    "    plt.xlabel('{},{:2.0f}% ({})'.format(class_names[predicted_label],\n",
    "                                         100 * np.max(predictions_array),\n",
    "                                         class_names[true_label]), color=color)\n",
    "# 将预测的结果以柱状图形状显示蓝对红错\n",
    "def plot_value_array(predictions_array, true_label):\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    this_plot = plt.bar(range(10), predictions_array, color='#777777')\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    this_plot[predicted_label].set_color('red')\n",
    "    this_plot[true_label].set_color('blue')\n",
    "\n",
    "import numpy as np\n",
    "def softmax_np(x):\n",
    "    x = x - np.max(x)\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x/np.sum(exp_x)\n",
    "    return softmax_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "ds_test, _ = create_dataset()\n",
    "test_ = ds_test.create_dict_iterator(output_numpy=True).__next__()\n",
    "predictions = model_Drop_BN.predict(Tensor(test_['x']))\n",
    "predictions = predictions.asnumpy()\n",
    "for i in range(15):\n",
    "    p_np = predictions[i, :]\n",
    "    p_list = p_np.tolist()\n",
    "    print('第' + str(i) + '个sample预测结果：', p_list.index(max(p_list)), '   真实结果：', test_['y'][i])\n",
    "\n",
    "# 预测15个图像与标签，并展现出来\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows * num_cols\n",
    "plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)\n",
    "    pred_np_ = predictions[i, :]\n",
    "    pred_np_ = softmax_np(pred_np_)\n",
    "    plot_image(pred_np_, test_['y'][i], test_['x'][i, 0, ...])\n",
    "    plt.subplot(num_rows, 2 * num_cols, 2 * i + 2)\n",
    "    plot_value_array(pred_np_, test_['y'][i])\n",
    "plt.show() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
