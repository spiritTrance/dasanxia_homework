{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入实验环境"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import sys\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train import Model\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore import Tensor\n",
    "\n",
    "# context.set_context(mode=context.GRAPH_MODE, device_target='Ascend') \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = edict({\n",
    "    'train_size': 60000,  # 训练集大小\n",
    "    'test_size': 10000,  # 测试集大小\n",
    "    'channel': 1,  # 图片通道数\n",
    "    'image_height': 28,  # 图片高度\n",
    "    'image_width': 28,  # 图片宽度\n",
    "    'batch_size': 64,\n",
    "    'num_classes': 10,  # 分类类别\n",
    "    'lr': 0.001,  # 学习率\n",
    "    'epoch_size': 20,  # 训练次数\n",
    "    'data_dir_train': os.path.join('fashion-mnist', 'train'),\n",
    "    'data_dir_test': os.path.join('fashion-mnist', 'test'),\n",
    "}) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取和预处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 步骤 1\t定义函数用于读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file_name):\n",
    "    '''\n",
    "    :param file_name: 文件路径\n",
    "    :return:  训练或者测试数据\n",
    "    如下是训练的图片的二进制格式\n",
    "    [offset] [type]          [value]          [description]\n",
    "    0000     32 bit integer  0x00000803(2051) magic number\n",
    "    0004     32 bit integer  60000            number of images\n",
    "    0008     32 bit integer  28               number of rows\n",
    "    0012     32 bit integer  28               number of columns\n",
    "    0016     unsigned byte   ??               pixel\n",
    "    0017     unsigned byte   ??               pixel\n",
    "    ........\n",
    "    xxxx     unsigned byte   ??               pixel\n",
    "    '''\n",
    "    file_handle = open(file_name, \"rb\")  # 以二进制打开文档\n",
    "    file_content = file_handle.read()  # 读取到缓冲区中\n",
    "    head = struct.unpack_from('>IIII', file_content, 0)  # 取前4个整数，返回一个元组\n",
    "    offset = struct.calcsize('>IIII')\n",
    "    imgNum = head[1]  # 图片数\n",
    "    width = head[2]  # 宽度\n",
    "    height = head[3]  # 高度\n",
    "    bits = imgNum * width * height  # data一共有60000*28*28个像素值\n",
    "    bitsString = '>' + str(bits) + 'B'  # fmt格式：'>47040000B'\n",
    "    imgs = struct.unpack_from(bitsString, file_content, offset)  # 取data数据，返回一个元组\n",
    "    imgs_array = np.array(imgs, np.float32).reshape((imgNum, width * height))  # 最后将读取的数据reshape成 【图片数，图片像素】二维数组\n",
    "    return imgs_array\n",
    "\n",
    "\n",
    "def read_label(file_name):\n",
    "    '''\n",
    "    :param file_name:\n",
    "    :return:\n",
    "    标签的格式如下：\n",
    "    [offset] [type]          [value]          [description]\n",
    "    0000     32 bit integer  0x00000801(2049) magic number (MSB first)\n",
    "    0004     32 bit integer  60000            number of items\n",
    "    0008     unsigned byte   ??               label\n",
    "    0009     unsigned byte   ??               label\n",
    "    ........\n",
    "    xxxx     unsigned byte   ??               label\n",
    "    The labels values are 0 to 9.\n",
    "    '''\n",
    "    file_handle = open(file_name, \"rb\")  # 以二进制打开文档\n",
    "    file_content = file_handle.read()  # 读取到缓冲区中\n",
    "    head = struct.unpack_from('>II', file_content, 0)  # 取前2个整数，返回一个元组\n",
    "    offset = struct.calcsize('>II')\n",
    "    labelNum = head[1]  # label数\n",
    "    bitsString = '>' + str(labelNum) + 'B'  # fmt格式：'>47040000B'\n",
    "    label = struct.unpack_from(bitsString, file_content, offset)  # 取data数据，返回一个元组\n",
    "    return np.array(label, np.int32)\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    # 文件获取\n",
    "    train_image = os.path.join(cfg.data_dir_train, 'train-images-idx3-ubyte')\n",
    "    test_image = os.path.join(cfg.data_dir_test, \"t10k-images-idx3-ubyte\")\n",
    "    train_label = os.path.join(cfg.data_dir_train, \"train-labels-idx1-ubyte\")\n",
    "    test_label = os.path.join(cfg.data_dir_test, \"t10k-labels-idx1-ubyte\")\n",
    "    # 读取数据\n",
    "    train_x = read_image(train_image)\n",
    "    test_x = read_image(test_image)\n",
    "    train_y = read_label(train_label)\n",
    "    test_y = read_label(test_label)\n",
    "    return train_x, train_y, test_x, test_y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = get_data()\n",
    "train_x = train_x.reshape(-1, 1, cfg.image_height, cfg.image_width)\n",
    "test_x = test_x.reshape(-1, 1, cfg.image_height, cfg.image_width)\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "print('训练数据集样本数：', train_x.shape[0])\n",
    "print('测试数据集样本数：', test_y.shape[0])\n",
    "print('通道数/图像长/宽：', train_x.shape[1:])\n",
    "print('一张图像的标签样式：', train_y[0])  # 一共10类，用0-9的数字表达类别。\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(train_x[0,0,...])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换数据类型为Dataset\n",
    "def create_dataset():\n",
    "    XY_train = list(zip(train_x, train_y))\n",
    "    ds_train = ds.GeneratorDataset(XY_train, ['x', 'y'])\n",
    "    ds_train = ds_train.shuffle(buffer_size=1000).batch(cfg.batch_size, drop_remainder=True)\n",
    "    XY_test = list(zip(test_x, test_y))\n",
    "    ds_test = ds.GeneratorDataset(XY_test, ['x', 'y'])\n",
    "    ds_test = ds_test.shuffle(buffer_size=1000).batch(cfg.batch_size, drop_remainder=True)\n",
    "    return ds_train, ds_test \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义卷积神经网络，无正则化\n",
    "class ForwardFashion(nn.Cell):\n",
    "    def __init__(self, num_class=10):  # 一共分十类，图片通道数是1\n",
    "        super(ForwardFashion, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.conv1 = nn.Conv2d(1, 32,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv2 = nn.Conv2d(32, 64,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv3 = nn.Conv2d(64, 128,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(128 * 11 * 11, 128)\n",
    "        self.fc2 = nn.Dense(128, self.num_class)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义卷积神经网络，有正则化\n",
    "class ForwardFashionRegularization(nn.Cell):\n",
    "    def __init__(self, num_class=10):  # 一共分十类，图片通道数是1\n",
    "        super(ForwardFashionRegularization, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.conv1 = nn.Conv2d(1, 32,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv2 = nn.Conv2d(32, 64,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv3 = nn.Conv2d(64, 128,kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(3200, 128)\n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Dense(128, self.num_class)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Net):\n",
    "    ds_train, ds_test = create_dataset()\n",
    "    # 构建网络\n",
    "    network = Net(cfg.num_classes)\n",
    "    # 定义模型的损失函数，优化器\n",
    "    net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "    net_opt = nn.Adam(network.trainable_params(), cfg.lr)\n",
    "    # 训练模型\n",
    "    model = Model(network, loss_fn=net_loss, optimizer=net_opt, metrics={'acc': Accuracy()})\n",
    "    loss_cb = LossMonitor()\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    model.train(30, ds_train, callbacks=[loss_cb], dataset_sink_mode=True)\n",
    "    # 验证\n",
    "    metric = model.eval(ds_test)\n",
    "    print(metric)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 2.302584409713745\n",
      "epoch: 1 step: 2, loss is 2.3025755882263184\n",
      "epoch: 1 step: 3, loss is 2.301689863204956\n",
      "epoch: 1 step: 4, loss is 2.301267147064209\n",
      "epoch: 1 step: 5, loss is 2.2968130111694336\n",
      "epoch: 1 step: 6, loss is 2.2899179458618164\n",
      "epoch: 1 step: 7, loss is 2.2672300338745117\n",
      "epoch: 1 step: 8, loss is 2.2277050018310547\n",
      "epoch: 1 step: 9, loss is 2.171910524368286\n",
      "epoch: 1 step: 10, loss is 2.0779671669006348\n",
      "epoch: 1 step: 11, loss is 1.925502061843872\n",
      "epoch: 1 step: 12, loss is 1.8303489685058594\n",
      "epoch: 1 step: 13, loss is 1.620053768157959\n",
      "epoch: 1 step: 14, loss is 1.385157585144043\n",
      "epoch: 1 step: 15, loss is 1.3020071983337402\n",
      "epoch: 1 step: 16, loss is 1.1370216608047485\n",
      "epoch: 1 step: 17, loss is 1.1671555042266846\n",
      "epoch: 1 step: 18, loss is 1.235795021057129\n",
      "epoch: 1 step: 19, loss is 1.3539749383926392\n",
      "epoch: 1 step: 20, loss is 1.63235604763031\n",
      "epoch: 1 step: 21, loss is 0.9911004304885864\n",
      "epoch: 1 step: 22, loss is 1.427156925201416\n",
      "epoch: 1 step: 23, loss is 1.4486141204833984\n",
      "epoch: 1 step: 24, loss is 1.380081295967102\n",
      "epoch: 1 step: 25, loss is 1.2076780796051025\n",
      "epoch: 1 step: 26, loss is 1.2125154733657837\n",
      "epoch: 1 step: 27, loss is 1.2564104795455933\n",
      "epoch: 1 step: 28, loss is 1.2150774002075195\n",
      "epoch: 1 step: 29, loss is 1.4342281818389893\n",
      "epoch: 1 step: 30, loss is 1.0838605165481567\n",
      "epoch: 1 step: 31, loss is 1.1951322555541992\n",
      "epoch: 1 step: 32, loss is 1.3303111791610718\n",
      "epoch: 1 step: 33, loss is 1.050676941871643\n",
      "epoch: 1 step: 34, loss is 0.8829252123832703\n",
      "epoch: 1 step: 35, loss is 0.9650248289108276\n",
      "epoch: 1 step: 36, loss is 0.8742240071296692\n",
      "epoch: 1 step: 37, loss is 1.1264047622680664\n",
      "epoch: 1 step: 38, loss is 1.0521984100341797\n",
      "epoch: 1 step: 39, loss is 0.894359827041626\n",
      "epoch: 1 step: 40, loss is 0.8569425344467163\n",
      "epoch: 1 step: 41, loss is 1.0448795557022095\n",
      "epoch: 1 step: 42, loss is 1.0659815073013306\n",
      "epoch: 1 step: 43, loss is 0.6874154210090637\n",
      "epoch: 1 step: 44, loss is 1.2037417888641357\n",
      "epoch: 1 step: 45, loss is 1.1200480461120605\n",
      "epoch: 1 step: 46, loss is 0.8633442521095276\n",
      "epoch: 1 step: 47, loss is 0.8757739663124084\n",
      "epoch: 1 step: 48, loss is 0.7771862745285034\n",
      "epoch: 1 step: 49, loss is 0.9766644835472107\n",
      "epoch: 1 step: 50, loss is 1.0237303972244263\n",
      "epoch: 1 step: 51, loss is 1.2143795490264893\n",
      "epoch: 1 step: 52, loss is 1.0344460010528564\n",
      "epoch: 1 step: 53, loss is 0.9302226901054382\n",
      "epoch: 1 step: 54, loss is 0.8575655221939087\n",
      "epoch: 1 step: 55, loss is 1.0551073551177979\n",
      "epoch: 1 step: 56, loss is 0.7477219700813293\n",
      "epoch: 1 step: 57, loss is 0.8679042458534241\n",
      "epoch: 1 step: 58, loss is 0.8978688716888428\n",
      "epoch: 1 step: 59, loss is 0.9577116966247559\n",
      "epoch: 1 step: 60, loss is 0.9043409824371338\n",
      "epoch: 1 step: 61, loss is 0.8018947243690491\n",
      "epoch: 1 step: 62, loss is 0.9837542772293091\n",
      "epoch: 1 step: 63, loss is 0.8142107129096985\n",
      "epoch: 1 step: 64, loss is 0.7642964720726013\n",
      "epoch: 1 step: 65, loss is 0.8231373429298401\n",
      "epoch: 1 step: 66, loss is 0.699063241481781\n",
      "epoch: 1 step: 67, loss is 0.9900226593017578\n",
      "epoch: 1 step: 68, loss is 1.1144360303878784\n",
      "epoch: 1 step: 69, loss is 1.036829948425293\n",
      "epoch: 1 step: 70, loss is 0.976970911026001\n",
      "epoch: 1 step: 71, loss is 0.6901805400848389\n",
      "epoch: 1 step: 72, loss is 0.7966073155403137\n",
      "epoch: 1 step: 73, loss is 0.8976341485977173\n",
      "epoch: 1 step: 74, loss is 0.8796411752700806\n",
      "epoch: 1 step: 75, loss is 0.6610467433929443\n",
      "epoch: 1 step: 76, loss is 0.8401628136634827\n",
      "epoch: 1 step: 77, loss is 0.8408254981040955\n",
      "epoch: 1 step: 78, loss is 0.8039733171463013\n",
      "epoch: 1 step: 79, loss is 0.7958226799964905\n",
      "epoch: 1 step: 80, loss is 0.87653648853302\n",
      "epoch: 1 step: 81, loss is 0.6497328281402588\n",
      "epoch: 1 step: 82, loss is 0.801197350025177\n",
      "epoch: 1 step: 83, loss is 0.9342767596244812\n",
      "epoch: 1 step: 84, loss is 0.7386340498924255\n",
      "epoch: 1 step: 85, loss is 0.503261923789978\n",
      "epoch: 1 step: 86, loss is 0.6444199681282043\n",
      "epoch: 1 step: 87, loss is 0.8775979280471802\n",
      "epoch: 1 step: 88, loss is 0.7752964496612549\n",
      "epoch: 1 step: 89, loss is 0.8962657451629639\n",
      "epoch: 1 step: 90, loss is 0.9581377506256104\n",
      "epoch: 1 step: 91, loss is 0.9094454050064087\n",
      "epoch: 1 step: 92, loss is 0.6523675322532654\n",
      "epoch: 1 step: 93, loss is 0.727643609046936\n",
      "epoch: 1 step: 94, loss is 1.069299340248108\n",
      "epoch: 1 step: 95, loss is 0.8507900834083557\n",
      "epoch: 1 step: 96, loss is 0.6492797136306763\n",
      "epoch: 1 step: 97, loss is 0.9855616092681885\n",
      "epoch: 1 step: 98, loss is 0.7795779705047607\n",
      "epoch: 1 step: 99, loss is 0.745640218257904\n",
      "epoch: 1 step: 100, loss is 0.7565106749534607\n",
      "epoch: 1 step: 101, loss is 0.6716783046722412\n",
      "epoch: 1 step: 102, loss is 0.6287573575973511\n",
      "epoch: 1 step: 103, loss is 0.9240707159042358\n",
      "epoch: 1 step: 104, loss is 0.6559188961982727\n",
      "epoch: 1 step: 105, loss is 0.5628983378410339\n",
      "epoch: 1 step: 106, loss is 1.1731668710708618\n",
      "epoch: 1 step: 107, loss is 0.876937210559845\n",
      "epoch: 1 step: 108, loss is 0.5841206312179565\n",
      "epoch: 1 step: 109, loss is 0.8130629062652588\n",
      "epoch: 1 step: 110, loss is 0.6846392750740051\n",
      "epoch: 1 step: 111, loss is 0.6748872399330139\n",
      "epoch: 1 step: 112, loss is 0.7281920313835144\n",
      "epoch: 1 step: 113, loss is 0.9221375584602356\n",
      "epoch: 1 step: 114, loss is 0.5153570771217346\n",
      "epoch: 1 step: 115, loss is 0.5561738014221191\n",
      "epoch: 1 step: 116, loss is 0.9431338310241699\n",
      "epoch: 1 step: 117, loss is 0.7039068937301636\n",
      "epoch: 1 step: 118, loss is 0.5804455876350403\n",
      "epoch: 1 step: 119, loss is 0.6268067359924316\n",
      "epoch: 1 step: 120, loss is 0.5903990268707275\n",
      "epoch: 1 step: 121, loss is 0.7319254279136658\n",
      "epoch: 1 step: 122, loss is 0.5433424115180969\n",
      "epoch: 1 step: 123, loss is 0.6017593741416931\n",
      "epoch: 1 step: 124, loss is 0.8344636559486389\n",
      "epoch: 1 step: 125, loss is 0.6999446153640747\n",
      "epoch: 1 step: 126, loss is 0.47873812913894653\n",
      "epoch: 1 step: 127, loss is 0.6133227944374084\n",
      "epoch: 1 step: 128, loss is 0.5012767314910889\n",
      "epoch: 1 step: 129, loss is 0.7014017701148987\n",
      "epoch: 1 step: 130, loss is 0.8050025701522827\n",
      "epoch: 1 step: 131, loss is 0.7790014147758484\n",
      "epoch: 1 step: 132, loss is 0.7408042550086975\n",
      "epoch: 1 step: 133, loss is 0.5962711572647095\n",
      "epoch: 1 step: 134, loss is 0.7419260144233704\n",
      "epoch: 1 step: 135, loss is 0.54495769739151\n",
      "epoch: 1 step: 136, loss is 0.7899991273880005\n",
      "epoch: 1 step: 137, loss is 0.647205114364624\n",
      "epoch: 1 step: 138, loss is 0.6231963634490967\n",
      "epoch: 1 step: 139, loss is 0.5635651350021362\n",
      "epoch: 1 step: 140, loss is 0.5298110246658325\n",
      "epoch: 1 step: 141, loss is 0.589225172996521\n",
      "epoch: 1 step: 142, loss is 0.44058775901794434\n",
      "epoch: 1 step: 143, loss is 0.6197219491004944\n",
      "epoch: 1 step: 144, loss is 0.6056414246559143\n",
      "epoch: 1 step: 145, loss is 0.6870455145835876\n",
      "epoch: 1 step: 146, loss is 0.8473621606826782\n",
      "epoch: 1 step: 147, loss is 0.6472554802894592\n",
      "epoch: 1 step: 148, loss is 0.5200453400611877\n",
      "epoch: 1 step: 149, loss is 0.696978747844696\n",
      "epoch: 1 step: 150, loss is 0.44741469621658325\n",
      "epoch: 1 step: 151, loss is 0.5827103853225708\n",
      "epoch: 1 step: 152, loss is 0.5319174528121948\n",
      "epoch: 1 step: 153, loss is 0.6675307154655457\n",
      "epoch: 1 step: 154, loss is 0.8810803294181824\n",
      "epoch: 1 step: 155, loss is 0.7642458081245422\n",
      "epoch: 1 step: 156, loss is 0.49620163440704346\n",
      "epoch: 1 step: 157, loss is 0.3836168050765991\n",
      "epoch: 1 step: 158, loss is 0.7102265357971191\n",
      "epoch: 1 step: 159, loss is 0.6539356112480164\n",
      "epoch: 1 step: 160, loss is 0.6751660108566284\n",
      "epoch: 1 step: 161, loss is 0.6304712295532227\n",
      "epoch: 1 step: 162, loss is 0.8459116220474243\n",
      "epoch: 1 step: 163, loss is 0.4335005283355713\n",
      "epoch: 1 step: 164, loss is 0.6838268637657166\n",
      "epoch: 1 step: 165, loss is 0.42053231596946716\n",
      "epoch: 1 step: 166, loss is 0.6951633095741272\n",
      "epoch: 1 step: 167, loss is 0.5493558049201965\n",
      "epoch: 1 step: 168, loss is 0.5624215006828308\n",
      "epoch: 1 step: 169, loss is 0.5130364894866943\n",
      "epoch: 1 step: 170, loss is 0.5763767957687378\n",
      "epoch: 1 step: 171, loss is 0.43939974904060364\n",
      "epoch: 1 step: 172, loss is 0.5780758857727051\n",
      "epoch: 1 step: 173, loss is 0.8206875920295715\n",
      "epoch: 1 step: 174, loss is 0.801115870475769\n",
      "epoch: 1 step: 175, loss is 0.7934765815734863\n",
      "epoch: 1 step: 176, loss is 0.7985427975654602\n",
      "epoch: 1 step: 177, loss is 0.528719425201416\n",
      "epoch: 1 step: 178, loss is 0.6544314026832581\n",
      "epoch: 1 step: 179, loss is 0.6056951284408569\n",
      "epoch: 1 step: 180, loss is 0.6576201319694519\n",
      "epoch: 1 step: 181, loss is 0.6545267105102539\n",
      "epoch: 1 step: 182, loss is 0.9007232189178467\n",
      "epoch: 1 step: 183, loss is 0.6073129177093506\n",
      "epoch: 1 step: 184, loss is 0.6942571401596069\n",
      "epoch: 1 step: 185, loss is 0.6263959407806396\n",
      "epoch: 1 step: 186, loss is 0.4247804582118988\n",
      "epoch: 1 step: 187, loss is 0.5135186910629272\n",
      "epoch: 1 step: 188, loss is 0.6250903010368347\n",
      "epoch: 1 step: 189, loss is 0.44398340582847595\n",
      "epoch: 1 step: 190, loss is 0.5158183574676514\n",
      "epoch: 1 step: 191, loss is 0.49162280559539795\n",
      "epoch: 1 step: 192, loss is 0.8211105465888977\n",
      "epoch: 1 step: 193, loss is 0.5430170297622681\n",
      "epoch: 1 step: 194, loss is 0.5551693439483643\n",
      "epoch: 1 step: 195, loss is 0.61152184009552\n",
      "epoch: 1 step: 196, loss is 0.4224426746368408\n",
      "epoch: 1 step: 197, loss is 0.5890608429908752\n",
      "epoch: 1 step: 198, loss is 0.7000977993011475\n",
      "epoch: 1 step: 199, loss is 0.32802361249923706\n",
      "epoch: 1 step: 200, loss is 0.6785417199134827\n",
      "epoch: 1 step: 201, loss is 0.4013148546218872\n",
      "epoch: 1 step: 202, loss is 0.46736326813697815\n",
      "epoch: 1 step: 203, loss is 0.5548000335693359\n",
      "epoch: 1 step: 204, loss is 0.5029782652854919\n",
      "epoch: 1 step: 205, loss is 0.559410810470581\n",
      "epoch: 1 step: 206, loss is 0.645662248134613\n",
      "epoch: 1 step: 207, loss is 0.3321855664253235\n",
      "epoch: 1 step: 208, loss is 0.3244228661060333\n",
      "epoch: 1 step: 209, loss is 0.653001606464386\n",
      "epoch: 1 step: 210, loss is 0.40664607286453247\n",
      "epoch: 1 step: 211, loss is 0.8239966630935669\n",
      "epoch: 1 step: 212, loss is 0.5493823289871216\n",
      "epoch: 1 step: 213, loss is 0.547340989112854\n",
      "epoch: 1 step: 214, loss is 0.7461887001991272\n",
      "epoch: 1 step: 215, loss is 0.4245643615722656\n",
      "epoch: 1 step: 216, loss is 0.6158846020698547\n",
      "epoch: 1 step: 217, loss is 0.6881178617477417\n",
      "epoch: 1 step: 218, loss is 0.5491432547569275\n",
      "epoch: 1 step: 219, loss is 0.4419965445995331\n",
      "epoch: 1 step: 220, loss is 0.7223563194274902\n",
      "epoch: 1 step: 221, loss is 0.7588633298873901\n",
      "epoch: 1 step: 222, loss is 0.44761088490486145\n",
      "epoch: 1 step: 223, loss is 0.4340410828590393\n",
      "epoch: 1 step: 224, loss is 0.6574876308441162\n",
      "epoch: 1 step: 225, loss is 0.5721592903137207\n",
      "epoch: 1 step: 226, loss is 0.4834822416305542\n",
      "epoch: 1 step: 227, loss is 0.48916903138160706\n",
      "epoch: 1 step: 228, loss is 0.42741161584854126\n",
      "epoch: 1 step: 229, loss is 0.43431952595710754\n",
      "epoch: 1 step: 230, loss is 0.3738833963871002\n",
      "epoch: 1 step: 231, loss is 0.47871720790863037\n",
      "epoch: 1 step: 232, loss is 0.6797058582305908\n",
      "epoch: 1 step: 233, loss is 0.392238974571228\n",
      "epoch: 1 step: 234, loss is 0.6127027869224548\n",
      "epoch: 1 step: 235, loss is 0.33669644594192505\n",
      "epoch: 1 step: 236, loss is 0.5627936124801636\n",
      "epoch: 1 step: 237, loss is 0.4582364857196808\n",
      "epoch: 1 step: 238, loss is 0.4587143063545227\n",
      "epoch: 1 step: 239, loss is 0.9214972853660583\n",
      "epoch: 1 step: 240, loss is 0.6227126121520996\n",
      "epoch: 1 step: 241, loss is 0.5355229377746582\n",
      "epoch: 1 step: 242, loss is 0.612169086933136\n",
      "epoch: 1 step: 243, loss is 0.623012125492096\n",
      "epoch: 1 step: 244, loss is 0.7862719297409058\n",
      "epoch: 1 step: 245, loss is 0.5638056993484497\n",
      "epoch: 1 step: 246, loss is 0.5283675789833069\n",
      "epoch: 1 step: 247, loss is 0.6357366442680359\n",
      "epoch: 1 step: 248, loss is 0.59719318151474\n",
      "epoch: 1 step: 249, loss is 0.4868382513523102\n",
      "epoch: 1 step: 250, loss is 0.6057201027870178\n",
      "epoch: 1 step: 251, loss is 0.5377665758132935\n",
      "epoch: 1 step: 252, loss is 0.4532226324081421\n",
      "epoch: 1 step: 253, loss is 0.8439738154411316\n",
      "epoch: 1 step: 254, loss is 0.5233394503593445\n",
      "epoch: 1 step: 255, loss is 0.2976343631744385\n",
      "epoch: 1 step: 256, loss is 0.65097975730896\n",
      "epoch: 1 step: 257, loss is 0.6529681086540222\n",
      "epoch: 1 step: 258, loss is 0.5943472385406494\n",
      "epoch: 1 step: 259, loss is 0.33605557680130005\n",
      "epoch: 1 step: 260, loss is 0.42227885127067566\n",
      "epoch: 1 step: 261, loss is 0.48900261521339417\n",
      "epoch: 1 step: 262, loss is 0.5032995939254761\n",
      "epoch: 1 step: 263, loss is 0.5886896848678589\n",
      "epoch: 1 step: 264, loss is 0.48549988865852356\n",
      "epoch: 1 step: 265, loss is 0.6187108159065247\n",
      "epoch: 1 step: 266, loss is 0.6053984761238098\n",
      "epoch: 1 step: 267, loss is 0.5412372946739197\n",
      "epoch: 1 step: 268, loss is 0.45824968814849854\n",
      "epoch: 1 step: 269, loss is 0.45223090052604675\n",
      "epoch: 1 step: 270, loss is 0.6458450555801392\n",
      "epoch: 1 step: 271, loss is 0.49910497665405273\n",
      "epoch: 1 step: 272, loss is 0.5183861255645752\n",
      "epoch: 1 step: 273, loss is 0.5377304553985596\n",
      "epoch: 1 step: 274, loss is 0.36975228786468506\n",
      "epoch: 1 step: 275, loss is 0.42873361706733704\n",
      "epoch: 1 step: 276, loss is 0.6779990196228027\n",
      "epoch: 1 step: 277, loss is 0.5719858407974243\n",
      "epoch: 1 step: 278, loss is 0.37380528450012207\n",
      "epoch: 1 step: 279, loss is 0.6131534576416016\n",
      "epoch: 1 step: 280, loss is 0.5282182097434998\n",
      "epoch: 1 step: 281, loss is 0.5019542574882507\n",
      "epoch: 1 step: 282, loss is 0.4442513585090637\n",
      "epoch: 1 step: 283, loss is 0.5131316184997559\n",
      "epoch: 1 step: 284, loss is 0.3091197609901428\n",
      "epoch: 1 step: 285, loss is 0.5304577946662903\n",
      "epoch: 1 step: 286, loss is 0.7051845192909241\n",
      "epoch: 1 step: 287, loss is 0.9300826787948608\n",
      "epoch: 1 step: 288, loss is 0.5584586262702942\n",
      "epoch: 1 step: 289, loss is 0.40488511323928833\n",
      "epoch: 1 step: 290, loss is 0.38087448477745056\n",
      "epoch: 1 step: 291, loss is 0.5527120232582092\n",
      "epoch: 1 step: 292, loss is 0.5488793849945068\n",
      "epoch: 1 step: 293, loss is 0.38840582966804504\n",
      "epoch: 1 step: 294, loss is 0.4669441878795624\n",
      "epoch: 1 step: 295, loss is 0.6612814664840698\n",
      "epoch: 1 step: 296, loss is 0.5778880715370178\n",
      "epoch: 1 step: 297, loss is 0.6347198486328125\n",
      "epoch: 1 step: 298, loss is 0.44331875443458557\n",
      "epoch: 1 step: 299, loss is 0.5644587874412537\n",
      "epoch: 1 step: 300, loss is 0.42708396911621094\n",
      "epoch: 1 step: 301, loss is 0.49199509620666504\n",
      "epoch: 1 step: 302, loss is 0.6560202836990356\n",
      "epoch: 1 step: 303, loss is 0.27600452303886414\n",
      "epoch: 1 step: 304, loss is 0.3337242901325226\n",
      "epoch: 1 step: 305, loss is 0.6221615672111511\n",
      "epoch: 1 step: 306, loss is 0.3477904200553894\n",
      "epoch: 1 step: 307, loss is 0.2796279489994049\n",
      "epoch: 1 step: 308, loss is 0.49417927861213684\n",
      "epoch: 1 step: 309, loss is 0.5613061189651489\n",
      "epoch: 1 step: 310, loss is 0.45666828751564026\n",
      "epoch: 1 step: 311, loss is 0.5291376113891602\n",
      "epoch: 1 step: 312, loss is 0.39570850133895874\n",
      "epoch: 1 step: 313, loss is 0.4388483464717865\n",
      "epoch: 1 step: 314, loss is 0.6819230318069458\n",
      "epoch: 1 step: 315, loss is 0.5447749495506287\n",
      "epoch: 1 step: 316, loss is 0.5324229001998901\n",
      "epoch: 1 step: 317, loss is 0.2706087827682495\n",
      "epoch: 1 step: 318, loss is 0.6987242102622986\n",
      "epoch: 1 step: 319, loss is 0.5534142851829529\n",
      "epoch: 1 step: 320, loss is 0.5316615104675293\n",
      "epoch: 1 step: 321, loss is 0.5255166888237\n",
      "epoch: 1 step: 322, loss is 0.33865445852279663\n",
      "epoch: 1 step: 323, loss is 0.47583267092704773\n",
      "epoch: 1 step: 324, loss is 0.5341325402259827\n",
      "epoch: 1 step: 325, loss is 0.46546557545661926\n",
      "epoch: 1 step: 326, loss is 0.5157972574234009\n",
      "epoch: 1 step: 327, loss is 0.28003475069999695\n",
      "epoch: 1 step: 328, loss is 0.5070130825042725\n",
      "epoch: 1 step: 329, loss is 0.41894474625587463\n",
      "epoch: 1 step: 330, loss is 0.6033455729484558\n",
      "epoch: 1 step: 331, loss is 0.2893693745136261\n",
      "epoch: 1 step: 332, loss is 0.4484902024269104\n",
      "epoch: 1 step: 333, loss is 0.2867969572544098\n",
      "epoch: 1 step: 334, loss is 0.3914799392223358\n",
      "epoch: 1 step: 335, loss is 0.7423099875450134\n",
      "epoch: 1 step: 336, loss is 0.44945207238197327\n",
      "epoch: 1 step: 337, loss is 0.7419193387031555\n",
      "epoch: 1 step: 338, loss is 0.6484761834144592\n",
      "epoch: 1 step: 339, loss is 0.3459276854991913\n",
      "epoch: 1 step: 340, loss is 0.5382339358329773\n",
      "epoch: 1 step: 341, loss is 0.409292608499527\n",
      "epoch: 1 step: 342, loss is 0.38938555121421814\n",
      "epoch: 1 step: 343, loss is 0.3076041340827942\n",
      "epoch: 1 step: 344, loss is 0.43347862362861633\n",
      "epoch: 1 step: 345, loss is 0.4120651185512543\n",
      "epoch: 1 step: 346, loss is 0.5112653374671936\n",
      "epoch: 1 step: 347, loss is 0.5677617788314819\n",
      "epoch: 1 step: 348, loss is 0.46827441453933716\n",
      "epoch: 1 step: 349, loss is 0.5026834011077881\n",
      "epoch: 1 step: 350, loss is 0.44211524724960327\n",
      "epoch: 1 step: 351, loss is 0.3563135862350464\n",
      "epoch: 1 step: 352, loss is 0.395700603723526\n",
      "epoch: 1 step: 353, loss is 0.5694661736488342\n",
      "epoch: 1 step: 354, loss is 0.4362739026546478\n",
      "epoch: 1 step: 355, loss is 0.49250829219818115\n",
      "epoch: 1 step: 356, loss is 0.5972797870635986\n",
      "epoch: 1 step: 357, loss is 0.6885025501251221\n",
      "epoch: 1 step: 358, loss is 0.509048581123352\n",
      "epoch: 1 step: 359, loss is 0.6594245433807373\n",
      "epoch: 1 step: 360, loss is 0.4929417669773102\n",
      "epoch: 1 step: 361, loss is 0.49477899074554443\n",
      "epoch: 1 step: 362, loss is 0.32810184359550476\n",
      "epoch: 1 step: 363, loss is 0.3591859042644501\n",
      "epoch: 1 step: 364, loss is 0.3628966212272644\n",
      "epoch: 1 step: 365, loss is 0.3963218927383423\n",
      "epoch: 1 step: 366, loss is 0.4396313726902008\n",
      "epoch: 1 step: 367, loss is 0.6208623647689819\n",
      "epoch: 1 step: 368, loss is 0.3388252854347229\n",
      "epoch: 1 step: 369, loss is 0.4553641676902771\n",
      "epoch: 1 step: 370, loss is 0.431464284658432\n",
      "epoch: 1 step: 371, loss is 0.4101516306400299\n",
      "epoch: 1 step: 372, loss is 0.48455438017845154\n",
      "epoch: 1 step: 373, loss is 0.489891916513443\n",
      "epoch: 1 step: 374, loss is 0.4604870080947876\n",
      "epoch: 1 step: 375, loss is 0.48785918951034546\n",
      "epoch: 1 step: 376, loss is 0.507092297077179\n",
      "epoch: 1 step: 377, loss is 0.4854625463485718\n",
      "epoch: 1 step: 378, loss is 0.47267913818359375\n",
      "epoch: 1 step: 379, loss is 0.6921358704566956\n",
      "epoch: 1 step: 380, loss is 0.4163893163204193\n",
      "epoch: 1 step: 381, loss is 0.3692252039909363\n",
      "epoch: 1 step: 382, loss is 0.41749292612075806\n",
      "epoch: 1 step: 383, loss is 0.5050052404403687\n",
      "epoch: 1 step: 384, loss is 0.3888624608516693\n",
      "epoch: 1 step: 385, loss is 0.34589889645576477\n",
      "epoch: 1 step: 386, loss is 0.56870436668396\n",
      "epoch: 1 step: 387, loss is 0.3220459222793579\n",
      "epoch: 1 step: 388, loss is 0.5839809775352478\n",
      "epoch: 1 step: 389, loss is 0.7126700282096863\n",
      "epoch: 1 step: 390, loss is 0.5544040203094482\n",
      "epoch: 1 step: 391, loss is 0.595367431640625\n",
      "epoch: 1 step: 392, loss is 0.5171870589256287\n",
      "epoch: 1 step: 393, loss is 0.7111233472824097\n",
      "epoch: 1 step: 394, loss is 0.31828710436820984\n",
      "epoch: 1 step: 395, loss is 0.32399290800094604\n",
      "epoch: 1 step: 396, loss is 0.462590754032135\n",
      "epoch: 1 step: 397, loss is 0.4583561420440674\n",
      "epoch: 1 step: 398, loss is 0.37652018666267395\n",
      "epoch: 1 step: 399, loss is 0.3366927206516266\n",
      "epoch: 1 step: 400, loss is 0.710052490234375\n",
      "epoch: 1 step: 401, loss is 0.21598029136657715\n",
      "epoch: 1 step: 402, loss is 0.3882451355457306\n",
      "epoch: 1 step: 403, loss is 0.5601978302001953\n",
      "epoch: 1 step: 404, loss is 0.4431471526622772\n",
      "epoch: 1 step: 405, loss is 0.38198354840278625\n",
      "epoch: 1 step: 406, loss is 0.48911845684051514\n",
      "epoch: 1 step: 407, loss is 0.5243895649909973\n",
      "epoch: 1 step: 408, loss is 0.5186029672622681\n",
      "epoch: 1 step: 409, loss is 0.3854916989803314\n",
      "epoch: 1 step: 410, loss is 0.40818315744400024\n",
      "epoch: 1 step: 411, loss is 0.40524768829345703\n",
      "epoch: 1 step: 412, loss is 0.5204153060913086\n",
      "epoch: 1 step: 413, loss is 0.5887671709060669\n",
      "epoch: 1 step: 414, loss is 0.386756956577301\n",
      "epoch: 1 step: 415, loss is 0.35714399814605713\n",
      "epoch: 1 step: 416, loss is 0.25814318656921387\n",
      "epoch: 1 step: 417, loss is 0.489363431930542\n",
      "epoch: 1 step: 418, loss is 0.4463748335838318\n",
      "epoch: 1 step: 419, loss is 0.44332265853881836\n",
      "epoch: 1 step: 420, loss is 0.6957613229751587\n",
      "epoch: 1 step: 421, loss is 0.35793784260749817\n",
      "epoch: 1 step: 422, loss is 0.5216895341873169\n",
      "epoch: 1 step: 423, loss is 0.5564513802528381\n",
      "epoch: 1 step: 424, loss is 0.5012065172195435\n",
      "epoch: 1 step: 425, loss is 0.23598888516426086\n",
      "epoch: 1 step: 426, loss is 0.6564133167266846\n",
      "epoch: 1 step: 427, loss is 0.5372934341430664\n",
      "epoch: 1 step: 428, loss is 0.34805795550346375\n",
      "epoch: 1 step: 429, loss is 0.46307995915412903\n",
      "epoch: 1 step: 430, loss is 0.4884476661682129\n",
      "epoch: 1 step: 431, loss is 0.5684261322021484\n",
      "epoch: 1 step: 432, loss is 0.4175463616847992\n",
      "epoch: 1 step: 433, loss is 0.7208486199378967\n",
      "epoch: 1 step: 434, loss is 0.6530402302742004\n",
      "epoch: 1 step: 435, loss is 0.35105809569358826\n",
      "epoch: 1 step: 436, loss is 0.4946672320365906\n",
      "epoch: 1 step: 437, loss is 0.586601197719574\n",
      "epoch: 1 step: 438, loss is 0.5767742991447449\n",
      "epoch: 1 step: 439, loss is 0.3396108150482178\n",
      "epoch: 1 step: 440, loss is 0.4823428690433502\n",
      "epoch: 1 step: 441, loss is 0.4857315421104431\n",
      "epoch: 1 step: 442, loss is 0.45910489559173584\n",
      "epoch: 1 step: 443, loss is 0.45377281308174133\n",
      "epoch: 1 step: 444, loss is 0.32959723472595215\n",
      "epoch: 1 step: 445, loss is 0.4212118089199066\n",
      "epoch: 1 step: 446, loss is 0.6126072406768799\n",
      "epoch: 1 step: 447, loss is 0.31293874979019165\n",
      "epoch: 1 step: 448, loss is 0.5082384347915649\n",
      "epoch: 1 step: 449, loss is 0.45550549030303955\n",
      "epoch: 1 step: 450, loss is 0.35551971197128296\n",
      "epoch: 1 step: 451, loss is 0.4521985352039337\n",
      "epoch: 1 step: 452, loss is 0.6608782410621643\n",
      "epoch: 1 step: 453, loss is 0.41076532006263733\n",
      "epoch: 1 step: 454, loss is 0.5067592859268188\n",
      "epoch: 1 step: 455, loss is 0.4978102445602417\n",
      "epoch: 1 step: 456, loss is 0.3872617483139038\n",
      "epoch: 1 step: 457, loss is 0.35606124997138977\n",
      "epoch: 1 step: 458, loss is 0.49965518712997437\n",
      "epoch: 1 step: 459, loss is 0.5729286074638367\n",
      "epoch: 1 step: 460, loss is 0.7119606137275696\n",
      "epoch: 1 step: 461, loss is 0.2723664343357086\n",
      "epoch: 1 step: 462, loss is 0.5587126016616821\n",
      "epoch: 1 step: 463, loss is 0.4533524513244629\n",
      "epoch: 1 step: 464, loss is 0.4420175850391388\n",
      "epoch: 1 step: 465, loss is 0.3653651177883148\n",
      "epoch: 1 step: 466, loss is 0.41002827882766724\n",
      "epoch: 1 step: 467, loss is 0.5185908675193787\n",
      "epoch: 1 step: 468, loss is 0.5085775852203369\n",
      "epoch: 1 step: 469, loss is 0.45162656903266907\n",
      "epoch: 1 step: 470, loss is 0.40932250022888184\n",
      "epoch: 1 step: 471, loss is 0.21935787796974182\n",
      "epoch: 1 step: 472, loss is 0.3298904597759247\n",
      "epoch: 1 step: 473, loss is 0.5865774154663086\n",
      "epoch: 1 step: 474, loss is 0.35700324177742004\n",
      "epoch: 1 step: 475, loss is 0.5468043088912964\n",
      "epoch: 1 step: 476, loss is 0.5948207974433899\n",
      "epoch: 1 step: 477, loss is 0.309049516916275\n",
      "epoch: 1 step: 478, loss is 0.454385370016098\n",
      "epoch: 1 step: 479, loss is 0.3446296453475952\n",
      "epoch: 1 step: 480, loss is 0.3473700284957886\n",
      "epoch: 1 step: 481, loss is 0.4604145884513855\n",
      "epoch: 1 step: 482, loss is 0.34654751420021057\n",
      "epoch: 1 step: 483, loss is 0.43098047375679016\n",
      "epoch: 1 step: 484, loss is 0.2738138437271118\n",
      "epoch: 1 step: 485, loss is 0.5205383896827698\n",
      "epoch: 1 step: 486, loss is 0.38208720088005066\n",
      "epoch: 1 step: 487, loss is 0.3282911777496338\n",
      "epoch: 1 step: 488, loss is 0.7095847725868225\n",
      "epoch: 1 step: 489, loss is 0.24083079397678375\n",
      "epoch: 1 step: 490, loss is 0.28760361671447754\n",
      "epoch: 1 step: 491, loss is 0.3742229640483856\n",
      "epoch: 1 step: 492, loss is 0.352689653635025\n",
      "epoch: 1 step: 493, loss is 0.3675270676612854\n",
      "epoch: 1 step: 494, loss is 0.4508971869945526\n",
      "epoch: 1 step: 495, loss is 0.4412561058998108\n",
      "epoch: 1 step: 496, loss is 0.48109427094459534\n",
      "epoch: 1 step: 497, loss is 0.31982338428497314\n",
      "epoch: 1 step: 498, loss is 0.3055037558078766\n",
      "epoch: 1 step: 499, loss is 0.34207600355148315\n",
      "epoch: 1 step: 500, loss is 0.4297245144844055\n",
      "epoch: 1 step: 501, loss is 0.3890190124511719\n",
      "epoch: 1 step: 502, loss is 0.4592377245426178\n",
      "epoch: 1 step: 503, loss is 0.5075197815895081\n",
      "epoch: 1 step: 504, loss is 0.3671669661998749\n",
      "epoch: 1 step: 505, loss is 0.5169641375541687\n",
      "epoch: 1 step: 506, loss is 0.4933811128139496\n",
      "epoch: 1 step: 507, loss is 0.4542292654514313\n",
      "epoch: 1 step: 508, loss is 0.3913646340370178\n",
      "epoch: 1 step: 509, loss is 0.5471361875534058\n",
      "epoch: 1 step: 510, loss is 0.4634658098220825\n",
      "epoch: 1 step: 511, loss is 0.3620102107524872\n",
      "epoch: 1 step: 512, loss is 0.4391259253025055\n",
      "epoch: 1 step: 513, loss is 0.27518874406814575\n",
      "epoch: 1 step: 514, loss is 0.33409860730171204\n",
      "epoch: 1 step: 515, loss is 0.4432281255722046\n",
      "epoch: 1 step: 516, loss is 0.49466872215270996\n",
      "epoch: 1 step: 517, loss is 0.5504891276359558\n",
      "epoch: 1 step: 518, loss is 0.41969412565231323\n",
      "epoch: 1 step: 519, loss is 0.43172821402549744\n",
      "epoch: 1 step: 520, loss is 0.3962026834487915\n",
      "epoch: 1 step: 521, loss is 0.48532381653785706\n",
      "epoch: 1 step: 522, loss is 0.4089840352535248\n",
      "epoch: 1 step: 523, loss is 0.4353586435317993\n",
      "epoch: 1 step: 524, loss is 0.2591264843940735\n",
      "epoch: 1 step: 525, loss is 0.3272347152233124\n",
      "epoch: 1 step: 526, loss is 0.32138392329216003\n",
      "epoch: 1 step: 527, loss is 0.5391701459884644\n",
      "epoch: 1 step: 528, loss is 0.46993520855903625\n",
      "epoch: 1 step: 529, loss is 0.25427255034446716\n",
      "epoch: 1 step: 530, loss is 0.33957552909851074\n",
      "epoch: 1 step: 531, loss is 0.40309083461761475\n",
      "epoch: 1 step: 532, loss is 0.37436994910240173\n",
      "epoch: 1 step: 533, loss is 0.24251188337802887\n",
      "epoch: 1 step: 534, loss is 0.2482379972934723\n",
      "epoch: 1 step: 535, loss is 0.3833383321762085\n",
      "epoch: 1 step: 536, loss is 0.31308436393737793\n",
      "epoch: 1 step: 537, loss is 0.4936307370662689\n",
      "epoch: 1 step: 538, loss is 0.44555220007896423\n",
      "epoch: 1 step: 539, loss is 0.44717517495155334\n",
      "epoch: 1 step: 540, loss is 0.36691248416900635\n",
      "epoch: 1 step: 541, loss is 0.2592775225639343\n",
      "epoch: 1 step: 542, loss is 0.30864813923835754\n",
      "epoch: 1 step: 543, loss is 0.4905889630317688\n",
      "epoch: 1 step: 544, loss is 0.4881191849708557\n",
      "epoch: 1 step: 545, loss is 0.4042724072933197\n",
      "epoch: 1 step: 546, loss is 0.40934091806411743\n",
      "epoch: 1 step: 547, loss is 0.35064584016799927\n",
      "epoch: 1 step: 548, loss is 0.6982260346412659\n",
      "epoch: 1 step: 549, loss is 0.3507859408855438\n",
      "epoch: 1 step: 550, loss is 0.4652659595012665\n",
      "epoch: 1 step: 551, loss is 0.5490884780883789\n",
      "epoch: 1 step: 552, loss is 0.42345303297042847\n",
      "epoch: 1 step: 553, loss is 0.30279606580734253\n",
      "epoch: 1 step: 554, loss is 0.39304405450820923\n",
      "epoch: 1 step: 555, loss is 0.20391671359539032\n",
      "epoch: 1 step: 556, loss is 0.3614028990268707\n",
      "epoch: 1 step: 557, loss is 0.4860292375087738\n",
      "epoch: 1 step: 558, loss is 0.2876875102519989\n",
      "epoch: 1 step: 559, loss is 0.671447217464447\n",
      "epoch: 1 step: 560, loss is 0.5348724126815796\n",
      "epoch: 1 step: 561, loss is 0.39683693647384644\n",
      "epoch: 1 step: 562, loss is 0.2694801986217499\n",
      "epoch: 1 step: 563, loss is 0.2757086157798767\n",
      "epoch: 1 step: 564, loss is 0.3208868205547333\n",
      "epoch: 1 step: 565, loss is 0.5856740474700928\n",
      "epoch: 1 step: 566, loss is 0.45110762119293213\n",
      "epoch: 1 step: 567, loss is 0.36490577459335327\n",
      "epoch: 1 step: 568, loss is 0.3502708971500397\n",
      "epoch: 1 step: 569, loss is 0.3199549615383148\n",
      "epoch: 1 step: 570, loss is 0.4179558753967285\n",
      "epoch: 1 step: 571, loss is 0.36313825845718384\n",
      "epoch: 1 step: 572, loss is 0.6322727203369141\n",
      "epoch: 1 step: 573, loss is 0.3610852062702179\n",
      "epoch: 1 step: 574, loss is 0.2390364408493042\n",
      "epoch: 1 step: 575, loss is 0.42148709297180176\n",
      "epoch: 1 step: 576, loss is 0.35528185963630676\n",
      "epoch: 1 step: 577, loss is 0.2268075793981552\n",
      "epoch: 1 step: 578, loss is 0.4141424596309662\n",
      "epoch: 1 step: 579, loss is 0.41916003823280334\n",
      "epoch: 1 step: 580, loss is 0.4210358262062073\n",
      "epoch: 1 step: 581, loss is 0.4561328887939453\n",
      "epoch: 1 step: 582, loss is 0.4125162959098816\n",
      "epoch: 1 step: 583, loss is 0.33360156416893005\n",
      "epoch: 1 step: 584, loss is 0.4126908779144287\n",
      "epoch: 1 step: 585, loss is 0.32940009236335754\n",
      "epoch: 1 step: 586, loss is 0.5630441308021545\n",
      "epoch: 1 step: 587, loss is 0.43709611892700195\n",
      "epoch: 1 step: 588, loss is 0.2969527840614319\n",
      "epoch: 1 step: 589, loss is 0.5375882983207703\n",
      "epoch: 1 step: 590, loss is 0.39405980706214905\n",
      "epoch: 1 step: 591, loss is 0.419773131608963\n",
      "epoch: 1 step: 592, loss is 0.31134501099586487\n",
      "epoch: 1 step: 593, loss is 0.34061887860298157\n",
      "epoch: 1 step: 594, loss is 0.4970424175262451\n",
      "epoch: 1 step: 595, loss is 0.5493545532226562\n",
      "epoch: 1 step: 596, loss is 0.3366975784301758\n",
      "epoch: 1 step: 597, loss is 0.47982245683670044\n",
      "epoch: 1 step: 598, loss is 0.2424052655696869\n",
      "epoch: 1 step: 599, loss is 0.35618066787719727\n",
      "epoch: 1 step: 600, loss is 0.4135395884513855\n",
      "epoch: 1 step: 601, loss is 0.26214784383773804\n",
      "epoch: 1 step: 602, loss is 0.3097049593925476\n",
      "epoch: 1 step: 603, loss is 0.49038705229759216\n",
      "epoch: 1 step: 604, loss is 0.4420996904373169\n",
      "epoch: 1 step: 605, loss is 0.6244514584541321\n",
      "epoch: 1 step: 606, loss is 0.275052934885025\n",
      "epoch: 1 step: 607, loss is 0.46949878334999084\n",
      "epoch: 1 step: 608, loss is 0.4969283938407898\n",
      "epoch: 1 step: 609, loss is 0.24995720386505127\n",
      "epoch: 1 step: 610, loss is 0.32412055134773254\n",
      "epoch: 1 step: 611, loss is 0.45481353998184204\n",
      "epoch: 1 step: 612, loss is 0.3835330307483673\n",
      "epoch: 1 step: 613, loss is 0.3845299482345581\n",
      "epoch: 1 step: 614, loss is 0.3124382197856903\n",
      "epoch: 1 step: 615, loss is 0.4396921396255493\n",
      "epoch: 1 step: 616, loss is 0.388251394033432\n",
      "epoch: 1 step: 617, loss is 0.27323833107948303\n",
      "epoch: 1 step: 618, loss is 0.5453586578369141\n",
      "epoch: 1 step: 619, loss is 0.5129340887069702\n",
      "epoch: 1 step: 620, loss is 0.3507229685783386\n",
      "epoch: 1 step: 621, loss is 0.5576862692832947\n",
      "epoch: 1 step: 622, loss is 0.2731797695159912\n",
      "epoch: 1 step: 623, loss is 0.3083260655403137\n",
      "epoch: 1 step: 624, loss is 0.3647783696651459\n",
      "epoch: 1 step: 625, loss is 0.44074761867523193\n",
      "epoch: 1 step: 626, loss is 0.4044652581214905\n",
      "epoch: 1 step: 627, loss is 0.5238563418388367\n",
      "epoch: 1 step: 628, loss is 0.3519599139690399\n",
      "epoch: 1 step: 629, loss is 0.24933327734470367\n",
      "epoch: 1 step: 630, loss is 0.5061816573143005\n",
      "epoch: 1 step: 631, loss is 0.384863018989563\n",
      "epoch: 1 step: 632, loss is 0.49461084604263306\n",
      "epoch: 1 step: 633, loss is 0.38536301255226135\n",
      "epoch: 1 step: 634, loss is 0.43777382373809814\n",
      "epoch: 1 step: 635, loss is 0.38991138339042664\n",
      "epoch: 1 step: 636, loss is 0.42124757170677185\n",
      "epoch: 1 step: 637, loss is 0.5040337443351746\n",
      "epoch: 1 step: 638, loss is 0.4532938599586487\n",
      "epoch: 1 step: 639, loss is 0.3358738422393799\n",
      "epoch: 1 step: 640, loss is 0.4629770815372467\n",
      "epoch: 1 step: 641, loss is 0.29806700348854065\n",
      "epoch: 1 step: 642, loss is 0.49061331152915955\n",
      "epoch: 1 step: 643, loss is 0.43395617604255676\n",
      "epoch: 1 step: 644, loss is 0.44501742720603943\n",
      "epoch: 1 step: 645, loss is 0.5234617590904236\n",
      "epoch: 1 step: 646, loss is 0.20080262422561646\n",
      "epoch: 1 step: 647, loss is 0.6070893406867981\n",
      "epoch: 1 step: 648, loss is 0.3578595519065857\n",
      "epoch: 1 step: 649, loss is 0.27060380578041077\n",
      "epoch: 1 step: 650, loss is 0.3516598045825958\n",
      "epoch: 1 step: 651, loss is 0.35491490364074707\n",
      "epoch: 1 step: 652, loss is 0.4687895178794861\n",
      "epoch: 1 step: 653, loss is 0.39070644974708557\n",
      "epoch: 1 step: 654, loss is 0.45009952783584595\n",
      "epoch: 1 step: 655, loss is 0.27716633677482605\n",
      "epoch: 1 step: 656, loss is 0.4178261160850525\n",
      "epoch: 1 step: 657, loss is 0.44459012150764465\n",
      "epoch: 1 step: 658, loss is 0.4895327687263489\n",
      "epoch: 1 step: 659, loss is 0.39894795417785645\n",
      "epoch: 1 step: 660, loss is 0.3215821385383606\n",
      "epoch: 1 step: 661, loss is 0.4818294048309326\n",
      "epoch: 1 step: 662, loss is 0.4555068612098694\n",
      "epoch: 1 step: 663, loss is 0.417512983083725\n",
      "epoch: 1 step: 664, loss is 0.4746442139148712\n",
      "epoch: 1 step: 665, loss is 0.532078206539154\n",
      "epoch: 1 step: 666, loss is 0.43784695863723755\n",
      "epoch: 1 step: 667, loss is 0.4070587754249573\n",
      "epoch: 1 step: 668, loss is 0.331794410943985\n",
      "epoch: 1 step: 669, loss is 0.293472021818161\n",
      "epoch: 1 step: 670, loss is 0.3659270405769348\n",
      "epoch: 1 step: 671, loss is 0.37770113348960876\n",
      "epoch: 1 step: 672, loss is 0.3527834713459015\n",
      "epoch: 1 step: 673, loss is 0.6322316527366638\n",
      "epoch: 1 step: 674, loss is 0.5500355362892151\n",
      "epoch: 1 step: 675, loss is 0.50815349817276\n",
      "epoch: 1 step: 676, loss is 0.3752783536911011\n",
      "epoch: 1 step: 677, loss is 0.6243547201156616\n",
      "epoch: 1 step: 678, loss is 0.42856067419052124\n",
      "epoch: 1 step: 679, loss is 0.3096141219139099\n",
      "epoch: 1 step: 680, loss is 0.3901740312576294\n",
      "epoch: 1 step: 681, loss is 0.43760544061660767\n",
      "epoch: 1 step: 682, loss is 0.3029000759124756\n",
      "epoch: 1 step: 683, loss is 0.3729557394981384\n",
      "epoch: 1 step: 684, loss is 0.3670385181903839\n",
      "epoch: 1 step: 685, loss is 0.4450962245464325\n",
      "epoch: 1 step: 686, loss is 0.4231354892253876\n",
      "epoch: 1 step: 687, loss is 0.3218216896057129\n",
      "epoch: 1 step: 688, loss is 0.3040273189544678\n",
      "epoch: 1 step: 689, loss is 0.463810533285141\n",
      "epoch: 1 step: 690, loss is 0.31811094284057617\n",
      "epoch: 1 step: 691, loss is 0.22417087852954865\n",
      "epoch: 1 step: 692, loss is 0.2358686774969101\n",
      "epoch: 1 step: 693, loss is 0.28136250376701355\n",
      "epoch: 1 step: 694, loss is 0.5112887024879456\n",
      "epoch: 1 step: 695, loss is 0.30187177658081055\n",
      "epoch: 1 step: 696, loss is 0.31940996646881104\n",
      "epoch: 1 step: 697, loss is 0.30658116936683655\n",
      "epoch: 1 step: 698, loss is 0.38700324296951294\n",
      "epoch: 1 step: 699, loss is 0.3408689796924591\n",
      "epoch: 1 step: 700, loss is 0.3988521099090576\n",
      "epoch: 1 step: 701, loss is 0.3461730182170868\n",
      "epoch: 1 step: 702, loss is 0.27479541301727295\n",
      "epoch: 1 step: 703, loss is 0.3078077733516693\n",
      "epoch: 1 step: 704, loss is 0.2853284478187561\n",
      "epoch: 1 step: 705, loss is 0.25858956575393677\n",
      "epoch: 1 step: 706, loss is 0.32164788246154785\n",
      "epoch: 1 step: 707, loss is 0.3208228647708893\n",
      "epoch: 1 step: 708, loss is 0.5430344343185425\n",
      "epoch: 1 step: 709, loss is 0.24434711039066315\n",
      "epoch: 1 step: 710, loss is 0.46113839745521545\n",
      "epoch: 1 step: 711, loss is 0.358791708946228\n",
      "epoch: 1 step: 712, loss is 0.6518163681030273\n",
      "epoch: 1 step: 713, loss is 0.34453505277633667\n",
      "epoch: 1 step: 714, loss is 0.47109031677246094\n",
      "epoch: 1 step: 715, loss is 0.36655253171920776\n",
      "epoch: 1 step: 716, loss is 0.4057353138923645\n",
      "epoch: 1 step: 717, loss is 0.38250651955604553\n",
      "epoch: 1 step: 718, loss is 0.5961052179336548\n",
      "epoch: 1 step: 719, loss is 0.34286579489707947\n",
      "epoch: 1 step: 720, loss is 0.5597143173217773\n",
      "epoch: 1 step: 721, loss is 0.46035754680633545\n",
      "epoch: 1 step: 722, loss is 0.48289746046066284\n",
      "epoch: 1 step: 723, loss is 0.3180426359176636\n",
      "epoch: 1 step: 724, loss is 0.4463535249233246\n",
      "epoch: 1 step: 725, loss is 0.4166354835033417\n",
      "epoch: 1 step: 726, loss is 0.43007150292396545\n",
      "epoch: 1 step: 727, loss is 0.35146522521972656\n",
      "epoch: 1 step: 728, loss is 0.39812368154525757\n",
      "epoch: 1 step: 729, loss is 0.534320592880249\n",
      "epoch: 1 step: 730, loss is 0.36099502444267273\n",
      "epoch: 1 step: 731, loss is 0.42669418454170227\n",
      "epoch: 1 step: 732, loss is 0.30820873379707336\n",
      "epoch: 1 step: 733, loss is 0.4098411798477173\n",
      "epoch: 1 step: 734, loss is 0.3738124966621399\n",
      "epoch: 1 step: 735, loss is 0.5514205098152161\n",
      "epoch: 1 step: 736, loss is 0.34414419531822205\n",
      "epoch: 1 step: 737, loss is 0.4810805916786194\n",
      "epoch: 1 step: 738, loss is 0.4709584414958954\n",
      "epoch: 1 step: 739, loss is 0.390896201133728\n",
      "epoch: 1 step: 740, loss is 0.32064712047576904\n",
      "epoch: 1 step: 741, loss is 0.41148316860198975\n",
      "epoch: 1 step: 742, loss is 0.5991296172142029\n",
      "epoch: 1 step: 743, loss is 0.4494263529777527\n",
      "epoch: 1 step: 744, loss is 0.33448585867881775\n",
      "epoch: 1 step: 745, loss is 0.5155100226402283\n",
      "epoch: 1 step: 746, loss is 0.4961664378643036\n",
      "epoch: 1 step: 747, loss is 0.24601349234580994\n",
      "epoch: 1 step: 748, loss is 0.6567811965942383\n",
      "epoch: 1 step: 749, loss is 0.4371563494205475\n",
      "epoch: 1 step: 750, loss is 0.4074196517467499\n",
      "epoch: 1 step: 751, loss is 0.36545953154563904\n",
      "epoch: 1 step: 752, loss is 0.3085392117500305\n",
      "epoch: 1 step: 753, loss is 0.4497677683830261\n",
      "epoch: 1 step: 754, loss is 0.6342329978942871\n",
      "epoch: 1 step: 755, loss is 0.5087367296218872\n",
      "epoch: 1 step: 756, loss is 0.39359959959983826\n",
      "epoch: 1 step: 757, loss is 0.3976377248764038\n",
      "epoch: 1 step: 758, loss is 0.2838539481163025\n",
      "epoch: 1 step: 759, loss is 0.4230772852897644\n",
      "epoch: 1 step: 760, loss is 0.29643285274505615\n",
      "epoch: 1 step: 761, loss is 0.37842586636543274\n",
      "epoch: 1 step: 762, loss is 0.41833406686782837\n",
      "epoch: 1 step: 763, loss is 0.34763529896736145\n",
      "epoch: 1 step: 764, loss is 0.2534600496292114\n",
      "epoch: 1 step: 765, loss is 0.2604711949825287\n",
      "epoch: 1 step: 766, loss is 0.5045220851898193\n",
      "epoch: 1 step: 767, loss is 0.4928848147392273\n",
      "epoch: 1 step: 768, loss is 0.2259063869714737\n",
      "epoch: 1 step: 769, loss is 0.3628310561180115\n",
      "epoch: 1 step: 770, loss is 0.4439813196659088\n",
      "epoch: 1 step: 771, loss is 0.29831045866012573\n",
      "epoch: 1 step: 772, loss is 0.5160192251205444\n",
      "epoch: 1 step: 773, loss is 0.31794875860214233\n",
      "epoch: 1 step: 774, loss is 0.37136080861091614\n",
      "epoch: 1 step: 775, loss is 0.28326940536499023\n",
      "epoch: 1 step: 776, loss is 0.40457743406295776\n",
      "epoch: 1 step: 777, loss is 0.3390207290649414\n",
      "epoch: 1 step: 778, loss is 0.3046053647994995\n",
      "epoch: 1 step: 779, loss is 0.6482740640640259\n",
      "epoch: 1 step: 780, loss is 0.2921624481678009\n",
      "epoch: 1 step: 781, loss is 0.518320620059967\n",
      "epoch: 1 step: 782, loss is 0.3388277590274811\n",
      "epoch: 1 step: 783, loss is 0.36955761909484863\n",
      "epoch: 1 step: 784, loss is 0.21407490968704224\n",
      "epoch: 1 step: 785, loss is 0.3129066824913025\n",
      "epoch: 1 step: 786, loss is 0.3522900342941284\n",
      "epoch: 1 step: 787, loss is 0.291228711605072\n",
      "epoch: 1 step: 788, loss is 0.3352673649787903\n",
      "epoch: 1 step: 789, loss is 0.30593469738960266\n",
      "epoch: 1 step: 790, loss is 0.35113778710365295\n",
      "epoch: 1 step: 791, loss is 0.46515098214149475\n",
      "epoch: 1 step: 792, loss is 0.25139176845550537\n",
      "epoch: 1 step: 793, loss is 0.356067031621933\n",
      "epoch: 1 step: 794, loss is 0.4494559168815613\n",
      "epoch: 1 step: 795, loss is 0.3808719515800476\n",
      "epoch: 1 step: 796, loss is 0.5485823154449463\n",
      "epoch: 1 step: 797, loss is 0.2607240378856659\n",
      "epoch: 1 step: 798, loss is 0.34749680757522583\n",
      "epoch: 1 step: 799, loss is 0.2951807677745819\n",
      "epoch: 1 step: 800, loss is 0.8445980548858643\n",
      "epoch: 1 step: 801, loss is 0.38503143191337585\n",
      "epoch: 1 step: 802, loss is 0.7415659427642822\n",
      "epoch: 1 step: 803, loss is 0.471964567899704\n",
      "epoch: 1 step: 804, loss is 0.37372735142707825\n",
      "epoch: 1 step: 805, loss is 0.3821289539337158\n",
      "epoch: 1 step: 806, loss is 0.2878153920173645\n",
      "epoch: 1 step: 807, loss is 0.5201141238212585\n",
      "epoch: 1 step: 808, loss is 0.40779757499694824\n",
      "epoch: 1 step: 809, loss is 0.35983508825302124\n",
      "epoch: 1 step: 810, loss is 0.31658536195755005\n",
      "epoch: 1 step: 811, loss is 0.3671391010284424\n",
      "epoch: 1 step: 812, loss is 0.26541417837142944\n",
      "epoch: 1 step: 813, loss is 0.4282051920890808\n",
      "epoch: 1 step: 814, loss is 0.37230467796325684\n",
      "epoch: 1 step: 815, loss is 0.2866279184818268\n",
      "epoch: 1 step: 816, loss is 0.3184104263782501\n",
      "epoch: 1 step: 817, loss is 0.38722848892211914\n",
      "epoch: 1 step: 818, loss is 0.3601889908313751\n",
      "epoch: 1 step: 819, loss is 0.3300483822822571\n",
      "epoch: 1 step: 820, loss is 0.32570889592170715\n",
      "epoch: 1 step: 821, loss is 0.40596505999565125\n",
      "epoch: 1 step: 822, loss is 0.35808125138282776\n",
      "epoch: 1 step: 823, loss is 0.37956732511520386\n",
      "epoch: 1 step: 824, loss is 0.21064186096191406\n",
      "epoch: 1 step: 825, loss is 0.4714358448982239\n",
      "epoch: 1 step: 826, loss is 0.4465009868144989\n",
      "epoch: 1 step: 827, loss is 0.4651615619659424\n",
      "epoch: 1 step: 828, loss is 0.45633113384246826\n",
      "epoch: 1 step: 829, loss is 0.35079118609428406\n",
      "epoch: 1 step: 830, loss is 0.2845979928970337\n",
      "epoch: 1 step: 831, loss is 0.22310630977153778\n",
      "epoch: 1 step: 832, loss is 0.37534111738204956\n",
      "epoch: 1 step: 833, loss is 0.5105428695678711\n",
      "epoch: 1 step: 834, loss is 0.27588126063346863\n",
      "epoch: 1 step: 835, loss is 0.4190012514591217\n",
      "epoch: 1 step: 836, loss is 0.3328982889652252\n",
      "epoch: 1 step: 837, loss is 0.26074597239494324\n",
      "epoch: 1 step: 838, loss is 0.33946195244789124\n",
      "epoch: 1 step: 839, loss is 0.30758923292160034\n",
      "epoch: 1 step: 840, loss is 0.48906245827674866\n",
      "epoch: 1 step: 841, loss is 0.24509917199611664\n",
      "epoch: 1 step: 842, loss is 0.31453895568847656\n",
      "epoch: 1 step: 843, loss is 0.33595454692840576\n",
      "epoch: 1 step: 844, loss is 0.30225926637649536\n",
      "epoch: 1 step: 845, loss is 0.23430268466472626\n",
      "epoch: 1 step: 846, loss is 0.35406655073165894\n",
      "epoch: 1 step: 847, loss is 0.3982224464416504\n",
      "epoch: 1 step: 848, loss is 0.43091610074043274\n",
      "epoch: 1 step: 849, loss is 0.3740193545818329\n",
      "epoch: 1 step: 850, loss is 0.3414694368839264\n",
      "epoch: 1 step: 851, loss is 0.5057094693183899\n",
      "epoch: 1 step: 852, loss is 0.1425255835056305\n",
      "epoch: 1 step: 853, loss is 0.2852073609828949\n",
      "epoch: 1 step: 854, loss is 0.31218409538269043\n",
      "epoch: 1 step: 855, loss is 0.18203021585941315\n",
      "epoch: 1 step: 856, loss is 0.4631589651107788\n",
      "epoch: 1 step: 857, loss is 0.33129915595054626\n",
      "epoch: 1 step: 858, loss is 0.23166358470916748\n",
      "epoch: 1 step: 859, loss is 0.5324928760528564\n",
      "epoch: 1 step: 860, loss is 0.31104812026023865\n",
      "epoch: 1 step: 861, loss is 0.4161912500858307\n",
      "epoch: 1 step: 862, loss is 0.44393160939216614\n",
      "epoch: 1 step: 863, loss is 0.34903091192245483\n",
      "epoch: 1 step: 864, loss is 0.12644247710704803\n",
      "epoch: 1 step: 865, loss is 0.3931708037853241\n",
      "epoch: 1 step: 866, loss is 0.3588433563709259\n",
      "epoch: 1 step: 867, loss is 0.3649676442146301\n",
      "epoch: 1 step: 868, loss is 0.28207406401634216\n",
      "epoch: 1 step: 869, loss is 0.31192073225975037\n",
      "epoch: 1 step: 870, loss is 0.5209660530090332\n",
      "epoch: 1 step: 871, loss is 0.2762829065322876\n",
      "epoch: 1 step: 872, loss is 0.29688796401023865\n",
      "epoch: 1 step: 873, loss is 0.21502842009067535\n",
      "epoch: 1 step: 874, loss is 0.15542298555374146\n",
      "epoch: 1 step: 875, loss is 0.4486267566680908\n",
      "epoch: 1 step: 876, loss is 0.3705342411994934\n",
      "epoch: 1 step: 877, loss is 0.6115641593933105\n",
      "epoch: 1 step: 878, loss is 0.2824813723564148\n",
      "epoch: 1 step: 879, loss is 0.2995055615901947\n",
      "epoch: 1 step: 880, loss is 0.29584330320358276\n",
      "epoch: 1 step: 881, loss is 0.315213143825531\n",
      "epoch: 1 step: 882, loss is 0.3643331527709961\n",
      "epoch: 1 step: 883, loss is 0.31710681319236755\n",
      "epoch: 1 step: 884, loss is 0.36886271834373474\n",
      "epoch: 1 step: 885, loss is 0.40659230947494507\n",
      "epoch: 1 step: 886, loss is 0.2765904664993286\n",
      "epoch: 1 step: 887, loss is 0.27360326051712036\n",
      "epoch: 1 step: 888, loss is 0.2482244223356247\n",
      "epoch: 1 step: 889, loss is 0.32823166251182556\n",
      "epoch: 1 step: 890, loss is 0.40251317620277405\n",
      "epoch: 1 step: 891, loss is 0.26335427165031433\n",
      "epoch: 1 step: 892, loss is 0.5261141657829285\n",
      "epoch: 1 step: 893, loss is 0.47676196694374084\n",
      "epoch: 1 step: 894, loss is 0.4141618013381958\n",
      "epoch: 1 step: 895, loss is 0.6939030885696411\n",
      "epoch: 1 step: 896, loss is 0.3569377660751343\n",
      "epoch: 1 step: 897, loss is 0.35742586851119995\n",
      "epoch: 1 step: 898, loss is 0.28927165269851685\n",
      "epoch: 1 step: 899, loss is 0.2280750870704651\n",
      "epoch: 1 step: 900, loss is 0.4117806851863861\n",
      "epoch: 1 step: 901, loss is 0.318065881729126\n",
      "epoch: 1 step: 902, loss is 0.4396511912345886\n",
      "epoch: 1 step: 903, loss is 0.36947280168533325\n",
      "epoch: 1 step: 904, loss is 0.37605464458465576\n",
      "epoch: 1 step: 905, loss is 0.5211360454559326\n",
      "epoch: 1 step: 906, loss is 0.3179456889629364\n",
      "epoch: 1 step: 907, loss is 0.3912562429904938\n",
      "epoch: 1 step: 908, loss is 0.29553714394569397\n",
      "epoch: 1 step: 909, loss is 0.3476439416408539\n",
      "epoch: 1 step: 910, loss is 0.31224778294563293\n",
      "epoch: 1 step: 911, loss is 0.3516567647457123\n",
      "epoch: 1 step: 912, loss is 0.29625481367111206\n",
      "epoch: 1 step: 913, loss is 0.3213122487068176\n",
      "epoch: 1 step: 914, loss is 0.3023504614830017\n",
      "epoch: 1 step: 915, loss is 0.41519397497177124\n",
      "epoch: 1 step: 916, loss is 0.31208905577659607\n",
      "epoch: 1 step: 917, loss is 0.44926324486732483\n",
      "epoch: 1 step: 918, loss is 0.33364203572273254\n",
      "epoch: 1 step: 919, loss is 0.26767832040786743\n",
      "epoch: 1 step: 920, loss is 0.3338192403316498\n",
      "epoch: 1 step: 921, loss is 0.3162882328033447\n",
      "epoch: 1 step: 922, loss is 0.30355629324913025\n",
      "epoch: 1 step: 923, loss is 0.589097261428833\n",
      "epoch: 1 step: 924, loss is 0.28986403346061707\n",
      "epoch: 1 step: 925, loss is 0.2075093388557434\n",
      "epoch: 1 step: 926, loss is 0.27291807532310486\n",
      "epoch: 1 step: 927, loss is 0.3813718259334564\n",
      "epoch: 1 step: 928, loss is 0.5281702280044556\n",
      "epoch: 1 step: 929, loss is 0.26661989092826843\n",
      "epoch: 1 step: 930, loss is 0.4745408594608307\n",
      "epoch: 1 step: 931, loss is 0.4780021607875824\n",
      "epoch: 1 step: 932, loss is 0.3450445234775543\n",
      "epoch: 1 step: 933, loss is 0.2572179436683655\n",
      "epoch: 1 step: 934, loss is 0.3107181787490845\n",
      "epoch: 1 step: 935, loss is 0.49770745635032654\n",
      "epoch: 1 step: 936, loss is 0.19668832421302795\n",
      "epoch: 1 step: 937, loss is 0.44384267926216125\n",
      "epoch: 2 step: 1, loss is 0.4209202826023102\n",
      "epoch: 2 step: 2, loss is 0.4081305265426636\n",
      "epoch: 2 step: 3, loss is 0.2343057543039322\n",
      "epoch: 2 step: 4, loss is 0.44743266701698303\n",
      "epoch: 2 step: 5, loss is 0.34496092796325684\n",
      "epoch: 2 step: 6, loss is 0.26830407977104187\n",
      "epoch: 2 step: 7, loss is 0.3437652885913849\n",
      "epoch: 2 step: 8, loss is 0.3298581838607788\n",
      "epoch: 2 step: 9, loss is 0.3373294472694397\n",
      "epoch: 2 step: 10, loss is 0.3771926760673523\n",
      "epoch: 2 step: 11, loss is 0.2707671523094177\n",
      "epoch: 2 step: 12, loss is 0.34673550724983215\n",
      "epoch: 2 step: 13, loss is 0.27683889865875244\n",
      "epoch: 2 step: 14, loss is 0.3779967725276947\n",
      "epoch: 2 step: 15, loss is 0.18490666151046753\n",
      "epoch: 2 step: 16, loss is 0.2594873905181885\n",
      "epoch: 2 step: 17, loss is 0.29029569029808044\n",
      "epoch: 2 step: 18, loss is 0.2777480483055115\n",
      "epoch: 2 step: 19, loss is 0.4440685212612152\n",
      "epoch: 2 step: 20, loss is 0.45540276169776917\n",
      "epoch: 2 step: 21, loss is 0.3199478089809418\n",
      "epoch: 2 step: 22, loss is 0.21786284446716309\n",
      "epoch: 2 step: 23, loss is 0.26930463314056396\n",
      "epoch: 2 step: 24, loss is 0.4108051359653473\n",
      "epoch: 2 step: 25, loss is 0.5036463141441345\n",
      "epoch: 2 step: 26, loss is 0.3282201886177063\n",
      "epoch: 2 step: 27, loss is 0.21256454288959503\n",
      "epoch: 2 step: 28, loss is 0.3177398145198822\n",
      "epoch: 2 step: 29, loss is 0.6052113771438599\n",
      "epoch: 2 step: 30, loss is 0.23315328359603882\n",
      "epoch: 2 step: 31, loss is 0.1709669828414917\n",
      "epoch: 2 step: 32, loss is 0.45189985632896423\n",
      "epoch: 2 step: 33, loss is 0.4634435474872589\n",
      "epoch: 2 step: 34, loss is 0.30295005440711975\n",
      "epoch: 2 step: 35, loss is 0.474867045879364\n",
      "epoch: 2 step: 36, loss is 0.5228686332702637\n",
      "epoch: 2 step: 37, loss is 0.3447621166706085\n",
      "epoch: 2 step: 38, loss is 0.26939743757247925\n",
      "epoch: 2 step: 39, loss is 0.2757200002670288\n",
      "epoch: 2 step: 40, loss is 0.19862207770347595\n",
      "epoch: 2 step: 41, loss is 0.3466039001941681\n",
      "epoch: 2 step: 42, loss is 0.33119338750839233\n",
      "epoch: 2 step: 43, loss is 0.5324496626853943\n",
      "epoch: 2 step: 44, loss is 0.29882171750068665\n",
      "epoch: 2 step: 45, loss is 0.45977282524108887\n",
      "epoch: 2 step: 46, loss is 0.33517807722091675\n",
      "epoch: 2 step: 47, loss is 0.36670181155204773\n",
      "epoch: 2 step: 48, loss is 0.3990332782268524\n",
      "epoch: 2 step: 49, loss is 0.44549494981765747\n",
      "epoch: 2 step: 50, loss is 0.47778695821762085\n",
      "epoch: 2 step: 51, loss is 0.31097161769866943\n",
      "epoch: 2 step: 52, loss is 0.7020580768585205\n",
      "epoch: 2 step: 53, loss is 0.23365744948387146\n",
      "epoch: 2 step: 54, loss is 0.42722073197364807\n",
      "epoch: 2 step: 55, loss is 0.26012369990348816\n",
      "epoch: 2 step: 56, loss is 0.4216119349002838\n",
      "epoch: 2 step: 57, loss is 0.3044784963130951\n",
      "epoch: 2 step: 58, loss is 0.2713562548160553\n",
      "epoch: 2 step: 59, loss is 0.27131399512290955\n",
      "epoch: 2 step: 60, loss is 0.3670366108417511\n",
      "epoch: 2 step: 61, loss is 0.4562649428844452\n",
      "epoch: 2 step: 62, loss is 0.4104631841182709\n",
      "epoch: 2 step: 63, loss is 0.26587021350860596\n",
      "epoch: 2 step: 64, loss is 0.2213815450668335\n",
      "epoch: 2 step: 65, loss is 0.3906797468662262\n",
      "epoch: 2 step: 66, loss is 0.34177955985069275\n",
      "epoch: 2 step: 67, loss is 0.3329305648803711\n",
      "epoch: 2 step: 68, loss is 0.32292640209198\n",
      "epoch: 2 step: 69, loss is 0.30538466572761536\n",
      "epoch: 2 step: 70, loss is 0.24480867385864258\n",
      "epoch: 2 step: 71, loss is 0.41437649726867676\n",
      "epoch: 2 step: 72, loss is 0.11912348121404648\n",
      "epoch: 2 step: 73, loss is 0.3827962279319763\n",
      "epoch: 2 step: 74, loss is 0.41497883200645447\n",
      "epoch: 2 step: 75, loss is 0.3883788287639618\n",
      "epoch: 2 step: 76, loss is 0.2917676568031311\n",
      "epoch: 2 step: 77, loss is 0.2979973256587982\n",
      "epoch: 2 step: 78, loss is 0.35587918758392334\n",
      "epoch: 2 step: 79, loss is 0.3917047083377838\n",
      "epoch: 2 step: 80, loss is 0.5427922606468201\n",
      "epoch: 2 step: 81, loss is 0.31150931119918823\n",
      "epoch: 2 step: 82, loss is 0.25023362040519714\n",
      "epoch: 2 step: 83, loss is 0.1623305380344391\n",
      "epoch: 2 step: 84, loss is 0.361286997795105\n",
      "epoch: 2 step: 85, loss is 0.2914065420627594\n",
      "epoch: 2 step: 86, loss is 0.35407400131225586\n",
      "epoch: 2 step: 87, loss is 0.2737041711807251\n",
      "epoch: 2 step: 88, loss is 0.28863802552223206\n",
      "epoch: 2 step: 89, loss is 0.3268392086029053\n",
      "epoch: 2 step: 90, loss is 0.45151492953300476\n",
      "epoch: 2 step: 91, loss is 0.3783442974090576\n",
      "epoch: 2 step: 92, loss is 0.38213878870010376\n",
      "epoch: 2 step: 93, loss is 0.27786096930503845\n",
      "epoch: 2 step: 94, loss is 0.2299848198890686\n",
      "epoch: 2 step: 95, loss is 0.29555845260620117\n",
      "epoch: 2 step: 96, loss is 0.27997973561286926\n",
      "epoch: 2 step: 97, loss is 0.3652615547180176\n",
      "epoch: 2 step: 98, loss is 0.3238829970359802\n",
      "epoch: 2 step: 99, loss is 0.2804381251335144\n",
      "epoch: 2 step: 100, loss is 0.268839567899704\n",
      "epoch: 2 step: 101, loss is 0.22740694880485535\n",
      "epoch: 2 step: 102, loss is 0.28591224551200867\n",
      "epoch: 2 step: 103, loss is 0.3562735617160797\n",
      "epoch: 2 step: 104, loss is 0.42484739422798157\n",
      "epoch: 2 step: 105, loss is 0.3597598969936371\n",
      "epoch: 2 step: 106, loss is 0.3439156115055084\n",
      "epoch: 2 step: 107, loss is 0.4767136871814728\n",
      "epoch: 2 step: 108, loss is 0.22579170763492584\n",
      "epoch: 2 step: 109, loss is 0.3323402404785156\n",
      "epoch: 2 step: 110, loss is 0.5953917503356934\n",
      "epoch: 2 step: 111, loss is 0.3550523817539215\n",
      "epoch: 2 step: 112, loss is 0.3692341148853302\n",
      "epoch: 2 step: 113, loss is 0.39839065074920654\n",
      "epoch: 2 step: 114, loss is 0.3955336809158325\n",
      "epoch: 2 step: 115, loss is 0.322491317987442\n",
      "epoch: 2 step: 116, loss is 0.32533538341522217\n",
      "epoch: 2 step: 117, loss is 0.34106069803237915\n",
      "epoch: 2 step: 118, loss is 0.340179443359375\n",
      "epoch: 2 step: 119, loss is 0.34198009967803955\n",
      "epoch: 2 step: 120, loss is 0.31437504291534424\n",
      "epoch: 2 step: 121, loss is 0.29170793294906616\n",
      "epoch: 2 step: 122, loss is 0.3526679575443268\n",
      "epoch: 2 step: 123, loss is 0.4774470627307892\n",
      "epoch: 2 step: 124, loss is 0.3516741096973419\n",
      "epoch: 2 step: 125, loss is 0.42417553067207336\n",
      "epoch: 2 step: 126, loss is 0.30971264839172363\n",
      "epoch: 2 step: 127, loss is 0.31700217723846436\n",
      "epoch: 2 step: 128, loss is 0.4241564869880676\n",
      "epoch: 2 step: 129, loss is 0.36934709548950195\n",
      "epoch: 2 step: 130, loss is 0.3452144265174866\n",
      "epoch: 2 step: 131, loss is 0.3574244976043701\n",
      "epoch: 2 step: 132, loss is 0.3125450015068054\n",
      "epoch: 2 step: 133, loss is 0.4357479512691498\n",
      "epoch: 2 step: 134, loss is 0.4446980059146881\n",
      "epoch: 2 step: 135, loss is 0.13982635736465454\n",
      "epoch: 2 step: 136, loss is 0.35081279277801514\n",
      "epoch: 2 step: 137, loss is 0.42022469639778137\n",
      "epoch: 2 step: 138, loss is 0.32710862159729004\n",
      "epoch: 2 step: 139, loss is 0.3444695472717285\n",
      "epoch: 2 step: 140, loss is 0.4255450367927551\n",
      "epoch: 2 step: 141, loss is 0.23179827630519867\n",
      "epoch: 2 step: 142, loss is 0.28141969442367554\n",
      "epoch: 2 step: 143, loss is 0.3435344994068146\n",
      "epoch: 2 step: 144, loss is 0.26814767718315125\n",
      "epoch: 2 step: 145, loss is 0.4240526258945465\n",
      "epoch: 2 step: 146, loss is 0.241119384765625\n",
      "epoch: 2 step: 147, loss is 0.19377318024635315\n",
      "epoch: 2 step: 148, loss is 0.4505440890789032\n",
      "epoch: 2 step: 149, loss is 0.27272000908851624\n",
      "epoch: 2 step: 150, loss is 0.09506294131278992\n",
      "epoch: 2 step: 151, loss is 0.2500017285346985\n",
      "epoch: 2 step: 152, loss is 0.515601634979248\n",
      "epoch: 2 step: 153, loss is 0.35257813334465027\n",
      "epoch: 2 step: 154, loss is 0.2779862582683563\n",
      "epoch: 2 step: 155, loss is 0.3292083442211151\n",
      "epoch: 2 step: 156, loss is 0.5983178019523621\n",
      "epoch: 2 step: 157, loss is 0.31748923659324646\n",
      "epoch: 2 step: 158, loss is 0.3640381693840027\n",
      "epoch: 2 step: 159, loss is 0.2745682895183563\n",
      "epoch: 2 step: 160, loss is 0.26599907875061035\n",
      "epoch: 2 step: 161, loss is 0.5253412127494812\n",
      "epoch: 2 step: 162, loss is 0.43277353048324585\n",
      "epoch: 2 step: 163, loss is 0.27616509795188904\n",
      "epoch: 2 step: 164, loss is 0.2673102915287018\n",
      "epoch: 2 step: 165, loss is 0.3378397226333618\n",
      "epoch: 2 step: 166, loss is 0.24090388417243958\n",
      "epoch: 2 step: 167, loss is 0.35448649525642395\n",
      "epoch: 2 step: 168, loss is 0.25164034962654114\n",
      "epoch: 2 step: 169, loss is 0.4711593985557556\n",
      "epoch: 2 step: 170, loss is 0.23693445324897766\n",
      "epoch: 2 step: 171, loss is 0.27643853425979614\n",
      "epoch: 2 step: 172, loss is 0.3072989583015442\n",
      "epoch: 2 step: 173, loss is 0.23161615431308746\n",
      "epoch: 2 step: 174, loss is 0.18541142344474792\n",
      "epoch: 2 step: 175, loss is 0.22637701034545898\n",
      "epoch: 2 step: 176, loss is 0.3955205976963043\n",
      "epoch: 2 step: 177, loss is 0.2985338568687439\n",
      "epoch: 2 step: 178, loss is 0.34151196479797363\n",
      "epoch: 2 step: 179, loss is 0.33395761251449585\n",
      "epoch: 2 step: 180, loss is 0.2832293212413788\n",
      "epoch: 2 step: 181, loss is 0.20398907363414764\n",
      "epoch: 2 step: 182, loss is 0.25157174468040466\n",
      "epoch: 2 step: 183, loss is 0.2839379608631134\n",
      "epoch: 2 step: 184, loss is 0.4000249207019806\n",
      "epoch: 2 step: 185, loss is 0.375456839799881\n",
      "epoch: 2 step: 186, loss is 0.29083266854286194\n",
      "epoch: 2 step: 187, loss is 0.18066149950027466\n",
      "epoch: 2 step: 188, loss is 0.28133445978164673\n",
      "epoch: 2 step: 189, loss is 0.34634190797805786\n",
      "epoch: 2 step: 190, loss is 0.28421956300735474\n",
      "epoch: 2 step: 191, loss is 0.30634358525276184\n",
      "epoch: 2 step: 192, loss is 0.39360320568084717\n",
      "epoch: 2 step: 193, loss is 0.18026305735111237\n",
      "epoch: 2 step: 194, loss is 0.3161695599555969\n",
      "epoch: 2 step: 195, loss is 0.33184289932250977\n",
      "epoch: 2 step: 196, loss is 0.4255138337612152\n",
      "epoch: 2 step: 197, loss is 0.4025784730911255\n",
      "epoch: 2 step: 198, loss is 0.29983246326446533\n",
      "epoch: 2 step: 199, loss is 0.12729479372501373\n",
      "epoch: 2 step: 200, loss is 0.34182652831077576\n",
      "epoch: 2 step: 201, loss is 0.4302010238170624\n",
      "epoch: 2 step: 202, loss is 0.20399689674377441\n",
      "epoch: 2 step: 203, loss is 0.48826706409454346\n",
      "epoch: 2 step: 204, loss is 0.19687913358211517\n",
      "epoch: 2 step: 205, loss is 0.48977264761924744\n",
      "epoch: 2 step: 206, loss is 0.3185139000415802\n",
      "epoch: 2 step: 207, loss is 0.2631900906562805\n",
      "epoch: 2 step: 208, loss is 0.3109707534313202\n",
      "epoch: 2 step: 209, loss is 0.3065766990184784\n",
      "epoch: 2 step: 210, loss is 0.3884546756744385\n",
      "epoch: 2 step: 211, loss is 0.26831355690956116\n",
      "epoch: 2 step: 212, loss is 0.28248360753059387\n",
      "epoch: 2 step: 213, loss is 0.2614545524120331\n",
      "epoch: 2 step: 214, loss is 0.3515886664390564\n",
      "epoch: 2 step: 215, loss is 0.2316482812166214\n",
      "epoch: 2 step: 216, loss is 0.33730220794677734\n",
      "epoch: 2 step: 217, loss is 0.210691437125206\n",
      "epoch: 2 step: 218, loss is 0.25842469930648804\n",
      "epoch: 2 step: 219, loss is 0.30879008769989014\n",
      "epoch: 2 step: 220, loss is 0.4950679838657379\n",
      "epoch: 2 step: 221, loss is 0.26227253675460815\n",
      "epoch: 2 step: 222, loss is 0.18534356355667114\n",
      "epoch: 2 step: 223, loss is 0.35757067799568176\n",
      "epoch: 2 step: 224, loss is 0.5630322098731995\n",
      "epoch: 2 step: 225, loss is 0.3798850476741791\n",
      "epoch: 2 step: 226, loss is 0.28417328000068665\n",
      "epoch: 2 step: 227, loss is 0.268354207277298\n",
      "epoch: 2 step: 228, loss is 0.331219881772995\n",
      "epoch: 2 step: 229, loss is 0.25108233094215393\n",
      "epoch: 2 step: 230, loss is 0.24704214930534363\n",
      "epoch: 2 step: 231, loss is 0.2531190514564514\n",
      "epoch: 2 step: 232, loss is 0.3945556581020355\n",
      "epoch: 2 step: 233, loss is 0.2950707674026489\n",
      "epoch: 2 step: 234, loss is 0.20039553940296173\n",
      "epoch: 2 step: 235, loss is 0.3033897578716278\n",
      "epoch: 2 step: 236, loss is 0.3309725821018219\n",
      "epoch: 2 step: 237, loss is 0.25143125653266907\n",
      "epoch: 2 step: 238, loss is 0.26501476764678955\n",
      "epoch: 2 step: 239, loss is 0.3213619291782379\n",
      "epoch: 2 step: 240, loss is 0.3454083502292633\n",
      "epoch: 2 step: 241, loss is 0.37973299622535706\n",
      "epoch: 2 step: 242, loss is 0.36213019490242004\n",
      "epoch: 2 step: 243, loss is 0.2902085781097412\n",
      "epoch: 2 step: 244, loss is 0.23628130555152893\n",
      "epoch: 2 step: 245, loss is 0.34023332595825195\n",
      "epoch: 2 step: 246, loss is 0.43167465925216675\n",
      "epoch: 2 step: 247, loss is 0.3325151801109314\n",
      "epoch: 2 step: 248, loss is 0.3379288613796234\n",
      "epoch: 2 step: 249, loss is 0.3246956467628479\n",
      "epoch: 2 step: 250, loss is 0.24071261286735535\n",
      "epoch: 2 step: 251, loss is 0.1884182095527649\n",
      "epoch: 2 step: 252, loss is 0.35160547494888306\n",
      "epoch: 2 step: 253, loss is 0.330810010433197\n",
      "epoch: 2 step: 254, loss is 0.35131174325942993\n",
      "epoch: 2 step: 255, loss is 0.4154910445213318\n",
      "epoch: 2 step: 256, loss is 0.24566017091274261\n",
      "epoch: 2 step: 257, loss is 0.42321664094924927\n",
      "epoch: 2 step: 258, loss is 0.398114949464798\n",
      "epoch: 2 step: 259, loss is 0.21674145758152008\n",
      "epoch: 2 step: 260, loss is 0.45471063256263733\n",
      "epoch: 2 step: 261, loss is 0.27415162324905396\n",
      "epoch: 2 step: 262, loss is 0.3684888184070587\n",
      "epoch: 2 step: 263, loss is 0.17579390108585358\n",
      "epoch: 2 step: 264, loss is 0.24585303664207458\n",
      "epoch: 2 step: 265, loss is 0.44401848316192627\n",
      "epoch: 2 step: 266, loss is 0.2544858455657959\n",
      "epoch: 2 step: 267, loss is 0.3256252110004425\n",
      "epoch: 2 step: 268, loss is 0.22292445600032806\n",
      "epoch: 2 step: 269, loss is 0.39207252860069275\n",
      "epoch: 2 step: 270, loss is 0.22108757495880127\n",
      "epoch: 2 step: 271, loss is 0.3194328844547272\n",
      "epoch: 2 step: 272, loss is 0.18279901146888733\n",
      "epoch: 2 step: 273, loss is 0.2534932792186737\n",
      "epoch: 2 step: 274, loss is 0.45749562978744507\n",
      "epoch: 2 step: 275, loss is 0.3105979859828949\n",
      "epoch: 2 step: 276, loss is 0.33166104555130005\n",
      "epoch: 2 step: 277, loss is 0.38084855675697327\n",
      "epoch: 2 step: 278, loss is 0.31483566761016846\n",
      "epoch: 2 step: 279, loss is 0.3344042897224426\n",
      "epoch: 2 step: 280, loss is 0.2918831706047058\n",
      "epoch: 2 step: 281, loss is 0.35426628589630127\n",
      "epoch: 2 step: 282, loss is 0.3101661801338196\n",
      "epoch: 2 step: 283, loss is 0.263300359249115\n",
      "epoch: 2 step: 284, loss is 0.3275966942310333\n",
      "epoch: 2 step: 285, loss is 0.2487623691558838\n",
      "epoch: 2 step: 286, loss is 0.24593977630138397\n",
      "epoch: 2 step: 287, loss is 0.416482537984848\n",
      "epoch: 2 step: 288, loss is 0.28345364332199097\n",
      "epoch: 2 step: 289, loss is 0.48652923107147217\n",
      "epoch: 2 step: 290, loss is 0.2880997657775879\n",
      "epoch: 2 step: 291, loss is 0.20250366628170013\n",
      "epoch: 2 step: 292, loss is 0.22525924444198608\n",
      "epoch: 2 step: 293, loss is 0.21032817661762238\n",
      "epoch: 2 step: 294, loss is 0.2818666696548462\n",
      "epoch: 2 step: 295, loss is 0.27994444966316223\n",
      "epoch: 2 step: 296, loss is 0.31431645154953003\n",
      "epoch: 2 step: 297, loss is 0.31674736738204956\n",
      "epoch: 2 step: 298, loss is 0.32315120100975037\n",
      "epoch: 2 step: 299, loss is 0.220052108168602\n",
      "epoch: 2 step: 300, loss is 0.21013149619102478\n",
      "epoch: 2 step: 301, loss is 0.5065019130706787\n",
      "epoch: 2 step: 302, loss is 0.3374117612838745\n",
      "epoch: 2 step: 303, loss is 0.67741858959198\n",
      "epoch: 2 step: 304, loss is 0.34601515531539917\n",
      "epoch: 2 step: 305, loss is 0.40709978342056274\n",
      "epoch: 2 step: 306, loss is 0.3060119152069092\n",
      "epoch: 2 step: 307, loss is 0.3674473762512207\n",
      "epoch: 2 step: 308, loss is 0.27847641706466675\n",
      "epoch: 2 step: 309, loss is 0.4211486279964447\n",
      "epoch: 2 step: 310, loss is 0.3812185227870941\n",
      "epoch: 2 step: 311, loss is 0.3623531460762024\n",
      "epoch: 2 step: 312, loss is 0.2878521680831909\n",
      "epoch: 2 step: 313, loss is 0.4198022186756134\n",
      "epoch: 2 step: 314, loss is 0.2769908607006073\n",
      "epoch: 2 step: 315, loss is 0.36270594596862793\n",
      "epoch: 2 step: 316, loss is 0.3572537899017334\n",
      "epoch: 2 step: 317, loss is 0.3774119019508362\n",
      "epoch: 2 step: 318, loss is 0.2580866515636444\n",
      "epoch: 2 step: 319, loss is 0.3868962228298187\n",
      "epoch: 2 step: 320, loss is 0.25693801045417786\n",
      "epoch: 2 step: 321, loss is 0.3250586986541748\n",
      "epoch: 2 step: 322, loss is 0.24699366092681885\n",
      "epoch: 2 step: 323, loss is 0.292854368686676\n",
      "epoch: 2 step: 324, loss is 0.30212947726249695\n",
      "epoch: 2 step: 325, loss is 0.5431991219520569\n",
      "epoch: 2 step: 326, loss is 0.27389299869537354\n",
      "epoch: 2 step: 327, loss is 0.27299925684928894\n",
      "epoch: 2 step: 328, loss is 0.29654809832572937\n",
      "epoch: 2 step: 329, loss is 0.3328728973865509\n",
      "epoch: 2 step: 330, loss is 0.47393420338630676\n",
      "epoch: 2 step: 331, loss is 0.3745901882648468\n",
      "epoch: 2 step: 332, loss is 0.31009921431541443\n",
      "epoch: 2 step: 333, loss is 0.2293127328157425\n",
      "epoch: 2 step: 334, loss is 0.2871166169643402\n",
      "epoch: 2 step: 335, loss is 0.3582143485546112\n",
      "epoch: 2 step: 336, loss is 0.31636351346969604\n",
      "epoch: 2 step: 337, loss is 0.37683627009391785\n",
      "epoch: 2 step: 338, loss is 0.22074155509471893\n",
      "epoch: 2 step: 339, loss is 0.2508001923561096\n",
      "epoch: 2 step: 340, loss is 0.336247980594635\n",
      "epoch: 2 step: 341, loss is 0.22015608847141266\n",
      "epoch: 2 step: 342, loss is 0.21520383656024933\n",
      "epoch: 2 step: 343, loss is 0.3041328489780426\n",
      "epoch: 2 step: 344, loss is 0.3775469958782196\n",
      "epoch: 2 step: 345, loss is 0.25479868054389954\n",
      "epoch: 2 step: 346, loss is 0.24031679332256317\n",
      "epoch: 2 step: 347, loss is 0.36260226368904114\n",
      "epoch: 2 step: 348, loss is 0.3781821131706238\n",
      "epoch: 2 step: 349, loss is 0.3326810598373413\n",
      "epoch: 2 step: 350, loss is 0.48097172379493713\n",
      "epoch: 2 step: 351, loss is 0.2648141086101532\n",
      "epoch: 2 step: 352, loss is 0.3468637466430664\n",
      "epoch: 2 step: 353, loss is 0.3552652597427368\n",
      "epoch: 2 step: 354, loss is 0.2791120111942291\n",
      "epoch: 2 step: 355, loss is 0.31219029426574707\n",
      "epoch: 2 step: 356, loss is 0.34874507784843445\n",
      "epoch: 2 step: 357, loss is 0.3690823018550873\n",
      "epoch: 2 step: 358, loss is 0.24755731225013733\n",
      "epoch: 2 step: 359, loss is 0.29649797081947327\n",
      "epoch: 2 step: 360, loss is 0.21999651193618774\n",
      "epoch: 2 step: 361, loss is 0.39472493529319763\n",
      "epoch: 2 step: 362, loss is 0.41631823778152466\n",
      "epoch: 2 step: 363, loss is 0.1712004542350769\n",
      "epoch: 2 step: 364, loss is 0.23260092735290527\n",
      "epoch: 2 step: 365, loss is 0.2479761391878128\n",
      "epoch: 2 step: 366, loss is 0.29890990257263184\n",
      "epoch: 2 step: 367, loss is 0.3495821952819824\n",
      "epoch: 2 step: 368, loss is 0.376933217048645\n",
      "epoch: 2 step: 369, loss is 0.23604638874530792\n",
      "epoch: 2 step: 370, loss is 0.22746886312961578\n",
      "epoch: 2 step: 371, loss is 0.388441264629364\n",
      "epoch: 2 step: 372, loss is 0.29559141397476196\n",
      "epoch: 2 step: 373, loss is 0.2615773379802704\n",
      "epoch: 2 step: 374, loss is 0.25366973876953125\n",
      "epoch: 2 step: 375, loss is 0.3130320608615875\n",
      "epoch: 2 step: 376, loss is 0.2994921803474426\n",
      "epoch: 2 step: 377, loss is 0.24082741141319275\n",
      "epoch: 2 step: 378, loss is 0.2818760275840759\n",
      "epoch: 2 step: 379, loss is 0.31702178716659546\n",
      "epoch: 2 step: 380, loss is 0.3354407250881195\n",
      "epoch: 2 step: 381, loss is 0.2584853172302246\n",
      "epoch: 2 step: 382, loss is 0.3382874131202698\n",
      "epoch: 2 step: 383, loss is 0.3555620610713959\n",
      "epoch: 2 step: 384, loss is 0.19931790232658386\n",
      "epoch: 2 step: 385, loss is 0.2800264358520508\n",
      "epoch: 2 step: 386, loss is 0.31838327646255493\n",
      "epoch: 2 step: 387, loss is 0.49206435680389404\n",
      "epoch: 2 step: 388, loss is 0.2402195781469345\n",
      "epoch: 2 step: 389, loss is 0.2789659798145294\n",
      "epoch: 2 step: 390, loss is 0.4640454947948456\n",
      "epoch: 2 step: 391, loss is 0.23752054572105408\n",
      "epoch: 2 step: 392, loss is 0.26806360483169556\n",
      "epoch: 2 step: 393, loss is 0.39242514967918396\n",
      "epoch: 2 step: 394, loss is 0.4209350645542145\n",
      "epoch: 2 step: 395, loss is 0.3437643051147461\n",
      "epoch: 2 step: 396, loss is 0.34353023767471313\n",
      "epoch: 2 step: 397, loss is 0.33669400215148926\n",
      "epoch: 2 step: 398, loss is 0.3106076121330261\n",
      "epoch: 2 step: 399, loss is 0.37103235721588135\n",
      "epoch: 2 step: 400, loss is 0.32333067059516907\n",
      "epoch: 2 step: 401, loss is 0.40128809213638306\n",
      "epoch: 2 step: 402, loss is 0.17351506650447845\n",
      "epoch: 2 step: 403, loss is 0.5763480067253113\n",
      "epoch: 2 step: 404, loss is 0.3409564197063446\n",
      "epoch: 2 step: 405, loss is 0.256806343793869\n",
      "epoch: 2 step: 406, loss is 0.3539201617240906\n",
      "epoch: 2 step: 407, loss is 0.2943247854709625\n",
      "epoch: 2 step: 408, loss is 0.5531960725784302\n",
      "epoch: 2 step: 409, loss is 0.5682151913642883\n",
      "epoch: 2 step: 410, loss is 0.16381345689296722\n",
      "epoch: 2 step: 411, loss is 0.2580864727497101\n",
      "epoch: 2 step: 412, loss is 0.3012869954109192\n",
      "epoch: 2 step: 413, loss is 0.20975807309150696\n",
      "epoch: 2 step: 414, loss is 0.262524276971817\n",
      "epoch: 2 step: 415, loss is 0.3807583153247833\n",
      "epoch: 2 step: 416, loss is 0.25950679183006287\n",
      "epoch: 2 step: 417, loss is 0.3886229693889618\n",
      "epoch: 2 step: 418, loss is 0.45613253116607666\n",
      "epoch: 2 step: 419, loss is 0.2463158667087555\n",
      "epoch: 2 step: 420, loss is 0.3890288770198822\n",
      "epoch: 2 step: 421, loss is 0.4773622453212738\n",
      "epoch: 2 step: 422, loss is 0.37382346391677856\n",
      "epoch: 2 step: 423, loss is 0.2497076690196991\n",
      "epoch: 2 step: 424, loss is 0.39997220039367676\n",
      "epoch: 2 step: 425, loss is 0.3023512363433838\n",
      "epoch: 2 step: 426, loss is 0.23818302154541016\n",
      "epoch: 2 step: 427, loss is 0.17536787688732147\n",
      "epoch: 2 step: 428, loss is 0.2636417746543884\n",
      "epoch: 2 step: 429, loss is 0.3898410201072693\n",
      "epoch: 2 step: 430, loss is 0.19112122058868408\n",
      "epoch: 2 step: 431, loss is 0.22992296516895294\n",
      "epoch: 2 step: 432, loss is 0.2851634919643402\n",
      "epoch: 2 step: 433, loss is 0.3792709410190582\n",
      "epoch: 2 step: 434, loss is 0.4091072082519531\n",
      "epoch: 2 step: 435, loss is 0.32237449288368225\n",
      "epoch: 2 step: 436, loss is 0.35593074560165405\n",
      "epoch: 2 step: 437, loss is 0.22413472831249237\n",
      "epoch: 2 step: 438, loss is 0.33826935291290283\n",
      "epoch: 2 step: 439, loss is 0.3594040274620056\n",
      "epoch: 2 step: 440, loss is 0.48267337679862976\n",
      "epoch: 2 step: 441, loss is 0.35052570700645447\n",
      "epoch: 2 step: 442, loss is 0.19784216582775116\n",
      "epoch: 2 step: 443, loss is 0.2510249614715576\n",
      "epoch: 2 step: 444, loss is 0.22371703386306763\n",
      "epoch: 2 step: 445, loss is 0.2980928421020508\n",
      "epoch: 2 step: 446, loss is 0.30675193667411804\n",
      "epoch: 2 step: 447, loss is 0.3272990584373474\n",
      "epoch: 2 step: 448, loss is 0.25869888067245483\n",
      "epoch: 2 step: 449, loss is 0.40200918912887573\n",
      "epoch: 2 step: 450, loss is 0.2592327892780304\n",
      "epoch: 2 step: 451, loss is 0.49744880199432373\n",
      "epoch: 2 step: 452, loss is 0.41561177372932434\n",
      "epoch: 2 step: 453, loss is 0.4907275140285492\n",
      "epoch: 2 step: 454, loss is 0.2733217179775238\n",
      "epoch: 2 step: 455, loss is 0.5602880120277405\n",
      "epoch: 2 step: 456, loss is 0.3743476867675781\n",
      "epoch: 2 step: 457, loss is 0.3987371623516083\n",
      "epoch: 2 step: 458, loss is 0.3550818860530853\n",
      "epoch: 2 step: 459, loss is 0.29415032267570496\n",
      "epoch: 2 step: 460, loss is 0.4005252420902252\n",
      "epoch: 2 step: 461, loss is 0.23475942015647888\n",
      "epoch: 2 step: 462, loss is 0.45830509066581726\n",
      "epoch: 2 step: 463, loss is 0.2777658700942993\n",
      "epoch: 2 step: 464, loss is 0.3848600387573242\n",
      "epoch: 2 step: 465, loss is 0.11256130039691925\n",
      "epoch: 2 step: 466, loss is 0.34154435992240906\n",
      "epoch: 2 step: 467, loss is 0.509719967842102\n",
      "epoch: 2 step: 468, loss is 0.48122939467430115\n",
      "epoch: 2 step: 469, loss is 0.338623583316803\n",
      "epoch: 2 step: 470, loss is 0.222122460603714\n",
      "epoch: 2 step: 471, loss is 0.3151107728481293\n",
      "epoch: 2 step: 472, loss is 0.37236902117729187\n",
      "epoch: 2 step: 473, loss is 0.3995445668697357\n",
      "epoch: 2 step: 474, loss is 0.5089864134788513\n",
      "epoch: 2 step: 475, loss is 0.3278183341026306\n",
      "epoch: 2 step: 476, loss is 0.24906036257743835\n",
      "epoch: 2 step: 477, loss is 0.2958422601222992\n",
      "epoch: 2 step: 478, loss is 0.22164219617843628\n",
      "epoch: 2 step: 479, loss is 0.3465377390384674\n",
      "epoch: 2 step: 480, loss is 0.4178483188152313\n",
      "epoch: 2 step: 481, loss is 0.18804901838302612\n",
      "epoch: 2 step: 482, loss is 0.22784514725208282\n",
      "epoch: 2 step: 483, loss is 0.2090875655412674\n",
      "epoch: 2 step: 484, loss is 0.5909609794616699\n",
      "epoch: 2 step: 485, loss is 0.22637619078159332\n",
      "epoch: 2 step: 486, loss is 0.2610776424407959\n",
      "epoch: 2 step: 487, loss is 0.27496153116226196\n",
      "epoch: 2 step: 488, loss is 0.32662060856819153\n",
      "epoch: 2 step: 489, loss is 0.2703942656517029\n",
      "epoch: 2 step: 490, loss is 0.18482807278633118\n",
      "epoch: 2 step: 491, loss is 0.38563209772109985\n",
      "epoch: 2 step: 492, loss is 0.3284395635128021\n",
      "epoch: 2 step: 493, loss is 0.4024812579154968\n",
      "epoch: 2 step: 494, loss is 0.16878217458724976\n",
      "epoch: 2 step: 495, loss is 0.3672294616699219\n",
      "epoch: 2 step: 496, loss is 0.2157350480556488\n",
      "epoch: 2 step: 497, loss is 0.250064879655838\n",
      "epoch: 2 step: 498, loss is 0.41625145077705383\n",
      "epoch: 2 step: 499, loss is 0.30231496691703796\n",
      "epoch: 2 step: 500, loss is 0.32909032702445984\n",
      "epoch: 2 step: 501, loss is 0.6369645595550537\n",
      "epoch: 2 step: 502, loss is 0.19763033092021942\n",
      "epoch: 2 step: 503, loss is 0.3456305265426636\n",
      "epoch: 2 step: 504, loss is 0.3402675688266754\n",
      "epoch: 2 step: 505, loss is 0.3972555994987488\n",
      "epoch: 2 step: 506, loss is 0.2617044746875763\n",
      "epoch: 2 step: 507, loss is 0.30825483798980713\n",
      "epoch: 2 step: 508, loss is 0.2857903838157654\n",
      "epoch: 2 step: 509, loss is 0.26624444127082825\n",
      "epoch: 2 step: 510, loss is 0.23053227365016937\n",
      "epoch: 2 step: 511, loss is 0.31564778089523315\n",
      "epoch: 2 step: 512, loss is 0.2742791771888733\n",
      "epoch: 2 step: 513, loss is 0.2716120481491089\n",
      "epoch: 2 step: 514, loss is 0.262160062789917\n",
      "epoch: 2 step: 515, loss is 0.2584690451622009\n",
      "epoch: 2 step: 516, loss is 0.47710245847702026\n",
      "epoch: 2 step: 517, loss is 0.4241499602794647\n",
      "epoch: 2 step: 518, loss is 0.180218905210495\n",
      "epoch: 2 step: 519, loss is 0.4763129949569702\n",
      "epoch: 2 step: 520, loss is 0.15258300304412842\n",
      "epoch: 2 step: 521, loss is 0.4043464660644531\n",
      "epoch: 2 step: 522, loss is 0.2833772897720337\n",
      "epoch: 2 step: 523, loss is 0.2620907425880432\n",
      "epoch: 2 step: 524, loss is 0.28834807872772217\n",
      "epoch: 2 step: 525, loss is 0.421871542930603\n",
      "epoch: 2 step: 526, loss is 0.4823441505432129\n",
      "epoch: 2 step: 527, loss is 0.28613168001174927\n",
      "epoch: 2 step: 528, loss is 0.34810569882392883\n",
      "epoch: 2 step: 529, loss is 0.163442462682724\n",
      "epoch: 2 step: 530, loss is 0.3604174852371216\n",
      "epoch: 2 step: 531, loss is 0.44054901599884033\n",
      "epoch: 2 step: 532, loss is 0.2642267942428589\n",
      "epoch: 2 step: 533, loss is 0.3091657757759094\n",
      "epoch: 2 step: 534, loss is 0.20146402716636658\n",
      "epoch: 2 step: 535, loss is 0.2411634922027588\n",
      "epoch: 2 step: 536, loss is 0.3387421667575836\n",
      "epoch: 2 step: 537, loss is 0.2616690695285797\n",
      "epoch: 2 step: 538, loss is 0.32086533308029175\n",
      "epoch: 2 step: 539, loss is 0.255759060382843\n",
      "epoch: 2 step: 540, loss is 0.20453305542469025\n",
      "epoch: 2 step: 541, loss is 0.1482662558555603\n",
      "epoch: 2 step: 542, loss is 0.3814440667629242\n",
      "epoch: 2 step: 543, loss is 0.2566591799259186\n",
      "epoch: 2 step: 544, loss is 0.28392088413238525\n",
      "epoch: 2 step: 545, loss is 0.2668508291244507\n",
      "epoch: 2 step: 546, loss is 0.49620190262794495\n",
      "epoch: 2 step: 547, loss is 0.40404096245765686\n",
      "epoch: 2 step: 548, loss is 0.21180258691310883\n",
      "epoch: 2 step: 549, loss is 0.18329907953739166\n",
      "epoch: 2 step: 550, loss is 0.4723176956176758\n",
      "epoch: 2 step: 551, loss is 0.2612069845199585\n",
      "epoch: 2 step: 552, loss is 0.20389209687709808\n",
      "epoch: 2 step: 553, loss is 0.3281664252281189\n",
      "epoch: 2 step: 554, loss is 0.2134903520345688\n",
      "epoch: 2 step: 555, loss is 0.20381832122802734\n",
      "epoch: 2 step: 556, loss is 0.23264923691749573\n",
      "epoch: 2 step: 557, loss is 0.31678444147109985\n",
      "epoch: 2 step: 558, loss is 0.4161551892757416\n",
      "epoch: 2 step: 559, loss is 0.4101574718952179\n",
      "epoch: 2 step: 560, loss is 0.13311055302619934\n",
      "epoch: 2 step: 561, loss is 0.2623377740383148\n",
      "epoch: 2 step: 562, loss is 0.2477315217256546\n",
      "epoch: 2 step: 563, loss is 0.2056092470884323\n",
      "epoch: 2 step: 564, loss is 0.27040350437164307\n",
      "epoch: 2 step: 565, loss is 0.4912863075733185\n",
      "epoch: 2 step: 566, loss is 0.22443820536136627\n",
      "epoch: 2 step: 567, loss is 0.2905506491661072\n",
      "epoch: 2 step: 568, loss is 0.17877884209156036\n",
      "epoch: 2 step: 569, loss is 0.20416274666786194\n",
      "epoch: 2 step: 570, loss is 0.2723875343799591\n",
      "epoch: 2 step: 571, loss is 0.3341915011405945\n",
      "epoch: 2 step: 572, loss is 0.29845789074897766\n",
      "epoch: 2 step: 573, loss is 0.34439021348953247\n",
      "epoch: 2 step: 574, loss is 0.19966991245746613\n",
      "epoch: 2 step: 575, loss is 0.375615656375885\n",
      "epoch: 2 step: 576, loss is 0.27313652634620667\n",
      "epoch: 2 step: 577, loss is 0.14874830842018127\n",
      "epoch: 2 step: 578, loss is 0.22066067159175873\n",
      "epoch: 2 step: 579, loss is 0.38239991664886475\n",
      "epoch: 2 step: 580, loss is 0.269685834646225\n",
      "epoch: 2 step: 581, loss is 0.21007119119167328\n",
      "epoch: 2 step: 582, loss is 0.3909192979335785\n",
      "epoch: 2 step: 583, loss is 0.27087846398353577\n",
      "epoch: 2 step: 584, loss is 0.24587996304035187\n",
      "epoch: 2 step: 585, loss is 0.25465476512908936\n",
      "epoch: 2 step: 586, loss is 0.3575129806995392\n",
      "epoch: 2 step: 587, loss is 0.367443710565567\n",
      "epoch: 2 step: 588, loss is 0.2928083837032318\n",
      "epoch: 2 step: 589, loss is 0.16595807671546936\n",
      "epoch: 2 step: 590, loss is 0.3256993591785431\n",
      "epoch: 2 step: 591, loss is 0.30718833208084106\n",
      "epoch: 2 step: 592, loss is 0.3560540974140167\n",
      "epoch: 2 step: 593, loss is 0.20212341845035553\n",
      "epoch: 2 step: 594, loss is 0.17810213565826416\n",
      "epoch: 2 step: 595, loss is 0.35101062059402466\n",
      "epoch: 2 step: 596, loss is 0.42582055926322937\n",
      "epoch: 2 step: 597, loss is 0.26647087931632996\n",
      "epoch: 2 step: 598, loss is 0.37379908561706543\n",
      "epoch: 2 step: 599, loss is 0.418817400932312\n",
      "epoch: 2 step: 600, loss is 0.3249620199203491\n",
      "epoch: 2 step: 601, loss is 0.30195754766464233\n",
      "epoch: 2 step: 602, loss is 0.2385927140712738\n",
      "epoch: 2 step: 603, loss is 0.22673837840557098\n",
      "epoch: 2 step: 604, loss is 0.2587805986404419\n",
      "epoch: 2 step: 605, loss is 0.45340242981910706\n",
      "epoch: 2 step: 606, loss is 0.2817303240299225\n",
      "epoch: 2 step: 607, loss is 0.24682796001434326\n",
      "epoch: 2 step: 608, loss is 0.3820814788341522\n",
      "epoch: 2 step: 609, loss is 0.47673463821411133\n",
      "epoch: 2 step: 610, loss is 0.2710943818092346\n",
      "epoch: 2 step: 611, loss is 0.3697999119758606\n",
      "epoch: 2 step: 612, loss is 0.22618499398231506\n",
      "epoch: 2 step: 613, loss is 0.35459965467453003\n",
      "epoch: 2 step: 614, loss is 0.31800639629364014\n",
      "epoch: 2 step: 615, loss is 0.3866700232028961\n",
      "epoch: 2 step: 616, loss is 0.5242594480514526\n",
      "epoch: 2 step: 617, loss is 0.3106689751148224\n",
      "epoch: 2 step: 618, loss is 0.21534591913223267\n",
      "epoch: 2 step: 619, loss is 0.28424039483070374\n",
      "epoch: 2 step: 620, loss is 0.25497832894325256\n",
      "epoch: 2 step: 621, loss is 0.40688225626945496\n",
      "epoch: 2 step: 622, loss is 0.2975396513938904\n",
      "epoch: 2 step: 623, loss is 0.2957650423049927\n",
      "epoch: 2 step: 624, loss is 0.35092630982398987\n",
      "epoch: 2 step: 625, loss is 0.3063878118991852\n",
      "epoch: 2 step: 626, loss is 0.2675362825393677\n",
      "epoch: 2 step: 627, loss is 0.2430919110774994\n",
      "epoch: 2 step: 628, loss is 0.4808492660522461\n",
      "epoch: 2 step: 629, loss is 0.1981653869152069\n",
      "epoch: 2 step: 630, loss is 0.45498165488243103\n",
      "epoch: 2 step: 631, loss is 0.43556714057922363\n",
      "epoch: 2 step: 632, loss is 0.39115116000175476\n",
      "epoch: 2 step: 633, loss is 0.24096383154392242\n",
      "epoch: 2 step: 634, loss is 0.28619301319122314\n",
      "epoch: 2 step: 635, loss is 0.2013155072927475\n",
      "epoch: 2 step: 636, loss is 0.2992214858531952\n",
      "epoch: 2 step: 637, loss is 0.21134911477565765\n",
      "epoch: 2 step: 638, loss is 0.27330031991004944\n",
      "epoch: 2 step: 639, loss is 0.20215760171413422\n",
      "epoch: 2 step: 640, loss is 0.21919208765029907\n",
      "epoch: 2 step: 641, loss is 0.18662816286087036\n",
      "epoch: 2 step: 642, loss is 0.32194218039512634\n",
      "epoch: 2 step: 643, loss is 0.1735118180513382\n",
      "epoch: 2 step: 644, loss is 0.22403623163700104\n",
      "epoch: 2 step: 645, loss is 0.3160819113254547\n",
      "epoch: 2 step: 646, loss is 0.32082048058509827\n",
      "epoch: 2 step: 647, loss is 0.20466536283493042\n",
      "epoch: 2 step: 648, loss is 0.16808630526065826\n",
      "epoch: 2 step: 649, loss is 0.27379390597343445\n",
      "epoch: 2 step: 650, loss is 0.17013771831989288\n",
      "epoch: 2 step: 651, loss is 0.2692437171936035\n",
      "epoch: 2 step: 652, loss is 0.2490314394235611\n",
      "epoch: 2 step: 653, loss is 0.2242739349603653\n",
      "epoch: 2 step: 654, loss is 0.23519957065582275\n",
      "epoch: 2 step: 655, loss is 0.2777540385723114\n",
      "epoch: 2 step: 656, loss is 0.2342839390039444\n",
      "epoch: 2 step: 657, loss is 0.2825871706008911\n",
      "epoch: 2 step: 658, loss is 0.2955346405506134\n",
      "epoch: 2 step: 659, loss is 0.39571020007133484\n",
      "epoch: 2 step: 660, loss is 0.18693913519382477\n",
      "epoch: 2 step: 661, loss is 0.13830812275409698\n",
      "epoch: 2 step: 662, loss is 0.14562951028347015\n",
      "epoch: 2 step: 663, loss is 0.19191895425319672\n",
      "epoch: 2 step: 664, loss is 0.3187105655670166\n",
      "epoch: 2 step: 665, loss is 0.3192983567714691\n",
      "epoch: 2 step: 666, loss is 0.3633091151714325\n",
      "epoch: 2 step: 667, loss is 0.4625718593597412\n",
      "epoch: 2 step: 668, loss is 0.293662965297699\n",
      "epoch: 2 step: 669, loss is 0.32421037554740906\n",
      "epoch: 2 step: 670, loss is 0.34615781903266907\n",
      "epoch: 2 step: 671, loss is 0.2664259672164917\n",
      "epoch: 2 step: 672, loss is 0.15563571453094482\n",
      "epoch: 2 step: 673, loss is 0.35538309812545776\n",
      "epoch: 2 step: 674, loss is 0.20425327122211456\n",
      "epoch: 2 step: 675, loss is 0.3317984342575073\n",
      "epoch: 2 step: 676, loss is 0.19005712866783142\n",
      "epoch: 2 step: 677, loss is 0.2742476165294647\n",
      "epoch: 2 step: 678, loss is 0.358364999294281\n",
      "epoch: 2 step: 679, loss is 0.23822824656963348\n",
      "epoch: 2 step: 680, loss is 0.29387620091438293\n",
      "epoch: 2 step: 681, loss is 0.26273879408836365\n",
      "epoch: 2 step: 682, loss is 0.339891642332077\n",
      "epoch: 2 step: 683, loss is 0.3223872184753418\n",
      "epoch: 2 step: 684, loss is 0.19469964504241943\n",
      "epoch: 2 step: 685, loss is 0.2543642520904541\n",
      "epoch: 2 step: 686, loss is 0.2940148115158081\n",
      "epoch: 2 step: 687, loss is 0.2279406487941742\n",
      "epoch: 2 step: 688, loss is 0.4323248267173767\n",
      "epoch: 2 step: 689, loss is 0.43554601073265076\n",
      "epoch: 2 step: 690, loss is 0.216389462351799\n",
      "epoch: 2 step: 691, loss is 0.28069552779197693\n",
      "epoch: 2 step: 692, loss is 0.27965039014816284\n",
      "epoch: 2 step: 693, loss is 0.40735816955566406\n",
      "epoch: 2 step: 694, loss is 0.2862670421600342\n",
      "epoch: 2 step: 695, loss is 0.24143271148204803\n",
      "epoch: 2 step: 696, loss is 0.4830823540687561\n",
      "epoch: 2 step: 697, loss is 0.2536996603012085\n",
      "epoch: 2 step: 698, loss is 0.28405889868736267\n",
      "epoch: 2 step: 699, loss is 0.21813170611858368\n",
      "epoch: 2 step: 700, loss is 0.31182798743247986\n",
      "epoch: 2 step: 701, loss is 0.26840901374816895\n",
      "epoch: 2 step: 702, loss is 0.1776997596025467\n",
      "epoch: 2 step: 703, loss is 0.19351376593112946\n",
      "epoch: 2 step: 704, loss is 0.4284292459487915\n",
      "epoch: 2 step: 705, loss is 0.36250266432762146\n",
      "epoch: 2 step: 706, loss is 0.10160696506500244\n",
      "epoch: 2 step: 707, loss is 0.14810948073863983\n",
      "epoch: 2 step: 708, loss is 0.3767329454421997\n",
      "epoch: 2 step: 709, loss is 0.17063169181346893\n",
      "epoch: 2 step: 710, loss is 0.14118151366710663\n",
      "epoch: 2 step: 711, loss is 0.43164435029029846\n",
      "epoch: 2 step: 712, loss is 0.3400594890117645\n",
      "epoch: 2 step: 713, loss is 0.33279910683631897\n",
      "epoch: 2 step: 714, loss is 0.3769686222076416\n",
      "epoch: 2 step: 715, loss is 0.48267632722854614\n",
      "epoch: 2 step: 716, loss is 0.25093039870262146\n",
      "epoch: 2 step: 717, loss is 0.3172367513179779\n",
      "epoch: 2 step: 718, loss is 0.17115932703018188\n",
      "epoch: 2 step: 719, loss is 0.17530067265033722\n",
      "epoch: 2 step: 720, loss is 0.21565033495426178\n",
      "epoch: 2 step: 721, loss is 0.3825037181377411\n",
      "epoch: 2 step: 722, loss is 0.3759347200393677\n",
      "epoch: 2 step: 723, loss is 0.3550158143043518\n",
      "epoch: 2 step: 724, loss is 0.37466591596603394\n",
      "epoch: 2 step: 725, loss is 0.3958040177822113\n",
      "epoch: 2 step: 726, loss is 0.34192290902137756\n",
      "epoch: 2 step: 727, loss is 0.2606973350048065\n",
      "epoch: 2 step: 728, loss is 0.4072871506214142\n",
      "epoch: 2 step: 729, loss is 0.27004092931747437\n",
      "epoch: 2 step: 730, loss is 0.3339354395866394\n",
      "epoch: 2 step: 731, loss is 0.23178443312644958\n",
      "epoch: 2 step: 732, loss is 0.4020608365535736\n",
      "epoch: 2 step: 733, loss is 0.2749650478363037\n",
      "epoch: 2 step: 734, loss is 0.3175338804721832\n",
      "epoch: 2 step: 735, loss is 0.4590086340904236\n",
      "epoch: 2 step: 736, loss is 0.22707600891590118\n",
      "epoch: 2 step: 737, loss is 0.22415561974048615\n",
      "epoch: 2 step: 738, loss is 0.24145688116550446\n",
      "epoch: 2 step: 739, loss is 0.2922561764717102\n",
      "epoch: 2 step: 740, loss is 0.3841639757156372\n",
      "epoch: 2 step: 741, loss is 0.1708972156047821\n",
      "epoch: 2 step: 742, loss is 0.2898024916648865\n",
      "epoch: 2 step: 743, loss is 0.19344739615917206\n",
      "epoch: 2 step: 744, loss is 0.38241469860076904\n",
      "epoch: 2 step: 745, loss is 0.202286034822464\n",
      "epoch: 2 step: 746, loss is 0.22251689434051514\n",
      "epoch: 2 step: 747, loss is 0.2613316476345062\n",
      "epoch: 2 step: 748, loss is 0.2800692319869995\n",
      "epoch: 2 step: 749, loss is 0.23742543160915375\n",
      "epoch: 2 step: 750, loss is 0.17058658599853516\n",
      "epoch: 2 step: 751, loss is 0.13527540862560272\n",
      "epoch: 2 step: 752, loss is 0.3076106607913971\n",
      "epoch: 2 step: 753, loss is 0.23842450976371765\n",
      "epoch: 2 step: 754, loss is 0.2200728952884674\n",
      "epoch: 2 step: 755, loss is 0.30915626883506775\n",
      "epoch: 2 step: 756, loss is 0.14271260797977448\n",
      "epoch: 2 step: 757, loss is 0.3894234001636505\n",
      "epoch: 2 step: 758, loss is 0.18139317631721497\n",
      "epoch: 2 step: 759, loss is 0.45052993297576904\n",
      "epoch: 2 step: 760, loss is 0.40382659435272217\n",
      "epoch: 2 step: 761, loss is 0.23988372087478638\n",
      "epoch: 2 step: 762, loss is 0.2531198263168335\n",
      "epoch: 2 step: 763, loss is 0.604117214679718\n",
      "epoch: 2 step: 764, loss is 0.3369276821613312\n",
      "epoch: 2 step: 765, loss is 0.20325042307376862\n",
      "epoch: 2 step: 766, loss is 0.21813368797302246\n",
      "epoch: 2 step: 767, loss is 0.48845523595809937\n",
      "epoch: 2 step: 768, loss is 0.28067806363105774\n",
      "epoch: 2 step: 769, loss is 0.29680678248405457\n",
      "epoch: 2 step: 770, loss is 0.2521570026874542\n",
      "epoch: 2 step: 771, loss is 0.31545108556747437\n",
      "epoch: 2 step: 772, loss is 0.3265804648399353\n",
      "epoch: 2 step: 773, loss is 0.3676358759403229\n",
      "epoch: 2 step: 774, loss is 0.179914191365242\n",
      "epoch: 2 step: 775, loss is 0.23298141360282898\n",
      "epoch: 2 step: 776, loss is 0.22666476666927338\n",
      "epoch: 2 step: 777, loss is 0.23881161212921143\n",
      "epoch: 2 step: 778, loss is 0.22499090433120728\n",
      "epoch: 2 step: 779, loss is 0.3858725428581238\n",
      "epoch: 2 step: 780, loss is 0.25054603815078735\n",
      "epoch: 2 step: 781, loss is 0.3001693785190582\n",
      "epoch: 2 step: 782, loss is 0.16633279621601105\n",
      "epoch: 2 step: 783, loss is 0.3193833827972412\n",
      "epoch: 2 step: 784, loss is 0.2928517460823059\n",
      "epoch: 2 step: 785, loss is 0.22288931906223297\n",
      "epoch: 2 step: 786, loss is 0.2673793137073517\n",
      "epoch: 2 step: 787, loss is 0.15897850692272186\n",
      "epoch: 2 step: 788, loss is 0.3194126486778259\n",
      "epoch: 2 step: 789, loss is 0.5957136154174805\n",
      "epoch: 2 step: 790, loss is 0.37021172046661377\n",
      "epoch: 2 step: 791, loss is 0.436971515417099\n",
      "epoch: 2 step: 792, loss is 0.47872617840766907\n",
      "epoch: 2 step: 793, loss is 0.2987414300441742\n",
      "epoch: 2 step: 794, loss is 0.180779829621315\n",
      "epoch: 2 step: 795, loss is 0.2604604959487915\n",
      "epoch: 2 step: 796, loss is 0.2723771929740906\n",
      "epoch: 2 step: 797, loss is 0.3493763208389282\n",
      "epoch: 2 step: 798, loss is 0.16767670214176178\n",
      "epoch: 2 step: 799, loss is 0.3735083043575287\n",
      "epoch: 2 step: 800, loss is 0.3159273862838745\n",
      "epoch: 2 step: 801, loss is 0.26405996084213257\n",
      "epoch: 2 step: 802, loss is 0.32693904638290405\n",
      "epoch: 2 step: 803, loss is 0.25348034501075745\n",
      "epoch: 2 step: 804, loss is 0.2605207562446594\n",
      "epoch: 2 step: 805, loss is 0.23897095024585724\n",
      "epoch: 2 step: 806, loss is 0.46020522713661194\n",
      "epoch: 2 step: 807, loss is 0.21224264800548553\n",
      "epoch: 2 step: 808, loss is 0.2864306569099426\n",
      "epoch: 2 step: 809, loss is 0.13433904945850372\n",
      "epoch: 2 step: 810, loss is 0.23438602685928345\n",
      "epoch: 2 step: 811, loss is 0.2896324694156647\n",
      "epoch: 2 step: 812, loss is 0.3368520438671112\n",
      "epoch: 2 step: 813, loss is 0.3637222945690155\n",
      "epoch: 2 step: 814, loss is 0.36036545038223267\n",
      "epoch: 2 step: 815, loss is 0.2713143229484558\n",
      "epoch: 2 step: 816, loss is 0.30474498867988586\n",
      "epoch: 2 step: 817, loss is 0.3740842640399933\n",
      "epoch: 2 step: 818, loss is 0.2621636688709259\n",
      "epoch: 2 step: 819, loss is 0.21101179718971252\n",
      "epoch: 2 step: 820, loss is 0.1846175491809845\n",
      "epoch: 2 step: 821, loss is 0.14385886490345\n",
      "epoch: 2 step: 822, loss is 0.28449752926826477\n",
      "epoch: 2 step: 823, loss is 0.27206969261169434\n",
      "epoch: 2 step: 824, loss is 0.2510451376438141\n",
      "epoch: 2 step: 825, loss is 0.19232486188411713\n",
      "epoch: 2 step: 826, loss is 0.17902381718158722\n",
      "epoch: 2 step: 827, loss is 0.26916155219078064\n",
      "epoch: 2 step: 828, loss is 0.21674877405166626\n",
      "epoch: 2 step: 829, loss is 0.37952545285224915\n",
      "epoch: 2 step: 830, loss is 0.3482690453529358\n",
      "epoch: 2 step: 831, loss is 0.3095906376838684\n",
      "epoch: 2 step: 832, loss is 0.44710904359817505\n",
      "epoch: 2 step: 833, loss is 0.09137489646673203\n",
      "epoch: 2 step: 834, loss is 0.293484628200531\n",
      "epoch: 2 step: 835, loss is 0.3658451735973358\n",
      "epoch: 2 step: 836, loss is 0.41294556856155396\n",
      "epoch: 2 step: 837, loss is 0.18392208218574524\n",
      "epoch: 2 step: 838, loss is 0.6359968185424805\n",
      "epoch: 2 step: 839, loss is 0.25665029883384705\n",
      "epoch: 2 step: 840, loss is 0.3113457262516022\n",
      "epoch: 2 step: 841, loss is 0.32146164774894714\n",
      "epoch: 2 step: 842, loss is 0.1857035607099533\n",
      "epoch: 2 step: 843, loss is 0.23926851153373718\n",
      "epoch: 2 step: 844, loss is 0.2830771207809448\n",
      "epoch: 2 step: 845, loss is 0.3466663062572479\n",
      "epoch: 2 step: 846, loss is 0.2143721729516983\n",
      "epoch: 2 step: 847, loss is 0.3197365701198578\n",
      "epoch: 2 step: 848, loss is 0.21997550129890442\n",
      "epoch: 2 step: 849, loss is 0.2856976389884949\n",
      "epoch: 2 step: 850, loss is 0.2387397736310959\n",
      "epoch: 2 step: 851, loss is 0.15363192558288574\n",
      "epoch: 2 step: 852, loss is 0.18745778501033783\n",
      "epoch: 2 step: 853, loss is 0.3554425835609436\n",
      "epoch: 2 step: 854, loss is 0.29990774393081665\n",
      "epoch: 2 step: 855, loss is 0.37532198429107666\n",
      "epoch: 2 step: 856, loss is 0.16882255673408508\n",
      "epoch: 2 step: 857, loss is 0.23947079479694366\n",
      "epoch: 2 step: 858, loss is 0.5643941760063171\n",
      "epoch: 2 step: 859, loss is 0.2263813465833664\n",
      "epoch: 2 step: 860, loss is 0.16381770372390747\n",
      "epoch: 2 step: 861, loss is 0.31684646010398865\n",
      "epoch: 2 step: 862, loss is 0.15820099413394928\n",
      "epoch: 2 step: 863, loss is 0.2010086476802826\n",
      "epoch: 2 step: 864, loss is 0.18871793150901794\n",
      "epoch: 2 step: 865, loss is 0.33534154295921326\n",
      "epoch: 2 step: 866, loss is 0.24522724747657776\n",
      "epoch: 2 step: 867, loss is 0.4776230454444885\n",
      "epoch: 2 step: 868, loss is 0.36026081442832947\n",
      "epoch: 2 step: 869, loss is 0.26723822951316833\n",
      "epoch: 2 step: 870, loss is 0.15246620774269104\n",
      "epoch: 2 step: 871, loss is 0.1906578540802002\n",
      "epoch: 2 step: 872, loss is 0.22517617046833038\n",
      "epoch: 2 step: 873, loss is 0.3519710898399353\n",
      "epoch: 2 step: 874, loss is 0.3343944847583771\n",
      "epoch: 2 step: 875, loss is 0.2718842923641205\n",
      "epoch: 2 step: 876, loss is 0.1734478771686554\n",
      "epoch: 2 step: 877, loss is 0.4731781780719757\n",
      "epoch: 2 step: 878, loss is 0.37841376662254333\n",
      "epoch: 2 step: 879, loss is 0.2321748584508896\n",
      "epoch: 2 step: 880, loss is 0.18235887587070465\n",
      "epoch: 2 step: 881, loss is 0.2853231132030487\n",
      "epoch: 2 step: 882, loss is 0.3682407736778259\n",
      "epoch: 2 step: 883, loss is 0.18949708342552185\n",
      "epoch: 2 step: 884, loss is 0.29337278008461\n",
      "epoch: 2 step: 885, loss is 0.15536266565322876\n",
      "epoch: 2 step: 886, loss is 0.14651814103126526\n",
      "epoch: 2 step: 887, loss is 0.14764545857906342\n",
      "epoch: 2 step: 888, loss is 0.3358401954174042\n",
      "epoch: 2 step: 889, loss is 0.22420711815357208\n",
      "epoch: 2 step: 890, loss is 0.1875653862953186\n",
      "epoch: 2 step: 891, loss is 0.2027745544910431\n",
      "epoch: 2 step: 892, loss is 0.22405852377414703\n",
      "epoch: 2 step: 893, loss is 0.18194794654846191\n",
      "epoch: 2 step: 894, loss is 0.1335769146680832\n",
      "epoch: 2 step: 895, loss is 0.2776440680027008\n",
      "epoch: 2 step: 896, loss is 0.2517204284667969\n",
      "epoch: 2 step: 897, loss is 0.37661218643188477\n",
      "epoch: 2 step: 898, loss is 0.2649402320384979\n",
      "epoch: 2 step: 899, loss is 0.360304057598114\n",
      "epoch: 2 step: 900, loss is 0.44386011362075806\n",
      "epoch: 2 step: 901, loss is 0.21752820909023285\n",
      "epoch: 2 step: 902, loss is 0.39353999495506287\n",
      "epoch: 2 step: 903, loss is 0.17640940845012665\n",
      "epoch: 2 step: 904, loss is 0.3035616874694824\n",
      "epoch: 2 step: 905, loss is 0.28285232186317444\n",
      "epoch: 2 step: 906, loss is 0.14350542426109314\n",
      "epoch: 2 step: 907, loss is 0.2683684527873993\n",
      "epoch: 2 step: 908, loss is 0.2710812985897064\n",
      "epoch: 2 step: 909, loss is 0.2591220736503601\n",
      "epoch: 2 step: 910, loss is 0.20935502648353577\n",
      "epoch: 2 step: 911, loss is 0.22132810950279236\n",
      "epoch: 2 step: 912, loss is 0.29578980803489685\n",
      "epoch: 2 step: 913, loss is 0.3270423412322998\n",
      "epoch: 2 step: 914, loss is 0.19934310019016266\n",
      "epoch: 2 step: 915, loss is 0.4007422924041748\n",
      "epoch: 2 step: 916, loss is 0.3134591579437256\n",
      "epoch: 2 step: 917, loss is 0.3259830176830292\n",
      "epoch: 2 step: 918, loss is 0.2266816794872284\n",
      "epoch: 2 step: 919, loss is 0.27269667387008667\n",
      "epoch: 2 step: 920, loss is 0.2621973156929016\n",
      "epoch: 2 step: 921, loss is 0.31695979833602905\n",
      "epoch: 2 step: 922, loss is 0.36255142092704773\n",
      "epoch: 2 step: 923, loss is 0.391903281211853\n",
      "epoch: 2 step: 924, loss is 0.32613691687583923\n",
      "epoch: 2 step: 925, loss is 0.30826258659362793\n",
      "epoch: 2 step: 926, loss is 0.21292197704315186\n",
      "epoch: 2 step: 927, loss is 0.35830721259117126\n",
      "epoch: 2 step: 928, loss is 0.17194761335849762\n",
      "epoch: 2 step: 929, loss is 0.1922202855348587\n",
      "epoch: 2 step: 930, loss is 0.25058022141456604\n",
      "epoch: 2 step: 931, loss is 0.3428264856338501\n",
      "epoch: 2 step: 932, loss is 0.2581579089164734\n",
      "epoch: 2 step: 933, loss is 0.33206069469451904\n",
      "epoch: 2 step: 934, loss is 0.30240219831466675\n",
      "epoch: 2 step: 935, loss is 0.21809673309326172\n",
      "epoch: 2 step: 936, loss is 0.18252944946289062\n",
      "epoch: 2 step: 937, loss is 0.6366714835166931\n",
      "epoch: 3 step: 1, loss is 0.31495773792266846\n",
      "epoch: 3 step: 2, loss is 0.2578662633895874\n",
      "epoch: 3 step: 3, loss is 0.20476604998111725\n",
      "epoch: 3 step: 4, loss is 0.23953399062156677\n",
      "epoch: 3 step: 5, loss is 0.25375571846961975\n",
      "epoch: 3 step: 6, loss is 0.32676273584365845\n",
      "epoch: 3 step: 7, loss is 0.20427609980106354\n",
      "epoch: 3 step: 8, loss is 0.2584351599216461\n",
      "epoch: 3 step: 9, loss is 0.23590658605098724\n",
      "epoch: 3 step: 10, loss is 0.22377395629882812\n",
      "epoch: 3 step: 11, loss is 0.19239795207977295\n",
      "epoch: 3 step: 12, loss is 0.2690856456756592\n",
      "epoch: 3 step: 13, loss is 0.32531827688217163\n",
      "epoch: 3 step: 14, loss is 0.19405904412269592\n",
      "epoch: 3 step: 15, loss is 0.19018416106700897\n",
      "epoch: 3 step: 16, loss is 0.2329121232032776\n",
      "epoch: 3 step: 17, loss is 0.22268009185791016\n",
      "epoch: 3 step: 18, loss is 0.30581948161125183\n",
      "epoch: 3 step: 19, loss is 0.2807803452014923\n",
      "epoch: 3 step: 20, loss is 0.4061260223388672\n",
      "epoch: 3 step: 21, loss is 0.34523364901542664\n",
      "epoch: 3 step: 22, loss is 0.17056839168071747\n",
      "epoch: 3 step: 23, loss is 0.29292577505111694\n",
      "epoch: 3 step: 24, loss is 0.3780798614025116\n",
      "epoch: 3 step: 25, loss is 0.25417593121528625\n",
      "epoch: 3 step: 26, loss is 0.23582352697849274\n",
      "epoch: 3 step: 27, loss is 0.44040805101394653\n",
      "epoch: 3 step: 28, loss is 0.33685556054115295\n",
      "epoch: 3 step: 29, loss is 0.30828264355659485\n",
      "epoch: 3 step: 30, loss is 0.23412546515464783\n",
      "epoch: 3 step: 31, loss is 0.22977197170257568\n",
      "epoch: 3 step: 32, loss is 0.2786158621311188\n",
      "epoch: 3 step: 33, loss is 0.3663405179977417\n",
      "epoch: 3 step: 34, loss is 0.2328576147556305\n",
      "epoch: 3 step: 35, loss is 0.2500423192977905\n",
      "epoch: 3 step: 36, loss is 0.16047196090221405\n",
      "epoch: 3 step: 37, loss is 0.2545641362667084\n",
      "epoch: 3 step: 38, loss is 0.27277272939682007\n",
      "epoch: 3 step: 39, loss is 0.3801654875278473\n",
      "epoch: 3 step: 40, loss is 0.24673983454704285\n",
      "epoch: 3 step: 41, loss is 0.22402618825435638\n",
      "epoch: 3 step: 42, loss is 0.14534114301204681\n",
      "epoch: 3 step: 43, loss is 0.2316158264875412\n",
      "epoch: 3 step: 44, loss is 0.2148979753255844\n",
      "epoch: 3 step: 45, loss is 0.29672783613204956\n",
      "epoch: 3 step: 46, loss is 0.2613351345062256\n",
      "epoch: 3 step: 47, loss is 0.3344839811325073\n",
      "epoch: 3 step: 48, loss is 0.21181094646453857\n",
      "epoch: 3 step: 49, loss is 0.27483895421028137\n",
      "epoch: 3 step: 50, loss is 0.1967911422252655\n",
      "epoch: 3 step: 51, loss is 0.27437031269073486\n",
      "epoch: 3 step: 52, loss is 0.34545454382896423\n",
      "epoch: 3 step: 53, loss is 0.21692587435245514\n",
      "epoch: 3 step: 54, loss is 0.20344264805316925\n",
      "epoch: 3 step: 55, loss is 0.15819479525089264\n",
      "epoch: 3 step: 56, loss is 0.21285776793956757\n",
      "epoch: 3 step: 57, loss is 0.13534699380397797\n",
      "epoch: 3 step: 58, loss is 0.19184602797031403\n",
      "epoch: 3 step: 59, loss is 0.11694293469190598\n",
      "epoch: 3 step: 60, loss is 0.37520062923431396\n",
      "epoch: 3 step: 61, loss is 0.2756502628326416\n",
      "epoch: 3 step: 62, loss is 0.22285492718219757\n",
      "epoch: 3 step: 63, loss is 0.22499315440654755\n",
      "epoch: 3 step: 64, loss is 0.3449949622154236\n",
      "epoch: 3 step: 65, loss is 0.16649769246578217\n",
      "epoch: 3 step: 66, loss is 0.22537817060947418\n",
      "epoch: 3 step: 67, loss is 0.17841005325317383\n",
      "epoch: 3 step: 68, loss is 0.1361454725265503\n",
      "epoch: 3 step: 69, loss is 0.12096759676933289\n",
      "epoch: 3 step: 70, loss is 0.16550573706626892\n",
      "epoch: 3 step: 71, loss is 0.15790218114852905\n",
      "epoch: 3 step: 72, loss is 0.2599080204963684\n",
      "epoch: 3 step: 73, loss is 0.2858580946922302\n",
      "epoch: 3 step: 74, loss is 0.2723734378814697\n",
      "epoch: 3 step: 75, loss is 0.20488008856773376\n",
      "epoch: 3 step: 76, loss is 0.3373226821422577\n",
      "epoch: 3 step: 77, loss is 0.09377668052911758\n",
      "epoch: 3 step: 78, loss is 0.3723059594631195\n",
      "epoch: 3 step: 79, loss is 0.2870234251022339\n",
      "epoch: 3 step: 80, loss is 0.19037149846553802\n",
      "epoch: 3 step: 81, loss is 0.6240449547767639\n",
      "epoch: 3 step: 82, loss is 0.41019919514656067\n",
      "epoch: 3 step: 83, loss is 0.2731727659702301\n",
      "epoch: 3 step: 84, loss is 0.235655277967453\n",
      "epoch: 3 step: 85, loss is 0.2300579845905304\n",
      "epoch: 3 step: 86, loss is 0.30927595496177673\n",
      "epoch: 3 step: 87, loss is 0.2103717178106308\n",
      "epoch: 3 step: 88, loss is 0.11504195630550385\n",
      "epoch: 3 step: 89, loss is 0.5708016753196716\n",
      "epoch: 3 step: 90, loss is 0.19150298833847046\n",
      "epoch: 3 step: 91, loss is 0.1940741389989853\n",
      "epoch: 3 step: 92, loss is 0.20802150666713715\n",
      "epoch: 3 step: 93, loss is 0.2586204409599304\n",
      "epoch: 3 step: 94, loss is 0.5281134247779846\n",
      "epoch: 3 step: 95, loss is 0.24322578310966492\n",
      "epoch: 3 step: 96, loss is 0.21849781274795532\n",
      "epoch: 3 step: 97, loss is 0.39692890644073486\n",
      "epoch: 3 step: 98, loss is 0.2731921374797821\n",
      "epoch: 3 step: 99, loss is 0.2821251451969147\n",
      "epoch: 3 step: 100, loss is 0.30216166377067566\n",
      "epoch: 3 step: 101, loss is 0.2196110188961029\n",
      "epoch: 3 step: 102, loss is 0.15980814397335052\n",
      "epoch: 3 step: 103, loss is 0.15772049129009247\n",
      "epoch: 3 step: 104, loss is 0.1432311236858368\n",
      "epoch: 3 step: 105, loss is 0.2501973509788513\n",
      "epoch: 3 step: 106, loss is 0.18290381133556366\n",
      "epoch: 3 step: 107, loss is 0.3220176696777344\n",
      "epoch: 3 step: 108, loss is 0.33495989441871643\n",
      "epoch: 3 step: 109, loss is 0.36486315727233887\n",
      "epoch: 3 step: 110, loss is 0.23183602094650269\n",
      "epoch: 3 step: 111, loss is 0.30171263217926025\n",
      "epoch: 3 step: 112, loss is 0.2965014576911926\n",
      "epoch: 3 step: 113, loss is 0.18930277228355408\n",
      "epoch: 3 step: 114, loss is 0.22361508011817932\n",
      "epoch: 3 step: 115, loss is 0.19667334854602814\n",
      "epoch: 3 step: 116, loss is 0.2650921642780304\n",
      "epoch: 3 step: 117, loss is 0.2952682375907898\n",
      "epoch: 3 step: 118, loss is 0.27842384576797485\n",
      "epoch: 3 step: 119, loss is 0.28084808588027954\n",
      "epoch: 3 step: 120, loss is 0.25925272703170776\n",
      "epoch: 3 step: 121, loss is 0.21208573877811432\n",
      "epoch: 3 step: 122, loss is 0.1232919991016388\n",
      "epoch: 3 step: 123, loss is 0.08515488356351852\n",
      "epoch: 3 step: 124, loss is 0.26171809434890747\n",
      "epoch: 3 step: 125, loss is 0.33872899413108826\n",
      "epoch: 3 step: 126, loss is 0.10830558836460114\n",
      "epoch: 3 step: 127, loss is 0.19136273860931396\n",
      "epoch: 3 step: 128, loss is 0.2232530564069748\n",
      "epoch: 3 step: 129, loss is 0.2202637493610382\n",
      "epoch: 3 step: 130, loss is 0.058762531727552414\n",
      "epoch: 3 step: 131, loss is 0.20811453461647034\n",
      "epoch: 3 step: 132, loss is 0.1872922033071518\n",
      "epoch: 3 step: 133, loss is 0.12723353505134583\n",
      "epoch: 3 step: 134, loss is 0.052404262125492096\n",
      "epoch: 3 step: 135, loss is 0.25672173500061035\n",
      "epoch: 3 step: 136, loss is 0.28828346729278564\n",
      "epoch: 3 step: 137, loss is 0.2907525300979614\n",
      "epoch: 3 step: 138, loss is 0.519529402256012\n",
      "epoch: 3 step: 139, loss is 0.1496124416589737\n",
      "epoch: 3 step: 140, loss is 0.4095509350299835\n",
      "epoch: 3 step: 141, loss is 0.1270444691181183\n",
      "epoch: 3 step: 142, loss is 0.14296577870845795\n",
      "epoch: 3 step: 143, loss is 0.18852736055850983\n",
      "epoch: 3 step: 144, loss is 0.23580677807331085\n",
      "epoch: 3 step: 145, loss is 0.2337467223405838\n",
      "epoch: 3 step: 146, loss is 0.16002754867076874\n",
      "epoch: 3 step: 147, loss is 0.25735652446746826\n",
      "epoch: 3 step: 148, loss is 0.3165857791900635\n",
      "epoch: 3 step: 149, loss is 0.29952895641326904\n",
      "epoch: 3 step: 150, loss is 0.32612279057502747\n",
      "epoch: 3 step: 151, loss is 0.3019251525402069\n",
      "epoch: 3 step: 152, loss is 0.2836951017379761\n",
      "epoch: 3 step: 153, loss is 0.23836106061935425\n",
      "epoch: 3 step: 154, loss is 0.2561834454536438\n",
      "epoch: 3 step: 155, loss is 0.1543245017528534\n",
      "epoch: 3 step: 156, loss is 0.4899330735206604\n",
      "epoch: 3 step: 157, loss is 0.2128133475780487\n",
      "epoch: 3 step: 158, loss is 0.31243619322776794\n",
      "epoch: 3 step: 159, loss is 0.25170257687568665\n",
      "epoch: 3 step: 160, loss is 0.2330527901649475\n",
      "epoch: 3 step: 161, loss is 0.20570631325244904\n",
      "epoch: 3 step: 162, loss is 0.2302500605583191\n",
      "epoch: 3 step: 163, loss is 0.33903470635414124\n",
      "epoch: 3 step: 164, loss is 0.17679953575134277\n",
      "epoch: 3 step: 165, loss is 0.18486471474170685\n",
      "epoch: 3 step: 166, loss is 0.2282988280057907\n",
      "epoch: 3 step: 167, loss is 0.22831854224205017\n",
      "epoch: 3 step: 168, loss is 0.2229941189289093\n",
      "epoch: 3 step: 169, loss is 0.27000248432159424\n",
      "epoch: 3 step: 170, loss is 0.283238023519516\n",
      "epoch: 3 step: 171, loss is 0.18080173432826996\n",
      "epoch: 3 step: 172, loss is 0.24804754555225372\n",
      "epoch: 3 step: 173, loss is 0.19085773825645447\n",
      "epoch: 3 step: 174, loss is 0.14027361571788788\n",
      "epoch: 3 step: 175, loss is 0.2420118898153305\n",
      "epoch: 3 step: 176, loss is 0.2068951278924942\n",
      "epoch: 3 step: 177, loss is 0.38053497672080994\n",
      "epoch: 3 step: 178, loss is 0.2801317274570465\n",
      "epoch: 3 step: 179, loss is 0.22779028117656708\n",
      "epoch: 3 step: 180, loss is 0.25614774227142334\n",
      "epoch: 3 step: 181, loss is 0.28385186195373535\n",
      "epoch: 3 step: 182, loss is 0.21378064155578613\n",
      "epoch: 3 step: 183, loss is 0.2808448374271393\n",
      "epoch: 3 step: 184, loss is 0.36676034331321716\n",
      "epoch: 3 step: 185, loss is 0.24310126900672913\n",
      "epoch: 3 step: 186, loss is 0.1544818878173828\n",
      "epoch: 3 step: 187, loss is 0.2402326762676239\n",
      "epoch: 3 step: 188, loss is 0.09020151197910309\n",
      "epoch: 3 step: 189, loss is 0.14510616660118103\n",
      "epoch: 3 step: 190, loss is 0.10845496505498886\n",
      "epoch: 3 step: 191, loss is 0.2347027063369751\n",
      "epoch: 3 step: 192, loss is 0.24026553332805634\n",
      "epoch: 3 step: 193, loss is 0.2472400665283203\n",
      "epoch: 3 step: 194, loss is 0.2055688351392746\n",
      "epoch: 3 step: 195, loss is 0.22230303287506104\n",
      "epoch: 3 step: 196, loss is 0.4008675515651703\n",
      "epoch: 3 step: 197, loss is 0.3273167312145233\n",
      "epoch: 3 step: 198, loss is 0.18550559878349304\n",
      "epoch: 3 step: 199, loss is 0.2742033302783966\n",
      "epoch: 3 step: 200, loss is 0.2489403486251831\n",
      "epoch: 3 step: 201, loss is 0.25386765599250793\n",
      "epoch: 3 step: 202, loss is 0.25614479184150696\n",
      "epoch: 3 step: 203, loss is 0.32592299580574036\n",
      "epoch: 3 step: 204, loss is 0.40038129687309265\n",
      "epoch: 3 step: 205, loss is 0.17049111425876617\n",
      "epoch: 3 step: 206, loss is 0.23103968799114227\n",
      "epoch: 3 step: 207, loss is 0.1539609730243683\n",
      "epoch: 3 step: 208, loss is 0.29584598541259766\n",
      "epoch: 3 step: 209, loss is 0.3167262077331543\n",
      "epoch: 3 step: 210, loss is 0.13493086397647858\n",
      "epoch: 3 step: 211, loss is 0.21184217929840088\n",
      "epoch: 3 step: 212, loss is 0.14551247656345367\n",
      "epoch: 3 step: 213, loss is 0.2440909892320633\n",
      "epoch: 3 step: 214, loss is 0.3078189790248871\n",
      "epoch: 3 step: 215, loss is 0.18137231469154358\n",
      "epoch: 3 step: 216, loss is 0.3845994174480438\n",
      "epoch: 3 step: 217, loss is 0.20117025077342987\n",
      "epoch: 3 step: 218, loss is 0.40183839201927185\n",
      "epoch: 3 step: 219, loss is 0.2577630877494812\n",
      "epoch: 3 step: 220, loss is 0.3072052597999573\n",
      "epoch: 3 step: 221, loss is 0.19638684391975403\n",
      "epoch: 3 step: 222, loss is 0.43226897716522217\n",
      "epoch: 3 step: 223, loss is 0.19091059267520905\n",
      "epoch: 3 step: 224, loss is 0.1514807939529419\n",
      "epoch: 3 step: 225, loss is 0.2509317100048065\n",
      "epoch: 3 step: 226, loss is 0.27615174651145935\n",
      "epoch: 3 step: 227, loss is 0.3714156150817871\n",
      "epoch: 3 step: 228, loss is 0.2159050554037094\n",
      "epoch: 3 step: 229, loss is 0.1523253619670868\n",
      "epoch: 3 step: 230, loss is 0.36081376671791077\n",
      "epoch: 3 step: 231, loss is 0.17977213859558105\n",
      "epoch: 3 step: 232, loss is 0.3312208354473114\n",
      "epoch: 3 step: 233, loss is 0.22107404470443726\n",
      "epoch: 3 step: 234, loss is 0.21671096980571747\n",
      "epoch: 3 step: 235, loss is 0.24449993669986725\n",
      "epoch: 3 step: 236, loss is 0.25489935278892517\n",
      "epoch: 3 step: 237, loss is 0.251162052154541\n",
      "epoch: 3 step: 238, loss is 0.40868979692459106\n",
      "epoch: 3 step: 239, loss is 0.3829476833343506\n",
      "epoch: 3 step: 240, loss is 0.12378785759210587\n",
      "epoch: 3 step: 241, loss is 0.3934924602508545\n",
      "epoch: 3 step: 242, loss is 0.21310707926750183\n",
      "epoch: 3 step: 243, loss is 0.2512984275817871\n",
      "epoch: 3 step: 244, loss is 0.1581757366657257\n",
      "epoch: 3 step: 245, loss is 0.3060360848903656\n",
      "epoch: 3 step: 246, loss is 0.22256483137607574\n",
      "epoch: 3 step: 247, loss is 0.3245004713535309\n",
      "epoch: 3 step: 248, loss is 0.2167341262102127\n",
      "epoch: 3 step: 249, loss is 0.1734045147895813\n",
      "epoch: 3 step: 250, loss is 0.340389221906662\n",
      "epoch: 3 step: 251, loss is 0.19284915924072266\n",
      "epoch: 3 step: 252, loss is 0.2042449563741684\n",
      "epoch: 3 step: 253, loss is 0.1300765573978424\n",
      "epoch: 3 step: 254, loss is 0.1475091576576233\n",
      "epoch: 3 step: 255, loss is 0.18983228504657745\n",
      "epoch: 3 step: 256, loss is 0.23835188150405884\n",
      "epoch: 3 step: 257, loss is 0.32243892550468445\n",
      "epoch: 3 step: 258, loss is 0.23825620114803314\n",
      "epoch: 3 step: 259, loss is 0.3583008646965027\n",
      "epoch: 3 step: 260, loss is 0.1863543540239334\n",
      "epoch: 3 step: 261, loss is 0.11880326271057129\n",
      "epoch: 3 step: 262, loss is 0.1807193160057068\n",
      "epoch: 3 step: 263, loss is 0.2298736572265625\n",
      "epoch: 3 step: 264, loss is 0.3796005845069885\n",
      "epoch: 3 step: 265, loss is 0.11482278257608414\n",
      "epoch: 3 step: 266, loss is 0.13875117897987366\n",
      "epoch: 3 step: 267, loss is 0.40504902601242065\n",
      "epoch: 3 step: 268, loss is 0.18758028745651245\n",
      "epoch: 3 step: 269, loss is 0.23556169867515564\n",
      "epoch: 3 step: 270, loss is 0.29076293110847473\n",
      "epoch: 3 step: 271, loss is 0.3510185182094574\n",
      "epoch: 3 step: 272, loss is 0.1685970574617386\n",
      "epoch: 3 step: 273, loss is 0.2349568009376526\n",
      "epoch: 3 step: 274, loss is 0.5258175730705261\n",
      "epoch: 3 step: 275, loss is 0.20238475501537323\n",
      "epoch: 3 step: 276, loss is 0.21873442828655243\n",
      "epoch: 3 step: 277, loss is 0.21243423223495483\n",
      "epoch: 3 step: 278, loss is 0.13299165666103363\n",
      "epoch: 3 step: 279, loss is 0.20184586942195892\n",
      "epoch: 3 step: 280, loss is 0.23676680028438568\n",
      "epoch: 3 step: 281, loss is 0.3392128050327301\n",
      "epoch: 3 step: 282, loss is 0.2380295991897583\n",
      "epoch: 3 step: 283, loss is 0.33148691058158875\n",
      "epoch: 3 step: 284, loss is 0.2966875433921814\n",
      "epoch: 3 step: 285, loss is 0.25882288813591003\n",
      "epoch: 3 step: 286, loss is 0.1284462809562683\n",
      "epoch: 3 step: 287, loss is 0.4635210335254669\n",
      "epoch: 3 step: 288, loss is 0.3320671319961548\n",
      "epoch: 3 step: 289, loss is 0.2663038372993469\n",
      "epoch: 3 step: 290, loss is 0.3204028308391571\n",
      "epoch: 3 step: 291, loss is 0.16103459894657135\n",
      "epoch: 3 step: 292, loss is 0.2889399528503418\n",
      "epoch: 3 step: 293, loss is 0.26882293820381165\n",
      "epoch: 3 step: 294, loss is 0.24498431384563446\n",
      "epoch: 3 step: 295, loss is 0.3501436114311218\n",
      "epoch: 3 step: 296, loss is 0.2944711148738861\n",
      "epoch: 3 step: 297, loss is 0.22156894207000732\n",
      "epoch: 3 step: 298, loss is 0.10742609202861786\n",
      "epoch: 3 step: 299, loss is 0.3950784504413605\n",
      "epoch: 3 step: 300, loss is 0.3618829846382141\n",
      "epoch: 3 step: 301, loss is 0.46406644582748413\n",
      "epoch: 3 step: 302, loss is 0.27174147963523865\n",
      "epoch: 3 step: 303, loss is 0.1502036303281784\n",
      "epoch: 3 step: 304, loss is 0.288726270198822\n",
      "epoch: 3 step: 305, loss is 0.4243433475494385\n",
      "epoch: 3 step: 306, loss is 0.15777091681957245\n",
      "epoch: 3 step: 307, loss is 0.5523016452789307\n",
      "epoch: 3 step: 308, loss is 0.29532039165496826\n",
      "epoch: 3 step: 309, loss is 0.17599891126155853\n",
      "epoch: 3 step: 310, loss is 0.19696706533432007\n",
      "epoch: 3 step: 311, loss is 0.15249215066432953\n",
      "epoch: 3 step: 312, loss is 0.23530548810958862\n",
      "epoch: 3 step: 313, loss is 0.29101505875587463\n",
      "epoch: 3 step: 314, loss is 0.23199133574962616\n",
      "epoch: 3 step: 315, loss is 0.2862198054790497\n",
      "epoch: 3 step: 316, loss is 0.3433896005153656\n",
      "epoch: 3 step: 317, loss is 0.3362047076225281\n",
      "epoch: 3 step: 318, loss is 0.35637885332107544\n",
      "epoch: 3 step: 319, loss is 0.5677588582038879\n",
      "epoch: 3 step: 320, loss is 0.23660174012184143\n",
      "epoch: 3 step: 321, loss is 0.31167876720428467\n",
      "epoch: 3 step: 322, loss is 0.2362109273672104\n",
      "epoch: 3 step: 323, loss is 0.314823180437088\n",
      "epoch: 3 step: 324, loss is 0.23367786407470703\n",
      "epoch: 3 step: 325, loss is 0.14970967173576355\n",
      "epoch: 3 step: 326, loss is 0.31282487511634827\n",
      "epoch: 3 step: 327, loss is 0.3274593949317932\n",
      "epoch: 3 step: 328, loss is 0.27631038427352905\n",
      "epoch: 3 step: 329, loss is 0.20754413306713104\n",
      "epoch: 3 step: 330, loss is 0.1353892982006073\n",
      "epoch: 3 step: 331, loss is 0.1567721664905548\n",
      "epoch: 3 step: 332, loss is 0.43714308738708496\n",
      "epoch: 3 step: 333, loss is 0.2361067235469818\n",
      "epoch: 3 step: 334, loss is 0.33106759190559387\n",
      "epoch: 3 step: 335, loss is 0.2243242859840393\n",
      "epoch: 3 step: 336, loss is 0.4839574098587036\n",
      "epoch: 3 step: 337, loss is 0.17679078876972198\n",
      "epoch: 3 step: 338, loss is 0.17770816385746002\n",
      "epoch: 3 step: 339, loss is 0.15071289241313934\n",
      "epoch: 3 step: 340, loss is 0.2917705476284027\n",
      "epoch: 3 step: 341, loss is 0.1922053098678589\n",
      "epoch: 3 step: 342, loss is 0.26211902499198914\n",
      "epoch: 3 step: 343, loss is 0.25097259879112244\n",
      "epoch: 3 step: 344, loss is 0.2421678900718689\n",
      "epoch: 3 step: 345, loss is 0.2488280087709427\n",
      "epoch: 3 step: 346, loss is 0.15014685690402985\n",
      "epoch: 3 step: 347, loss is 0.1898258775472641\n",
      "epoch: 3 step: 348, loss is 0.3752530515193939\n",
      "epoch: 3 step: 349, loss is 0.30539533495903015\n",
      "epoch: 3 step: 350, loss is 0.3047613501548767\n",
      "epoch: 3 step: 351, loss is 0.3579089939594269\n",
      "epoch: 3 step: 352, loss is 0.20235541462898254\n",
      "epoch: 3 step: 353, loss is 0.28702688217163086\n",
      "epoch: 3 step: 354, loss is 0.31208381056785583\n",
      "epoch: 3 step: 355, loss is 0.2953658103942871\n",
      "epoch: 3 step: 356, loss is 0.285881906747818\n",
      "epoch: 3 step: 357, loss is 0.15577073395252228\n",
      "epoch: 3 step: 358, loss is 0.15366224944591522\n",
      "epoch: 3 step: 359, loss is 0.21483471989631653\n",
      "epoch: 3 step: 360, loss is 0.36639145016670227\n",
      "epoch: 3 step: 361, loss is 0.23722271621227264\n",
      "epoch: 3 step: 362, loss is 0.19242046773433685\n",
      "epoch: 3 step: 363, loss is 0.2999178469181061\n",
      "epoch: 3 step: 364, loss is 0.3042997121810913\n",
      "epoch: 3 step: 365, loss is 0.1781136393547058\n",
      "epoch: 3 step: 366, loss is 0.1531279981136322\n",
      "epoch: 3 step: 367, loss is 0.2838733196258545\n",
      "epoch: 3 step: 368, loss is 0.2004845291376114\n",
      "epoch: 3 step: 369, loss is 0.15824125707149506\n",
      "epoch: 3 step: 370, loss is 0.15542088449001312\n",
      "epoch: 3 step: 371, loss is 0.1835002601146698\n",
      "epoch: 3 step: 372, loss is 0.24770762026309967\n",
      "epoch: 3 step: 373, loss is 0.27780020236968994\n",
      "epoch: 3 step: 374, loss is 0.2234918624162674\n",
      "epoch: 3 step: 375, loss is 0.2502064108848572\n",
      "epoch: 3 step: 376, loss is 0.20749740302562714\n",
      "epoch: 3 step: 377, loss is 0.282441645860672\n",
      "epoch: 3 step: 378, loss is 0.2786748707294464\n",
      "epoch: 3 step: 379, loss is 0.2394411563873291\n",
      "epoch: 3 step: 380, loss is 0.34721946716308594\n",
      "epoch: 3 step: 381, loss is 0.24591511487960815\n",
      "epoch: 3 step: 382, loss is 0.1730971336364746\n",
      "epoch: 3 step: 383, loss is 0.08688845485448837\n",
      "epoch: 3 step: 384, loss is 0.16176506876945496\n",
      "epoch: 3 step: 385, loss is 0.26311805844306946\n",
      "epoch: 3 step: 386, loss is 0.3621975779533386\n",
      "epoch: 3 step: 387, loss is 0.24402093887329102\n",
      "epoch: 3 step: 388, loss is 0.20203226804733276\n",
      "epoch: 3 step: 389, loss is 0.15425531566143036\n",
      "epoch: 3 step: 390, loss is 0.23831340670585632\n",
      "epoch: 3 step: 391, loss is 0.16822296380996704\n",
      "epoch: 3 step: 392, loss is 0.12684319913387299\n",
      "epoch: 3 step: 393, loss is 0.3262384235858917\n",
      "epoch: 3 step: 394, loss is 0.23843799531459808\n",
      "epoch: 3 step: 395, loss is 0.1866808384656906\n",
      "epoch: 3 step: 396, loss is 0.2433127611875534\n",
      "epoch: 3 step: 397, loss is 0.3535063862800598\n",
      "epoch: 3 step: 398, loss is 0.1383352428674698\n",
      "epoch: 3 step: 399, loss is 0.14211434125900269\n",
      "epoch: 3 step: 400, loss is 0.3646335005760193\n",
      "epoch: 3 step: 401, loss is 0.37317410111427307\n",
      "epoch: 3 step: 402, loss is 0.30297917127609253\n",
      "epoch: 3 step: 403, loss is 0.15439778566360474\n",
      "epoch: 3 step: 404, loss is 0.2099113166332245\n",
      "epoch: 3 step: 405, loss is 0.2169172316789627\n",
      "epoch: 3 step: 406, loss is 0.16298912465572357\n",
      "epoch: 3 step: 407, loss is 0.25755009055137634\n",
      "epoch: 3 step: 408, loss is 0.2686437964439392\n",
      "epoch: 3 step: 409, loss is 0.12091486155986786\n",
      "epoch: 3 step: 410, loss is 0.17517788708209991\n",
      "epoch: 3 step: 411, loss is 0.29265516996383667\n",
      "epoch: 3 step: 412, loss is 0.3248298168182373\n",
      "epoch: 3 step: 413, loss is 0.21891720592975616\n",
      "epoch: 3 step: 414, loss is 0.24117930233478546\n",
      "epoch: 3 step: 415, loss is 0.3446022570133209\n",
      "epoch: 3 step: 416, loss is 0.17560003697872162\n",
      "epoch: 3 step: 417, loss is 0.30317139625549316\n",
      "epoch: 3 step: 418, loss is 0.1999392956495285\n",
      "epoch: 3 step: 419, loss is 0.37049832940101624\n",
      "epoch: 3 step: 420, loss is 0.21366596221923828\n",
      "epoch: 3 step: 421, loss is 0.31162306666374207\n",
      "epoch: 3 step: 422, loss is 0.26851511001586914\n",
      "epoch: 3 step: 423, loss is 0.2932502329349518\n",
      "epoch: 3 step: 424, loss is 0.19697953760623932\n",
      "epoch: 3 step: 425, loss is 0.3132858872413635\n",
      "epoch: 3 step: 426, loss is 0.2875749468803406\n",
      "epoch: 3 step: 427, loss is 0.3324741721153259\n",
      "epoch: 3 step: 428, loss is 0.2620757520198822\n",
      "epoch: 3 step: 429, loss is 0.2356257438659668\n",
      "epoch: 3 step: 430, loss is 0.24539828300476074\n",
      "epoch: 3 step: 431, loss is 0.32397910952568054\n",
      "epoch: 3 step: 432, loss is 0.44089314341545105\n",
      "epoch: 3 step: 433, loss is 0.08807323127985\n",
      "epoch: 3 step: 434, loss is 0.22222958505153656\n",
      "epoch: 3 step: 435, loss is 0.15753193199634552\n",
      "epoch: 3 step: 436, loss is 0.2818860411643982\n",
      "epoch: 3 step: 437, loss is 0.30631163716316223\n",
      "epoch: 3 step: 438, loss is 0.18217453360557556\n",
      "epoch: 3 step: 439, loss is 0.48557525873184204\n",
      "epoch: 3 step: 440, loss is 0.18427419662475586\n",
      "epoch: 3 step: 441, loss is 0.4113196134567261\n",
      "epoch: 3 step: 442, loss is 0.2574419677257538\n",
      "epoch: 3 step: 443, loss is 0.15932443737983704\n",
      "epoch: 3 step: 444, loss is 0.25250014662742615\n",
      "epoch: 3 step: 445, loss is 0.31299227476119995\n",
      "epoch: 3 step: 446, loss is 0.19821259379386902\n",
      "epoch: 3 step: 447, loss is 0.26192304491996765\n",
      "epoch: 3 step: 448, loss is 0.17299528419971466\n",
      "epoch: 3 step: 449, loss is 0.19050346314907074\n",
      "epoch: 3 step: 450, loss is 0.2385762631893158\n",
      "epoch: 3 step: 451, loss is 0.21143387258052826\n",
      "epoch: 3 step: 452, loss is 0.14347322285175323\n",
      "epoch: 3 step: 453, loss is 0.2994118630886078\n",
      "epoch: 3 step: 454, loss is 0.18995118141174316\n",
      "epoch: 3 step: 455, loss is 0.2756097614765167\n",
      "epoch: 3 step: 456, loss is 0.17585207521915436\n",
      "epoch: 3 step: 457, loss is 0.3811143934726715\n",
      "epoch: 3 step: 458, loss is 0.32602182030677795\n",
      "epoch: 3 step: 459, loss is 0.3013947308063507\n",
      "epoch: 3 step: 460, loss is 0.35970547795295715\n",
      "epoch: 3 step: 461, loss is 0.19608727097511292\n",
      "epoch: 3 step: 462, loss is 0.19435849785804749\n",
      "epoch: 3 step: 463, loss is 0.18691371381282806\n",
      "epoch: 3 step: 464, loss is 0.32293590903282166\n",
      "epoch: 3 step: 465, loss is 0.2106112539768219\n",
      "epoch: 3 step: 466, loss is 0.21836470067501068\n",
      "epoch: 3 step: 467, loss is 0.1646515130996704\n",
      "epoch: 3 step: 468, loss is 0.11854781210422516\n",
      "epoch: 3 step: 469, loss is 0.5170442461967468\n",
      "epoch: 3 step: 470, loss is 0.3629865050315857\n",
      "epoch: 3 step: 471, loss is 0.3322770297527313\n",
      "epoch: 3 step: 472, loss is 0.2507299780845642\n",
      "epoch: 3 step: 473, loss is 0.22284583747386932\n",
      "epoch: 3 step: 474, loss is 0.41123121976852417\n",
      "epoch: 3 step: 475, loss is 0.10685247927904129\n",
      "epoch: 3 step: 476, loss is 0.17843219637870789\n",
      "epoch: 3 step: 477, loss is 0.2709546983242035\n",
      "epoch: 3 step: 478, loss is 0.22485120594501495\n",
      "epoch: 3 step: 479, loss is 0.17381347715854645\n",
      "epoch: 3 step: 480, loss is 0.14867018163204193\n",
      "epoch: 3 step: 481, loss is 0.13571226596832275\n",
      "epoch: 3 step: 482, loss is 0.16399575769901276\n",
      "epoch: 3 step: 483, loss is 0.20231707394123077\n",
      "epoch: 3 step: 484, loss is 0.31150415539741516\n",
      "epoch: 3 step: 485, loss is 0.29958122968673706\n",
      "epoch: 3 step: 486, loss is 0.2286224365234375\n",
      "epoch: 3 step: 487, loss is 0.16572174429893494\n",
      "epoch: 3 step: 488, loss is 0.3010095953941345\n",
      "epoch: 3 step: 489, loss is 0.2345932424068451\n",
      "epoch: 3 step: 490, loss is 0.24155902862548828\n",
      "epoch: 3 step: 491, loss is 0.19373996555805206\n",
      "epoch: 3 step: 492, loss is 0.17159390449523926\n",
      "epoch: 3 step: 493, loss is 0.21538588404655457\n",
      "epoch: 3 step: 494, loss is 0.2520194351673126\n",
      "epoch: 3 step: 495, loss is 0.234071746468544\n",
      "epoch: 3 step: 496, loss is 0.26408401131629944\n",
      "epoch: 3 step: 497, loss is 0.19910453259944916\n",
      "epoch: 3 step: 498, loss is 0.35968831181526184\n",
      "epoch: 3 step: 499, loss is 0.18764865398406982\n",
      "epoch: 3 step: 500, loss is 0.1322333961725235\n",
      "epoch: 3 step: 501, loss is 0.12039771676063538\n",
      "epoch: 3 step: 502, loss is 0.15147089958190918\n",
      "epoch: 3 step: 503, loss is 0.5785462856292725\n",
      "epoch: 3 step: 504, loss is 0.3144591450691223\n",
      "epoch: 3 step: 505, loss is 0.21074123680591583\n",
      "epoch: 3 step: 506, loss is 0.19704265892505646\n",
      "epoch: 3 step: 507, loss is 0.2456825226545334\n",
      "epoch: 3 step: 508, loss is 0.25612765550613403\n",
      "epoch: 3 step: 509, loss is 0.25672441720962524\n",
      "epoch: 3 step: 510, loss is 0.24226218461990356\n",
      "epoch: 3 step: 511, loss is 0.27966415882110596\n",
      "epoch: 3 step: 512, loss is 0.1942416876554489\n",
      "epoch: 3 step: 513, loss is 0.1879466027021408\n",
      "epoch: 3 step: 514, loss is 0.13437071442604065\n",
      "epoch: 3 step: 515, loss is 0.21116487681865692\n",
      "epoch: 3 step: 516, loss is 0.12491168081760406\n",
      "epoch: 3 step: 517, loss is 0.2699912488460541\n",
      "epoch: 3 step: 518, loss is 0.16175350546836853\n",
      "epoch: 3 step: 519, loss is 0.1673453003168106\n",
      "epoch: 3 step: 520, loss is 0.24983057379722595\n",
      "epoch: 3 step: 521, loss is 0.20728875696659088\n",
      "epoch: 3 step: 522, loss is 0.353932648897171\n",
      "epoch: 3 step: 523, loss is 0.262235552072525\n",
      "epoch: 3 step: 524, loss is 0.3914780020713806\n",
      "epoch: 3 step: 525, loss is 0.31286588311195374\n",
      "epoch: 3 step: 526, loss is 0.2663889229297638\n",
      "epoch: 3 step: 527, loss is 0.29406607151031494\n",
      "epoch: 3 step: 528, loss is 0.2091985046863556\n",
      "epoch: 3 step: 529, loss is 0.2628079950809479\n",
      "epoch: 3 step: 530, loss is 0.19198065996170044\n",
      "epoch: 3 step: 531, loss is 0.34396928548812866\n",
      "epoch: 3 step: 532, loss is 0.2447984367609024\n",
      "epoch: 3 step: 533, loss is 0.2942405045032501\n",
      "epoch: 3 step: 534, loss is 0.1486000120639801\n",
      "epoch: 3 step: 535, loss is 0.34775757789611816\n",
      "epoch: 3 step: 536, loss is 0.14881765842437744\n",
      "epoch: 3 step: 537, loss is 0.25528037548065186\n",
      "epoch: 3 step: 538, loss is 0.15002448856830597\n",
      "epoch: 3 step: 539, loss is 0.23536042869091034\n",
      "epoch: 3 step: 540, loss is 0.1949538141489029\n",
      "epoch: 3 step: 541, loss is 0.15035179257392883\n",
      "epoch: 3 step: 542, loss is 0.21036404371261597\n",
      "epoch: 3 step: 543, loss is 0.3354751765727997\n",
      "epoch: 3 step: 544, loss is 0.2978983223438263\n",
      "epoch: 3 step: 545, loss is 0.2805497944355011\n",
      "epoch: 3 step: 546, loss is 0.17075514793395996\n",
      "epoch: 3 step: 547, loss is 0.20671331882476807\n",
      "epoch: 3 step: 548, loss is 0.14172686636447906\n",
      "epoch: 3 step: 549, loss is 0.3116310238838196\n",
      "epoch: 3 step: 550, loss is 0.2436256855726242\n",
      "epoch: 3 step: 551, loss is 0.12483248114585876\n",
      "epoch: 3 step: 552, loss is 0.2790902554988861\n",
      "epoch: 3 step: 553, loss is 0.17350710928440094\n",
      "epoch: 3 step: 554, loss is 0.23778794705867767\n",
      "epoch: 3 step: 555, loss is 0.3712061643600464\n",
      "epoch: 3 step: 556, loss is 0.2549647390842438\n",
      "epoch: 3 step: 557, loss is 0.2062319666147232\n",
      "epoch: 3 step: 558, loss is 0.2763579785823822\n",
      "epoch: 3 step: 559, loss is 0.2985028922557831\n",
      "epoch: 3 step: 560, loss is 0.24227355420589447\n",
      "epoch: 3 step: 561, loss is 0.23597612977027893\n",
      "epoch: 3 step: 562, loss is 0.22923247516155243\n",
      "epoch: 3 step: 563, loss is 0.25725239515304565\n",
      "epoch: 3 step: 564, loss is 0.3134654462337494\n",
      "epoch: 3 step: 565, loss is 0.33926987648010254\n",
      "epoch: 3 step: 566, loss is 0.1967783272266388\n",
      "epoch: 3 step: 567, loss is 0.3109542727470398\n",
      "epoch: 3 step: 568, loss is 0.15406593680381775\n",
      "epoch: 3 step: 569, loss is 0.3673361837863922\n",
      "epoch: 3 step: 570, loss is 0.43611419200897217\n",
      "epoch: 3 step: 571, loss is 0.20658189058303833\n",
      "epoch: 3 step: 572, loss is 0.23915283381938934\n",
      "epoch: 3 step: 573, loss is 0.2764171063899994\n",
      "epoch: 3 step: 574, loss is 0.20326370000839233\n",
      "epoch: 3 step: 575, loss is 0.1745389699935913\n",
      "epoch: 3 step: 576, loss is 0.15146765112876892\n",
      "epoch: 3 step: 577, loss is 0.3477553427219391\n",
      "epoch: 3 step: 578, loss is 0.15524864196777344\n",
      "epoch: 3 step: 579, loss is 0.21024636924266815\n",
      "epoch: 3 step: 580, loss is 0.14597386121749878\n",
      "epoch: 3 step: 581, loss is 0.2226378619670868\n",
      "epoch: 3 step: 582, loss is 0.3041127622127533\n",
      "epoch: 3 step: 583, loss is 0.3533148169517517\n",
      "epoch: 3 step: 584, loss is 0.301661878824234\n",
      "epoch: 3 step: 585, loss is 0.33009740710258484\n",
      "epoch: 3 step: 586, loss is 0.22606196999549866\n",
      "epoch: 3 step: 587, loss is 0.23237226903438568\n",
      "epoch: 3 step: 588, loss is 0.17870593070983887\n",
      "epoch: 3 step: 589, loss is 0.2215210348367691\n",
      "epoch: 3 step: 590, loss is 0.27436304092407227\n",
      "epoch: 3 step: 591, loss is 0.08431865274906158\n",
      "epoch: 3 step: 592, loss is 0.06989303976297379\n",
      "epoch: 3 step: 593, loss is 0.1644410640001297\n",
      "epoch: 3 step: 594, loss is 0.17798329889774323\n",
      "epoch: 3 step: 595, loss is 0.273809015750885\n",
      "epoch: 3 step: 596, loss is 0.22895212471485138\n",
      "epoch: 3 step: 597, loss is 0.2382282018661499\n",
      "epoch: 3 step: 598, loss is 0.24760442972183228\n",
      "epoch: 3 step: 599, loss is 0.19477121531963348\n",
      "epoch: 3 step: 600, loss is 0.2747182548046112\n",
      "epoch: 3 step: 601, loss is 0.1901387721300125\n",
      "epoch: 3 step: 602, loss is 0.44335460662841797\n",
      "epoch: 3 step: 603, loss is 0.3151603639125824\n",
      "epoch: 3 step: 604, loss is 0.1381136029958725\n",
      "epoch: 3 step: 605, loss is 0.1785667985677719\n",
      "epoch: 3 step: 606, loss is 0.21411195397377014\n",
      "epoch: 3 step: 607, loss is 0.16458876430988312\n",
      "epoch: 3 step: 608, loss is 0.1925264447927475\n",
      "epoch: 3 step: 609, loss is 0.15301793813705444\n",
      "epoch: 3 step: 610, loss is 0.2256385087966919\n",
      "epoch: 3 step: 611, loss is 0.224183589220047\n",
      "epoch: 3 step: 612, loss is 0.3470155894756317\n",
      "epoch: 3 step: 613, loss is 0.27844324707984924\n",
      "epoch: 3 step: 614, loss is 0.1344236135482788\n",
      "epoch: 3 step: 615, loss is 0.256957471370697\n",
      "epoch: 3 step: 616, loss is 0.44922998547554016\n",
      "epoch: 3 step: 617, loss is 0.16736142337322235\n",
      "epoch: 3 step: 618, loss is 0.2522036135196686\n",
      "epoch: 3 step: 619, loss is 0.23348481953144073\n",
      "epoch: 3 step: 620, loss is 0.2074291706085205\n",
      "epoch: 3 step: 621, loss is 0.25811368227005005\n",
      "epoch: 3 step: 622, loss is 0.13651043176651\n",
      "epoch: 3 step: 623, loss is 0.1944657862186432\n",
      "epoch: 3 step: 624, loss is 0.3235732913017273\n",
      "epoch: 3 step: 625, loss is 0.28355857729911804\n",
      "epoch: 3 step: 626, loss is 0.23459722101688385\n",
      "epoch: 3 step: 627, loss is 0.2982434332370758\n",
      "epoch: 3 step: 628, loss is 0.31697627902030945\n",
      "epoch: 3 step: 629, loss is 0.22181513905525208\n",
      "epoch: 3 step: 630, loss is 0.14591427147388458\n",
      "epoch: 3 step: 631, loss is 0.08420614898204803\n",
      "epoch: 3 step: 632, loss is 0.1060604453086853\n",
      "epoch: 3 step: 633, loss is 0.24826842546463013\n",
      "epoch: 3 step: 634, loss is 0.4925890862941742\n",
      "epoch: 3 step: 635, loss is 0.14772550761699677\n",
      "epoch: 3 step: 636, loss is 0.20425696671009064\n",
      "epoch: 3 step: 637, loss is 0.2114303857088089\n",
      "epoch: 3 step: 638, loss is 0.2666776776313782\n",
      "epoch: 3 step: 639, loss is 0.39138996601104736\n",
      "epoch: 3 step: 640, loss is 0.23134499788284302\n",
      "epoch: 3 step: 641, loss is 0.38702839612960815\n",
      "epoch: 3 step: 642, loss is 0.39728254079818726\n",
      "epoch: 3 step: 643, loss is 0.04829680919647217\n",
      "epoch: 3 step: 644, loss is 0.07487962394952774\n",
      "epoch: 3 step: 645, loss is 0.2598375976085663\n",
      "epoch: 3 step: 646, loss is 0.3726617097854614\n",
      "epoch: 3 step: 647, loss is 0.31572672724723816\n",
      "epoch: 3 step: 648, loss is 0.33714160323143005\n",
      "epoch: 3 step: 649, loss is 0.1976461410522461\n",
      "epoch: 3 step: 650, loss is 0.18513430655002594\n",
      "epoch: 3 step: 651, loss is 0.2930813431739807\n",
      "epoch: 3 step: 652, loss is 0.28842082619667053\n",
      "epoch: 3 step: 653, loss is 0.24392826855182648\n",
      "epoch: 3 step: 654, loss is 0.1761263757944107\n",
      "epoch: 3 step: 655, loss is 0.35610201954841614\n",
      "epoch: 3 step: 656, loss is 0.2258787751197815\n",
      "epoch: 3 step: 657, loss is 0.2730424404144287\n",
      "epoch: 3 step: 658, loss is 0.24053363502025604\n",
      "epoch: 3 step: 659, loss is 0.2338819056749344\n",
      "epoch: 3 step: 660, loss is 0.16501697897911072\n",
      "epoch: 3 step: 661, loss is 0.2478400319814682\n",
      "epoch: 3 step: 662, loss is 0.40429630875587463\n",
      "epoch: 3 step: 663, loss is 0.2846035957336426\n",
      "epoch: 3 step: 664, loss is 0.21736663579940796\n",
      "epoch: 3 step: 665, loss is 0.22901645302772522\n",
      "epoch: 3 step: 666, loss is 0.26382678747177124\n",
      "epoch: 3 step: 667, loss is 0.20135398209095\n",
      "epoch: 3 step: 668, loss is 0.22383415699005127\n",
      "epoch: 3 step: 669, loss is 0.15750752389431\n",
      "epoch: 3 step: 670, loss is 0.24457766115665436\n",
      "epoch: 3 step: 671, loss is 0.17398889362812042\n",
      "epoch: 3 step: 672, loss is 0.19799110293388367\n",
      "epoch: 3 step: 673, loss is 0.1981772929430008\n",
      "epoch: 3 step: 674, loss is 0.14740173518657684\n",
      "epoch: 3 step: 675, loss is 0.25694143772125244\n",
      "epoch: 3 step: 676, loss is 0.22208581864833832\n",
      "epoch: 3 step: 677, loss is 0.3067725598812103\n",
      "epoch: 3 step: 678, loss is 0.23251089453697205\n",
      "epoch: 3 step: 679, loss is 0.318127304315567\n",
      "epoch: 3 step: 680, loss is 0.26146402955055237\n",
      "epoch: 3 step: 681, loss is 0.31894370913505554\n",
      "epoch: 3 step: 682, loss is 0.12768696248531342\n",
      "epoch: 3 step: 683, loss is 0.20116302371025085\n",
      "epoch: 3 step: 684, loss is 0.4000265598297119\n",
      "epoch: 3 step: 685, loss is 0.400244802236557\n",
      "epoch: 3 step: 686, loss is 0.2591530978679657\n",
      "epoch: 3 step: 687, loss is 0.27992305159568787\n",
      "epoch: 3 step: 688, loss is 0.2237606942653656\n",
      "epoch: 3 step: 689, loss is 0.23236815631389618\n",
      "epoch: 3 step: 690, loss is 0.22166262567043304\n",
      "epoch: 3 step: 691, loss is 0.2723207175731659\n",
      "epoch: 3 step: 692, loss is 0.40458205342292786\n",
      "epoch: 3 step: 693, loss is 0.23053140938282013\n",
      "epoch: 3 step: 694, loss is 0.34489884972572327\n",
      "epoch: 3 step: 695, loss is 0.36478158831596375\n",
      "epoch: 3 step: 696, loss is 0.2470504492521286\n",
      "epoch: 3 step: 697, loss is 0.30045637488365173\n",
      "epoch: 3 step: 698, loss is 0.2400086671113968\n",
      "epoch: 3 step: 699, loss is 0.15772750973701477\n",
      "epoch: 3 step: 700, loss is 0.16069258749485016\n",
      "epoch: 3 step: 701, loss is 0.23552584648132324\n",
      "epoch: 3 step: 702, loss is 0.36604809761047363\n",
      "epoch: 3 step: 703, loss is 0.21958579123020172\n",
      "epoch: 3 step: 704, loss is 0.24859443306922913\n",
      "epoch: 3 step: 705, loss is 0.12188702821731567\n",
      "epoch: 3 step: 706, loss is 0.271788090467453\n",
      "epoch: 3 step: 707, loss is 0.31108179688453674\n",
      "epoch: 3 step: 708, loss is 0.31985318660736084\n",
      "epoch: 3 step: 709, loss is 0.2982994616031647\n",
      "epoch: 3 step: 710, loss is 0.3688313663005829\n",
      "epoch: 3 step: 711, loss is 0.334084689617157\n",
      "epoch: 3 step: 712, loss is 0.17538748681545258\n",
      "epoch: 3 step: 713, loss is 0.1801123172044754\n",
      "epoch: 3 step: 714, loss is 0.12925636768341064\n",
      "epoch: 3 step: 715, loss is 0.16038745641708374\n",
      "epoch: 3 step: 716, loss is 0.2901127338409424\n",
      "epoch: 3 step: 717, loss is 0.1664789766073227\n",
      "epoch: 3 step: 718, loss is 0.2670789062976837\n",
      "epoch: 3 step: 719, loss is 0.22921009361743927\n",
      "epoch: 3 step: 720, loss is 0.24962837994098663\n",
      "epoch: 3 step: 721, loss is 0.19507530331611633\n",
      "epoch: 3 step: 722, loss is 0.35494983196258545\n",
      "epoch: 3 step: 723, loss is 0.23410087823867798\n",
      "epoch: 3 step: 724, loss is 0.4182426929473877\n",
      "epoch: 3 step: 725, loss is 0.38646337389945984\n",
      "epoch: 3 step: 726, loss is 0.1701652556657791\n",
      "epoch: 3 step: 727, loss is 0.11493128538131714\n",
      "epoch: 3 step: 728, loss is 0.23203395307064056\n",
      "epoch: 3 step: 729, loss is 0.25774967670440674\n",
      "epoch: 3 step: 730, loss is 0.3454931676387787\n",
      "epoch: 3 step: 731, loss is 0.22790610790252686\n",
      "epoch: 3 step: 732, loss is 0.2809343636035919\n",
      "epoch: 3 step: 733, loss is 0.15523359179496765\n",
      "epoch: 3 step: 734, loss is 0.18353943526744843\n",
      "epoch: 3 step: 735, loss is 0.20646275579929352\n",
      "epoch: 3 step: 736, loss is 0.18880069255828857\n",
      "epoch: 3 step: 737, loss is 0.2980373501777649\n",
      "epoch: 3 step: 738, loss is 0.19339561462402344\n",
      "epoch: 3 step: 739, loss is 0.26353511214256287\n",
      "epoch: 3 step: 740, loss is 0.31785014271736145\n",
      "epoch: 3 step: 741, loss is 0.286624550819397\n",
      "epoch: 3 step: 742, loss is 0.2779698669910431\n",
      "epoch: 3 step: 743, loss is 0.4268715977668762\n",
      "epoch: 3 step: 744, loss is 0.2336808294057846\n",
      "epoch: 3 step: 745, loss is 0.31140831112861633\n",
      "epoch: 3 step: 746, loss is 0.15433566272258759\n",
      "epoch: 3 step: 747, loss is 0.2985987961292267\n",
      "epoch: 3 step: 748, loss is 0.24591854214668274\n",
      "epoch: 3 step: 749, loss is 0.23166139423847198\n",
      "epoch: 3 step: 750, loss is 0.24077385663986206\n",
      "epoch: 3 step: 751, loss is 0.2661454379558563\n",
      "epoch: 3 step: 752, loss is 0.14213892817497253\n",
      "epoch: 3 step: 753, loss is 0.1219443827867508\n",
      "epoch: 3 step: 754, loss is 0.21849462389945984\n",
      "epoch: 3 step: 755, loss is 0.10106746852397919\n",
      "epoch: 3 step: 756, loss is 0.12191161513328552\n",
      "epoch: 3 step: 757, loss is 0.22197550535202026\n",
      "epoch: 3 step: 758, loss is 0.2396063208580017\n",
      "epoch: 3 step: 759, loss is 0.2847093343734741\n",
      "epoch: 3 step: 760, loss is 0.16741199791431427\n",
      "epoch: 3 step: 761, loss is 0.18410971760749817\n",
      "epoch: 3 step: 762, loss is 0.16906584799289703\n",
      "epoch: 3 step: 763, loss is 0.18050633370876312\n",
      "epoch: 3 step: 764, loss is 0.24477775394916534\n",
      "epoch: 3 step: 765, loss is 0.11324566602706909\n",
      "epoch: 3 step: 766, loss is 0.24748972058296204\n",
      "epoch: 3 step: 767, loss is 0.2002868801355362\n",
      "epoch: 3 step: 768, loss is 0.2600303590297699\n",
      "epoch: 3 step: 769, loss is 0.19128230214118958\n",
      "epoch: 3 step: 770, loss is 0.54738450050354\n",
      "epoch: 3 step: 771, loss is 0.21824795007705688\n",
      "epoch: 3 step: 772, loss is 0.34509119391441345\n",
      "epoch: 3 step: 773, loss is 0.1709485650062561\n",
      "epoch: 3 step: 774, loss is 0.32889866828918457\n",
      "epoch: 3 step: 775, loss is 0.14907413721084595\n",
      "epoch: 3 step: 776, loss is 0.29436326026916504\n",
      "epoch: 3 step: 777, loss is 0.24401457607746124\n",
      "epoch: 3 step: 778, loss is 0.2294972836971283\n",
      "epoch: 3 step: 779, loss is 0.419670969247818\n",
      "epoch: 3 step: 780, loss is 0.29545092582702637\n",
      "epoch: 3 step: 781, loss is 0.25191259384155273\n",
      "epoch: 3 step: 782, loss is 0.21524669229984283\n",
      "epoch: 3 step: 783, loss is 0.16618916392326355\n",
      "epoch: 3 step: 784, loss is 0.20686811208724976\n",
      "epoch: 3 step: 785, loss is 0.3899713158607483\n",
      "epoch: 3 step: 786, loss is 0.3316408395767212\n",
      "epoch: 3 step: 787, loss is 0.3611542880535126\n",
      "epoch: 3 step: 788, loss is 0.2678825855255127\n",
      "epoch: 3 step: 789, loss is 0.1991366446018219\n",
      "epoch: 3 step: 790, loss is 0.26987534761428833\n",
      "epoch: 3 step: 791, loss is 0.274586021900177\n",
      "epoch: 3 step: 792, loss is 0.36326563358306885\n",
      "epoch: 3 step: 793, loss is 0.19338178634643555\n",
      "epoch: 3 step: 794, loss is 0.1813076287508011\n",
      "epoch: 3 step: 795, loss is 0.22488011419773102\n",
      "epoch: 3 step: 796, loss is 0.2399105429649353\n",
      "epoch: 3 step: 797, loss is 0.26745814085006714\n",
      "epoch: 3 step: 798, loss is 0.11669216305017471\n",
      "epoch: 3 step: 799, loss is 0.11060948669910431\n",
      "epoch: 3 step: 800, loss is 0.22783257067203522\n",
      "epoch: 3 step: 801, loss is 0.17716102302074432\n",
      "epoch: 3 step: 802, loss is 0.2412230223417282\n",
      "epoch: 3 step: 803, loss is 0.43961939215660095\n",
      "epoch: 3 step: 804, loss is 0.26271891593933105\n",
      "epoch: 3 step: 805, loss is 0.32938531041145325\n",
      "epoch: 3 step: 806, loss is 0.2836613655090332\n",
      "epoch: 3 step: 807, loss is 0.19772982597351074\n",
      "epoch: 3 step: 808, loss is 0.1223423182964325\n",
      "epoch: 3 step: 809, loss is 0.2873072624206543\n",
      "epoch: 3 step: 810, loss is 0.2177593857049942\n",
      "epoch: 3 step: 811, loss is 0.19583149254322052\n",
      "epoch: 3 step: 812, loss is 0.2746090590953827\n",
      "epoch: 3 step: 813, loss is 0.21441695094108582\n",
      "epoch: 3 step: 814, loss is 0.2927350699901581\n",
      "epoch: 3 step: 815, loss is 0.3441247045993805\n",
      "epoch: 3 step: 816, loss is 0.2168206423521042\n",
      "epoch: 3 step: 817, loss is 0.2579810917377472\n",
      "epoch: 3 step: 818, loss is 0.12173286080360413\n",
      "epoch: 3 step: 819, loss is 0.23565125465393066\n",
      "epoch: 3 step: 820, loss is 0.19552552700042725\n",
      "epoch: 3 step: 821, loss is 0.18344740569591522\n",
      "epoch: 3 step: 822, loss is 0.13123217225074768\n",
      "epoch: 3 step: 823, loss is 0.2627605199813843\n",
      "epoch: 3 step: 824, loss is 0.1703491508960724\n",
      "epoch: 3 step: 825, loss is 0.2780897617340088\n",
      "epoch: 3 step: 826, loss is 0.18786898255348206\n",
      "epoch: 3 step: 827, loss is 0.2104935497045517\n",
      "epoch: 3 step: 828, loss is 0.24118350446224213\n",
      "epoch: 3 step: 829, loss is 0.19420871138572693\n",
      "epoch: 3 step: 830, loss is 0.22356581687927246\n",
      "epoch: 3 step: 831, loss is 0.34176623821258545\n",
      "epoch: 3 step: 832, loss is 0.0551576167345047\n",
      "epoch: 3 step: 833, loss is 0.1339372992515564\n",
      "epoch: 3 step: 834, loss is 0.29751768708229065\n",
      "epoch: 3 step: 835, loss is 0.3736609220504761\n",
      "epoch: 3 step: 836, loss is 0.34068137407302856\n",
      "epoch: 3 step: 837, loss is 0.21085163950920105\n",
      "epoch: 3 step: 838, loss is 0.33302462100982666\n",
      "epoch: 3 step: 839, loss is 0.30515366792678833\n",
      "epoch: 3 step: 840, loss is 0.1805090606212616\n",
      "epoch: 3 step: 841, loss is 0.09937529265880585\n",
      "epoch: 3 step: 842, loss is 0.21413885056972504\n",
      "epoch: 3 step: 843, loss is 0.21943722665309906\n",
      "epoch: 3 step: 844, loss is 0.22911934554576874\n",
      "epoch: 3 step: 845, loss is 0.2958281934261322\n",
      "epoch: 3 step: 846, loss is 0.4333973526954651\n",
      "epoch: 3 step: 847, loss is 0.20951586961746216\n",
      "epoch: 3 step: 848, loss is 0.20996204018592834\n",
      "epoch: 3 step: 849, loss is 0.23227792978286743\n",
      "epoch: 3 step: 850, loss is 0.30275750160217285\n",
      "epoch: 3 step: 851, loss is 0.11906421184539795\n",
      "epoch: 3 step: 852, loss is 0.30143871903419495\n",
      "epoch: 3 step: 853, loss is 0.4170386791229248\n",
      "epoch: 3 step: 854, loss is 0.19017556309700012\n",
      "epoch: 3 step: 855, loss is 0.1937895268201828\n",
      "epoch: 3 step: 856, loss is 0.2774345278739929\n",
      "epoch: 3 step: 857, loss is 0.3138119578361511\n",
      "epoch: 3 step: 858, loss is 0.26635244488716125\n",
      "epoch: 3 step: 859, loss is 0.26122480630874634\n",
      "epoch: 3 step: 860, loss is 0.2022014558315277\n",
      "epoch: 3 step: 861, loss is 0.23662175238132477\n",
      "epoch: 3 step: 862, loss is 0.14643360674381256\n",
      "epoch: 3 step: 863, loss is 0.1813395917415619\n",
      "epoch: 3 step: 864, loss is 0.17646275460720062\n",
      "epoch: 3 step: 865, loss is 0.40808409452438354\n",
      "epoch: 3 step: 866, loss is 0.3563651442527771\n",
      "epoch: 3 step: 867, loss is 0.2907632291316986\n",
      "epoch: 3 step: 868, loss is 0.2008284479379654\n",
      "epoch: 3 step: 869, loss is 0.429701566696167\n",
      "epoch: 3 step: 870, loss is 0.4918662905693054\n",
      "epoch: 3 step: 871, loss is 0.2299230694770813\n",
      "epoch: 3 step: 872, loss is 0.12996353209018707\n",
      "epoch: 3 step: 873, loss is 0.22083154320716858\n",
      "epoch: 3 step: 874, loss is 0.20903833210468292\n",
      "epoch: 3 step: 875, loss is 0.24696272611618042\n",
      "epoch: 3 step: 876, loss is 0.13922984898090363\n",
      "epoch: 3 step: 877, loss is 0.3585492670536041\n",
      "epoch: 3 step: 878, loss is 0.16199429333209991\n",
      "epoch: 3 step: 879, loss is 0.22362542152404785\n",
      "epoch: 3 step: 880, loss is 0.2547643482685089\n",
      "epoch: 3 step: 881, loss is 0.19501721858978271\n",
      "epoch: 3 step: 882, loss is 0.1498652696609497\n",
      "epoch: 3 step: 883, loss is 0.21884745359420776\n",
      "epoch: 3 step: 884, loss is 0.3400147259235382\n",
      "epoch: 3 step: 885, loss is 0.23518282175064087\n",
      "epoch: 3 step: 886, loss is 0.2440413534641266\n",
      "epoch: 3 step: 887, loss is 0.395907461643219\n",
      "epoch: 3 step: 888, loss is 0.20893308520317078\n",
      "epoch: 3 step: 889, loss is 0.18057410418987274\n",
      "epoch: 3 step: 890, loss is 0.222471684217453\n",
      "epoch: 3 step: 891, loss is 0.2563708424568176\n",
      "epoch: 3 step: 892, loss is 0.4382578730583191\n",
      "epoch: 3 step: 893, loss is 0.37088099122047424\n",
      "epoch: 3 step: 894, loss is 0.16645334661006927\n",
      "epoch: 3 step: 895, loss is 0.19600781798362732\n",
      "epoch: 3 step: 896, loss is 0.3556284010410309\n",
      "epoch: 3 step: 897, loss is 0.28015416860580444\n",
      "epoch: 3 step: 898, loss is 0.21586759388446808\n",
      "epoch: 3 step: 899, loss is 0.28376129269599915\n",
      "epoch: 3 step: 900, loss is 0.2080276608467102\n",
      "epoch: 3 step: 901, loss is 0.19517722725868225\n",
      "epoch: 3 step: 902, loss is 0.23731668293476105\n",
      "epoch: 3 step: 903, loss is 0.3266269862651825\n",
      "epoch: 3 step: 904, loss is 0.11551825702190399\n",
      "epoch: 3 step: 905, loss is 0.29348599910736084\n",
      "epoch: 3 step: 906, loss is 0.23955471813678741\n",
      "epoch: 3 step: 907, loss is 0.2772890329360962\n",
      "epoch: 3 step: 908, loss is 0.20726719498634338\n",
      "epoch: 3 step: 909, loss is 0.3112775683403015\n",
      "epoch: 3 step: 910, loss is 0.1711387187242508\n",
      "epoch: 3 step: 911, loss is 0.3264547288417816\n",
      "epoch: 3 step: 912, loss is 0.12788204848766327\n",
      "epoch: 3 step: 913, loss is 0.24215571582317352\n",
      "epoch: 3 step: 914, loss is 0.24602453410625458\n",
      "epoch: 3 step: 915, loss is 0.10578372329473495\n",
      "epoch: 3 step: 916, loss is 0.18859612941741943\n",
      "epoch: 3 step: 917, loss is 0.18201658129692078\n",
      "epoch: 3 step: 918, loss is 0.2726996839046478\n",
      "epoch: 3 step: 919, loss is 0.41464611887931824\n",
      "epoch: 3 step: 920, loss is 0.10609562695026398\n",
      "epoch: 3 step: 921, loss is 0.2217930257320404\n",
      "epoch: 3 step: 922, loss is 0.35667744278907776\n",
      "epoch: 3 step: 923, loss is 0.11295188218355179\n",
      "epoch: 3 step: 924, loss is 0.2995504140853882\n",
      "epoch: 3 step: 925, loss is 0.12575757503509521\n",
      "epoch: 3 step: 926, loss is 0.13773830235004425\n",
      "epoch: 3 step: 927, loss is 0.21367453038692474\n",
      "epoch: 3 step: 928, loss is 0.2765049338340759\n",
      "epoch: 3 step: 929, loss is 0.2397085577249527\n",
      "epoch: 3 step: 930, loss is 0.30329760909080505\n",
      "epoch: 3 step: 931, loss is 0.16130250692367554\n",
      "epoch: 3 step: 932, loss is 0.16017742455005646\n",
      "epoch: 3 step: 933, loss is 0.18718229234218597\n",
      "epoch: 3 step: 934, loss is 0.12492738664150238\n",
      "epoch: 3 step: 935, loss is 0.32662883400917053\n",
      "epoch: 3 step: 936, loss is 0.34565362334251404\n",
      "epoch: 3 step: 937, loss is 0.25493624806404114\n",
      "epoch: 4 step: 1, loss is 0.18446412682533264\n",
      "epoch: 4 step: 2, loss is 0.18445509672164917\n",
      "epoch: 4 step: 3, loss is 0.16585345566272736\n",
      "epoch: 4 step: 4, loss is 0.31010088324546814\n",
      "epoch: 4 step: 5, loss is 0.17195086181163788\n",
      "epoch: 4 step: 6, loss is 0.18113015592098236\n",
      "epoch: 4 step: 7, loss is 0.18726155161857605\n",
      "epoch: 4 step: 8, loss is 0.22675688564777374\n",
      "epoch: 4 step: 9, loss is 0.19400718808174133\n",
      "epoch: 4 step: 10, loss is 0.11540936678647995\n",
      "epoch: 4 step: 11, loss is 0.25388994812965393\n",
      "epoch: 4 step: 12, loss is 0.42235642671585083\n",
      "epoch: 4 step: 13, loss is 0.15561260282993317\n",
      "epoch: 4 step: 14, loss is 0.4036031663417816\n",
      "epoch: 4 step: 15, loss is 0.13241854310035706\n",
      "epoch: 4 step: 16, loss is 0.2240029126405716\n",
      "epoch: 4 step: 17, loss is 0.14234764873981476\n",
      "epoch: 4 step: 18, loss is 0.16045141220092773\n",
      "epoch: 4 step: 19, loss is 0.20412366092205048\n",
      "epoch: 4 step: 20, loss is 0.20730483531951904\n",
      "epoch: 4 step: 21, loss is 0.18774181604385376\n",
      "epoch: 4 step: 22, loss is 0.1332540214061737\n",
      "epoch: 4 step: 23, loss is 0.20199225842952728\n",
      "epoch: 4 step: 24, loss is 0.3741637170314789\n",
      "epoch: 4 step: 25, loss is 0.20772092044353485\n",
      "epoch: 4 step: 26, loss is 0.0736343264579773\n",
      "epoch: 4 step: 27, loss is 0.2506752908229828\n",
      "epoch: 4 step: 28, loss is 0.19291621446609497\n",
      "epoch: 4 step: 29, loss is 0.28480735421180725\n",
      "epoch: 4 step: 30, loss is 0.2220691442489624\n",
      "epoch: 4 step: 31, loss is 0.2166823297739029\n",
      "epoch: 4 step: 32, loss is 0.09887491166591644\n",
      "epoch: 4 step: 33, loss is 0.10827454179525375\n",
      "epoch: 4 step: 34, loss is 0.20957447588443756\n",
      "epoch: 4 step: 35, loss is 0.16223935782909393\n",
      "epoch: 4 step: 36, loss is 0.1233857050538063\n",
      "epoch: 4 step: 37, loss is 0.31382066011428833\n",
      "epoch: 4 step: 38, loss is 0.18386401236057281\n",
      "epoch: 4 step: 39, loss is 0.155141219496727\n",
      "epoch: 4 step: 40, loss is 0.39162299036979675\n",
      "epoch: 4 step: 41, loss is 0.28750061988830566\n",
      "epoch: 4 step: 42, loss is 0.20574551820755005\n",
      "epoch: 4 step: 43, loss is 0.2221989631652832\n",
      "epoch: 4 step: 44, loss is 0.2281411588191986\n",
      "epoch: 4 step: 45, loss is 0.13152287900447845\n",
      "epoch: 4 step: 46, loss is 0.15005707740783691\n",
      "epoch: 4 step: 47, loss is 0.2410254329442978\n",
      "epoch: 4 step: 48, loss is 0.2259974628686905\n",
      "epoch: 4 step: 49, loss is 0.19558316469192505\n",
      "epoch: 4 step: 50, loss is 0.28775709867477417\n",
      "epoch: 4 step: 51, loss is 0.2034480720758438\n",
      "epoch: 4 step: 52, loss is 0.3149351179599762\n",
      "epoch: 4 step: 53, loss is 0.19567595422267914\n",
      "epoch: 4 step: 54, loss is 0.24656863510608673\n",
      "epoch: 4 step: 55, loss is 0.19957558810710907\n",
      "epoch: 4 step: 56, loss is 0.27916988730430603\n",
      "epoch: 4 step: 57, loss is 0.21222524344921112\n",
      "epoch: 4 step: 58, loss is 0.31182175874710083\n",
      "epoch: 4 step: 59, loss is 0.1206749975681305\n",
      "epoch: 4 step: 60, loss is 0.19576743245124817\n",
      "epoch: 4 step: 61, loss is 0.2195599228143692\n",
      "epoch: 4 step: 62, loss is 0.091277115046978\n",
      "epoch: 4 step: 63, loss is 0.21224680542945862\n",
      "epoch: 4 step: 64, loss is 0.16413681209087372\n",
      "epoch: 4 step: 65, loss is 0.11100199073553085\n",
      "epoch: 4 step: 66, loss is 0.1739315390586853\n",
      "epoch: 4 step: 67, loss is 0.3999781608581543\n",
      "epoch: 4 step: 68, loss is 0.2025454193353653\n",
      "epoch: 4 step: 69, loss is 0.1510380208492279\n",
      "epoch: 4 step: 70, loss is 0.15161968767642975\n",
      "epoch: 4 step: 71, loss is 0.18775108456611633\n",
      "epoch: 4 step: 72, loss is 0.08978509157896042\n",
      "epoch: 4 step: 73, loss is 0.23880746960639954\n",
      "epoch: 4 step: 74, loss is 0.32696864008903503\n",
      "epoch: 4 step: 75, loss is 0.41489213705062866\n",
      "epoch: 4 step: 76, loss is 0.1827169954776764\n",
      "epoch: 4 step: 77, loss is 0.16082143783569336\n",
      "epoch: 4 step: 78, loss is 0.22144204378128052\n",
      "epoch: 4 step: 79, loss is 0.16945600509643555\n",
      "epoch: 4 step: 80, loss is 0.25794729590415955\n",
      "epoch: 4 step: 81, loss is 0.16796018183231354\n",
      "epoch: 4 step: 82, loss is 0.2395811527967453\n",
      "epoch: 4 step: 83, loss is 0.2642601728439331\n",
      "epoch: 4 step: 84, loss is 0.17411953210830688\n",
      "epoch: 4 step: 85, loss is 0.42559534311294556\n",
      "epoch: 4 step: 86, loss is 0.17881357669830322\n",
      "epoch: 4 step: 87, loss is 0.08837785571813583\n",
      "epoch: 4 step: 88, loss is 0.1876353919506073\n",
      "epoch: 4 step: 89, loss is 0.3156512975692749\n",
      "epoch: 4 step: 90, loss is 0.14802861213684082\n",
      "epoch: 4 step: 91, loss is 0.20542490482330322\n",
      "epoch: 4 step: 92, loss is 0.2645316421985626\n",
      "epoch: 4 step: 93, loss is 0.25338849425315857\n",
      "epoch: 4 step: 94, loss is 0.21520976722240448\n",
      "epoch: 4 step: 95, loss is 0.19790329039096832\n",
      "epoch: 4 step: 96, loss is 0.13850238919258118\n",
      "epoch: 4 step: 97, loss is 0.20823177695274353\n",
      "epoch: 4 step: 98, loss is 0.20231330394744873\n",
      "epoch: 4 step: 99, loss is 0.1698818951845169\n",
      "epoch: 4 step: 100, loss is 0.19161976873874664\n",
      "epoch: 4 step: 101, loss is 0.5165585875511169\n",
      "epoch: 4 step: 102, loss is 0.2648940980434418\n",
      "epoch: 4 step: 103, loss is 0.1787351816892624\n",
      "epoch: 4 step: 104, loss is 0.2205098271369934\n",
      "epoch: 4 step: 105, loss is 0.21726445853710175\n",
      "epoch: 4 step: 106, loss is 0.1783982217311859\n",
      "epoch: 4 step: 107, loss is 0.15495294332504272\n",
      "epoch: 4 step: 108, loss is 0.22613053023815155\n",
      "epoch: 4 step: 109, loss is 0.23723697662353516\n",
      "epoch: 4 step: 110, loss is 0.25412505865097046\n",
      "epoch: 4 step: 111, loss is 0.19392552971839905\n",
      "epoch: 4 step: 112, loss is 0.3885995149612427\n",
      "epoch: 4 step: 113, loss is 0.29125022888183594\n",
      "epoch: 4 step: 114, loss is 0.14686457812786102\n",
      "epoch: 4 step: 115, loss is 0.18817220628261566\n",
      "epoch: 4 step: 116, loss is 0.19878509640693665\n",
      "epoch: 4 step: 117, loss is 0.06348948925733566\n",
      "epoch: 4 step: 118, loss is 0.29301488399505615\n",
      "epoch: 4 step: 119, loss is 0.09667810052633286\n",
      "epoch: 4 step: 120, loss is 0.2572409510612488\n",
      "epoch: 4 step: 121, loss is 0.23256522417068481\n",
      "epoch: 4 step: 122, loss is 0.1149320900440216\n",
      "epoch: 4 step: 123, loss is 0.24204276502132416\n",
      "epoch: 4 step: 124, loss is 0.19686487317085266\n",
      "epoch: 4 step: 125, loss is 0.4474616050720215\n",
      "epoch: 4 step: 126, loss is 0.10556913167238235\n",
      "epoch: 4 step: 127, loss is 0.27011024951934814\n",
      "epoch: 4 step: 128, loss is 0.31915050745010376\n",
      "epoch: 4 step: 129, loss is 0.2236555516719818\n",
      "epoch: 4 step: 130, loss is 0.2351904660463333\n",
      "epoch: 4 step: 131, loss is 0.20930199325084686\n",
      "epoch: 4 step: 132, loss is 0.14949768781661987\n",
      "epoch: 4 step: 133, loss is 0.32594436407089233\n",
      "epoch: 4 step: 134, loss is 0.22630161046981812\n",
      "epoch: 4 step: 135, loss is 0.18110720813274384\n",
      "epoch: 4 step: 136, loss is 0.19790630042552948\n",
      "epoch: 4 step: 137, loss is 0.14850139617919922\n",
      "epoch: 4 step: 138, loss is 0.14659222960472107\n",
      "epoch: 4 step: 139, loss is 0.17479024827480316\n",
      "epoch: 4 step: 140, loss is 0.10636415332555771\n",
      "epoch: 4 step: 141, loss is 0.21304653584957123\n",
      "epoch: 4 step: 142, loss is 0.32276925444602966\n",
      "epoch: 4 step: 143, loss is 0.2783588469028473\n",
      "epoch: 4 step: 144, loss is 0.1297968029975891\n",
      "epoch: 4 step: 145, loss is 0.06576766818761826\n",
      "epoch: 4 step: 146, loss is 0.17123599350452423\n",
      "epoch: 4 step: 147, loss is 0.21427656710147858\n",
      "epoch: 4 step: 148, loss is 0.07736517488956451\n",
      "epoch: 4 step: 149, loss is 0.19666045904159546\n",
      "epoch: 4 step: 150, loss is 0.23239396512508392\n",
      "epoch: 4 step: 151, loss is 0.15118466317653656\n",
      "epoch: 4 step: 152, loss is 0.18584054708480835\n",
      "epoch: 4 step: 153, loss is 0.24254772067070007\n",
      "epoch: 4 step: 154, loss is 0.2707238495349884\n",
      "epoch: 4 step: 155, loss is 0.1704094260931015\n",
      "epoch: 4 step: 156, loss is 0.375228613615036\n",
      "epoch: 4 step: 157, loss is 0.22015303373336792\n",
      "epoch: 4 step: 158, loss is 0.11651051789522171\n",
      "epoch: 4 step: 159, loss is 0.14051732420921326\n",
      "epoch: 4 step: 160, loss is 0.24942147731781006\n",
      "epoch: 4 step: 161, loss is 0.45017990469932556\n",
      "epoch: 4 step: 162, loss is 0.10074366629123688\n",
      "epoch: 4 step: 163, loss is 0.19878284633159637\n",
      "epoch: 4 step: 164, loss is 0.23690985143184662\n",
      "epoch: 4 step: 165, loss is 0.2710786461830139\n",
      "epoch: 4 step: 166, loss is 0.25480663776397705\n",
      "epoch: 4 step: 167, loss is 0.14490564167499542\n",
      "epoch: 4 step: 168, loss is 0.19192752242088318\n",
      "epoch: 4 step: 169, loss is 0.17981506884098053\n",
      "epoch: 4 step: 170, loss is 0.1750573366880417\n",
      "epoch: 4 step: 171, loss is 0.08473745733499527\n",
      "epoch: 4 step: 172, loss is 0.1912144124507904\n",
      "epoch: 4 step: 173, loss is 0.2604634761810303\n",
      "epoch: 4 step: 174, loss is 0.15702611207962036\n",
      "epoch: 4 step: 175, loss is 0.19725094735622406\n",
      "epoch: 4 step: 176, loss is 0.2250492125749588\n",
      "epoch: 4 step: 177, loss is 0.08443188667297363\n",
      "epoch: 4 step: 178, loss is 0.17843329906463623\n",
      "epoch: 4 step: 179, loss is 0.15645743906497955\n",
      "epoch: 4 step: 180, loss is 0.21685683727264404\n",
      "epoch: 4 step: 181, loss is 0.19895610213279724\n",
      "epoch: 4 step: 182, loss is 0.14987698197364807\n",
      "epoch: 4 step: 183, loss is 0.18487855792045593\n",
      "epoch: 4 step: 184, loss is 0.22526633739471436\n",
      "epoch: 4 step: 185, loss is 0.24836501479148865\n",
      "epoch: 4 step: 186, loss is 0.22602128982543945\n",
      "epoch: 4 step: 187, loss is 0.16418564319610596\n",
      "epoch: 4 step: 188, loss is 0.19955359399318695\n",
      "epoch: 4 step: 189, loss is 0.23948557674884796\n",
      "epoch: 4 step: 190, loss is 0.05517496541142464\n",
      "epoch: 4 step: 191, loss is 0.22333863377571106\n",
      "epoch: 4 step: 192, loss is 0.25858762860298157\n",
      "epoch: 4 step: 193, loss is 0.062015797942876816\n",
      "epoch: 4 step: 194, loss is 0.17312586307525635\n",
      "epoch: 4 step: 195, loss is 0.1328897625207901\n",
      "epoch: 4 step: 196, loss is 0.4259932041168213\n",
      "epoch: 4 step: 197, loss is 0.281117707490921\n",
      "epoch: 4 step: 198, loss is 0.1716250628232956\n",
      "epoch: 4 step: 199, loss is 0.1786949187517166\n",
      "epoch: 4 step: 200, loss is 0.17841115593910217\n",
      "epoch: 4 step: 201, loss is 0.062285516411066055\n",
      "epoch: 4 step: 202, loss is 0.12892158329486847\n",
      "epoch: 4 step: 203, loss is 0.13081413507461548\n",
      "epoch: 4 step: 204, loss is 0.11520889401435852\n",
      "epoch: 4 step: 205, loss is 0.11285275965929031\n",
      "epoch: 4 step: 206, loss is 0.22649742662906647\n",
      "epoch: 4 step: 207, loss is 0.16432666778564453\n",
      "epoch: 4 step: 208, loss is 0.26834774017333984\n",
      "epoch: 4 step: 209, loss is 0.21899516880512238\n",
      "epoch: 4 step: 210, loss is 0.2210673689842224\n",
      "epoch: 4 step: 211, loss is 0.11972934007644653\n",
      "epoch: 4 step: 212, loss is 0.19562973082065582\n",
      "epoch: 4 step: 213, loss is 0.242270827293396\n",
      "epoch: 4 step: 214, loss is 0.12269219011068344\n",
      "epoch: 4 step: 215, loss is 0.13543622195720673\n",
      "epoch: 4 step: 216, loss is 0.3223930597305298\n",
      "epoch: 4 step: 217, loss is 0.1550879031419754\n",
      "epoch: 4 step: 218, loss is 0.22009901702404022\n",
      "epoch: 4 step: 219, loss is 0.17913657426834106\n",
      "epoch: 4 step: 220, loss is 0.20254069566726685\n",
      "epoch: 4 step: 221, loss is 0.1306440830230713\n",
      "epoch: 4 step: 222, loss is 0.19423851370811462\n",
      "epoch: 4 step: 223, loss is 0.2764088809490204\n",
      "epoch: 4 step: 224, loss is 0.23042799532413483\n",
      "epoch: 4 step: 225, loss is 0.3354918658733368\n",
      "epoch: 4 step: 226, loss is 0.30257776379585266\n",
      "epoch: 4 step: 227, loss is 0.21414996683597565\n",
      "epoch: 4 step: 228, loss is 0.16104137897491455\n",
      "epoch: 4 step: 229, loss is 0.15452595055103302\n",
      "epoch: 4 step: 230, loss is 0.16266430914402008\n",
      "epoch: 4 step: 231, loss is 0.22643248736858368\n",
      "epoch: 4 step: 232, loss is 0.3600294589996338\n",
      "epoch: 4 step: 233, loss is 0.06987346708774567\n",
      "epoch: 4 step: 234, loss is 0.19558890163898468\n",
      "epoch: 4 step: 235, loss is 0.13952156901359558\n",
      "epoch: 4 step: 236, loss is 0.4381466507911682\n",
      "epoch: 4 step: 237, loss is 0.2792547941207886\n",
      "epoch: 4 step: 238, loss is 0.2922341525554657\n",
      "epoch: 4 step: 239, loss is 0.29622575640678406\n",
      "epoch: 4 step: 240, loss is 0.2974707782268524\n",
      "epoch: 4 step: 241, loss is 0.16340738534927368\n",
      "epoch: 4 step: 242, loss is 0.2967274487018585\n",
      "epoch: 4 step: 243, loss is 0.19563637673854828\n",
      "epoch: 4 step: 244, loss is 0.19533975422382355\n",
      "epoch: 4 step: 245, loss is 0.1834578663110733\n",
      "epoch: 4 step: 246, loss is 0.20549607276916504\n",
      "epoch: 4 step: 247, loss is 0.36160290241241455\n",
      "epoch: 4 step: 248, loss is 0.16382230818271637\n",
      "epoch: 4 step: 249, loss is 0.30794307589530945\n",
      "epoch: 4 step: 250, loss is 0.13421429693698883\n",
      "epoch: 4 step: 251, loss is 0.20998254418373108\n",
      "epoch: 4 step: 252, loss is 0.37637442350387573\n",
      "epoch: 4 step: 253, loss is 0.21564698219299316\n",
      "epoch: 4 step: 254, loss is 0.10437905788421631\n",
      "epoch: 4 step: 255, loss is 0.2412308305501938\n",
      "epoch: 4 step: 256, loss is 0.2349579781293869\n",
      "epoch: 4 step: 257, loss is 0.14836063981056213\n",
      "epoch: 4 step: 258, loss is 0.2674216628074646\n",
      "epoch: 4 step: 259, loss is 0.24160648882389069\n",
      "epoch: 4 step: 260, loss is 0.2754327058792114\n",
      "epoch: 4 step: 261, loss is 0.13791029155254364\n",
      "epoch: 4 step: 262, loss is 0.10911284387111664\n",
      "epoch: 4 step: 263, loss is 0.17579138278961182\n",
      "epoch: 4 step: 264, loss is 0.13443240523338318\n",
      "epoch: 4 step: 265, loss is 0.25780826807022095\n",
      "epoch: 4 step: 266, loss is 0.11312791705131531\n",
      "epoch: 4 step: 267, loss is 0.1418014019727707\n",
      "epoch: 4 step: 268, loss is 0.3085729777812958\n",
      "epoch: 4 step: 269, loss is 0.17484556138515472\n",
      "epoch: 4 step: 270, loss is 0.15902048349380493\n",
      "epoch: 4 step: 271, loss is 0.25558605790138245\n",
      "epoch: 4 step: 272, loss is 0.1014813780784607\n",
      "epoch: 4 step: 273, loss is 0.18176552653312683\n",
      "epoch: 4 step: 274, loss is 0.2533811628818512\n",
      "epoch: 4 step: 275, loss is 0.20338836312294006\n",
      "epoch: 4 step: 276, loss is 0.2697291076183319\n",
      "epoch: 4 step: 277, loss is 0.07695837318897247\n",
      "epoch: 4 step: 278, loss is 0.1979699581861496\n",
      "epoch: 4 step: 279, loss is 0.08963289111852646\n",
      "epoch: 4 step: 280, loss is 0.11812534928321838\n",
      "epoch: 4 step: 281, loss is 0.13734635710716248\n",
      "epoch: 4 step: 282, loss is 0.35273098945617676\n",
      "epoch: 4 step: 283, loss is 0.21369625627994537\n",
      "epoch: 4 step: 284, loss is 0.15523222088813782\n",
      "epoch: 4 step: 285, loss is 0.15383782982826233\n",
      "epoch: 4 step: 286, loss is 0.12584957480430603\n",
      "epoch: 4 step: 287, loss is 0.25426989793777466\n",
      "epoch: 4 step: 288, loss is 0.24936698377132416\n",
      "epoch: 4 step: 289, loss is 0.3477334678173065\n",
      "epoch: 4 step: 290, loss is 0.2557935416698456\n",
      "epoch: 4 step: 291, loss is 0.36098358035087585\n",
      "epoch: 4 step: 292, loss is 0.13341116905212402\n",
      "epoch: 4 step: 293, loss is 0.27947983145713806\n",
      "epoch: 4 step: 294, loss is 0.0814763680100441\n",
      "epoch: 4 step: 295, loss is 0.16086071729660034\n",
      "epoch: 4 step: 296, loss is 0.10774647444486618\n",
      "epoch: 4 step: 297, loss is 0.19275833666324615\n",
      "epoch: 4 step: 298, loss is 0.09544488042593002\n",
      "epoch: 4 step: 299, loss is 0.1170194149017334\n",
      "epoch: 4 step: 300, loss is 0.20293526351451874\n",
      "epoch: 4 step: 301, loss is 0.22488729655742645\n",
      "epoch: 4 step: 302, loss is 0.24525202810764313\n",
      "epoch: 4 step: 303, loss is 0.24244073033332825\n",
      "epoch: 4 step: 304, loss is 0.20196150243282318\n",
      "epoch: 4 step: 305, loss is 0.2064230591058731\n",
      "epoch: 4 step: 306, loss is 0.11388787627220154\n",
      "epoch: 4 step: 307, loss is 0.34255722165107727\n",
      "epoch: 4 step: 308, loss is 0.07737716287374496\n",
      "epoch: 4 step: 309, loss is 0.26434311270713806\n",
      "epoch: 4 step: 310, loss is 0.15338997542858124\n",
      "epoch: 4 step: 311, loss is 0.19318334758281708\n",
      "epoch: 4 step: 312, loss is 0.12135004252195358\n",
      "epoch: 4 step: 313, loss is 0.21893669664859772\n",
      "epoch: 4 step: 314, loss is 0.22055986523628235\n",
      "epoch: 4 step: 315, loss is 0.25989845395088196\n",
      "epoch: 4 step: 316, loss is 0.35234135389328003\n",
      "epoch: 4 step: 317, loss is 0.22603338956832886\n",
      "epoch: 4 step: 318, loss is 0.18479643762111664\n",
      "epoch: 4 step: 319, loss is 0.2842426002025604\n",
      "epoch: 4 step: 320, loss is 0.32341253757476807\n",
      "epoch: 4 step: 321, loss is 0.24112726747989655\n",
      "epoch: 4 step: 322, loss is 0.24112601578235626\n",
      "epoch: 4 step: 323, loss is 0.19871968030929565\n",
      "epoch: 4 step: 324, loss is 0.20482578873634338\n",
      "epoch: 4 step: 325, loss is 0.1710616946220398\n",
      "epoch: 4 step: 326, loss is 0.15345242619514465\n",
      "epoch: 4 step: 327, loss is 0.25948381423950195\n",
      "epoch: 4 step: 328, loss is 0.17074736952781677\n",
      "epoch: 4 step: 329, loss is 0.34588688611984253\n",
      "epoch: 4 step: 330, loss is 0.1848863959312439\n",
      "epoch: 4 step: 331, loss is 0.18491922318935394\n",
      "epoch: 4 step: 332, loss is 0.13430893421173096\n",
      "epoch: 4 step: 333, loss is 0.1336052119731903\n",
      "epoch: 4 step: 334, loss is 0.1655752807855606\n",
      "epoch: 4 step: 335, loss is 0.16596932709217072\n",
      "epoch: 4 step: 336, loss is 0.16099296510219574\n",
      "epoch: 4 step: 337, loss is 0.1789180487394333\n",
      "epoch: 4 step: 338, loss is 0.1495535671710968\n",
      "epoch: 4 step: 339, loss is 0.10668408870697021\n",
      "epoch: 4 step: 340, loss is 0.12173361331224442\n",
      "epoch: 4 step: 341, loss is 0.2497366964817047\n",
      "epoch: 4 step: 342, loss is 0.18326593935489655\n",
      "epoch: 4 step: 343, loss is 0.151457279920578\n",
      "epoch: 4 step: 344, loss is 0.26474952697753906\n",
      "epoch: 4 step: 345, loss is 0.20897352695465088\n",
      "epoch: 4 step: 346, loss is 0.15604442358016968\n",
      "epoch: 4 step: 347, loss is 0.13564586639404297\n",
      "epoch: 4 step: 348, loss is 0.27500462532043457\n",
      "epoch: 4 step: 349, loss is 0.11691837012767792\n",
      "epoch: 4 step: 350, loss is 0.1610058844089508\n",
      "epoch: 4 step: 351, loss is 0.1526530534029007\n",
      "epoch: 4 step: 352, loss is 0.13005225360393524\n",
      "epoch: 4 step: 353, loss is 0.42587506771087646\n",
      "epoch: 4 step: 354, loss is 0.5139478445053101\n",
      "epoch: 4 step: 355, loss is 0.3486229479312897\n",
      "epoch: 4 step: 356, loss is 0.19139450788497925\n",
      "epoch: 4 step: 357, loss is 0.16105885803699493\n",
      "epoch: 4 step: 358, loss is 0.27655768394470215\n",
      "epoch: 4 step: 359, loss is 0.3618004024028778\n",
      "epoch: 4 step: 360, loss is 0.3127302825450897\n",
      "epoch: 4 step: 361, loss is 0.226870596408844\n",
      "epoch: 4 step: 362, loss is 0.21291834115982056\n",
      "epoch: 4 step: 363, loss is 0.1058131605386734\n",
      "epoch: 4 step: 364, loss is 0.29889988899230957\n",
      "epoch: 4 step: 365, loss is 0.19899742305278778\n",
      "epoch: 4 step: 366, loss is 0.1856745034456253\n",
      "epoch: 4 step: 367, loss is 0.2537955939769745\n",
      "epoch: 4 step: 368, loss is 0.08790802210569382\n",
      "epoch: 4 step: 369, loss is 0.09534810483455658\n",
      "epoch: 4 step: 370, loss is 0.28044193983078003\n",
      "epoch: 4 step: 371, loss is 0.24316434562206268\n",
      "epoch: 4 step: 372, loss is 0.1377888321876526\n",
      "epoch: 4 step: 373, loss is 0.17545834183692932\n",
      "epoch: 4 step: 374, loss is 0.2292107492685318\n",
      "epoch: 4 step: 375, loss is 0.3230763375759125\n",
      "epoch: 4 step: 376, loss is 0.11520077288150787\n",
      "epoch: 4 step: 377, loss is 0.2965136766433716\n",
      "epoch: 4 step: 378, loss is 0.18414334952831268\n",
      "epoch: 4 step: 379, loss is 0.22462497651576996\n",
      "epoch: 4 step: 380, loss is 0.27811160683631897\n",
      "epoch: 4 step: 381, loss is 0.28556790947914124\n",
      "epoch: 4 step: 382, loss is 0.1284300982952118\n",
      "epoch: 4 step: 383, loss is 0.21334736049175262\n",
      "epoch: 4 step: 384, loss is 0.21812379360198975\n",
      "epoch: 4 step: 385, loss is 0.3329567015171051\n",
      "epoch: 4 step: 386, loss is 0.24511811137199402\n",
      "epoch: 4 step: 387, loss is 0.21106132864952087\n",
      "epoch: 4 step: 388, loss is 0.34788596630096436\n",
      "epoch: 4 step: 389, loss is 0.3295876979827881\n",
      "epoch: 4 step: 390, loss is 0.12603554129600525\n",
      "epoch: 4 step: 391, loss is 0.15413066744804382\n",
      "epoch: 4 step: 392, loss is 0.17542889714241028\n",
      "epoch: 4 step: 393, loss is 0.2594708502292633\n",
      "epoch: 4 step: 394, loss is 0.23345932364463806\n",
      "epoch: 4 step: 395, loss is 0.2851494550704956\n",
      "epoch: 4 step: 396, loss is 0.15753942728042603\n",
      "epoch: 4 step: 397, loss is 0.13215120136737823\n",
      "epoch: 4 step: 398, loss is 0.11953332275152206\n",
      "epoch: 4 step: 399, loss is 0.1291365623474121\n",
      "epoch: 4 step: 400, loss is 0.14494693279266357\n",
      "epoch: 4 step: 401, loss is 0.20426028966903687\n",
      "epoch: 4 step: 402, loss is 0.05133763328194618\n",
      "epoch: 4 step: 403, loss is 0.2273443639278412\n",
      "epoch: 4 step: 404, loss is 0.30686771869659424\n",
      "epoch: 4 step: 405, loss is 0.17759287357330322\n",
      "epoch: 4 step: 406, loss is 0.22863872349262238\n",
      "epoch: 4 step: 407, loss is 0.20994523167610168\n",
      "epoch: 4 step: 408, loss is 0.27159878611564636\n",
      "epoch: 4 step: 409, loss is 0.15678508579730988\n",
      "epoch: 4 step: 410, loss is 0.13411368429660797\n",
      "epoch: 4 step: 411, loss is 0.12499606609344482\n",
      "epoch: 4 step: 412, loss is 0.14156143367290497\n",
      "epoch: 4 step: 413, loss is 0.18502353131771088\n",
      "epoch: 4 step: 414, loss is 0.2498219758272171\n",
      "epoch: 4 step: 415, loss is 0.11870112270116806\n",
      "epoch: 4 step: 416, loss is 0.1445290893316269\n",
      "epoch: 4 step: 417, loss is 0.18217070400714874\n",
      "epoch: 4 step: 418, loss is 0.19112077355384827\n",
      "epoch: 4 step: 419, loss is 0.2767180800437927\n",
      "epoch: 4 step: 420, loss is 0.22825218737125397\n",
      "epoch: 4 step: 421, loss is 0.26376500725746155\n",
      "epoch: 4 step: 422, loss is 0.20630432665348053\n",
      "epoch: 4 step: 423, loss is 0.22560343146324158\n",
      "epoch: 4 step: 424, loss is 0.21264438331127167\n",
      "epoch: 4 step: 425, loss is 0.19446779787540436\n",
      "epoch: 4 step: 426, loss is 0.38607633113861084\n",
      "epoch: 4 step: 427, loss is 0.2988727390766144\n",
      "epoch: 4 step: 428, loss is 0.18337009847164154\n",
      "epoch: 4 step: 429, loss is 0.185133695602417\n",
      "epoch: 4 step: 430, loss is 0.35199952125549316\n",
      "epoch: 4 step: 431, loss is 0.3141060173511505\n",
      "epoch: 4 step: 432, loss is 0.17439588904380798\n",
      "epoch: 4 step: 433, loss is 0.2074851244688034\n",
      "epoch: 4 step: 434, loss is 0.14045186340808868\n",
      "epoch: 4 step: 435, loss is 0.08958600461483002\n",
      "epoch: 4 step: 436, loss is 0.22608047723770142\n",
      "epoch: 4 step: 437, loss is 0.23571622371673584\n",
      "epoch: 4 step: 438, loss is 0.12760333716869354\n",
      "epoch: 4 step: 439, loss is 0.34054702520370483\n",
      "epoch: 4 step: 440, loss is 0.2385793775320053\n",
      "epoch: 4 step: 441, loss is 0.29614490270614624\n",
      "epoch: 4 step: 442, loss is 0.18324843049049377\n",
      "epoch: 4 step: 443, loss is 0.09534209221601486\n",
      "epoch: 4 step: 444, loss is 0.1833970993757248\n",
      "epoch: 4 step: 445, loss is 0.21475979685783386\n",
      "epoch: 4 step: 446, loss is 0.33198416233062744\n",
      "epoch: 4 step: 447, loss is 0.29656559228897095\n",
      "epoch: 4 step: 448, loss is 0.17359519004821777\n",
      "epoch: 4 step: 449, loss is 0.19293946027755737\n",
      "epoch: 4 step: 450, loss is 0.15453796088695526\n",
      "epoch: 4 step: 451, loss is 0.18418045341968536\n",
      "epoch: 4 step: 452, loss is 0.2265022248029709\n",
      "epoch: 4 step: 453, loss is 0.3426058888435364\n",
      "epoch: 4 step: 454, loss is 0.23731279373168945\n",
      "epoch: 4 step: 455, loss is 0.1952088177204132\n",
      "epoch: 4 step: 456, loss is 0.18469126522541046\n",
      "epoch: 4 step: 457, loss is 0.2104773223400116\n",
      "epoch: 4 step: 458, loss is 0.42585840821266174\n",
      "epoch: 4 step: 459, loss is 0.23074966669082642\n",
      "epoch: 4 step: 460, loss is 0.30805453658103943\n",
      "epoch: 4 step: 461, loss is 0.17385493218898773\n",
      "epoch: 4 step: 462, loss is 0.13672225177288055\n",
      "epoch: 4 step: 463, loss is 0.2173464447259903\n",
      "epoch: 4 step: 464, loss is 0.10611487925052643\n",
      "epoch: 4 step: 465, loss is 0.17048470675945282\n",
      "epoch: 4 step: 466, loss is 0.32482242584228516\n",
      "epoch: 4 step: 467, loss is 0.17750157415866852\n",
      "epoch: 4 step: 468, loss is 0.11929163336753845\n",
      "epoch: 4 step: 469, loss is 0.2781248390674591\n",
      "epoch: 4 step: 470, loss is 0.13829952478408813\n",
      "epoch: 4 step: 471, loss is 0.3051193654537201\n",
      "epoch: 4 step: 472, loss is 0.18338294327259064\n",
      "epoch: 4 step: 473, loss is 0.1348956674337387\n",
      "epoch: 4 step: 474, loss is 0.2852329611778259\n",
      "epoch: 4 step: 475, loss is 0.429320365190506\n",
      "epoch: 4 step: 476, loss is 0.17753957211971283\n",
      "epoch: 4 step: 477, loss is 0.2314263880252838\n",
      "epoch: 4 step: 478, loss is 0.1412886381149292\n",
      "epoch: 4 step: 479, loss is 0.183048814535141\n",
      "epoch: 4 step: 480, loss is 0.2477365881204605\n",
      "epoch: 4 step: 481, loss is 0.1392705738544464\n",
      "epoch: 4 step: 482, loss is 0.13027900457382202\n",
      "epoch: 4 step: 483, loss is 0.20532065629959106\n",
      "epoch: 4 step: 484, loss is 0.11743343621492386\n",
      "epoch: 4 step: 485, loss is 0.1284032016992569\n",
      "epoch: 4 step: 486, loss is 0.2627623975276947\n",
      "epoch: 4 step: 487, loss is 0.11353231221437454\n",
      "epoch: 4 step: 488, loss is 0.10748792439699173\n",
      "epoch: 4 step: 489, loss is 0.2355809062719345\n",
      "epoch: 4 step: 490, loss is 0.09170907735824585\n",
      "epoch: 4 step: 491, loss is 0.33550187945365906\n",
      "epoch: 4 step: 492, loss is 0.1930353194475174\n",
      "epoch: 4 step: 493, loss is 0.23928922414779663\n",
      "epoch: 4 step: 494, loss is 0.23623253405094147\n",
      "epoch: 4 step: 495, loss is 0.17615818977355957\n",
      "epoch: 4 step: 496, loss is 0.2227017879486084\n",
      "epoch: 4 step: 497, loss is 0.22287783026695251\n",
      "epoch: 4 step: 498, loss is 0.2763853073120117\n",
      "epoch: 4 step: 499, loss is 0.10775041580200195\n",
      "epoch: 4 step: 500, loss is 0.17656873166561127\n",
      "epoch: 4 step: 501, loss is 0.2373616099357605\n",
      "epoch: 4 step: 502, loss is 0.3662625253200531\n",
      "epoch: 4 step: 503, loss is 0.1018211767077446\n",
      "epoch: 4 step: 504, loss is 0.3141617774963379\n",
      "epoch: 4 step: 505, loss is 0.31250837445259094\n",
      "epoch: 4 step: 506, loss is 0.36462756991386414\n",
      "epoch: 4 step: 507, loss is 0.21234045922756195\n",
      "epoch: 4 step: 508, loss is 0.1957325041294098\n",
      "epoch: 4 step: 509, loss is 0.26065489649772644\n",
      "epoch: 4 step: 510, loss is 0.24021302163600922\n",
      "epoch: 4 step: 511, loss is 0.21156702935695648\n",
      "epoch: 4 step: 512, loss is 0.1555008441209793\n",
      "epoch: 4 step: 513, loss is 0.33893445134162903\n",
      "epoch: 4 step: 514, loss is 0.2395678609609604\n",
      "epoch: 4 step: 515, loss is 0.2186051458120346\n",
      "epoch: 4 step: 516, loss is 0.11171837151050568\n",
      "epoch: 4 step: 517, loss is 0.18696613609790802\n",
      "epoch: 4 step: 518, loss is 0.1937476098537445\n",
      "epoch: 4 step: 519, loss is 0.2999083399772644\n",
      "epoch: 4 step: 520, loss is 0.12607316672801971\n",
      "epoch: 4 step: 521, loss is 0.1166001409292221\n",
      "epoch: 4 step: 522, loss is 0.2288692444562912\n",
      "epoch: 4 step: 523, loss is 0.19645261764526367\n",
      "epoch: 4 step: 524, loss is 0.24370619654655457\n",
      "epoch: 4 step: 525, loss is 0.24389922618865967\n",
      "epoch: 4 step: 526, loss is 0.13816006481647491\n",
      "epoch: 4 step: 527, loss is 0.2126198261976242\n",
      "epoch: 4 step: 528, loss is 0.16055813431739807\n",
      "epoch: 4 step: 529, loss is 0.2443961650133133\n",
      "epoch: 4 step: 530, loss is 0.19964227080345154\n",
      "epoch: 4 step: 531, loss is 0.2664858400821686\n",
      "epoch: 4 step: 532, loss is 0.22293801605701447\n",
      "epoch: 4 step: 533, loss is 0.26333922147750854\n",
      "epoch: 4 step: 534, loss is 0.1985412985086441\n",
      "epoch: 4 step: 535, loss is 0.25973716378211975\n",
      "epoch: 4 step: 536, loss is 0.15609745681285858\n",
      "epoch: 4 step: 537, loss is 0.12676218152046204\n",
      "epoch: 4 step: 538, loss is 0.3773573040962219\n",
      "epoch: 4 step: 539, loss is 0.22692817449569702\n",
      "epoch: 4 step: 540, loss is 0.3518221378326416\n",
      "epoch: 4 step: 541, loss is 0.16254453361034393\n",
      "epoch: 4 step: 542, loss is 0.20013226568698883\n",
      "epoch: 4 step: 543, loss is 0.24920549988746643\n",
      "epoch: 4 step: 544, loss is 0.2656841576099396\n",
      "epoch: 4 step: 545, loss is 0.1680307686328888\n",
      "epoch: 4 step: 546, loss is 0.18545368313789368\n",
      "epoch: 4 step: 547, loss is 0.09051249176263809\n",
      "epoch: 4 step: 548, loss is 0.1828458458185196\n",
      "epoch: 4 step: 549, loss is 0.14758308231830597\n",
      "epoch: 4 step: 550, loss is 0.08007369190454483\n",
      "epoch: 4 step: 551, loss is 0.14296609163284302\n",
      "epoch: 4 step: 552, loss is 0.35290026664733887\n",
      "epoch: 4 step: 553, loss is 0.5172988176345825\n",
      "epoch: 4 step: 554, loss is 0.12064754217863083\n",
      "epoch: 4 step: 555, loss is 0.2148149162530899\n",
      "epoch: 4 step: 556, loss is 0.20089925825595856\n",
      "epoch: 4 step: 557, loss is 0.2680044174194336\n",
      "epoch: 4 step: 558, loss is 0.1670796126127243\n",
      "epoch: 4 step: 559, loss is 0.16404718160629272\n",
      "epoch: 4 step: 560, loss is 0.16577771306037903\n",
      "epoch: 4 step: 561, loss is 0.2153848558664322\n",
      "epoch: 4 step: 562, loss is 0.1846633106470108\n",
      "epoch: 4 step: 563, loss is 0.2852088212966919\n",
      "epoch: 4 step: 564, loss is 0.10305892676115036\n",
      "epoch: 4 step: 565, loss is 0.2849175035953522\n",
      "epoch: 4 step: 566, loss is 0.22066357731819153\n",
      "epoch: 4 step: 567, loss is 0.23189039528369904\n",
      "epoch: 4 step: 568, loss is 0.10606006532907486\n",
      "epoch: 4 step: 569, loss is 0.23033945262432098\n",
      "epoch: 4 step: 570, loss is 0.2148151695728302\n",
      "epoch: 4 step: 571, loss is 0.13388170301914215\n",
      "epoch: 4 step: 572, loss is 0.19638805091381073\n",
      "epoch: 4 step: 573, loss is 0.27773967385292053\n",
      "epoch: 4 step: 574, loss is 0.2555992603302002\n",
      "epoch: 4 step: 575, loss is 0.18630725145339966\n",
      "epoch: 4 step: 576, loss is 0.23257675766944885\n",
      "epoch: 4 step: 577, loss is 0.16001833975315094\n",
      "epoch: 4 step: 578, loss is 0.20574496686458588\n",
      "epoch: 4 step: 579, loss is 0.18085063993930817\n",
      "epoch: 4 step: 580, loss is 0.18170975148677826\n",
      "epoch: 4 step: 581, loss is 0.2182377129793167\n",
      "epoch: 4 step: 582, loss is 0.09178300946950912\n",
      "epoch: 4 step: 583, loss is 0.2642512619495392\n",
      "epoch: 4 step: 584, loss is 0.15444986522197723\n",
      "epoch: 4 step: 585, loss is 0.1588757485151291\n",
      "epoch: 4 step: 586, loss is 0.11131703853607178\n",
      "epoch: 4 step: 587, loss is 0.12856684625148773\n",
      "epoch: 4 step: 588, loss is 0.25570812821388245\n",
      "epoch: 4 step: 589, loss is 0.21576659381389618\n",
      "epoch: 4 step: 590, loss is 0.19089533388614655\n",
      "epoch: 4 step: 591, loss is 0.1290544718503952\n",
      "epoch: 4 step: 592, loss is 0.2794714570045471\n",
      "epoch: 4 step: 593, loss is 0.2098110467195511\n",
      "epoch: 4 step: 594, loss is 0.2554493248462677\n",
      "epoch: 4 step: 595, loss is 0.271028995513916\n",
      "epoch: 4 step: 596, loss is 0.31350621581077576\n",
      "epoch: 4 step: 597, loss is 0.24092042446136475\n",
      "epoch: 4 step: 598, loss is 0.15693454444408417\n",
      "epoch: 4 step: 599, loss is 0.2976449728012085\n",
      "epoch: 4 step: 600, loss is 0.23608948290348053\n",
      "epoch: 4 step: 601, loss is 0.13670527935028076\n",
      "epoch: 4 step: 602, loss is 0.30950862169265747\n",
      "epoch: 4 step: 603, loss is 0.2132064253091812\n",
      "epoch: 4 step: 604, loss is 0.2741566002368927\n",
      "epoch: 4 step: 605, loss is 0.27884432673454285\n",
      "epoch: 4 step: 606, loss is 0.27694275975227356\n",
      "epoch: 4 step: 607, loss is 0.20645307004451752\n",
      "epoch: 4 step: 608, loss is 0.3182998597621918\n",
      "epoch: 4 step: 609, loss is 0.08476700633764267\n",
      "epoch: 4 step: 610, loss is 0.13329726457595825\n",
      "epoch: 4 step: 611, loss is 0.3162732720375061\n",
      "epoch: 4 step: 612, loss is 0.20646709203720093\n",
      "epoch: 4 step: 613, loss is 0.13645628094673157\n",
      "epoch: 4 step: 614, loss is 0.17120754718780518\n",
      "epoch: 4 step: 615, loss is 0.15398414433002472\n",
      "epoch: 4 step: 616, loss is 0.29979535937309265\n",
      "epoch: 4 step: 617, loss is 0.09202715754508972\n",
      "epoch: 4 step: 618, loss is 0.17655253410339355\n",
      "epoch: 4 step: 619, loss is 0.14797498285770416\n",
      "epoch: 4 step: 620, loss is 0.2044699639081955\n",
      "epoch: 4 step: 621, loss is 0.08416532725095749\n",
      "epoch: 4 step: 622, loss is 0.2756505310535431\n",
      "epoch: 4 step: 623, loss is 0.23655109107494354\n",
      "epoch: 4 step: 624, loss is 0.13309228420257568\n",
      "epoch: 4 step: 625, loss is 0.11150983721017838\n",
      "epoch: 4 step: 626, loss is 0.12916316092014313\n",
      "epoch: 4 step: 627, loss is 0.16669845581054688\n",
      "epoch: 4 step: 628, loss is 0.19027429819107056\n",
      "epoch: 4 step: 629, loss is 0.2669128477573395\n",
      "epoch: 4 step: 630, loss is 0.11886661499738693\n",
      "epoch: 4 step: 631, loss is 0.2254316508769989\n",
      "epoch: 4 step: 632, loss is 0.245937317609787\n",
      "epoch: 4 step: 633, loss is 0.18180501461029053\n",
      "epoch: 4 step: 634, loss is 0.10791206359863281\n",
      "epoch: 4 step: 635, loss is 0.21737471222877502\n",
      "epoch: 4 step: 636, loss is 0.11878639459609985\n",
      "epoch: 4 step: 637, loss is 0.325470894575119\n",
      "epoch: 4 step: 638, loss is 0.20820102095603943\n",
      "epoch: 4 step: 639, loss is 0.10263895988464355\n",
      "epoch: 4 step: 640, loss is 0.20257672667503357\n",
      "epoch: 4 step: 641, loss is 0.1347561478614807\n",
      "epoch: 4 step: 642, loss is 0.41198694705963135\n",
      "epoch: 4 step: 643, loss is 0.23049196600914001\n",
      "epoch: 4 step: 644, loss is 0.17899563908576965\n",
      "epoch: 4 step: 645, loss is 0.1999460905790329\n",
      "epoch: 4 step: 646, loss is 0.13453242182731628\n",
      "epoch: 4 step: 647, loss is 0.15009403228759766\n",
      "epoch: 4 step: 648, loss is 0.1959417313337326\n",
      "epoch: 4 step: 649, loss is 0.2739253044128418\n",
      "epoch: 4 step: 650, loss is 0.3425344228744507\n",
      "epoch: 4 step: 651, loss is 0.23667261004447937\n",
      "epoch: 4 step: 652, loss is 0.28895190358161926\n",
      "epoch: 4 step: 653, loss is 0.07555624097585678\n",
      "epoch: 4 step: 654, loss is 0.1879103034734726\n",
      "epoch: 4 step: 655, loss is 0.18573260307312012\n",
      "epoch: 4 step: 656, loss is 0.32626351714134216\n",
      "epoch: 4 step: 657, loss is 0.15587235987186432\n",
      "epoch: 4 step: 658, loss is 0.09921471029520035\n",
      "epoch: 4 step: 659, loss is 0.18657606840133667\n",
      "epoch: 4 step: 660, loss is 0.3627556562423706\n",
      "epoch: 4 step: 661, loss is 0.232116237282753\n",
      "epoch: 4 step: 662, loss is 0.22596390545368195\n",
      "epoch: 4 step: 663, loss is 0.23940111696720123\n",
      "epoch: 4 step: 664, loss is 0.22723710536956787\n",
      "epoch: 4 step: 665, loss is 0.2534298300743103\n",
      "epoch: 4 step: 666, loss is 0.14559949934482574\n",
      "epoch: 4 step: 667, loss is 0.14563722908496857\n",
      "epoch: 4 step: 668, loss is 0.16559115052223206\n",
      "epoch: 4 step: 669, loss is 0.16126015782356262\n",
      "epoch: 4 step: 670, loss is 0.2393501251935959\n",
      "epoch: 4 step: 671, loss is 0.2075691819190979\n",
      "epoch: 4 step: 672, loss is 0.18898430466651917\n",
      "epoch: 4 step: 673, loss is 0.2855807840824127\n",
      "epoch: 4 step: 674, loss is 0.2686435878276825\n",
      "epoch: 4 step: 675, loss is 0.2348327487707138\n",
      "epoch: 4 step: 676, loss is 0.11347982287406921\n",
      "epoch: 4 step: 677, loss is 0.14244940876960754\n",
      "epoch: 4 step: 678, loss is 0.21685709059238434\n",
      "epoch: 4 step: 679, loss is 0.17183324694633484\n",
      "epoch: 4 step: 680, loss is 0.23239465057849884\n",
      "epoch: 4 step: 681, loss is 0.11035480350255966\n",
      "epoch: 4 step: 682, loss is 0.16385723650455475\n",
      "epoch: 4 step: 683, loss is 0.07764105498790741\n",
      "epoch: 4 step: 684, loss is 0.15696509182453156\n",
      "epoch: 4 step: 685, loss is 0.23898908495903015\n",
      "epoch: 4 step: 686, loss is 0.11020060628652573\n",
      "epoch: 4 step: 687, loss is 0.050998054444789886\n",
      "epoch: 4 step: 688, loss is 0.12281942367553711\n",
      "epoch: 4 step: 689, loss is 0.10266458243131638\n",
      "epoch: 4 step: 690, loss is 0.1355609893798828\n",
      "epoch: 4 step: 691, loss is 0.1909399777650833\n",
      "epoch: 4 step: 692, loss is 0.339353084564209\n",
      "epoch: 4 step: 693, loss is 0.19764241576194763\n",
      "epoch: 4 step: 694, loss is 0.3091481029987335\n",
      "epoch: 4 step: 695, loss is 0.18352244794368744\n",
      "epoch: 4 step: 696, loss is 0.147952601313591\n",
      "epoch: 4 step: 697, loss is 0.1890636682510376\n",
      "epoch: 4 step: 698, loss is 0.08614270389080048\n",
      "epoch: 4 step: 699, loss is 0.17641790211200714\n",
      "epoch: 4 step: 700, loss is 0.30218377709388733\n",
      "epoch: 4 step: 701, loss is 0.26599064469337463\n",
      "epoch: 4 step: 702, loss is 0.3239439129829407\n",
      "epoch: 4 step: 703, loss is 0.26606231927871704\n",
      "epoch: 4 step: 704, loss is 0.24535910785198212\n",
      "epoch: 4 step: 705, loss is 0.35508057475090027\n",
      "epoch: 4 step: 706, loss is 0.1740867793560028\n",
      "epoch: 4 step: 707, loss is 0.3236302137374878\n",
      "epoch: 4 step: 708, loss is 0.26021426916122437\n",
      "epoch: 4 step: 709, loss is 0.19523096084594727\n",
      "epoch: 4 step: 710, loss is 0.351774662733078\n",
      "epoch: 4 step: 711, loss is 0.25356727838516235\n",
      "epoch: 4 step: 712, loss is 0.35808253288269043\n",
      "epoch: 4 step: 713, loss is 0.20692484080791473\n",
      "epoch: 4 step: 714, loss is 0.20866291224956512\n",
      "epoch: 4 step: 715, loss is 0.10486368089914322\n",
      "epoch: 4 step: 716, loss is 0.24783070385456085\n",
      "epoch: 4 step: 717, loss is 0.10701620578765869\n",
      "epoch: 4 step: 718, loss is 0.19417616724967957\n",
      "epoch: 4 step: 719, loss is 0.16274230182170868\n",
      "epoch: 4 step: 720, loss is 0.21262331306934357\n",
      "epoch: 4 step: 721, loss is 0.10837934166193008\n",
      "epoch: 4 step: 722, loss is 0.1743035763502121\n",
      "epoch: 4 step: 723, loss is 0.2239452600479126\n",
      "epoch: 4 step: 724, loss is 0.16389620304107666\n",
      "epoch: 4 step: 725, loss is 0.33808502554893494\n",
      "epoch: 4 step: 726, loss is 0.1658192276954651\n",
      "epoch: 4 step: 727, loss is 0.10780094563961029\n",
      "epoch: 4 step: 728, loss is 0.2801085412502289\n",
      "epoch: 4 step: 729, loss is 0.21516014635562897\n",
      "epoch: 4 step: 730, loss is 0.1427876502275467\n",
      "epoch: 4 step: 731, loss is 0.20257222652435303\n",
      "epoch: 4 step: 732, loss is 0.09385263174772263\n",
      "epoch: 4 step: 733, loss is 0.22266806662082672\n",
      "epoch: 4 step: 734, loss is 0.18216802179813385\n",
      "epoch: 4 step: 735, loss is 0.10158699005842209\n",
      "epoch: 4 step: 736, loss is 0.2558731436729431\n",
      "epoch: 4 step: 737, loss is 0.2306511402130127\n",
      "epoch: 4 step: 738, loss is 0.1924036294221878\n",
      "epoch: 4 step: 739, loss is 0.2758792042732239\n",
      "epoch: 4 step: 740, loss is 0.0840190127491951\n",
      "epoch: 4 step: 741, loss is 0.07263952493667603\n",
      "epoch: 4 step: 742, loss is 0.11201433837413788\n",
      "epoch: 4 step: 743, loss is 0.28686046600341797\n",
      "epoch: 4 step: 744, loss is 0.18389207124710083\n",
      "epoch: 4 step: 745, loss is 0.2122521996498108\n",
      "epoch: 4 step: 746, loss is 0.09202729165554047\n",
      "epoch: 4 step: 747, loss is 0.2258862406015396\n",
      "epoch: 4 step: 748, loss is 0.21969549357891083\n",
      "epoch: 4 step: 749, loss is 0.19091133773326874\n",
      "epoch: 4 step: 750, loss is 0.23072288930416107\n",
      "epoch: 4 step: 751, loss is 0.0793318897485733\n",
      "epoch: 4 step: 752, loss is 0.3561428487300873\n",
      "epoch: 4 step: 753, loss is 0.2005322426557541\n",
      "epoch: 4 step: 754, loss is 0.42138954997062683\n",
      "epoch: 4 step: 755, loss is 0.3035716116428375\n",
      "epoch: 4 step: 756, loss is 0.25237777829170227\n",
      "epoch: 4 step: 757, loss is 0.14523667097091675\n",
      "epoch: 4 step: 758, loss is 0.17638808488845825\n",
      "epoch: 4 step: 759, loss is 0.15464186668395996\n",
      "epoch: 4 step: 760, loss is 0.27249157428741455\n",
      "epoch: 4 step: 761, loss is 0.2290712594985962\n",
      "epoch: 4 step: 762, loss is 0.15235094726085663\n",
      "epoch: 4 step: 763, loss is 0.12995778024196625\n",
      "epoch: 4 step: 764, loss is 0.3045702278614044\n",
      "epoch: 4 step: 765, loss is 0.34563249349594116\n",
      "epoch: 4 step: 766, loss is 0.13904662430286407\n",
      "epoch: 4 step: 767, loss is 0.1856883317232132\n",
      "epoch: 4 step: 768, loss is 0.2911882698535919\n",
      "epoch: 4 step: 769, loss is 0.12111343443393707\n",
      "epoch: 4 step: 770, loss is 0.22746632993221283\n",
      "epoch: 4 step: 771, loss is 0.10979350656270981\n",
      "epoch: 4 step: 772, loss is 0.2559421956539154\n",
      "epoch: 4 step: 773, loss is 0.18634553253650665\n",
      "epoch: 4 step: 774, loss is 0.17304237186908722\n",
      "epoch: 4 step: 775, loss is 0.13244876265525818\n",
      "epoch: 4 step: 776, loss is 0.14249885082244873\n",
      "epoch: 4 step: 777, loss is 0.2940985858440399\n",
      "epoch: 4 step: 778, loss is 0.1432407945394516\n",
      "epoch: 4 step: 779, loss is 0.11853495240211487\n",
      "epoch: 4 step: 780, loss is 0.06280074268579483\n",
      "epoch: 4 step: 781, loss is 0.23026873171329498\n",
      "epoch: 4 step: 782, loss is 0.19737370312213898\n",
      "epoch: 4 step: 783, loss is 0.12197686731815338\n",
      "epoch: 4 step: 784, loss is 0.11746607720851898\n",
      "epoch: 4 step: 785, loss is 0.16272075474262238\n",
      "epoch: 4 step: 786, loss is 0.1681838482618332\n",
      "epoch: 4 step: 787, loss is 0.1460852324962616\n",
      "epoch: 4 step: 788, loss is 0.20616337656974792\n",
      "epoch: 4 step: 789, loss is 0.33532217144966125\n",
      "epoch: 4 step: 790, loss is 0.24063557386398315\n",
      "epoch: 4 step: 791, loss is 0.0985943153500557\n",
      "epoch: 4 step: 792, loss is 0.2542629539966583\n",
      "epoch: 4 step: 793, loss is 0.21885184943675995\n",
      "epoch: 4 step: 794, loss is 0.19172511994838715\n",
      "epoch: 4 step: 795, loss is 0.3421936631202698\n",
      "epoch: 4 step: 796, loss is 0.14119745790958405\n",
      "epoch: 4 step: 797, loss is 0.31595951318740845\n",
      "epoch: 4 step: 798, loss is 0.3678821325302124\n",
      "epoch: 4 step: 799, loss is 0.2406223714351654\n",
      "epoch: 4 step: 800, loss is 0.17374420166015625\n",
      "epoch: 4 step: 801, loss is 0.19085706770420074\n",
      "epoch: 4 step: 802, loss is 0.22780853509902954\n",
      "epoch: 4 step: 803, loss is 0.25546881556510925\n",
      "epoch: 4 step: 804, loss is 0.09137646108865738\n",
      "epoch: 4 step: 805, loss is 0.15444496273994446\n",
      "epoch: 4 step: 806, loss is 0.17169801890850067\n",
      "epoch: 4 step: 807, loss is 0.10214325040578842\n",
      "epoch: 4 step: 808, loss is 0.23340459167957306\n",
      "epoch: 4 step: 809, loss is 0.2712757885456085\n",
      "epoch: 4 step: 810, loss is 0.3371066451072693\n",
      "epoch: 4 step: 811, loss is 0.16311043500900269\n",
      "epoch: 4 step: 812, loss is 0.18727147579193115\n",
      "epoch: 4 step: 813, loss is 0.1321515440940857\n",
      "epoch: 4 step: 814, loss is 0.13388128578662872\n",
      "epoch: 4 step: 815, loss is 0.2979317009449005\n",
      "epoch: 4 step: 816, loss is 0.22239740192890167\n",
      "epoch: 4 step: 817, loss is 0.4063791036605835\n",
      "epoch: 4 step: 818, loss is 0.18402090668678284\n",
      "epoch: 4 step: 819, loss is 0.14661429822444916\n",
      "epoch: 4 step: 820, loss is 0.2752361297607422\n",
      "epoch: 4 step: 821, loss is 0.3083498775959015\n",
      "epoch: 4 step: 822, loss is 0.1956682801246643\n",
      "epoch: 4 step: 823, loss is 0.07184860110282898\n",
      "epoch: 4 step: 824, loss is 0.33007559180259705\n",
      "epoch: 4 step: 825, loss is 0.23422284424304962\n",
      "epoch: 4 step: 826, loss is 0.18403537571430206\n",
      "epoch: 4 step: 827, loss is 0.25028038024902344\n",
      "epoch: 4 step: 828, loss is 0.19441789388656616\n",
      "epoch: 4 step: 829, loss is 0.19294065237045288\n",
      "epoch: 4 step: 830, loss is 0.09605710208415985\n",
      "epoch: 4 step: 831, loss is 0.1521303653717041\n",
      "epoch: 4 step: 832, loss is 0.12372669577598572\n",
      "epoch: 4 step: 833, loss is 0.2004673182964325\n",
      "epoch: 4 step: 834, loss is 0.3097735047340393\n",
      "epoch: 4 step: 835, loss is 0.17775191366672516\n",
      "epoch: 4 step: 836, loss is 0.11884208023548126\n",
      "epoch: 4 step: 837, loss is 0.11321742832660675\n",
      "epoch: 4 step: 838, loss is 0.2802017033100128\n",
      "epoch: 4 step: 839, loss is 0.11908106505870819\n",
      "epoch: 4 step: 840, loss is 0.25223711133003235\n",
      "epoch: 4 step: 841, loss is 0.21014761924743652\n",
      "epoch: 4 step: 842, loss is 0.14121384918689728\n",
      "epoch: 4 step: 843, loss is 0.1597820669412613\n",
      "epoch: 4 step: 844, loss is 0.2848842740058899\n",
      "epoch: 4 step: 845, loss is 0.21962279081344604\n",
      "epoch: 4 step: 846, loss is 0.11541492491960526\n",
      "epoch: 4 step: 847, loss is 0.09844440966844559\n",
      "epoch: 4 step: 848, loss is 0.29449447989463806\n",
      "epoch: 4 step: 849, loss is 0.22205202281475067\n",
      "epoch: 4 step: 850, loss is 0.3287273347377777\n",
      "epoch: 4 step: 851, loss is 0.17216090857982635\n",
      "epoch: 4 step: 852, loss is 0.1406608670949936\n",
      "epoch: 4 step: 853, loss is 0.3708181381225586\n",
      "epoch: 4 step: 854, loss is 0.14383460581302643\n",
      "epoch: 4 step: 855, loss is 0.14072993397712708\n",
      "epoch: 4 step: 856, loss is 0.19626134634017944\n",
      "epoch: 4 step: 857, loss is 0.3127624988555908\n",
      "epoch: 4 step: 858, loss is 0.1426691859960556\n",
      "epoch: 4 step: 859, loss is 0.21882115304470062\n",
      "epoch: 4 step: 860, loss is 0.2222871035337448\n",
      "epoch: 4 step: 861, loss is 0.12856049835681915\n",
      "epoch: 4 step: 862, loss is 0.11483926326036453\n",
      "epoch: 4 step: 863, loss is 0.10541321337223053\n",
      "epoch: 4 step: 864, loss is 0.36969512701034546\n",
      "epoch: 4 step: 865, loss is 0.12124931812286377\n",
      "epoch: 4 step: 866, loss is 0.32318738102912903\n",
      "epoch: 4 step: 867, loss is 0.26221156120300293\n",
      "epoch: 4 step: 868, loss is 0.20064933598041534\n",
      "epoch: 4 step: 869, loss is 0.2245771586894989\n",
      "epoch: 4 step: 870, loss is 0.2450975924730301\n",
      "epoch: 4 step: 871, loss is 0.2559782862663269\n",
      "epoch: 4 step: 872, loss is 0.28391411900520325\n",
      "epoch: 4 step: 873, loss is 0.2265157699584961\n",
      "epoch: 4 step: 874, loss is 0.19578564167022705\n",
      "epoch: 4 step: 875, loss is 0.186636283993721\n",
      "epoch: 4 step: 876, loss is 0.16369684040546417\n",
      "epoch: 4 step: 877, loss is 0.23713524639606476\n",
      "epoch: 4 step: 878, loss is 0.17420348525047302\n",
      "epoch: 4 step: 879, loss is 0.33749836683273315\n",
      "epoch: 4 step: 880, loss is 0.23406481742858887\n",
      "epoch: 4 step: 881, loss is 0.09101312607526779\n",
      "epoch: 4 step: 882, loss is 0.1403810828924179\n",
      "epoch: 4 step: 883, loss is 0.13987059891223907\n",
      "epoch: 4 step: 884, loss is 0.1919192373752594\n",
      "epoch: 4 step: 885, loss is 0.15444070100784302\n",
      "epoch: 4 step: 886, loss is 0.1282968968153\n",
      "epoch: 4 step: 887, loss is 0.16887159645557404\n",
      "epoch: 4 step: 888, loss is 0.1641717404127121\n",
      "epoch: 4 step: 889, loss is 0.11976269632577896\n",
      "epoch: 4 step: 890, loss is 0.3125256896018982\n",
      "epoch: 4 step: 891, loss is 0.30655282735824585\n",
      "epoch: 4 step: 892, loss is 0.11196201294660568\n",
      "epoch: 4 step: 893, loss is 0.18851809203624725\n",
      "epoch: 4 step: 894, loss is 0.15385450422763824\n",
      "epoch: 4 step: 895, loss is 0.19769351184368134\n",
      "epoch: 4 step: 896, loss is 0.17681945860385895\n",
      "epoch: 4 step: 897, loss is 0.15243539214134216\n",
      "epoch: 4 step: 898, loss is 0.20227445662021637\n",
      "epoch: 4 step: 899, loss is 0.15307369828224182\n",
      "epoch: 4 step: 900, loss is 0.10263620316982269\n",
      "epoch: 4 step: 901, loss is 0.12178361415863037\n",
      "epoch: 4 step: 902, loss is 0.27245983481407166\n",
      "epoch: 4 step: 903, loss is 0.07520794868469238\n",
      "epoch: 4 step: 904, loss is 0.1602019965648651\n",
      "epoch: 4 step: 905, loss is 0.13863153755664825\n",
      "epoch: 4 step: 906, loss is 0.20941883325576782\n",
      "epoch: 4 step: 907, loss is 0.2809326648712158\n",
      "epoch: 4 step: 908, loss is 0.24619179964065552\n",
      "epoch: 4 step: 909, loss is 0.25220566987991333\n",
      "epoch: 4 step: 910, loss is 0.3947470486164093\n",
      "epoch: 4 step: 911, loss is 0.25081774592399597\n",
      "epoch: 4 step: 912, loss is 0.3387051224708557\n",
      "epoch: 4 step: 913, loss is 0.31106600165367126\n",
      "epoch: 4 step: 914, loss is 0.15247057378292084\n",
      "epoch: 4 step: 915, loss is 0.39184123277664185\n",
      "epoch: 4 step: 916, loss is 0.1679425686597824\n",
      "epoch: 4 step: 917, loss is 0.3411348760128021\n",
      "epoch: 4 step: 918, loss is 0.29321974515914917\n",
      "epoch: 4 step: 919, loss is 0.27387458086013794\n",
      "epoch: 4 step: 920, loss is 0.1081625372171402\n",
      "epoch: 4 step: 921, loss is 0.16881728172302246\n",
      "epoch: 4 step: 922, loss is 0.16236408054828644\n",
      "epoch: 4 step: 923, loss is 0.3077557682991028\n",
      "epoch: 4 step: 924, loss is 0.15380246937274933\n",
      "epoch: 4 step: 925, loss is 0.15392516553401947\n",
      "epoch: 4 step: 926, loss is 0.2005375623703003\n",
      "epoch: 4 step: 927, loss is 0.18829670548439026\n",
      "epoch: 4 step: 928, loss is 0.2718062400817871\n",
      "epoch: 4 step: 929, loss is 0.16889894008636475\n",
      "epoch: 4 step: 930, loss is 0.208247572183609\n",
      "epoch: 4 step: 931, loss is 0.19477161765098572\n",
      "epoch: 4 step: 932, loss is 0.33311450481414795\n",
      "epoch: 4 step: 933, loss is 0.20442675054073334\n",
      "epoch: 4 step: 934, loss is 0.0947536751627922\n",
      "epoch: 4 step: 935, loss is 0.1081828624010086\n",
      "epoch: 4 step: 936, loss is 0.27053654193878174\n",
      "epoch: 4 step: 937, loss is 0.26933300495147705\n",
      "epoch: 5 step: 1, loss is 0.14534729719161987\n",
      "epoch: 5 step: 2, loss is 0.1342441439628601\n",
      "epoch: 5 step: 3, loss is 0.24892549216747284\n",
      "epoch: 5 step: 4, loss is 0.15460118651390076\n",
      "epoch: 5 step: 5, loss is 0.1250317543745041\n",
      "epoch: 5 step: 6, loss is 0.1551486998796463\n",
      "epoch: 5 step: 7, loss is 0.15699128806591034\n",
      "epoch: 5 step: 8, loss is 0.16176673769950867\n",
      "epoch: 5 step: 9, loss is 0.08547006547451019\n",
      "epoch: 5 step: 10, loss is 0.29813045263290405\n",
      "epoch: 5 step: 11, loss is 0.11530417203903198\n",
      "epoch: 5 step: 12, loss is 0.21766918897628784\n",
      "epoch: 5 step: 13, loss is 0.210841566324234\n",
      "epoch: 5 step: 14, loss is 0.18317845463752747\n",
      "epoch: 5 step: 15, loss is 0.1197192594408989\n",
      "epoch: 5 step: 16, loss is 0.21923795342445374\n",
      "epoch: 5 step: 17, loss is 0.23125115036964417\n",
      "epoch: 5 step: 18, loss is 0.17113149166107178\n",
      "epoch: 5 step: 19, loss is 0.09895654767751694\n",
      "epoch: 5 step: 20, loss is 0.09263240545988083\n",
      "epoch: 5 step: 21, loss is 0.16233842074871063\n",
      "epoch: 5 step: 22, loss is 0.11973090469837189\n",
      "epoch: 5 step: 23, loss is 0.23097021877765656\n",
      "epoch: 5 step: 24, loss is 0.19644075632095337\n",
      "epoch: 5 step: 25, loss is 0.16968601942062378\n",
      "epoch: 5 step: 26, loss is 0.10849924385547638\n",
      "epoch: 5 step: 27, loss is 0.12756891548633575\n",
      "epoch: 5 step: 28, loss is 0.09989473223686218\n",
      "epoch: 5 step: 29, loss is 0.18512341380119324\n",
      "epoch: 5 step: 30, loss is 0.13399720191955566\n",
      "epoch: 5 step: 31, loss is 0.1771211475133896\n",
      "epoch: 5 step: 32, loss is 0.24993237853050232\n",
      "epoch: 5 step: 33, loss is 0.14547637104988098\n",
      "epoch: 5 step: 34, loss is 0.09691818058490753\n",
      "epoch: 5 step: 35, loss is 0.3592095375061035\n",
      "epoch: 5 step: 36, loss is 0.08624249696731567\n",
      "epoch: 5 step: 37, loss is 0.12554797530174255\n",
      "epoch: 5 step: 38, loss is 0.18446870148181915\n",
      "epoch: 5 step: 39, loss is 0.08136338740587234\n",
      "epoch: 5 step: 40, loss is 0.08157229423522949\n",
      "epoch: 5 step: 41, loss is 0.022763995453715324\n",
      "epoch: 5 step: 42, loss is 0.3066549301147461\n",
      "epoch: 5 step: 43, loss is 0.22506332397460938\n",
      "epoch: 5 step: 44, loss is 0.09516371786594391\n",
      "epoch: 5 step: 45, loss is 0.07965004444122314\n",
      "epoch: 5 step: 46, loss is 0.18398192524909973\n",
      "epoch: 5 step: 47, loss is 0.18909935653209686\n",
      "epoch: 5 step: 48, loss is 0.09494836628437042\n",
      "epoch: 5 step: 49, loss is 0.12919384241104126\n",
      "epoch: 5 step: 50, loss is 0.19175124168395996\n",
      "epoch: 5 step: 51, loss is 0.2333555966615677\n",
      "epoch: 5 step: 52, loss is 0.19396159052848816\n",
      "epoch: 5 step: 53, loss is 0.4123859405517578\n",
      "epoch: 5 step: 54, loss is 0.07644625008106232\n",
      "epoch: 5 step: 55, loss is 0.09391996264457703\n",
      "epoch: 5 step: 56, loss is 0.14382240176200867\n",
      "epoch: 5 step: 57, loss is 0.24673175811767578\n",
      "epoch: 5 step: 58, loss is 0.11020126193761826\n",
      "epoch: 5 step: 59, loss is 0.2616617679595947\n",
      "epoch: 5 step: 60, loss is 0.2888670563697815\n",
      "epoch: 5 step: 61, loss is 0.11765535920858383\n",
      "epoch: 5 step: 62, loss is 0.1273835450410843\n",
      "epoch: 5 step: 63, loss is 0.0861876904964447\n",
      "epoch: 5 step: 64, loss is 0.07377701252698898\n",
      "epoch: 5 step: 65, loss is 0.10770367830991745\n",
      "epoch: 5 step: 66, loss is 0.1311054676771164\n",
      "epoch: 5 step: 67, loss is 0.1568077802658081\n",
      "epoch: 5 step: 68, loss is 0.1457972526550293\n",
      "epoch: 5 step: 69, loss is 0.1570185273885727\n",
      "epoch: 5 step: 70, loss is 0.09480844438076019\n",
      "epoch: 5 step: 71, loss is 0.15306994318962097\n",
      "epoch: 5 step: 72, loss is 0.12551407516002655\n",
      "epoch: 5 step: 73, loss is 0.08348355442285538\n",
      "epoch: 5 step: 74, loss is 0.3403286635875702\n",
      "epoch: 5 step: 75, loss is 0.28695884346961975\n",
      "epoch: 5 step: 76, loss is 0.16413262486457825\n",
      "epoch: 5 step: 77, loss is 0.16126693785190582\n",
      "epoch: 5 step: 78, loss is 0.11109676212072372\n",
      "epoch: 5 step: 79, loss is 0.34241652488708496\n",
      "epoch: 5 step: 80, loss is 0.2952207922935486\n",
      "epoch: 5 step: 81, loss is 0.10053536295890808\n",
      "epoch: 5 step: 82, loss is 0.11620631814002991\n",
      "epoch: 5 step: 83, loss is 0.10584760457277298\n",
      "epoch: 5 step: 84, loss is 0.16664357483386993\n",
      "epoch: 5 step: 85, loss is 0.33716022968292236\n",
      "epoch: 5 step: 86, loss is 0.14003126323223114\n",
      "epoch: 5 step: 87, loss is 0.09665249288082123\n",
      "epoch: 5 step: 88, loss is 0.28206077218055725\n",
      "epoch: 5 step: 89, loss is 0.0830141007900238\n",
      "epoch: 5 step: 90, loss is 0.1887824386358261\n",
      "epoch: 5 step: 91, loss is 0.16721688210964203\n",
      "epoch: 5 step: 92, loss is 0.1171245202422142\n",
      "epoch: 5 step: 93, loss is 0.12552617490291595\n",
      "epoch: 5 step: 94, loss is 0.0610303059220314\n",
      "epoch: 5 step: 95, loss is 0.19958099722862244\n",
      "epoch: 5 step: 96, loss is 0.09710624814033508\n",
      "epoch: 5 step: 97, loss is 0.18040668964385986\n",
      "epoch: 5 step: 98, loss is 0.14647246897220612\n",
      "epoch: 5 step: 99, loss is 0.14998574554920197\n",
      "epoch: 5 step: 100, loss is 0.1465258151292801\n",
      "epoch: 5 step: 101, loss is 0.188567116856575\n",
      "epoch: 5 step: 102, loss is 0.16700564324855804\n",
      "epoch: 5 step: 103, loss is 0.12440312653779984\n",
      "epoch: 5 step: 104, loss is 0.23116432130336761\n",
      "epoch: 5 step: 105, loss is 0.05924678593873978\n",
      "epoch: 5 step: 106, loss is 0.20912021398544312\n",
      "epoch: 5 step: 107, loss is 0.060281120240688324\n",
      "epoch: 5 step: 108, loss is 0.11944781988859177\n",
      "epoch: 5 step: 109, loss is 0.16385892033576965\n",
      "epoch: 5 step: 110, loss is 0.25554656982421875\n",
      "epoch: 5 step: 111, loss is 0.15541890263557434\n",
      "epoch: 5 step: 112, loss is 0.08933490514755249\n",
      "epoch: 5 step: 113, loss is 0.041214700788259506\n",
      "epoch: 5 step: 114, loss is 0.13583165407180786\n",
      "epoch: 5 step: 115, loss is 0.06143445894122124\n",
      "epoch: 5 step: 116, loss is 0.15109796822071075\n",
      "epoch: 5 step: 117, loss is 0.3073170781135559\n",
      "epoch: 5 step: 118, loss is 0.18124108016490936\n",
      "epoch: 5 step: 119, loss is 0.17824384570121765\n",
      "epoch: 5 step: 120, loss is 0.08407088369131088\n",
      "epoch: 5 step: 121, loss is 0.08622273802757263\n",
      "epoch: 5 step: 122, loss is 0.15141502022743225\n",
      "epoch: 5 step: 123, loss is 0.22306124866008759\n",
      "epoch: 5 step: 124, loss is 0.18069124221801758\n",
      "epoch: 5 step: 125, loss is 0.23108772933483124\n",
      "epoch: 5 step: 126, loss is 0.27278056740760803\n",
      "epoch: 5 step: 127, loss is 0.18400397896766663\n",
      "epoch: 5 step: 128, loss is 0.17854349315166473\n",
      "epoch: 5 step: 129, loss is 0.11476308852434158\n",
      "epoch: 5 step: 130, loss is 0.1305834949016571\n",
      "epoch: 5 step: 131, loss is 0.262197881937027\n",
      "epoch: 5 step: 132, loss is 0.2173784375190735\n",
      "epoch: 5 step: 133, loss is 0.11526881903409958\n",
      "epoch: 5 step: 134, loss is 0.18418611586093903\n",
      "epoch: 5 step: 135, loss is 0.1670263558626175\n",
      "epoch: 5 step: 136, loss is 0.11020757257938385\n",
      "epoch: 5 step: 137, loss is 0.22121068835258484\n",
      "epoch: 5 step: 138, loss is 0.15257896482944489\n",
      "epoch: 5 step: 139, loss is 0.27531301975250244\n",
      "epoch: 5 step: 140, loss is 0.13624784350395203\n",
      "epoch: 5 step: 141, loss is 0.13868801295757294\n",
      "epoch: 5 step: 142, loss is 0.08037842065095901\n",
      "epoch: 5 step: 143, loss is 0.36178138852119446\n",
      "epoch: 5 step: 144, loss is 0.11881334334611893\n",
      "epoch: 5 step: 145, loss is 0.20151494443416595\n",
      "epoch: 5 step: 146, loss is 0.16446781158447266\n",
      "epoch: 5 step: 147, loss is 0.06452833861112595\n",
      "epoch: 5 step: 148, loss is 0.18070444464683533\n",
      "epoch: 5 step: 149, loss is 0.25903481245040894\n",
      "epoch: 5 step: 150, loss is 0.20054952800273895\n",
      "epoch: 5 step: 151, loss is 0.29878804087638855\n",
      "epoch: 5 step: 152, loss is 0.07637770473957062\n",
      "epoch: 5 step: 153, loss is 0.11617429554462433\n",
      "epoch: 5 step: 154, loss is 0.14854909479618073\n",
      "epoch: 5 step: 155, loss is 0.2702062427997589\n",
      "epoch: 5 step: 156, loss is 0.19994980096817017\n",
      "epoch: 5 step: 157, loss is 0.08565838634967804\n",
      "epoch: 5 step: 158, loss is 0.13694077730178833\n",
      "epoch: 5 step: 159, loss is 0.10343335568904877\n",
      "epoch: 5 step: 160, loss is 0.1251380443572998\n",
      "epoch: 5 step: 161, loss is 0.20599359273910522\n",
      "epoch: 5 step: 162, loss is 0.40977272391319275\n",
      "epoch: 5 step: 163, loss is 0.10436728596687317\n",
      "epoch: 5 step: 164, loss is 0.31766727566719055\n",
      "epoch: 5 step: 165, loss is 0.20842961966991425\n",
      "epoch: 5 step: 166, loss is 0.13058620691299438\n",
      "epoch: 5 step: 167, loss is 0.18283095955848694\n",
      "epoch: 5 step: 168, loss is 0.3168850839138031\n",
      "epoch: 5 step: 169, loss is 0.08838263899087906\n",
      "epoch: 5 step: 170, loss is 0.30571743845939636\n",
      "epoch: 5 step: 171, loss is 0.17094175517559052\n",
      "epoch: 5 step: 172, loss is 0.18298694491386414\n",
      "epoch: 5 step: 173, loss is 0.18304447829723358\n",
      "epoch: 5 step: 174, loss is 0.1971471756696701\n",
      "epoch: 5 step: 175, loss is 0.1117674708366394\n",
      "epoch: 5 step: 176, loss is 0.15383364260196686\n",
      "epoch: 5 step: 177, loss is 0.15741366147994995\n",
      "epoch: 5 step: 178, loss is 0.15631048381328583\n",
      "epoch: 5 step: 179, loss is 0.30110859870910645\n",
      "epoch: 5 step: 180, loss is 0.11888018995523453\n",
      "epoch: 5 step: 181, loss is 0.16597743332386017\n",
      "epoch: 5 step: 182, loss is 0.21865878999233246\n",
      "epoch: 5 step: 183, loss is 0.09454863518476486\n",
      "epoch: 5 step: 184, loss is 0.1455376148223877\n",
      "epoch: 5 step: 185, loss is 0.11861292272806168\n",
      "epoch: 5 step: 186, loss is 0.086048923432827\n",
      "epoch: 5 step: 187, loss is 0.1328815072774887\n",
      "epoch: 5 step: 188, loss is 0.22891762852668762\n",
      "epoch: 5 step: 189, loss is 0.10288114845752716\n",
      "epoch: 5 step: 190, loss is 0.10509616136550903\n",
      "epoch: 5 step: 191, loss is 0.14352451264858246\n",
      "epoch: 5 step: 192, loss is 0.27976325154304504\n",
      "epoch: 5 step: 193, loss is 0.10921982675790787\n",
      "epoch: 5 step: 194, loss is 0.21364624798297882\n",
      "epoch: 5 step: 195, loss is 0.2024945765733719\n",
      "epoch: 5 step: 196, loss is 0.27985402941703796\n",
      "epoch: 5 step: 197, loss is 0.131259024143219\n",
      "epoch: 5 step: 198, loss is 0.49922364950180054\n",
      "epoch: 5 step: 199, loss is 0.23329928517341614\n",
      "epoch: 5 step: 200, loss is 0.16789354383945465\n",
      "epoch: 5 step: 201, loss is 0.23054029047489166\n",
      "epoch: 5 step: 202, loss is 0.09505293518304825\n",
      "epoch: 5 step: 203, loss is 0.19322605431079865\n",
      "epoch: 5 step: 204, loss is 0.3044475317001343\n",
      "epoch: 5 step: 205, loss is 0.24739797413349152\n",
      "epoch: 5 step: 206, loss is 0.21637021005153656\n",
      "epoch: 5 step: 207, loss is 0.12927600741386414\n",
      "epoch: 5 step: 208, loss is 0.2541072368621826\n",
      "epoch: 5 step: 209, loss is 0.2492515593767166\n",
      "epoch: 5 step: 210, loss is 0.21771444380283356\n",
      "epoch: 5 step: 211, loss is 0.15069027245044708\n",
      "epoch: 5 step: 212, loss is 0.1757930964231491\n",
      "epoch: 5 step: 213, loss is 0.32005706429481506\n",
      "epoch: 5 step: 214, loss is 0.15905827283859253\n",
      "epoch: 5 step: 215, loss is 0.23941220343112946\n",
      "epoch: 5 step: 216, loss is 0.25551095604896545\n",
      "epoch: 5 step: 217, loss is 0.15212905406951904\n",
      "epoch: 5 step: 218, loss is 0.14523270726203918\n",
      "epoch: 5 step: 219, loss is 0.22522664070129395\n",
      "epoch: 5 step: 220, loss is 0.1298215091228485\n",
      "epoch: 5 step: 221, loss is 0.21042653918266296\n",
      "epoch: 5 step: 222, loss is 0.13760261237621307\n",
      "epoch: 5 step: 223, loss is 0.15264372527599335\n",
      "epoch: 5 step: 224, loss is 0.08598156273365021\n",
      "epoch: 5 step: 225, loss is 0.18457868695259094\n",
      "epoch: 5 step: 226, loss is 0.2602226734161377\n",
      "epoch: 5 step: 227, loss is 0.1435588002204895\n",
      "epoch: 5 step: 228, loss is 0.10987818241119385\n",
      "epoch: 5 step: 229, loss is 0.2072208821773529\n",
      "epoch: 5 step: 230, loss is 0.27060550451278687\n",
      "epoch: 5 step: 231, loss is 0.15540078282356262\n",
      "epoch: 5 step: 232, loss is 0.11667811870574951\n",
      "epoch: 5 step: 233, loss is 0.2970786392688751\n",
      "epoch: 5 step: 234, loss is 0.14309222996234894\n",
      "epoch: 5 step: 235, loss is 0.2706514000892639\n",
      "epoch: 5 step: 236, loss is 0.10233531892299652\n",
      "epoch: 5 step: 237, loss is 0.1903718262910843\n",
      "epoch: 5 step: 238, loss is 0.18168433010578156\n",
      "epoch: 5 step: 239, loss is 0.22044068574905396\n",
      "epoch: 5 step: 240, loss is 0.21267840266227722\n",
      "epoch: 5 step: 241, loss is 0.11421599239110947\n",
      "epoch: 5 step: 242, loss is 0.15537725389003754\n",
      "epoch: 5 step: 243, loss is 0.11263874918222427\n",
      "epoch: 5 step: 244, loss is 0.09620922058820724\n",
      "epoch: 5 step: 245, loss is 0.07373975217342377\n",
      "epoch: 5 step: 246, loss is 0.19095692038536072\n",
      "epoch: 5 step: 247, loss is 0.23222306370735168\n",
      "epoch: 5 step: 248, loss is 0.16013407707214355\n",
      "epoch: 5 step: 249, loss is 0.2001839131116867\n",
      "epoch: 5 step: 250, loss is 0.11522459238767624\n",
      "epoch: 5 step: 251, loss is 0.2329632192850113\n",
      "epoch: 5 step: 252, loss is 0.24077166616916656\n",
      "epoch: 5 step: 253, loss is 0.27083611488342285\n",
      "epoch: 5 step: 254, loss is 0.2924177646636963\n",
      "epoch: 5 step: 255, loss is 0.21908965706825256\n",
      "epoch: 5 step: 256, loss is 0.20703844726085663\n",
      "epoch: 5 step: 257, loss is 0.17943145334720612\n",
      "epoch: 5 step: 258, loss is 0.1394776701927185\n",
      "epoch: 5 step: 259, loss is 0.23059360682964325\n",
      "epoch: 5 step: 260, loss is 0.11256873607635498\n",
      "epoch: 5 step: 261, loss is 0.19453951716423035\n",
      "epoch: 5 step: 262, loss is 0.07679910957813263\n",
      "epoch: 5 step: 263, loss is 0.2146959751844406\n",
      "epoch: 5 step: 264, loss is 0.1463090032339096\n",
      "epoch: 5 step: 265, loss is 0.19049666821956635\n",
      "epoch: 5 step: 266, loss is 0.25256189703941345\n",
      "epoch: 5 step: 267, loss is 0.19170257449150085\n",
      "epoch: 5 step: 268, loss is 0.1924220472574234\n",
      "epoch: 5 step: 269, loss is 0.19791507720947266\n",
      "epoch: 5 step: 270, loss is 0.12376818060874939\n",
      "epoch: 5 step: 271, loss is 0.13938170671463013\n",
      "epoch: 5 step: 272, loss is 0.1891418695449829\n",
      "epoch: 5 step: 273, loss is 0.15532775223255157\n",
      "epoch: 5 step: 274, loss is 0.18176013231277466\n",
      "epoch: 5 step: 275, loss is 0.07064046710729599\n",
      "epoch: 5 step: 276, loss is 0.3376104235649109\n",
      "epoch: 5 step: 277, loss is 0.22502855956554413\n",
      "epoch: 5 step: 278, loss is 0.35288766026496887\n",
      "epoch: 5 step: 279, loss is 0.17495669424533844\n",
      "epoch: 5 step: 280, loss is 0.2616438865661621\n",
      "epoch: 5 step: 281, loss is 0.16932913661003113\n",
      "epoch: 5 step: 282, loss is 0.27450841665267944\n",
      "epoch: 5 step: 283, loss is 0.18905717134475708\n",
      "epoch: 5 step: 284, loss is 0.24626100063323975\n",
      "epoch: 5 step: 285, loss is 0.11513406783342361\n",
      "epoch: 5 step: 286, loss is 0.33005091547966003\n",
      "epoch: 5 step: 287, loss is 0.16052143275737762\n",
      "epoch: 5 step: 288, loss is 0.21805287897586823\n",
      "epoch: 5 step: 289, loss is 0.17188337445259094\n",
      "epoch: 5 step: 290, loss is 0.15082526206970215\n",
      "epoch: 5 step: 291, loss is 0.15704968571662903\n",
      "epoch: 5 step: 292, loss is 0.11006586253643036\n",
      "epoch: 5 step: 293, loss is 0.252317875623703\n",
      "epoch: 5 step: 294, loss is 0.17108231782913208\n",
      "epoch: 5 step: 295, loss is 0.1354692429304123\n",
      "epoch: 5 step: 296, loss is 0.1468363255262375\n",
      "epoch: 5 step: 297, loss is 0.2485455572605133\n",
      "epoch: 5 step: 298, loss is 0.22787123918533325\n",
      "epoch: 5 step: 299, loss is 0.13790790736675262\n",
      "epoch: 5 step: 300, loss is 0.19737018644809723\n",
      "epoch: 5 step: 301, loss is 0.2161085307598114\n",
      "epoch: 5 step: 302, loss is 0.3577101230621338\n",
      "epoch: 5 step: 303, loss is 0.22198154032230377\n",
      "epoch: 5 step: 304, loss is 0.0985882505774498\n",
      "epoch: 5 step: 305, loss is 0.0691094845533371\n",
      "epoch: 5 step: 306, loss is 0.18928496539592743\n",
      "epoch: 5 step: 307, loss is 0.16676607728004456\n",
      "epoch: 5 step: 308, loss is 0.19386211037635803\n",
      "epoch: 5 step: 309, loss is 0.16521647572517395\n",
      "epoch: 5 step: 310, loss is 0.15221446752548218\n",
      "epoch: 5 step: 311, loss is 0.15863573551177979\n",
      "epoch: 5 step: 312, loss is 0.12792298197746277\n",
      "epoch: 5 step: 313, loss is 0.2922244668006897\n",
      "epoch: 5 step: 314, loss is 0.14084847271442413\n",
      "epoch: 5 step: 315, loss is 0.17016904056072235\n",
      "epoch: 5 step: 316, loss is 0.25926461815834045\n",
      "epoch: 5 step: 317, loss is 0.09298957884311676\n",
      "epoch: 5 step: 318, loss is 0.15477141737937927\n",
      "epoch: 5 step: 319, loss is 0.29743558168411255\n",
      "epoch: 5 step: 320, loss is 0.12827619910240173\n",
      "epoch: 5 step: 321, loss is 0.12173309922218323\n",
      "epoch: 5 step: 322, loss is 0.07544657588005066\n",
      "epoch: 5 step: 323, loss is 0.16256068646907806\n",
      "epoch: 5 step: 324, loss is 0.22849445044994354\n",
      "epoch: 5 step: 325, loss is 0.06685812771320343\n",
      "epoch: 5 step: 326, loss is 0.2257474660873413\n",
      "epoch: 5 step: 327, loss is 0.18715012073516846\n",
      "epoch: 5 step: 328, loss is 0.2729395031929016\n",
      "epoch: 5 step: 329, loss is 0.3339381814002991\n",
      "epoch: 5 step: 330, loss is 0.17001278698444366\n",
      "epoch: 5 step: 331, loss is 0.17133137583732605\n",
      "epoch: 5 step: 332, loss is 0.26354366540908813\n",
      "epoch: 5 step: 333, loss is 0.1429879069328308\n",
      "epoch: 5 step: 334, loss is 0.14251166582107544\n",
      "epoch: 5 step: 335, loss is 0.12516435980796814\n",
      "epoch: 5 step: 336, loss is 0.18903106451034546\n",
      "epoch: 5 step: 337, loss is 0.2186373770236969\n",
      "epoch: 5 step: 338, loss is 0.19867314398288727\n",
      "epoch: 5 step: 339, loss is 0.11729089170694351\n",
      "epoch: 5 step: 340, loss is 0.15403907001018524\n",
      "epoch: 5 step: 341, loss is 0.18681983649730682\n",
      "epoch: 5 step: 342, loss is 0.07324010878801346\n",
      "epoch: 5 step: 343, loss is 0.14890293776988983\n",
      "epoch: 5 step: 344, loss is 0.0606456883251667\n",
      "epoch: 5 step: 345, loss is 0.14199146628379822\n",
      "epoch: 5 step: 346, loss is 0.23397628962993622\n",
      "epoch: 5 step: 347, loss is 0.131678506731987\n",
      "epoch: 5 step: 348, loss is 0.10215423256158829\n",
      "epoch: 5 step: 349, loss is 0.1390705108642578\n",
      "epoch: 5 step: 350, loss is 0.12196926772594452\n",
      "epoch: 5 step: 351, loss is 0.17926403880119324\n",
      "epoch: 5 step: 352, loss is 0.0852854922413826\n",
      "epoch: 5 step: 353, loss is 0.2667466998100281\n",
      "epoch: 5 step: 354, loss is 0.4261557459831238\n",
      "epoch: 5 step: 355, loss is 0.16808350384235382\n",
      "epoch: 5 step: 356, loss is 0.12788550555706024\n",
      "epoch: 5 step: 357, loss is 0.1324498951435089\n",
      "epoch: 5 step: 358, loss is 0.2561682462692261\n",
      "epoch: 5 step: 359, loss is 0.23101739585399628\n",
      "epoch: 5 step: 360, loss is 0.12880314886569977\n",
      "epoch: 5 step: 361, loss is 0.08494244515895844\n",
      "epoch: 5 step: 362, loss is 0.10108505934476852\n",
      "epoch: 5 step: 363, loss is 0.14585700631141663\n",
      "epoch: 5 step: 364, loss is 0.1427460014820099\n",
      "epoch: 5 step: 365, loss is 0.16117733716964722\n",
      "epoch: 5 step: 366, loss is 0.22669997811317444\n",
      "epoch: 5 step: 367, loss is 0.17278452217578888\n",
      "epoch: 5 step: 368, loss is 0.15625672042369843\n",
      "epoch: 5 step: 369, loss is 0.09895405918359756\n",
      "epoch: 5 step: 370, loss is 0.11741803586483002\n",
      "epoch: 5 step: 371, loss is 0.16148683428764343\n",
      "epoch: 5 step: 372, loss is 0.11105608940124512\n",
      "epoch: 5 step: 373, loss is 0.1486157327890396\n",
      "epoch: 5 step: 374, loss is 0.11134444922208786\n",
      "epoch: 5 step: 375, loss is 0.2242690771818161\n",
      "epoch: 5 step: 376, loss is 0.21852818131446838\n",
      "epoch: 5 step: 377, loss is 0.25981441140174866\n",
      "epoch: 5 step: 378, loss is 0.18347571790218353\n",
      "epoch: 5 step: 379, loss is 0.3232394754886627\n",
      "epoch: 5 step: 380, loss is 0.11673305928707123\n",
      "epoch: 5 step: 381, loss is 0.11931732296943665\n",
      "epoch: 5 step: 382, loss is 0.2704370617866516\n",
      "epoch: 5 step: 383, loss is 0.11984393745660782\n",
      "epoch: 5 step: 384, loss is 0.20599335432052612\n",
      "epoch: 5 step: 385, loss is 0.1776946634054184\n",
      "epoch: 5 step: 386, loss is 0.1729482114315033\n",
      "epoch: 5 step: 387, loss is 0.24532799422740936\n",
      "epoch: 5 step: 388, loss is 0.11972877383232117\n",
      "epoch: 5 step: 389, loss is 0.12618952989578247\n",
      "epoch: 5 step: 390, loss is 0.19711309671401978\n",
      "epoch: 5 step: 391, loss is 0.15512795746326447\n",
      "epoch: 5 step: 392, loss is 0.22180315852165222\n",
      "epoch: 5 step: 393, loss is 0.10175143927335739\n",
      "epoch: 5 step: 394, loss is 0.1300131231546402\n",
      "epoch: 5 step: 395, loss is 0.2092237025499344\n",
      "epoch: 5 step: 396, loss is 0.14403985440731049\n",
      "epoch: 5 step: 397, loss is 0.24212336540222168\n",
      "epoch: 5 step: 398, loss is 0.22182200849056244\n",
      "epoch: 5 step: 399, loss is 0.22927358746528625\n",
      "epoch: 5 step: 400, loss is 0.09435399621725082\n",
      "epoch: 5 step: 401, loss is 0.33336085081100464\n",
      "epoch: 5 step: 402, loss is 0.19143801927566528\n",
      "epoch: 5 step: 403, loss is 0.22042278945446014\n",
      "epoch: 5 step: 404, loss is 0.07085034996271133\n",
      "epoch: 5 step: 405, loss is 0.2677997350692749\n",
      "epoch: 5 step: 406, loss is 0.14539560675621033\n",
      "epoch: 5 step: 407, loss is 0.11388836055994034\n",
      "epoch: 5 step: 408, loss is 0.11876849085092545\n",
      "epoch: 5 step: 409, loss is 0.11894987523555756\n",
      "epoch: 5 step: 410, loss is 0.17081588506698608\n",
      "epoch: 5 step: 411, loss is 0.303811639547348\n",
      "epoch: 5 step: 412, loss is 0.11245575547218323\n",
      "epoch: 5 step: 413, loss is 0.27812111377716064\n",
      "epoch: 5 step: 414, loss is 0.1409304440021515\n",
      "epoch: 5 step: 415, loss is 0.18840181827545166\n",
      "epoch: 5 step: 416, loss is 0.22715161740779877\n",
      "epoch: 5 step: 417, loss is 0.04866364598274231\n",
      "epoch: 5 step: 418, loss is 0.19874262809753418\n",
      "epoch: 5 step: 419, loss is 0.23342092335224152\n",
      "epoch: 5 step: 420, loss is 0.11570511758327484\n",
      "epoch: 5 step: 421, loss is 0.21530400216579437\n",
      "epoch: 5 step: 422, loss is 0.09534045308828354\n",
      "epoch: 5 step: 423, loss is 0.11714067310094833\n",
      "epoch: 5 step: 424, loss is 0.10881529748439789\n",
      "epoch: 5 step: 425, loss is 0.3088337779045105\n",
      "epoch: 5 step: 426, loss is 0.11103235930204391\n",
      "epoch: 5 step: 427, loss is 0.10719291120767593\n",
      "epoch: 5 step: 428, loss is 0.14269278943538666\n",
      "epoch: 5 step: 429, loss is 0.07552268356084824\n",
      "epoch: 5 step: 430, loss is 0.15302136540412903\n",
      "epoch: 5 step: 431, loss is 0.05962210148572922\n",
      "epoch: 5 step: 432, loss is 0.1439584195613861\n",
      "epoch: 5 step: 433, loss is 0.08740780502557755\n",
      "epoch: 5 step: 434, loss is 0.1552063226699829\n",
      "epoch: 5 step: 435, loss is 0.13757027685642242\n",
      "epoch: 5 step: 436, loss is 0.16421008110046387\n",
      "epoch: 5 step: 437, loss is 0.09324415028095245\n",
      "epoch: 5 step: 438, loss is 0.2767217457294464\n",
      "epoch: 5 step: 439, loss is 0.4335637092590332\n",
      "epoch: 5 step: 440, loss is 0.1285635381937027\n",
      "epoch: 5 step: 441, loss is 0.2529361844062805\n",
      "epoch: 5 step: 442, loss is 0.1637101173400879\n",
      "epoch: 5 step: 443, loss is 0.10719594359397888\n",
      "epoch: 5 step: 444, loss is 0.1894300878047943\n",
      "epoch: 5 step: 445, loss is 0.08188999444246292\n",
      "epoch: 5 step: 446, loss is 0.1514207273721695\n",
      "epoch: 5 step: 447, loss is 0.2532484233379364\n",
      "epoch: 5 step: 448, loss is 0.09386460483074188\n",
      "epoch: 5 step: 449, loss is 0.3463553190231323\n",
      "epoch: 5 step: 450, loss is 0.06675755977630615\n",
      "epoch: 5 step: 451, loss is 0.134710431098938\n",
      "epoch: 5 step: 452, loss is 0.14030000567436218\n",
      "epoch: 5 step: 453, loss is 0.19662974774837494\n",
      "epoch: 5 step: 454, loss is 0.19985485076904297\n",
      "epoch: 5 step: 455, loss is 0.41958969831466675\n",
      "epoch: 5 step: 456, loss is 0.150121808052063\n",
      "epoch: 5 step: 457, loss is 0.347434401512146\n",
      "epoch: 5 step: 458, loss is 0.1339213103055954\n",
      "epoch: 5 step: 459, loss is 0.26198703050613403\n",
      "epoch: 5 step: 460, loss is 0.17057429254055023\n",
      "epoch: 5 step: 461, loss is 0.12149106711149216\n",
      "epoch: 5 step: 462, loss is 0.2567344307899475\n",
      "epoch: 5 step: 463, loss is 0.1952308714389801\n",
      "epoch: 5 step: 464, loss is 0.17795062065124512\n",
      "epoch: 5 step: 465, loss is 0.16768990457057953\n",
      "epoch: 5 step: 466, loss is 0.30401721596717834\n",
      "epoch: 5 step: 467, loss is 0.24943678081035614\n",
      "epoch: 5 step: 468, loss is 0.20451387763023376\n",
      "epoch: 5 step: 469, loss is 0.10922735184431076\n",
      "epoch: 5 step: 470, loss is 0.12063717842102051\n",
      "epoch: 5 step: 471, loss is 0.13843987882137299\n",
      "epoch: 5 step: 472, loss is 0.06382086873054504\n",
      "epoch: 5 step: 473, loss is 0.29464587569236755\n",
      "epoch: 5 step: 474, loss is 0.1416068971157074\n",
      "epoch: 5 step: 475, loss is 0.23006227612495422\n",
      "epoch: 5 step: 476, loss is 0.19856451451778412\n",
      "epoch: 5 step: 477, loss is 0.24697518348693848\n",
      "epoch: 5 step: 478, loss is 0.1263485848903656\n",
      "epoch: 5 step: 479, loss is 0.13127121329307556\n",
      "epoch: 5 step: 480, loss is 0.1634349673986435\n",
      "epoch: 5 step: 481, loss is 0.14890949428081512\n",
      "epoch: 5 step: 482, loss is 0.10171819478273392\n",
      "epoch: 5 step: 483, loss is 0.0990365669131279\n",
      "epoch: 5 step: 484, loss is 0.2855896055698395\n",
      "epoch: 5 step: 485, loss is 0.1406141221523285\n",
      "epoch: 5 step: 486, loss is 0.14191670715808868\n",
      "epoch: 5 step: 487, loss is 0.19456785917282104\n",
      "epoch: 5 step: 488, loss is 0.05678471550345421\n",
      "epoch: 5 step: 489, loss is 0.12416265904903412\n",
      "epoch: 5 step: 490, loss is 0.19033707678318024\n",
      "epoch: 5 step: 491, loss is 0.1336268186569214\n",
      "epoch: 5 step: 492, loss is 0.4324031174182892\n",
      "epoch: 5 step: 493, loss is 0.14661288261413574\n",
      "epoch: 5 step: 494, loss is 0.12838000059127808\n",
      "epoch: 5 step: 495, loss is 0.22751116752624512\n",
      "epoch: 5 step: 496, loss is 0.1076657846570015\n",
      "epoch: 5 step: 497, loss is 0.12370666116476059\n",
      "epoch: 5 step: 498, loss is 0.35449811816215515\n",
      "epoch: 5 step: 499, loss is 0.283557653427124\n",
      "epoch: 5 step: 500, loss is 0.355599969625473\n",
      "epoch: 5 step: 501, loss is 0.23432229459285736\n",
      "epoch: 5 step: 502, loss is 0.1943524032831192\n",
      "epoch: 5 step: 503, loss is 0.07823242247104645\n",
      "epoch: 5 step: 504, loss is 0.19309577345848083\n",
      "epoch: 5 step: 505, loss is 0.12910215556621552\n",
      "epoch: 5 step: 506, loss is 0.10942083597183228\n",
      "epoch: 5 step: 507, loss is 0.18787740170955658\n",
      "epoch: 5 step: 508, loss is 0.2879627048969269\n",
      "epoch: 5 step: 509, loss is 0.15991094708442688\n",
      "epoch: 5 step: 510, loss is 0.19231262803077698\n",
      "epoch: 5 step: 511, loss is 0.09204895049333572\n",
      "epoch: 5 step: 512, loss is 0.24954792857170105\n",
      "epoch: 5 step: 513, loss is 0.16395477950572968\n",
      "epoch: 5 step: 514, loss is 0.24914495646953583\n",
      "epoch: 5 step: 515, loss is 0.14407587051391602\n",
      "epoch: 5 step: 516, loss is 0.14575831592082977\n",
      "epoch: 5 step: 517, loss is 0.08434495329856873\n",
      "epoch: 5 step: 518, loss is 0.17953641712665558\n",
      "epoch: 5 step: 519, loss is 0.11034373939037323\n",
      "epoch: 5 step: 520, loss is 0.09580665081739426\n",
      "epoch: 5 step: 521, loss is 0.21429499983787537\n",
      "epoch: 5 step: 522, loss is 0.16250328719615936\n",
      "epoch: 5 step: 523, loss is 0.20141452550888062\n",
      "epoch: 5 step: 524, loss is 0.10813542455434799\n",
      "epoch: 5 step: 525, loss is 0.15914995968341827\n",
      "epoch: 5 step: 526, loss is 0.24788567423820496\n",
      "epoch: 5 step: 527, loss is 0.10700810700654984\n",
      "epoch: 5 step: 528, loss is 0.04518813267350197\n",
      "epoch: 5 step: 529, loss is 0.1529681235551834\n",
      "epoch: 5 step: 530, loss is 0.13025327026844025\n",
      "epoch: 5 step: 531, loss is 0.21677859127521515\n",
      "epoch: 5 step: 532, loss is 0.3241218328475952\n",
      "epoch: 5 step: 533, loss is 0.20843584835529327\n",
      "epoch: 5 step: 534, loss is 0.2526695430278778\n",
      "epoch: 5 step: 535, loss is 0.1977507621049881\n",
      "epoch: 5 step: 536, loss is 0.21171878278255463\n",
      "epoch: 5 step: 537, loss is 0.08143096417188644\n",
      "epoch: 5 step: 538, loss is 0.26726943254470825\n",
      "epoch: 5 step: 539, loss is 0.11783885210752487\n",
      "epoch: 5 step: 540, loss is 0.09426575154066086\n",
      "epoch: 5 step: 541, loss is 0.05736978352069855\n",
      "epoch: 5 step: 542, loss is 0.1871386170387268\n",
      "epoch: 5 step: 543, loss is 0.24538542330265045\n",
      "epoch: 5 step: 544, loss is 0.08220556378364563\n",
      "epoch: 5 step: 545, loss is 0.25824469327926636\n",
      "epoch: 5 step: 546, loss is 0.09140751510858536\n",
      "epoch: 5 step: 547, loss is 0.14380651712417603\n",
      "epoch: 5 step: 548, loss is 0.29907307028770447\n",
      "epoch: 5 step: 549, loss is 0.21273788809776306\n",
      "epoch: 5 step: 550, loss is 0.14704273641109467\n",
      "epoch: 5 step: 551, loss is 0.22064657509326935\n",
      "epoch: 5 step: 552, loss is 0.12131704390048981\n",
      "epoch: 5 step: 553, loss is 0.19044868648052216\n",
      "epoch: 5 step: 554, loss is 0.1807790994644165\n",
      "epoch: 5 step: 555, loss is 0.18883132934570312\n",
      "epoch: 5 step: 556, loss is 0.12260033935308456\n",
      "epoch: 5 step: 557, loss is 0.16632236540317535\n",
      "epoch: 5 step: 558, loss is 0.1958877295255661\n",
      "epoch: 5 step: 559, loss is 0.18371587991714478\n",
      "epoch: 5 step: 560, loss is 0.21665704250335693\n",
      "epoch: 5 step: 561, loss is 0.21322453022003174\n",
      "epoch: 5 step: 562, loss is 0.19335044920444489\n",
      "epoch: 5 step: 563, loss is 0.12682993710041046\n",
      "epoch: 5 step: 564, loss is 0.194728285074234\n",
      "epoch: 5 step: 565, loss is 0.10941220819950104\n",
      "epoch: 5 step: 566, loss is 0.1869514137506485\n",
      "epoch: 5 step: 567, loss is 0.177247554063797\n",
      "epoch: 5 step: 568, loss is 0.19748106598854065\n",
      "epoch: 5 step: 569, loss is 0.19432173669338226\n",
      "epoch: 5 step: 570, loss is 0.16704796254634857\n",
      "epoch: 5 step: 571, loss is 0.11268812417984009\n",
      "epoch: 5 step: 572, loss is 0.19433537125587463\n",
      "epoch: 5 step: 573, loss is 0.16330896317958832\n",
      "epoch: 5 step: 574, loss is 0.090381920337677\n",
      "epoch: 5 step: 575, loss is 0.1660633236169815\n",
      "epoch: 5 step: 576, loss is 0.17984801530838013\n",
      "epoch: 5 step: 577, loss is 0.10216827690601349\n",
      "epoch: 5 step: 578, loss is 0.10791057348251343\n",
      "epoch: 5 step: 579, loss is 0.2593717575073242\n",
      "epoch: 5 step: 580, loss is 0.26245391368865967\n",
      "epoch: 5 step: 581, loss is 0.14400993287563324\n",
      "epoch: 5 step: 582, loss is 0.14903756976127625\n",
      "epoch: 5 step: 583, loss is 0.24832719564437866\n",
      "epoch: 5 step: 584, loss is 0.12299288809299469\n",
      "epoch: 5 step: 585, loss is 0.20196422934532166\n",
      "epoch: 5 step: 586, loss is 0.10543166100978851\n",
      "epoch: 5 step: 587, loss is 0.22138093411922455\n",
      "epoch: 5 step: 588, loss is 0.09654779732227325\n",
      "epoch: 5 step: 589, loss is 0.21280886232852936\n",
      "epoch: 5 step: 590, loss is 0.18278826773166656\n",
      "epoch: 5 step: 591, loss is 0.21367096900939941\n",
      "epoch: 5 step: 592, loss is 0.11473408341407776\n",
      "epoch: 5 step: 593, loss is 0.31750595569610596\n",
      "epoch: 5 step: 594, loss is 0.2267126590013504\n",
      "epoch: 5 step: 595, loss is 0.15397606790065765\n",
      "epoch: 5 step: 596, loss is 0.060535941272974014\n",
      "epoch: 5 step: 597, loss is 0.1740582287311554\n",
      "epoch: 5 step: 598, loss is 0.14395543932914734\n",
      "epoch: 5 step: 599, loss is 0.2816550135612488\n",
      "epoch: 5 step: 600, loss is 0.18418362736701965\n",
      "epoch: 5 step: 601, loss is 0.1317017674446106\n",
      "epoch: 5 step: 602, loss is 0.2009439915418625\n",
      "epoch: 5 step: 603, loss is 0.10590618848800659\n",
      "epoch: 5 step: 604, loss is 0.20541562139987946\n",
      "epoch: 5 step: 605, loss is 0.13822755217552185\n",
      "epoch: 5 step: 606, loss is 0.2126210331916809\n",
      "epoch: 5 step: 607, loss is 0.09832705557346344\n",
      "epoch: 5 step: 608, loss is 0.07874055206775665\n",
      "epoch: 5 step: 609, loss is 0.22195251286029816\n",
      "epoch: 5 step: 610, loss is 0.19482116401195526\n",
      "epoch: 5 step: 611, loss is 0.27565324306488037\n",
      "epoch: 5 step: 612, loss is 0.25748875737190247\n",
      "epoch: 5 step: 613, loss is 0.17433185875415802\n",
      "epoch: 5 step: 614, loss is 0.2480851262807846\n",
      "epoch: 5 step: 615, loss is 0.10436467081308365\n",
      "epoch: 5 step: 616, loss is 0.2288564145565033\n",
      "epoch: 5 step: 617, loss is 0.18813814222812653\n",
      "epoch: 5 step: 618, loss is 0.18856926262378693\n",
      "epoch: 5 step: 619, loss is 0.29702243208885193\n",
      "epoch: 5 step: 620, loss is 0.14351002871990204\n",
      "epoch: 5 step: 621, loss is 0.1420900970697403\n",
      "epoch: 5 step: 622, loss is 0.19988925755023956\n",
      "epoch: 5 step: 623, loss is 0.1715262532234192\n",
      "epoch: 5 step: 624, loss is 0.1326795220375061\n",
      "epoch: 5 step: 625, loss is 0.16208204627037048\n",
      "epoch: 5 step: 626, loss is 0.0875721350312233\n",
      "epoch: 5 step: 627, loss is 0.24526537954807281\n",
      "epoch: 5 step: 628, loss is 0.18983840942382812\n",
      "epoch: 5 step: 629, loss is 0.11606967449188232\n",
      "epoch: 5 step: 630, loss is 0.30934128165245056\n",
      "epoch: 5 step: 631, loss is 0.149319589138031\n",
      "epoch: 5 step: 632, loss is 0.11978340148925781\n",
      "epoch: 5 step: 633, loss is 0.22432971000671387\n",
      "epoch: 5 step: 634, loss is 0.14018450677394867\n",
      "epoch: 5 step: 635, loss is 0.2453436553478241\n",
      "epoch: 5 step: 636, loss is 0.12434045970439911\n",
      "epoch: 5 step: 637, loss is 0.2371300905942917\n",
      "epoch: 5 step: 638, loss is 0.10252963751554489\n",
      "epoch: 5 step: 639, loss is 0.1649332195520401\n",
      "epoch: 5 step: 640, loss is 0.3604968786239624\n",
      "epoch: 5 step: 641, loss is 0.22945967316627502\n",
      "epoch: 5 step: 642, loss is 0.3778619170188904\n",
      "epoch: 5 step: 643, loss is 0.12729428708553314\n",
      "epoch: 5 step: 644, loss is 0.19913892447948456\n",
      "epoch: 5 step: 645, loss is 0.1454489529132843\n",
      "epoch: 5 step: 646, loss is 0.16227997839450836\n",
      "epoch: 5 step: 647, loss is 0.15483514964580536\n",
      "epoch: 5 step: 648, loss is 0.1538817584514618\n",
      "epoch: 5 step: 649, loss is 0.21205662190914154\n",
      "epoch: 5 step: 650, loss is 0.23382385075092316\n",
      "epoch: 5 step: 651, loss is 0.16779568791389465\n",
      "epoch: 5 step: 652, loss is 0.19376428425312042\n",
      "epoch: 5 step: 653, loss is 0.258759468793869\n",
      "epoch: 5 step: 654, loss is 0.06274046748876572\n",
      "epoch: 5 step: 655, loss is 0.12399759888648987\n",
      "epoch: 5 step: 656, loss is 0.12138941884040833\n",
      "epoch: 5 step: 657, loss is 0.08092252910137177\n",
      "epoch: 5 step: 658, loss is 0.10605169832706451\n",
      "epoch: 5 step: 659, loss is 0.09843841195106506\n",
      "epoch: 5 step: 660, loss is 0.1536186933517456\n",
      "epoch: 5 step: 661, loss is 0.1402299702167511\n",
      "epoch: 5 step: 662, loss is 0.19481845200061798\n",
      "epoch: 5 step: 663, loss is 0.30072394013404846\n",
      "epoch: 5 step: 664, loss is 0.1486797332763672\n",
      "epoch: 5 step: 665, loss is 0.22638235986232758\n",
      "epoch: 5 step: 666, loss is 0.1817539781332016\n",
      "epoch: 5 step: 667, loss is 0.14809280633926392\n",
      "epoch: 5 step: 668, loss is 0.1225745677947998\n",
      "epoch: 5 step: 669, loss is 0.20607933402061462\n",
      "epoch: 5 step: 670, loss is 0.12142790853977203\n",
      "epoch: 5 step: 671, loss is 0.1444859653711319\n",
      "epoch: 5 step: 672, loss is 0.2779237926006317\n",
      "epoch: 5 step: 673, loss is 0.2699081301689148\n",
      "epoch: 5 step: 674, loss is 0.17875781655311584\n",
      "epoch: 5 step: 675, loss is 0.19416946172714233\n",
      "epoch: 5 step: 676, loss is 0.18024636805057526\n",
      "epoch: 5 step: 677, loss is 0.2684673070907593\n",
      "epoch: 5 step: 678, loss is 0.08150564134120941\n",
      "epoch: 5 step: 679, loss is 0.16335748136043549\n",
      "epoch: 5 step: 680, loss is 0.15590237081050873\n",
      "epoch: 5 step: 681, loss is 0.09794208407402039\n",
      "epoch: 5 step: 682, loss is 0.3247385323047638\n",
      "epoch: 5 step: 683, loss is 0.12812645733356476\n",
      "epoch: 5 step: 684, loss is 0.3084332048892975\n",
      "epoch: 5 step: 685, loss is 0.17795471847057343\n",
      "epoch: 5 step: 686, loss is 0.1372741311788559\n",
      "epoch: 5 step: 687, loss is 0.14978423714637756\n",
      "epoch: 5 step: 688, loss is 0.14725591242313385\n",
      "epoch: 5 step: 689, loss is 0.2910039722919464\n",
      "epoch: 5 step: 690, loss is 0.22795452177524567\n",
      "epoch: 5 step: 691, loss is 0.079033762216568\n",
      "epoch: 5 step: 692, loss is 0.052734240889549255\n",
      "epoch: 5 step: 693, loss is 0.11217768490314484\n",
      "epoch: 5 step: 694, loss is 0.05523034185171127\n",
      "epoch: 5 step: 695, loss is 0.13053151965141296\n",
      "epoch: 5 step: 696, loss is 0.3127412497997284\n",
      "epoch: 5 step: 697, loss is 0.1988331377506256\n",
      "epoch: 5 step: 698, loss is 0.16338087618350983\n",
      "epoch: 5 step: 699, loss is 0.18172897398471832\n",
      "epoch: 5 step: 700, loss is 0.10955707728862762\n",
      "epoch: 5 step: 701, loss is 0.21454891562461853\n",
      "epoch: 5 step: 702, loss is 0.19830986857414246\n",
      "epoch: 5 step: 703, loss is 0.08243183046579361\n",
      "epoch: 5 step: 704, loss is 0.1510700285434723\n",
      "epoch: 5 step: 705, loss is 0.13057594001293182\n",
      "epoch: 5 step: 706, loss is 0.23778420686721802\n",
      "epoch: 5 step: 707, loss is 0.13482655584812164\n",
      "epoch: 5 step: 708, loss is 0.24446754157543182\n",
      "epoch: 5 step: 709, loss is 0.14804179966449738\n",
      "epoch: 5 step: 710, loss is 0.21872888505458832\n",
      "epoch: 5 step: 711, loss is 0.3411606252193451\n",
      "epoch: 5 step: 712, loss is 0.21213561296463013\n",
      "epoch: 5 step: 713, loss is 0.25084421038627625\n",
      "epoch: 5 step: 714, loss is 0.15017428994178772\n",
      "epoch: 5 step: 715, loss is 0.08752598613500595\n",
      "epoch: 5 step: 716, loss is 0.18875257670879364\n",
      "epoch: 5 step: 717, loss is 0.2631809115409851\n",
      "epoch: 5 step: 718, loss is 0.2074005901813507\n",
      "epoch: 5 step: 719, loss is 0.19370441138744354\n",
      "epoch: 5 step: 720, loss is 0.20271632075309753\n",
      "epoch: 5 step: 721, loss is 0.1408243626356125\n",
      "epoch: 5 step: 722, loss is 0.1827978491783142\n",
      "epoch: 5 step: 723, loss is 0.1575680524110794\n",
      "epoch: 5 step: 724, loss is 0.24336516857147217\n",
      "epoch: 5 step: 725, loss is 0.10756470263004303\n",
      "epoch: 5 step: 726, loss is 0.1023249700665474\n",
      "epoch: 5 step: 727, loss is 0.11937619745731354\n",
      "epoch: 5 step: 728, loss is 0.20035062730312347\n",
      "epoch: 5 step: 729, loss is 0.18670836091041565\n",
      "epoch: 5 step: 730, loss is 0.10509268939495087\n",
      "epoch: 5 step: 731, loss is 0.12968315184116364\n",
      "epoch: 5 step: 732, loss is 0.1505027711391449\n",
      "epoch: 5 step: 733, loss is 0.16433793306350708\n",
      "epoch: 5 step: 734, loss is 0.3375152349472046\n",
      "epoch: 5 step: 735, loss is 0.15670201182365417\n",
      "epoch: 5 step: 736, loss is 0.18069781363010406\n",
      "epoch: 5 step: 737, loss is 0.07899592071771622\n",
      "epoch: 5 step: 738, loss is 0.11952132731676102\n",
      "epoch: 5 step: 739, loss is 0.16288068890571594\n",
      "epoch: 5 step: 740, loss is 0.29762083292007446\n",
      "epoch: 5 step: 741, loss is 0.16396376490592957\n",
      "epoch: 5 step: 742, loss is 0.1972246766090393\n",
      "epoch: 5 step: 743, loss is 0.08303862810134888\n",
      "epoch: 5 step: 744, loss is 0.11835313588380814\n",
      "epoch: 5 step: 745, loss is 0.25174257159233093\n",
      "epoch: 5 step: 746, loss is 0.10850545018911362\n",
      "epoch: 5 step: 747, loss is 0.13035528361797333\n",
      "epoch: 5 step: 748, loss is 0.16123761236667633\n",
      "epoch: 5 step: 749, loss is 0.1748228371143341\n",
      "epoch: 5 step: 750, loss is 0.3017277121543884\n",
      "epoch: 5 step: 751, loss is 0.08017394691705704\n",
      "epoch: 5 step: 752, loss is 0.15058940649032593\n",
      "epoch: 5 step: 753, loss is 0.24337324500083923\n",
      "epoch: 5 step: 754, loss is 0.1484140306711197\n",
      "epoch: 5 step: 755, loss is 0.36728858947753906\n",
      "epoch: 5 step: 756, loss is 0.17109961807727814\n",
      "epoch: 5 step: 757, loss is 0.2132004052400589\n",
      "epoch: 5 step: 758, loss is 0.29112327098846436\n",
      "epoch: 5 step: 759, loss is 0.07538028061389923\n",
      "epoch: 5 step: 760, loss is 0.25519227981567383\n",
      "epoch: 5 step: 761, loss is 0.1454659402370453\n",
      "epoch: 5 step: 762, loss is 0.09449932724237442\n",
      "epoch: 5 step: 763, loss is 0.07824937999248505\n",
      "epoch: 5 step: 764, loss is 0.139372318983078\n",
      "epoch: 5 step: 765, loss is 0.14249637722969055\n",
      "epoch: 5 step: 766, loss is 0.2141459882259369\n",
      "epoch: 5 step: 767, loss is 0.15794353187084198\n",
      "epoch: 5 step: 768, loss is 0.1772458702325821\n",
      "epoch: 5 step: 769, loss is 0.1457556188106537\n",
      "epoch: 5 step: 770, loss is 0.13396818935871124\n",
      "epoch: 5 step: 771, loss is 0.1513347625732422\n",
      "epoch: 5 step: 772, loss is 0.2691977918148041\n",
      "epoch: 5 step: 773, loss is 0.11111024022102356\n",
      "epoch: 5 step: 774, loss is 0.09888606518507004\n",
      "epoch: 5 step: 775, loss is 0.11278094351291656\n",
      "epoch: 5 step: 776, loss is 0.12971775233745575\n",
      "epoch: 5 step: 777, loss is 0.08592471480369568\n",
      "epoch: 5 step: 778, loss is 0.17774498462677002\n",
      "epoch: 5 step: 779, loss is 0.26919040083885193\n",
      "epoch: 5 step: 780, loss is 0.253635048866272\n",
      "epoch: 5 step: 781, loss is 0.18232250213623047\n",
      "epoch: 5 step: 782, loss is 0.22550703585147858\n",
      "epoch: 5 step: 783, loss is 0.09974213689565659\n",
      "epoch: 5 step: 784, loss is 0.16648989915847778\n",
      "epoch: 5 step: 785, loss is 0.19561591744422913\n",
      "epoch: 5 step: 786, loss is 0.3289910554885864\n",
      "epoch: 5 step: 787, loss is 0.45223572850227356\n",
      "epoch: 5 step: 788, loss is 0.21589569747447968\n",
      "epoch: 5 step: 789, loss is 0.19893130660057068\n",
      "epoch: 5 step: 790, loss is 0.12503045797348022\n",
      "epoch: 5 step: 791, loss is 0.20135632157325745\n",
      "epoch: 5 step: 792, loss is 0.3972477614879608\n",
      "epoch: 5 step: 793, loss is 0.2223452478647232\n",
      "epoch: 5 step: 794, loss is 0.07194393873214722\n",
      "epoch: 5 step: 795, loss is 0.09278770536184311\n",
      "epoch: 5 step: 796, loss is 0.15856145322322845\n",
      "epoch: 5 step: 797, loss is 0.15656717121601105\n",
      "epoch: 5 step: 798, loss is 0.12872503697872162\n",
      "epoch: 5 step: 799, loss is 0.28605639934539795\n",
      "epoch: 5 step: 800, loss is 0.13000833988189697\n",
      "epoch: 5 step: 801, loss is 0.15035775303840637\n",
      "epoch: 5 step: 802, loss is 0.14823175966739655\n",
      "epoch: 5 step: 803, loss is 0.1842106133699417\n",
      "epoch: 5 step: 804, loss is 0.22875522077083588\n",
      "epoch: 5 step: 805, loss is 0.10982924699783325\n",
      "epoch: 5 step: 806, loss is 0.17190569639205933\n",
      "epoch: 5 step: 807, loss is 0.33351027965545654\n",
      "epoch: 5 step: 808, loss is 0.20608094334602356\n",
      "epoch: 5 step: 809, loss is 0.14297352731227875\n",
      "epoch: 5 step: 810, loss is 0.23604750633239746\n",
      "epoch: 5 step: 811, loss is 0.4230761229991913\n",
      "epoch: 5 step: 812, loss is 0.11325113475322723\n",
      "epoch: 5 step: 813, loss is 0.11250969767570496\n",
      "epoch: 5 step: 814, loss is 0.14124509692192078\n",
      "epoch: 5 step: 815, loss is 0.13477326929569244\n",
      "epoch: 5 step: 816, loss is 0.14133352041244507\n",
      "epoch: 5 step: 817, loss is 0.138852059841156\n",
      "epoch: 5 step: 818, loss is 0.10679689794778824\n",
      "epoch: 5 step: 819, loss is 0.14935477077960968\n",
      "epoch: 5 step: 820, loss is 0.19456927478313446\n",
      "epoch: 5 step: 821, loss is 0.10197483003139496\n",
      "epoch: 5 step: 822, loss is 0.23269085586071014\n",
      "epoch: 5 step: 823, loss is 0.1972057819366455\n",
      "epoch: 5 step: 824, loss is 0.19470424950122833\n",
      "epoch: 5 step: 825, loss is 0.08033274859189987\n",
      "epoch: 5 step: 826, loss is 0.35362645983695984\n",
      "epoch: 5 step: 827, loss is 0.12420205771923065\n",
      "epoch: 5 step: 828, loss is 0.22281308472156525\n",
      "epoch: 5 step: 829, loss is 0.1607198417186737\n",
      "epoch: 5 step: 830, loss is 0.10747415572404861\n",
      "epoch: 5 step: 831, loss is 0.35610032081604004\n",
      "epoch: 5 step: 832, loss is 0.0695410668849945\n",
      "epoch: 5 step: 833, loss is 0.25287389755249023\n",
      "epoch: 5 step: 834, loss is 0.20702803134918213\n",
      "epoch: 5 step: 835, loss is 0.19713561236858368\n",
      "epoch: 5 step: 836, loss is 0.12107973545789719\n",
      "epoch: 5 step: 837, loss is 0.12801319360733032\n",
      "epoch: 5 step: 838, loss is 0.11331295222043991\n",
      "epoch: 5 step: 839, loss is 0.17393246293067932\n",
      "epoch: 5 step: 840, loss is 0.11300446093082428\n",
      "epoch: 5 step: 841, loss is 0.2345380038022995\n",
      "epoch: 5 step: 842, loss is 0.22056525945663452\n",
      "epoch: 5 step: 843, loss is 0.18786963820457458\n",
      "epoch: 5 step: 844, loss is 0.29135406017303467\n",
      "epoch: 5 step: 845, loss is 0.12056548148393631\n",
      "epoch: 5 step: 846, loss is 0.04000517353415489\n",
      "epoch: 5 step: 847, loss is 0.29479390382766724\n",
      "epoch: 5 step: 848, loss is 0.2561788856983185\n",
      "epoch: 5 step: 849, loss is 0.2530410587787628\n",
      "epoch: 5 step: 850, loss is 0.10334252566099167\n",
      "epoch: 5 step: 851, loss is 0.11155184358358383\n",
      "epoch: 5 step: 852, loss is 0.10339713841676712\n",
      "epoch: 5 step: 853, loss is 0.13269749283790588\n",
      "epoch: 5 step: 854, loss is 0.12293277680873871\n",
      "epoch: 5 step: 855, loss is 0.05515677109360695\n",
      "epoch: 5 step: 856, loss is 0.15749214589595795\n",
      "epoch: 5 step: 857, loss is 0.12521108984947205\n",
      "epoch: 5 step: 858, loss is 0.13573603332042694\n",
      "epoch: 5 step: 859, loss is 0.1412695199251175\n",
      "epoch: 5 step: 860, loss is 0.1886894404888153\n",
      "epoch: 5 step: 861, loss is 0.2425142526626587\n",
      "epoch: 5 step: 862, loss is 0.19856026768684387\n",
      "epoch: 5 step: 863, loss is 0.08414663374423981\n",
      "epoch: 5 step: 864, loss is 0.18427039682865143\n",
      "epoch: 5 step: 865, loss is 0.17766666412353516\n",
      "epoch: 5 step: 866, loss is 0.23376192152500153\n",
      "epoch: 5 step: 867, loss is 0.1298084408044815\n",
      "epoch: 5 step: 868, loss is 0.14614340662956238\n",
      "epoch: 5 step: 869, loss is 0.10880444198846817\n",
      "epoch: 5 step: 870, loss is 0.179183691740036\n",
      "epoch: 5 step: 871, loss is 0.1479129046201706\n",
      "epoch: 5 step: 872, loss is 0.2589673101902008\n",
      "epoch: 5 step: 873, loss is 0.14867937564849854\n",
      "epoch: 5 step: 874, loss is 0.1129845380783081\n",
      "epoch: 5 step: 875, loss is 0.19963274896144867\n",
      "epoch: 5 step: 876, loss is 0.15021851658821106\n",
      "epoch: 5 step: 877, loss is 0.3241058588027954\n",
      "epoch: 5 step: 878, loss is 0.15789352357387543\n",
      "epoch: 5 step: 879, loss is 0.18579454720020294\n",
      "epoch: 5 step: 880, loss is 0.18835528194904327\n",
      "epoch: 5 step: 881, loss is 0.10588746517896652\n",
      "epoch: 5 step: 882, loss is 0.14388954639434814\n",
      "epoch: 5 step: 883, loss is 0.23422393202781677\n",
      "epoch: 5 step: 884, loss is 0.07702532410621643\n",
      "epoch: 5 step: 885, loss is 0.3286474645137787\n",
      "epoch: 5 step: 886, loss is 0.13729430735111237\n",
      "epoch: 5 step: 887, loss is 0.16387484967708588\n",
      "epoch: 5 step: 888, loss is 0.2106071412563324\n",
      "epoch: 5 step: 889, loss is 0.1910986602306366\n",
      "epoch: 5 step: 890, loss is 0.21699821949005127\n",
      "epoch: 5 step: 891, loss is 0.20409296452999115\n",
      "epoch: 5 step: 892, loss is 0.2329729199409485\n",
      "epoch: 5 step: 893, loss is 0.07610012590885162\n",
      "epoch: 5 step: 894, loss is 0.1689777672290802\n",
      "epoch: 5 step: 895, loss is 0.1767847239971161\n",
      "epoch: 5 step: 896, loss is 0.26835161447525024\n",
      "epoch: 5 step: 897, loss is 0.24918164312839508\n",
      "epoch: 5 step: 898, loss is 0.04664720967411995\n",
      "epoch: 5 step: 899, loss is 0.16073289513587952\n",
      "epoch: 5 step: 900, loss is 0.22832763195037842\n",
      "epoch: 5 step: 901, loss is 0.12270275503396988\n",
      "epoch: 5 step: 902, loss is 0.1760241538286209\n",
      "epoch: 5 step: 903, loss is 0.05520278960466385\n",
      "epoch: 5 step: 904, loss is 0.1831730753183365\n",
      "epoch: 5 step: 905, loss is 0.35989922285079956\n",
      "epoch: 5 step: 906, loss is 0.11350438743829727\n",
      "epoch: 5 step: 907, loss is 0.14008261263370514\n",
      "epoch: 5 step: 908, loss is 0.1814824491739273\n",
      "epoch: 5 step: 909, loss is 0.17882786691188812\n",
      "epoch: 5 step: 910, loss is 0.09505081921815872\n",
      "epoch: 5 step: 911, loss is 0.23048080503940582\n",
      "epoch: 5 step: 912, loss is 0.2339666783809662\n",
      "epoch: 5 step: 913, loss is 0.10751226544380188\n",
      "epoch: 5 step: 914, loss is 0.12429583072662354\n",
      "epoch: 5 step: 915, loss is 0.1524304896593094\n",
      "epoch: 5 step: 916, loss is 0.17079178988933563\n",
      "epoch: 5 step: 917, loss is 0.16793087124824524\n",
      "epoch: 5 step: 918, loss is 0.13149338960647583\n",
      "epoch: 5 step: 919, loss is 0.20928168296813965\n",
      "epoch: 5 step: 920, loss is 0.14189499616622925\n",
      "epoch: 5 step: 921, loss is 0.37224167585372925\n",
      "epoch: 5 step: 922, loss is 0.2910444736480713\n",
      "epoch: 5 step: 923, loss is 0.2222500443458557\n",
      "epoch: 5 step: 924, loss is 0.1153750792145729\n",
      "epoch: 5 step: 925, loss is 0.06175883114337921\n",
      "epoch: 5 step: 926, loss is 0.2163148671388626\n",
      "epoch: 5 step: 927, loss is 0.22969114780426025\n",
      "epoch: 5 step: 928, loss is 0.1414213329553604\n",
      "epoch: 5 step: 929, loss is 0.1729729175567627\n",
      "epoch: 5 step: 930, loss is 0.3513152003288269\n",
      "epoch: 5 step: 931, loss is 0.16550230979919434\n",
      "epoch: 5 step: 932, loss is 0.18398180603981018\n",
      "epoch: 5 step: 933, loss is 0.2529575228691101\n",
      "epoch: 5 step: 934, loss is 0.1859225034713745\n",
      "epoch: 5 step: 935, loss is 0.1562572568655014\n",
      "epoch: 5 step: 936, loss is 0.16642305254936218\n",
      "epoch: 5 step: 937, loss is 0.08800657093524933\n",
      "epoch: 6 step: 1, loss is 0.0780918151140213\n",
      "epoch: 6 step: 2, loss is 0.12011212855577469\n",
      "epoch: 6 step: 3, loss is 0.2803695797920227\n",
      "epoch: 6 step: 4, loss is 0.1541038453578949\n",
      "epoch: 6 step: 5, loss is 0.11087638139724731\n",
      "epoch: 6 step: 6, loss is 0.18634124100208282\n",
      "epoch: 6 step: 7, loss is 0.08127303421497345\n",
      "epoch: 6 step: 8, loss is 0.2730545401573181\n",
      "epoch: 6 step: 9, loss is 0.10259755700826645\n",
      "epoch: 6 step: 10, loss is 0.19100338220596313\n",
      "epoch: 6 step: 11, loss is 0.09837150573730469\n",
      "epoch: 6 step: 12, loss is 0.18105749785900116\n",
      "epoch: 6 step: 13, loss is 0.12778089940547943\n",
      "epoch: 6 step: 14, loss is 0.1792910099029541\n",
      "epoch: 6 step: 15, loss is 0.22562475502490997\n",
      "epoch: 6 step: 16, loss is 0.10418803989887238\n",
      "epoch: 6 step: 17, loss is 0.08748936653137207\n",
      "epoch: 6 step: 18, loss is 0.171352356672287\n",
      "epoch: 6 step: 19, loss is 0.10668547451496124\n",
      "epoch: 6 step: 20, loss is 0.11504893004894257\n",
      "epoch: 6 step: 21, loss is 0.09396260231733322\n",
      "epoch: 6 step: 22, loss is 0.16123618185520172\n",
      "epoch: 6 step: 23, loss is 0.14033646881580353\n",
      "epoch: 6 step: 24, loss is 0.050265584141016006\n",
      "epoch: 6 step: 25, loss is 0.18965722620487213\n",
      "epoch: 6 step: 26, loss is 0.12847554683685303\n",
      "epoch: 6 step: 27, loss is 0.09253787994384766\n",
      "epoch: 6 step: 28, loss is 0.0892404168844223\n",
      "epoch: 6 step: 29, loss is 0.21493428945541382\n",
      "epoch: 6 step: 30, loss is 0.24911023676395416\n",
      "epoch: 6 step: 31, loss is 0.09147769212722778\n",
      "epoch: 6 step: 32, loss is 0.11220131814479828\n",
      "epoch: 6 step: 33, loss is 0.09177910536527634\n",
      "epoch: 6 step: 34, loss is 0.14965152740478516\n",
      "epoch: 6 step: 35, loss is 0.08663185685873032\n",
      "epoch: 6 step: 36, loss is 0.09613911807537079\n",
      "epoch: 6 step: 37, loss is 0.1025790125131607\n",
      "epoch: 6 step: 38, loss is 0.2546344995498657\n",
      "epoch: 6 step: 39, loss is 0.33773452043533325\n",
      "epoch: 6 step: 40, loss is 0.21629685163497925\n",
      "epoch: 6 step: 41, loss is 0.1347789168357849\n",
      "epoch: 6 step: 42, loss is 0.08120536804199219\n",
      "epoch: 6 step: 43, loss is 0.21064484119415283\n",
      "epoch: 6 step: 44, loss is 0.2721371352672577\n",
      "epoch: 6 step: 45, loss is 0.08588409423828125\n",
      "epoch: 6 step: 46, loss is 0.05038156732916832\n",
      "epoch: 6 step: 47, loss is 0.06856205314397812\n",
      "epoch: 6 step: 48, loss is 0.10106119513511658\n",
      "epoch: 6 step: 49, loss is 0.08846260607242584\n",
      "epoch: 6 step: 50, loss is 0.06301017105579376\n",
      "epoch: 6 step: 51, loss is 0.05566321313381195\n",
      "epoch: 6 step: 52, loss is 0.07365861535072327\n",
      "epoch: 6 step: 53, loss is 0.09209033101797104\n",
      "epoch: 6 step: 54, loss is 0.18542462587356567\n",
      "epoch: 6 step: 55, loss is 0.22817596793174744\n",
      "epoch: 6 step: 56, loss is 0.024312226101756096\n",
      "epoch: 6 step: 57, loss is 0.04786274954676628\n",
      "epoch: 6 step: 58, loss is 0.07968374341726303\n",
      "epoch: 6 step: 59, loss is 0.09831089526414871\n",
      "epoch: 6 step: 60, loss is 0.2552013099193573\n",
      "epoch: 6 step: 61, loss is 0.08497203886508942\n",
      "epoch: 6 step: 62, loss is 0.15295733511447906\n",
      "epoch: 6 step: 63, loss is 0.1677142083644867\n",
      "epoch: 6 step: 64, loss is 0.15524214506149292\n",
      "epoch: 6 step: 65, loss is 0.15536363422870636\n",
      "epoch: 6 step: 66, loss is 0.07137356698513031\n",
      "epoch: 6 step: 67, loss is 0.1274721473455429\n",
      "epoch: 6 step: 68, loss is 0.27158522605895996\n",
      "epoch: 6 step: 69, loss is 0.18101394176483154\n",
      "epoch: 6 step: 70, loss is 0.08689842373132706\n",
      "epoch: 6 step: 71, loss is 0.12726718187332153\n",
      "epoch: 6 step: 72, loss is 0.11676967144012451\n",
      "epoch: 6 step: 73, loss is 0.07179726660251617\n",
      "epoch: 6 step: 74, loss is 0.12115827202796936\n",
      "epoch: 6 step: 75, loss is 0.14312128722667694\n",
      "epoch: 6 step: 76, loss is 0.21908776462078094\n",
      "epoch: 6 step: 77, loss is 0.1570090651512146\n",
      "epoch: 6 step: 78, loss is 0.23490777611732483\n",
      "epoch: 6 step: 79, loss is 0.21704980731010437\n",
      "epoch: 6 step: 80, loss is 0.02955658733844757\n",
      "epoch: 6 step: 81, loss is 0.09137570858001709\n",
      "epoch: 6 step: 82, loss is 0.12131887674331665\n",
      "epoch: 6 step: 83, loss is 0.1490710973739624\n",
      "epoch: 6 step: 84, loss is 0.25626835227012634\n",
      "epoch: 6 step: 85, loss is 0.12829427421092987\n",
      "epoch: 6 step: 86, loss is 0.2051025927066803\n",
      "epoch: 6 step: 87, loss is 0.09488391131162643\n",
      "epoch: 6 step: 88, loss is 0.14043033123016357\n",
      "epoch: 6 step: 89, loss is 0.17742574214935303\n",
      "epoch: 6 step: 90, loss is 0.10320980846881866\n",
      "epoch: 6 step: 91, loss is 0.11860641092061996\n",
      "epoch: 6 step: 92, loss is 0.09985876828432083\n",
      "epoch: 6 step: 93, loss is 0.052467260509729385\n",
      "epoch: 6 step: 94, loss is 0.19092312455177307\n",
      "epoch: 6 step: 95, loss is 0.11248976737260818\n",
      "epoch: 6 step: 96, loss is 0.14843109250068665\n",
      "epoch: 6 step: 97, loss is 0.06827151030302048\n",
      "epoch: 6 step: 98, loss is 0.22991414368152618\n",
      "epoch: 6 step: 99, loss is 0.11932101100683212\n",
      "epoch: 6 step: 100, loss is 0.1734185665845871\n",
      "epoch: 6 step: 101, loss is 0.0792030468583107\n",
      "epoch: 6 step: 102, loss is 0.09331471472978592\n",
      "epoch: 6 step: 103, loss is 0.05936666205525398\n",
      "epoch: 6 step: 104, loss is 0.09907598793506622\n",
      "epoch: 6 step: 105, loss is 0.12026402354240417\n",
      "epoch: 6 step: 106, loss is 0.05952884629368782\n",
      "epoch: 6 step: 107, loss is 0.07001505047082901\n",
      "epoch: 6 step: 108, loss is 0.24001649022102356\n",
      "epoch: 6 step: 109, loss is 0.1527542769908905\n",
      "epoch: 6 step: 110, loss is 0.20462167263031006\n",
      "epoch: 6 step: 111, loss is 0.11739502847194672\n",
      "epoch: 6 step: 112, loss is 0.36309221386909485\n",
      "epoch: 6 step: 113, loss is 0.0738893672823906\n",
      "epoch: 6 step: 114, loss is 0.17913204431533813\n",
      "epoch: 6 step: 115, loss is 0.06656765192747116\n",
      "epoch: 6 step: 116, loss is 0.13484512269496918\n",
      "epoch: 6 step: 117, loss is 0.08428870141506195\n",
      "epoch: 6 step: 118, loss is 0.035430602729320526\n",
      "epoch: 6 step: 119, loss is 0.13823986053466797\n",
      "epoch: 6 step: 120, loss is 0.19522541761398315\n",
      "epoch: 6 step: 121, loss is 0.10936087369918823\n",
      "epoch: 6 step: 122, loss is 0.20888115465641022\n",
      "epoch: 6 step: 123, loss is 0.17814725637435913\n",
      "epoch: 6 step: 124, loss is 0.27829039096832275\n",
      "epoch: 6 step: 125, loss is 0.14760971069335938\n",
      "epoch: 6 step: 126, loss is 0.10679244250059128\n",
      "epoch: 6 step: 127, loss is 0.11316391825675964\n",
      "epoch: 6 step: 128, loss is 0.11883550882339478\n",
      "epoch: 6 step: 129, loss is 0.2368522584438324\n",
      "epoch: 6 step: 130, loss is 0.18852543830871582\n",
      "epoch: 6 step: 131, loss is 0.13562026619911194\n",
      "epoch: 6 step: 132, loss is 0.1864800602197647\n",
      "epoch: 6 step: 133, loss is 0.07286743819713593\n",
      "epoch: 6 step: 134, loss is 0.1281675547361374\n",
      "epoch: 6 step: 135, loss is 0.14609305560588837\n",
      "epoch: 6 step: 136, loss is 0.11269447207450867\n",
      "epoch: 6 step: 137, loss is 0.2598693370819092\n",
      "epoch: 6 step: 138, loss is 0.11246179044246674\n",
      "epoch: 6 step: 139, loss is 0.18477682769298553\n",
      "epoch: 6 step: 140, loss is 0.25690141320228577\n",
      "epoch: 6 step: 141, loss is 0.25189337134361267\n",
      "epoch: 6 step: 142, loss is 0.17841382324695587\n",
      "epoch: 6 step: 143, loss is 0.15974341332912445\n",
      "epoch: 6 step: 144, loss is 0.19401605427265167\n",
      "epoch: 6 step: 145, loss is 0.0494336262345314\n",
      "epoch: 6 step: 146, loss is 0.28601887822151184\n",
      "epoch: 6 step: 147, loss is 0.17319029569625854\n",
      "epoch: 6 step: 148, loss is 0.20386897027492523\n",
      "epoch: 6 step: 149, loss is 0.20042534172534943\n",
      "epoch: 6 step: 150, loss is 0.054668255150318146\n",
      "epoch: 6 step: 151, loss is 0.07031333446502686\n",
      "epoch: 6 step: 152, loss is 0.0749548152089119\n",
      "epoch: 6 step: 153, loss is 0.07845699042081833\n",
      "epoch: 6 step: 154, loss is 0.110933318734169\n",
      "epoch: 6 step: 155, loss is 0.08760152757167816\n",
      "epoch: 6 step: 156, loss is 0.10173676908016205\n",
      "epoch: 6 step: 157, loss is 0.11404412984848022\n",
      "epoch: 6 step: 158, loss is 0.24844282865524292\n",
      "epoch: 6 step: 159, loss is 0.16377483308315277\n",
      "epoch: 6 step: 160, loss is 0.1510634869337082\n",
      "epoch: 6 step: 161, loss is 0.12326615303754807\n",
      "epoch: 6 step: 162, loss is 0.09664692729711533\n",
      "epoch: 6 step: 163, loss is 0.08161826431751251\n",
      "epoch: 6 step: 164, loss is 0.1020345687866211\n",
      "epoch: 6 step: 165, loss is 0.10936293751001358\n",
      "epoch: 6 step: 166, loss is 0.1298709362745285\n",
      "epoch: 6 step: 167, loss is 0.15227040648460388\n",
      "epoch: 6 step: 168, loss is 0.15535029768943787\n",
      "epoch: 6 step: 169, loss is 0.2581179141998291\n",
      "epoch: 6 step: 170, loss is 0.17964768409729004\n",
      "epoch: 6 step: 171, loss is 0.15266112983226776\n",
      "epoch: 6 step: 172, loss is 0.11953739821910858\n",
      "epoch: 6 step: 173, loss is 0.11425836384296417\n",
      "epoch: 6 step: 174, loss is 0.19063298404216766\n",
      "epoch: 6 step: 175, loss is 0.1253586858510971\n",
      "epoch: 6 step: 176, loss is 0.06275015324354172\n",
      "epoch: 6 step: 177, loss is 0.1413375437259674\n",
      "epoch: 6 step: 178, loss is 0.13849188387393951\n",
      "epoch: 6 step: 179, loss is 0.14628319442272186\n",
      "epoch: 6 step: 180, loss is 0.07445883750915527\n",
      "epoch: 6 step: 181, loss is 0.0685741975903511\n",
      "epoch: 6 step: 182, loss is 0.1808835119009018\n",
      "epoch: 6 step: 183, loss is 0.15384702384471893\n",
      "epoch: 6 step: 184, loss is 0.09056948125362396\n",
      "epoch: 6 step: 185, loss is 0.25687485933303833\n",
      "epoch: 6 step: 186, loss is 0.04460868239402771\n",
      "epoch: 6 step: 187, loss is 0.2262755036354065\n",
      "epoch: 6 step: 188, loss is 0.06801772117614746\n",
      "epoch: 6 step: 189, loss is 0.1286390870809555\n",
      "epoch: 6 step: 190, loss is 0.07681199908256531\n",
      "epoch: 6 step: 191, loss is 0.20326457917690277\n",
      "epoch: 6 step: 192, loss is 0.10493398457765579\n",
      "epoch: 6 step: 193, loss is 0.11643461138010025\n",
      "epoch: 6 step: 194, loss is 0.22899867594242096\n",
      "epoch: 6 step: 195, loss is 0.1573651134967804\n",
      "epoch: 6 step: 196, loss is 0.10331367701292038\n",
      "epoch: 6 step: 197, loss is 0.0628427192568779\n",
      "epoch: 6 step: 198, loss is 0.180538609623909\n",
      "epoch: 6 step: 199, loss is 0.21448823809623718\n",
      "epoch: 6 step: 200, loss is 0.051340363919734955\n",
      "epoch: 6 step: 201, loss is 0.055223576724529266\n",
      "epoch: 6 step: 202, loss is 0.06757624447345734\n",
      "epoch: 6 step: 203, loss is 0.1186213418841362\n",
      "epoch: 6 step: 204, loss is 0.18490353226661682\n",
      "epoch: 6 step: 205, loss is 0.12729036808013916\n",
      "epoch: 6 step: 206, loss is 0.10695980489253998\n",
      "epoch: 6 step: 207, loss is 0.06704896688461304\n",
      "epoch: 6 step: 208, loss is 0.2443542331457138\n",
      "epoch: 6 step: 209, loss is 0.11899474263191223\n",
      "epoch: 6 step: 210, loss is 0.16601859033107758\n",
      "epoch: 6 step: 211, loss is 0.1080114096403122\n",
      "epoch: 6 step: 212, loss is 0.06554102897644043\n",
      "epoch: 6 step: 213, loss is 0.08681393414735794\n",
      "epoch: 6 step: 214, loss is 0.04794974997639656\n",
      "epoch: 6 step: 215, loss is 0.10659689456224442\n",
      "epoch: 6 step: 216, loss is 0.20642121136188507\n",
      "epoch: 6 step: 217, loss is 0.1501978188753128\n",
      "epoch: 6 step: 218, loss is 0.23598767817020416\n",
      "epoch: 6 step: 219, loss is 0.19677704572677612\n",
      "epoch: 6 step: 220, loss is 0.10005223751068115\n",
      "epoch: 6 step: 221, loss is 0.16107356548309326\n",
      "epoch: 6 step: 222, loss is 0.09720766544342041\n",
      "epoch: 6 step: 223, loss is 0.1929667890071869\n",
      "epoch: 6 step: 224, loss is 0.13394202291965485\n",
      "epoch: 6 step: 225, loss is 0.12993034720420837\n",
      "epoch: 6 step: 226, loss is 0.1107330322265625\n",
      "epoch: 6 step: 227, loss is 0.08549989014863968\n",
      "epoch: 6 step: 228, loss is 0.11796065419912338\n",
      "epoch: 6 step: 229, loss is 0.11730807274580002\n",
      "epoch: 6 step: 230, loss is 0.13257010281085968\n",
      "epoch: 6 step: 231, loss is 0.053138576447963715\n",
      "epoch: 6 step: 232, loss is 0.20228983461856842\n",
      "epoch: 6 step: 233, loss is 0.06238328665494919\n",
      "epoch: 6 step: 234, loss is 0.0722535103559494\n",
      "epoch: 6 step: 235, loss is 0.17600761353969574\n",
      "epoch: 6 step: 236, loss is 0.14507345855236053\n",
      "epoch: 6 step: 237, loss is 0.12317655980587006\n",
      "epoch: 6 step: 238, loss is 0.20623207092285156\n",
      "epoch: 6 step: 239, loss is 0.2639493942260742\n",
      "epoch: 6 step: 240, loss is 0.07468395680189133\n",
      "epoch: 6 step: 241, loss is 0.1480916142463684\n",
      "epoch: 6 step: 242, loss is 0.07718905806541443\n",
      "epoch: 6 step: 243, loss is 0.07013407349586487\n",
      "epoch: 6 step: 244, loss is 0.13358719646930695\n",
      "epoch: 6 step: 245, loss is 0.12969768047332764\n",
      "epoch: 6 step: 246, loss is 0.0894245132803917\n",
      "epoch: 6 step: 247, loss is 0.14683391153812408\n",
      "epoch: 6 step: 248, loss is 0.10860805958509445\n",
      "epoch: 6 step: 249, loss is 0.06369207054376602\n",
      "epoch: 6 step: 250, loss is 0.2549377977848053\n",
      "epoch: 6 step: 251, loss is 0.15735721588134766\n",
      "epoch: 6 step: 252, loss is 0.10636366158723831\n",
      "epoch: 6 step: 253, loss is 0.1321892887353897\n",
      "epoch: 6 step: 254, loss is 0.12049243599176407\n",
      "epoch: 6 step: 255, loss is 0.11246092617511749\n",
      "epoch: 6 step: 256, loss is 0.172724187374115\n",
      "epoch: 6 step: 257, loss is 0.11707167327404022\n",
      "epoch: 6 step: 258, loss is 0.08687852323055267\n",
      "epoch: 6 step: 259, loss is 0.1563309282064438\n",
      "epoch: 6 step: 260, loss is 0.17607799172401428\n",
      "epoch: 6 step: 261, loss is 0.06103922054171562\n",
      "epoch: 6 step: 262, loss is 0.11516980826854706\n",
      "epoch: 6 step: 263, loss is 0.13057075440883636\n",
      "epoch: 6 step: 264, loss is 0.047233834862709045\n",
      "epoch: 6 step: 265, loss is 0.06186310946941376\n",
      "epoch: 6 step: 266, loss is 0.13590700924396515\n",
      "epoch: 6 step: 267, loss is 0.19259004294872284\n",
      "epoch: 6 step: 268, loss is 0.1665501594543457\n",
      "epoch: 6 step: 269, loss is 0.07723835110664368\n",
      "epoch: 6 step: 270, loss is 0.21984253823757172\n",
      "epoch: 6 step: 271, loss is 0.1407981663942337\n",
      "epoch: 6 step: 272, loss is 0.2333928942680359\n",
      "epoch: 6 step: 273, loss is 0.2376645803451538\n",
      "epoch: 6 step: 274, loss is 0.14191243052482605\n",
      "epoch: 6 step: 275, loss is 0.14003005623817444\n",
      "epoch: 6 step: 276, loss is 0.16312113404273987\n",
      "epoch: 6 step: 277, loss is 0.16851234436035156\n",
      "epoch: 6 step: 278, loss is 0.13058727979660034\n",
      "epoch: 6 step: 279, loss is 0.15034891664981842\n",
      "epoch: 6 step: 280, loss is 0.09688693284988403\n",
      "epoch: 6 step: 281, loss is 0.07800935953855515\n",
      "epoch: 6 step: 282, loss is 0.1416691243648529\n",
      "epoch: 6 step: 283, loss is 0.2940232455730438\n",
      "epoch: 6 step: 284, loss is 0.07179487496614456\n",
      "epoch: 6 step: 285, loss is 0.2552134692668915\n",
      "epoch: 6 step: 286, loss is 0.05446965992450714\n",
      "epoch: 6 step: 287, loss is 0.20861735939979553\n",
      "epoch: 6 step: 288, loss is 0.19204050302505493\n",
      "epoch: 6 step: 289, loss is 0.06107344850897789\n",
      "epoch: 6 step: 290, loss is 0.16053909063339233\n",
      "epoch: 6 step: 291, loss is 0.2026437222957611\n",
      "epoch: 6 step: 292, loss is 0.21391865611076355\n",
      "epoch: 6 step: 293, loss is 0.04256178066134453\n",
      "epoch: 6 step: 294, loss is 0.1897886097431183\n",
      "epoch: 6 step: 295, loss is 0.16834166646003723\n",
      "epoch: 6 step: 296, loss is 0.12795886397361755\n",
      "epoch: 6 step: 297, loss is 0.13322344422340393\n",
      "epoch: 6 step: 298, loss is 0.09540081769227982\n",
      "epoch: 6 step: 299, loss is 0.06463741511106491\n",
      "epoch: 6 step: 300, loss is 0.13704298436641693\n",
      "epoch: 6 step: 301, loss is 0.11467093974351883\n",
      "epoch: 6 step: 302, loss is 0.16576021909713745\n",
      "epoch: 6 step: 303, loss is 0.22671934962272644\n",
      "epoch: 6 step: 304, loss is 0.1074453741312027\n",
      "epoch: 6 step: 305, loss is 0.2913692891597748\n",
      "epoch: 6 step: 306, loss is 0.13941437005996704\n",
      "epoch: 6 step: 307, loss is 0.20427808165550232\n",
      "epoch: 6 step: 308, loss is 0.20021095871925354\n",
      "epoch: 6 step: 309, loss is 0.2434309422969818\n",
      "epoch: 6 step: 310, loss is 0.18989557027816772\n",
      "epoch: 6 step: 311, loss is 0.17804110050201416\n",
      "epoch: 6 step: 312, loss is 0.04935815557837486\n",
      "epoch: 6 step: 313, loss is 0.14907582104206085\n",
      "epoch: 6 step: 314, loss is 0.16883903741836548\n",
      "epoch: 6 step: 315, loss is 0.11222502589225769\n",
      "epoch: 6 step: 316, loss is 0.15132951736450195\n",
      "epoch: 6 step: 317, loss is 0.14136403799057007\n",
      "epoch: 6 step: 318, loss is 0.12367436289787292\n",
      "epoch: 6 step: 319, loss is 0.07749147713184357\n",
      "epoch: 6 step: 320, loss is 0.34482595324516296\n",
      "epoch: 6 step: 321, loss is 0.07851441949605942\n",
      "epoch: 6 step: 322, loss is 0.09007687866687775\n",
      "epoch: 6 step: 323, loss is 0.1138373613357544\n",
      "epoch: 6 step: 324, loss is 0.14758652448654175\n",
      "epoch: 6 step: 325, loss is 0.1540926843881607\n",
      "epoch: 6 step: 326, loss is 0.14495937526226044\n",
      "epoch: 6 step: 327, loss is 0.14044690132141113\n",
      "epoch: 6 step: 328, loss is 0.10612627863883972\n",
      "epoch: 6 step: 329, loss is 0.1069740429520607\n",
      "epoch: 6 step: 330, loss is 0.08920140564441681\n",
      "epoch: 6 step: 331, loss is 0.11450257897377014\n",
      "epoch: 6 step: 332, loss is 0.1814606636762619\n",
      "epoch: 6 step: 333, loss is 0.19682885706424713\n",
      "epoch: 6 step: 334, loss is 0.16237175464630127\n",
      "epoch: 6 step: 335, loss is 0.13684509694576263\n",
      "epoch: 6 step: 336, loss is 0.07280991971492767\n",
      "epoch: 6 step: 337, loss is 0.15300871431827545\n",
      "epoch: 6 step: 338, loss is 0.09047983586788177\n",
      "epoch: 6 step: 339, loss is 0.1290324628353119\n",
      "epoch: 6 step: 340, loss is 0.10132432729005814\n",
      "epoch: 6 step: 341, loss is 0.1416468471288681\n",
      "epoch: 6 step: 342, loss is 0.118356853723526\n",
      "epoch: 6 step: 343, loss is 0.08021528273820877\n",
      "epoch: 6 step: 344, loss is 0.10010573267936707\n",
      "epoch: 6 step: 345, loss is 0.21078059077262878\n",
      "epoch: 6 step: 346, loss is 0.14905408024787903\n",
      "epoch: 6 step: 347, loss is 0.3202023208141327\n",
      "epoch: 6 step: 348, loss is 0.19654864072799683\n",
      "epoch: 6 step: 349, loss is 0.19874629378318787\n",
      "epoch: 6 step: 350, loss is 0.2580367922782898\n",
      "epoch: 6 step: 351, loss is 0.08847396075725555\n",
      "epoch: 6 step: 352, loss is 0.14332513511180878\n",
      "epoch: 6 step: 353, loss is 0.29317402839660645\n",
      "epoch: 6 step: 354, loss is 0.08454478532075882\n",
      "epoch: 6 step: 355, loss is 0.08787926286458969\n",
      "epoch: 6 step: 356, loss is 0.09792565554380417\n",
      "epoch: 6 step: 357, loss is 0.11357365548610687\n",
      "epoch: 6 step: 358, loss is 0.12768737971782684\n",
      "epoch: 6 step: 359, loss is 0.10489843785762787\n",
      "epoch: 6 step: 360, loss is 0.07967289537191391\n",
      "epoch: 6 step: 361, loss is 0.07415962964296341\n",
      "epoch: 6 step: 362, loss is 0.15196622908115387\n",
      "epoch: 6 step: 363, loss is 0.16712148487567902\n",
      "epoch: 6 step: 364, loss is 0.14925700426101685\n",
      "epoch: 6 step: 365, loss is 0.07673583924770355\n",
      "epoch: 6 step: 366, loss is 0.13253405690193176\n",
      "epoch: 6 step: 367, loss is 0.07781859487295151\n",
      "epoch: 6 step: 368, loss is 0.08792818337678909\n",
      "epoch: 6 step: 369, loss is 0.14356526732444763\n",
      "epoch: 6 step: 370, loss is 0.07323025912046432\n",
      "epoch: 6 step: 371, loss is 0.17032359540462494\n",
      "epoch: 6 step: 372, loss is 0.1786210834980011\n",
      "epoch: 6 step: 373, loss is 0.09269074350595474\n",
      "epoch: 6 step: 374, loss is 0.15603648126125336\n",
      "epoch: 6 step: 375, loss is 0.1189623475074768\n",
      "epoch: 6 step: 376, loss is 0.13793779909610748\n",
      "epoch: 6 step: 377, loss is 0.06353094428777695\n",
      "epoch: 6 step: 378, loss is 0.12023814022541046\n",
      "epoch: 6 step: 379, loss is 0.18973805010318756\n",
      "epoch: 6 step: 380, loss is 0.22075223922729492\n",
      "epoch: 6 step: 381, loss is 0.1278597116470337\n",
      "epoch: 6 step: 382, loss is 0.1016918197274208\n",
      "epoch: 6 step: 383, loss is 0.15700237452983856\n",
      "epoch: 6 step: 384, loss is 0.08416429907083511\n",
      "epoch: 6 step: 385, loss is 0.11541575193405151\n",
      "epoch: 6 step: 386, loss is 0.29782554507255554\n",
      "epoch: 6 step: 387, loss is 0.1894099861383438\n",
      "epoch: 6 step: 388, loss is 0.3100237548351288\n",
      "epoch: 6 step: 389, loss is 0.12826427817344666\n",
      "epoch: 6 step: 390, loss is 0.043993670493364334\n",
      "epoch: 6 step: 391, loss is 0.14859791100025177\n",
      "epoch: 6 step: 392, loss is 0.21409820020198822\n",
      "epoch: 6 step: 393, loss is 0.05901717022061348\n",
      "epoch: 6 step: 394, loss is 0.04896373674273491\n",
      "epoch: 6 step: 395, loss is 0.11710719019174576\n",
      "epoch: 6 step: 396, loss is 0.10197219252586365\n",
      "epoch: 6 step: 397, loss is 0.18966670334339142\n",
      "epoch: 6 step: 398, loss is 0.17429475486278534\n",
      "epoch: 6 step: 399, loss is 0.07926718890666962\n",
      "epoch: 6 step: 400, loss is 0.1074666902422905\n",
      "epoch: 6 step: 401, loss is 0.13126273453235626\n",
      "epoch: 6 step: 402, loss is 0.25000157952308655\n",
      "epoch: 6 step: 403, loss is 0.12724605202674866\n",
      "epoch: 6 step: 404, loss is 0.08415801078081131\n",
      "epoch: 6 step: 405, loss is 0.23420138657093048\n",
      "epoch: 6 step: 406, loss is 0.15548266470432281\n",
      "epoch: 6 step: 407, loss is 0.10476638376712799\n",
      "epoch: 6 step: 408, loss is 0.10487790405750275\n",
      "epoch: 6 step: 409, loss is 0.17924775183200836\n",
      "epoch: 6 step: 410, loss is 0.14274069666862488\n",
      "epoch: 6 step: 411, loss is 0.17590022087097168\n",
      "epoch: 6 step: 412, loss is 0.052294354885816574\n",
      "epoch: 6 step: 413, loss is 0.14344869554042816\n",
      "epoch: 6 step: 414, loss is 0.11931489408016205\n",
      "epoch: 6 step: 415, loss is 0.05057250335812569\n",
      "epoch: 6 step: 416, loss is 0.13433462381362915\n",
      "epoch: 6 step: 417, loss is 0.0510469414293766\n",
      "epoch: 6 step: 418, loss is 0.176214799284935\n",
      "epoch: 6 step: 419, loss is 0.21171995997428894\n",
      "epoch: 6 step: 420, loss is 0.09084805846214294\n",
      "epoch: 6 step: 421, loss is 0.2305528223514557\n",
      "epoch: 6 step: 422, loss is 0.13031132519245148\n",
      "epoch: 6 step: 423, loss is 0.15150590240955353\n",
      "epoch: 6 step: 424, loss is 0.13957330584526062\n",
      "epoch: 6 step: 425, loss is 0.11953561007976532\n",
      "epoch: 6 step: 426, loss is 0.29259708523750305\n",
      "epoch: 6 step: 427, loss is 0.17708824574947357\n",
      "epoch: 6 step: 428, loss is 0.13643808662891388\n",
      "epoch: 6 step: 429, loss is 0.1460416167974472\n",
      "epoch: 6 step: 430, loss is 0.17065072059631348\n",
      "epoch: 6 step: 431, loss is 0.17741085588932037\n",
      "epoch: 6 step: 432, loss is 0.07548977434635162\n",
      "epoch: 6 step: 433, loss is 0.12818090617656708\n",
      "epoch: 6 step: 434, loss is 0.15047690272331238\n",
      "epoch: 6 step: 435, loss is 0.22779060900211334\n",
      "epoch: 6 step: 436, loss is 0.14496058225631714\n",
      "epoch: 6 step: 437, loss is 0.2998795807361603\n",
      "epoch: 6 step: 438, loss is 0.1524021476507187\n",
      "epoch: 6 step: 439, loss is 0.1476147621870041\n",
      "epoch: 6 step: 440, loss is 0.11599381268024445\n",
      "epoch: 6 step: 441, loss is 0.11621720343828201\n",
      "epoch: 6 step: 442, loss is 0.07022445648908615\n",
      "epoch: 6 step: 443, loss is 0.19356730580329895\n",
      "epoch: 6 step: 444, loss is 0.10173256695270538\n",
      "epoch: 6 step: 445, loss is 0.03475504741072655\n",
      "epoch: 6 step: 446, loss is 0.28560250997543335\n",
      "epoch: 6 step: 447, loss is 0.16339831054210663\n",
      "epoch: 6 step: 448, loss is 0.25160011649131775\n",
      "epoch: 6 step: 449, loss is 0.15330569446086884\n",
      "epoch: 6 step: 450, loss is 0.16501325368881226\n",
      "epoch: 6 step: 451, loss is 0.08882873505353928\n",
      "epoch: 6 step: 452, loss is 0.2075027972459793\n",
      "epoch: 6 step: 453, loss is 0.09790759533643723\n",
      "epoch: 6 step: 454, loss is 0.13203944265842438\n",
      "epoch: 6 step: 455, loss is 0.12265443801879883\n",
      "epoch: 6 step: 456, loss is 0.18294581770896912\n",
      "epoch: 6 step: 457, loss is 0.15727490186691284\n",
      "epoch: 6 step: 458, loss is 0.2380158007144928\n",
      "epoch: 6 step: 459, loss is 0.12981662154197693\n",
      "epoch: 6 step: 460, loss is 0.08283787220716476\n",
      "epoch: 6 step: 461, loss is 0.19482414424419403\n",
      "epoch: 6 step: 462, loss is 0.10357918590307236\n",
      "epoch: 6 step: 463, loss is 0.19548742473125458\n",
      "epoch: 6 step: 464, loss is 0.0997319445014\n",
      "epoch: 6 step: 465, loss is 0.1913638710975647\n",
      "epoch: 6 step: 466, loss is 0.12472550570964813\n",
      "epoch: 6 step: 467, loss is 0.0871395543217659\n",
      "epoch: 6 step: 468, loss is 0.1035647764801979\n",
      "epoch: 6 step: 469, loss is 0.16762246191501617\n",
      "epoch: 6 step: 470, loss is 0.17684127390384674\n",
      "epoch: 6 step: 471, loss is 0.16936686635017395\n",
      "epoch: 6 step: 472, loss is 0.06258217245340347\n",
      "epoch: 6 step: 473, loss is 0.22247101366519928\n",
      "epoch: 6 step: 474, loss is 0.10783912986516953\n",
      "epoch: 6 step: 475, loss is 0.06495620310306549\n",
      "epoch: 6 step: 476, loss is 0.10641704499721527\n",
      "epoch: 6 step: 477, loss is 0.23483920097351074\n",
      "epoch: 6 step: 478, loss is 0.2109016627073288\n",
      "epoch: 6 step: 479, loss is 0.04727968946099281\n",
      "epoch: 6 step: 480, loss is 0.09319354593753815\n",
      "epoch: 6 step: 481, loss is 0.06106715276837349\n",
      "epoch: 6 step: 482, loss is 0.12804128229618073\n",
      "epoch: 6 step: 483, loss is 0.15468239784240723\n",
      "epoch: 6 step: 484, loss is 0.1766907125711441\n",
      "epoch: 6 step: 485, loss is 0.13221675157546997\n",
      "epoch: 6 step: 486, loss is 0.08189921081066132\n",
      "epoch: 6 step: 487, loss is 0.3097791075706482\n",
      "epoch: 6 step: 488, loss is 0.12329665571451187\n",
      "epoch: 6 step: 489, loss is 0.23960387706756592\n",
      "epoch: 6 step: 490, loss is 0.15273703634738922\n",
      "epoch: 6 step: 491, loss is 0.1802128404378891\n",
      "epoch: 6 step: 492, loss is 0.16676050424575806\n",
      "epoch: 6 step: 493, loss is 0.21877256035804749\n",
      "epoch: 6 step: 494, loss is 0.15255944430828094\n",
      "epoch: 6 step: 495, loss is 0.12758107483386993\n",
      "epoch: 6 step: 496, loss is 0.05408719927072525\n",
      "epoch: 6 step: 497, loss is 0.1783447265625\n",
      "epoch: 6 step: 498, loss is 0.12465736269950867\n",
      "epoch: 6 step: 499, loss is 0.16206960380077362\n",
      "epoch: 6 step: 500, loss is 0.1281626671552658\n",
      "epoch: 6 step: 501, loss is 0.10127948224544525\n",
      "epoch: 6 step: 502, loss is 0.15382593870162964\n",
      "epoch: 6 step: 503, loss is 0.19553013145923615\n",
      "epoch: 6 step: 504, loss is 0.17113015055656433\n",
      "epoch: 6 step: 505, loss is 0.1590370088815689\n",
      "epoch: 6 step: 506, loss is 0.1809081733226776\n",
      "epoch: 6 step: 507, loss is 0.1330273449420929\n",
      "epoch: 6 step: 508, loss is 0.1946648508310318\n",
      "epoch: 6 step: 509, loss is 0.18062511086463928\n",
      "epoch: 6 step: 510, loss is 0.19611167907714844\n",
      "epoch: 6 step: 511, loss is 0.17987675964832306\n",
      "epoch: 6 step: 512, loss is 0.12230075150728226\n",
      "epoch: 6 step: 513, loss is 0.055439289659261703\n",
      "epoch: 6 step: 514, loss is 0.13923390209674835\n",
      "epoch: 6 step: 515, loss is 0.12219534069299698\n",
      "epoch: 6 step: 516, loss is 0.08860409259796143\n",
      "epoch: 6 step: 517, loss is 0.1302984654903412\n",
      "epoch: 6 step: 518, loss is 0.22186285257339478\n",
      "epoch: 6 step: 519, loss is 0.21680177748203278\n",
      "epoch: 6 step: 520, loss is 0.08593882620334625\n",
      "epoch: 6 step: 521, loss is 0.052775219082832336\n",
      "epoch: 6 step: 522, loss is 0.1317850798368454\n",
      "epoch: 6 step: 523, loss is 0.06890694797039032\n",
      "epoch: 6 step: 524, loss is 0.1311406046152115\n",
      "epoch: 6 step: 525, loss is 0.16708645224571228\n",
      "epoch: 6 step: 526, loss is 0.16445313394069672\n",
      "epoch: 6 step: 527, loss is 0.05307859927415848\n",
      "epoch: 6 step: 528, loss is 0.12934383749961853\n",
      "epoch: 6 step: 529, loss is 0.17283296585083008\n",
      "epoch: 6 step: 530, loss is 0.1643580198287964\n",
      "epoch: 6 step: 531, loss is 0.16743123531341553\n",
      "epoch: 6 step: 532, loss is 0.15176960825920105\n",
      "epoch: 6 step: 533, loss is 0.09081584960222244\n",
      "epoch: 6 step: 534, loss is 0.1820475161075592\n",
      "epoch: 6 step: 535, loss is 0.12691503763198853\n",
      "epoch: 6 step: 536, loss is 0.16108638048171997\n",
      "epoch: 6 step: 537, loss is 0.17433156073093414\n",
      "epoch: 6 step: 538, loss is 0.2043406367301941\n",
      "epoch: 6 step: 539, loss is 0.21969850361347198\n",
      "epoch: 6 step: 540, loss is 0.08060577511787415\n",
      "epoch: 6 step: 541, loss is 0.1430315226316452\n",
      "epoch: 6 step: 542, loss is 0.06962566822767258\n",
      "epoch: 6 step: 543, loss is 0.1604297161102295\n",
      "epoch: 6 step: 544, loss is 0.3107694387435913\n",
      "epoch: 6 step: 545, loss is 0.1837504506111145\n",
      "epoch: 6 step: 546, loss is 0.19771318137645721\n",
      "epoch: 6 step: 547, loss is 0.10961522161960602\n",
      "epoch: 6 step: 548, loss is 0.17634044587612152\n",
      "epoch: 6 step: 549, loss is 0.09361470490694046\n",
      "epoch: 6 step: 550, loss is 0.19983284175395966\n",
      "epoch: 6 step: 551, loss is 0.19786480069160461\n",
      "epoch: 6 step: 552, loss is 0.1422395408153534\n",
      "epoch: 6 step: 553, loss is 0.11805536597967148\n",
      "epoch: 6 step: 554, loss is 0.15305288136005402\n",
      "epoch: 6 step: 555, loss is 0.13456808030605316\n",
      "epoch: 6 step: 556, loss is 0.1521053910255432\n",
      "epoch: 6 step: 557, loss is 0.12760961055755615\n",
      "epoch: 6 step: 558, loss is 0.1575552225112915\n",
      "epoch: 6 step: 559, loss is 0.15282896161079407\n",
      "epoch: 6 step: 560, loss is 0.11416608840227127\n",
      "epoch: 6 step: 561, loss is 0.2350139319896698\n",
      "epoch: 6 step: 562, loss is 0.23915618658065796\n",
      "epoch: 6 step: 563, loss is 0.1367788165807724\n",
      "epoch: 6 step: 564, loss is 0.2340647280216217\n",
      "epoch: 6 step: 565, loss is 0.10647807270288467\n",
      "epoch: 6 step: 566, loss is 0.19381418824195862\n",
      "epoch: 6 step: 567, loss is 0.21385972201824188\n",
      "epoch: 6 step: 568, loss is 0.11899691820144653\n",
      "epoch: 6 step: 569, loss is 0.04130662977695465\n",
      "epoch: 6 step: 570, loss is 0.0843665823340416\n",
      "epoch: 6 step: 571, loss is 0.17834894359111786\n",
      "epoch: 6 step: 572, loss is 0.11732642352581024\n",
      "epoch: 6 step: 573, loss is 0.09789098054170609\n",
      "epoch: 6 step: 574, loss is 0.18195652961730957\n",
      "epoch: 6 step: 575, loss is 0.12782490253448486\n",
      "epoch: 6 step: 576, loss is 0.13267600536346436\n",
      "epoch: 6 step: 577, loss is 0.10625477135181427\n",
      "epoch: 6 step: 578, loss is 0.11535266786813736\n",
      "epoch: 6 step: 579, loss is 0.08574799448251724\n",
      "epoch: 6 step: 580, loss is 0.10749232023954391\n",
      "epoch: 6 step: 581, loss is 0.3760553300380707\n",
      "epoch: 6 step: 582, loss is 0.04008789733052254\n",
      "epoch: 6 step: 583, loss is 0.07218104600906372\n",
      "epoch: 6 step: 584, loss is 0.12632441520690918\n",
      "epoch: 6 step: 585, loss is 0.19316278398036957\n",
      "epoch: 6 step: 586, loss is 0.23166920244693756\n",
      "epoch: 6 step: 587, loss is 0.17327404022216797\n",
      "epoch: 6 step: 588, loss is 0.10424277931451797\n",
      "epoch: 6 step: 589, loss is 0.1435895711183548\n",
      "epoch: 6 step: 590, loss is 0.11381582170724869\n",
      "epoch: 6 step: 591, loss is 0.16550256311893463\n",
      "epoch: 6 step: 592, loss is 0.16098324954509735\n",
      "epoch: 6 step: 593, loss is 0.09591273963451385\n",
      "epoch: 6 step: 594, loss is 0.1230243593454361\n",
      "epoch: 6 step: 595, loss is 0.0996553972363472\n",
      "epoch: 6 step: 596, loss is 0.15764938294887543\n",
      "epoch: 6 step: 597, loss is 0.07861372828483582\n",
      "epoch: 6 step: 598, loss is 0.10257398337125778\n",
      "epoch: 6 step: 599, loss is 0.13439524173736572\n",
      "epoch: 6 step: 600, loss is 0.15050406754016876\n",
      "epoch: 6 step: 601, loss is 0.08135346323251724\n",
      "epoch: 6 step: 602, loss is 0.14237767457962036\n",
      "epoch: 6 step: 603, loss is 0.07544836401939392\n",
      "epoch: 6 step: 604, loss is 0.11737078428268433\n",
      "epoch: 6 step: 605, loss is 0.09631699323654175\n",
      "epoch: 6 step: 606, loss is 0.15574833750724792\n",
      "epoch: 6 step: 607, loss is 0.1703614443540573\n",
      "epoch: 6 step: 608, loss is 0.11238255351781845\n",
      "epoch: 6 step: 609, loss is 0.1332375705242157\n",
      "epoch: 6 step: 610, loss is 0.25445273518562317\n",
      "epoch: 6 step: 611, loss is 0.12296014279127121\n",
      "epoch: 6 step: 612, loss is 0.10638578236103058\n",
      "epoch: 6 step: 613, loss is 0.16736271977424622\n",
      "epoch: 6 step: 614, loss is 0.1387866884469986\n",
      "epoch: 6 step: 615, loss is 0.06738670915365219\n",
      "epoch: 6 step: 616, loss is 0.18813391029834747\n",
      "epoch: 6 step: 617, loss is 0.1909671127796173\n",
      "epoch: 6 step: 618, loss is 0.27062875032424927\n",
      "epoch: 6 step: 619, loss is 0.21699179708957672\n",
      "epoch: 6 step: 620, loss is 0.10504484176635742\n",
      "epoch: 6 step: 621, loss is 0.07672667503356934\n",
      "epoch: 6 step: 622, loss is 0.09506067633628845\n",
      "epoch: 6 step: 623, loss is 0.2074604332447052\n",
      "epoch: 6 step: 624, loss is 0.16154928505420685\n",
      "epoch: 6 step: 625, loss is 0.27392518520355225\n",
      "epoch: 6 step: 626, loss is 0.16640588641166687\n",
      "epoch: 6 step: 627, loss is 0.16750220954418182\n",
      "epoch: 6 step: 628, loss is 0.11883817613124847\n",
      "epoch: 6 step: 629, loss is 0.29124215245246887\n",
      "epoch: 6 step: 630, loss is 0.07010143250226974\n",
      "epoch: 6 step: 631, loss is 0.052606020122766495\n",
      "epoch: 6 step: 632, loss is 0.2886289954185486\n",
      "epoch: 6 step: 633, loss is 0.09853032231330872\n",
      "epoch: 6 step: 634, loss is 0.1412922441959381\n",
      "epoch: 6 step: 635, loss is 0.1424361914396286\n",
      "epoch: 6 step: 636, loss is 0.23091554641723633\n",
      "epoch: 6 step: 637, loss is 0.1596250832080841\n",
      "epoch: 6 step: 638, loss is 0.14078804850578308\n",
      "epoch: 6 step: 639, loss is 0.12924310564994812\n",
      "epoch: 6 step: 640, loss is 0.2982027530670166\n",
      "epoch: 6 step: 641, loss is 0.18926849961280823\n",
      "epoch: 6 step: 642, loss is 0.21396329998970032\n",
      "epoch: 6 step: 643, loss is 0.23454958200454712\n",
      "epoch: 6 step: 644, loss is 0.11547738313674927\n",
      "epoch: 6 step: 645, loss is 0.19102351367473602\n",
      "epoch: 6 step: 646, loss is 0.06927528232336044\n",
      "epoch: 6 step: 647, loss is 0.15168902277946472\n",
      "epoch: 6 step: 648, loss is 0.17448024451732635\n",
      "epoch: 6 step: 649, loss is 0.23614157736301422\n",
      "epoch: 6 step: 650, loss is 0.22888849675655365\n",
      "epoch: 6 step: 651, loss is 0.1383076012134552\n",
      "epoch: 6 step: 652, loss is 0.08414236456155777\n",
      "epoch: 6 step: 653, loss is 0.12791606783866882\n",
      "epoch: 6 step: 654, loss is 0.10928252339363098\n",
      "epoch: 6 step: 655, loss is 0.1249702200293541\n",
      "epoch: 6 step: 656, loss is 0.06281518936157227\n",
      "epoch: 6 step: 657, loss is 0.2115732878446579\n",
      "epoch: 6 step: 658, loss is 0.14178580045700073\n",
      "epoch: 6 step: 659, loss is 0.3584868311882019\n",
      "epoch: 6 step: 660, loss is 0.10933226346969604\n",
      "epoch: 6 step: 661, loss is 0.1146160289645195\n",
      "epoch: 6 step: 662, loss is 0.15862113237380981\n",
      "epoch: 6 step: 663, loss is 0.19503089785575867\n",
      "epoch: 6 step: 664, loss is 0.08642391860485077\n",
      "epoch: 6 step: 665, loss is 0.07760433852672577\n",
      "epoch: 6 step: 666, loss is 0.09400226175785065\n",
      "epoch: 6 step: 667, loss is 0.1427663117647171\n",
      "epoch: 6 step: 668, loss is 0.2425697147846222\n",
      "epoch: 6 step: 669, loss is 0.09553064405918121\n",
      "epoch: 6 step: 670, loss is 0.07630150765180588\n",
      "epoch: 6 step: 671, loss is 0.0962790921330452\n",
      "epoch: 6 step: 672, loss is 0.18594765663146973\n",
      "epoch: 6 step: 673, loss is 0.14470545947551727\n",
      "epoch: 6 step: 674, loss is 0.12988895177841187\n",
      "epoch: 6 step: 675, loss is 0.2437380701303482\n",
      "epoch: 6 step: 676, loss is 0.17657214403152466\n",
      "epoch: 6 step: 677, loss is 0.10282765328884125\n",
      "epoch: 6 step: 678, loss is 0.06730664521455765\n",
      "epoch: 6 step: 679, loss is 0.13596859574317932\n",
      "epoch: 6 step: 680, loss is 0.09021035581827164\n",
      "epoch: 6 step: 681, loss is 0.11977595835924149\n",
      "epoch: 6 step: 682, loss is 0.12540385127067566\n",
      "epoch: 6 step: 683, loss is 0.12223799526691437\n",
      "epoch: 6 step: 684, loss is 0.14010433852672577\n",
      "epoch: 6 step: 685, loss is 0.06920073181390762\n",
      "epoch: 6 step: 686, loss is 0.0625975951552391\n",
      "epoch: 6 step: 687, loss is 0.03773577883839607\n",
      "epoch: 6 step: 688, loss is 0.16844072937965393\n",
      "epoch: 6 step: 689, loss is 0.08395274728536606\n",
      "epoch: 6 step: 690, loss is 0.1416422426700592\n",
      "epoch: 6 step: 691, loss is 0.1676606833934784\n",
      "epoch: 6 step: 692, loss is 0.06728963553905487\n",
      "epoch: 6 step: 693, loss is 0.12895844876766205\n",
      "epoch: 6 step: 694, loss is 0.28663402795791626\n",
      "epoch: 6 step: 695, loss is 0.09687797725200653\n",
      "epoch: 6 step: 696, loss is 0.13074883818626404\n",
      "epoch: 6 step: 697, loss is 0.11369655281305313\n",
      "epoch: 6 step: 698, loss is 0.19398610293865204\n",
      "epoch: 6 step: 699, loss is 0.2324473112821579\n",
      "epoch: 6 step: 700, loss is 0.23071525990962982\n",
      "epoch: 6 step: 701, loss is 0.10953895002603531\n",
      "epoch: 6 step: 702, loss is 0.09961032122373581\n",
      "epoch: 6 step: 703, loss is 0.16247060894966125\n",
      "epoch: 6 step: 704, loss is 0.12098143249750137\n",
      "epoch: 6 step: 705, loss is 0.2365732043981552\n",
      "epoch: 6 step: 706, loss is 0.1959320455789566\n",
      "epoch: 6 step: 707, loss is 0.15351195633411407\n",
      "epoch: 6 step: 708, loss is 0.18237920105457306\n",
      "epoch: 6 step: 709, loss is 0.10775701701641083\n",
      "epoch: 6 step: 710, loss is 0.13914209604263306\n",
      "epoch: 6 step: 711, loss is 0.13495752215385437\n",
      "epoch: 6 step: 712, loss is 0.11751671135425568\n",
      "epoch: 6 step: 713, loss is 0.09187131375074387\n",
      "epoch: 6 step: 714, loss is 0.054447636008262634\n",
      "epoch: 6 step: 715, loss is 0.20252302289009094\n",
      "epoch: 6 step: 716, loss is 0.15771308541297913\n",
      "epoch: 6 step: 717, loss is 0.23885317146778107\n",
      "epoch: 6 step: 718, loss is 0.2329631745815277\n",
      "epoch: 6 step: 719, loss is 0.19136910140514374\n",
      "epoch: 6 step: 720, loss is 0.1327279508113861\n",
      "epoch: 6 step: 721, loss is 0.18705615401268005\n",
      "epoch: 6 step: 722, loss is 0.18114745616912842\n",
      "epoch: 6 step: 723, loss is 0.11992321908473969\n",
      "epoch: 6 step: 724, loss is 0.1486954689025879\n",
      "epoch: 6 step: 725, loss is 0.06024305894970894\n",
      "epoch: 6 step: 726, loss is 0.25919821858406067\n",
      "epoch: 6 step: 727, loss is 0.2085510492324829\n",
      "epoch: 6 step: 728, loss is 0.03583982214331627\n",
      "epoch: 6 step: 729, loss is 0.1552671641111374\n",
      "epoch: 6 step: 730, loss is 0.12495234608650208\n",
      "epoch: 6 step: 731, loss is 0.07071846723556519\n",
      "epoch: 6 step: 732, loss is 0.07430172711610794\n",
      "epoch: 6 step: 733, loss is 0.1983404904603958\n",
      "epoch: 6 step: 734, loss is 0.17070889472961426\n",
      "epoch: 6 step: 735, loss is 0.06430473178625107\n",
      "epoch: 6 step: 736, loss is 0.2447633445262909\n",
      "epoch: 6 step: 737, loss is 0.03833698481321335\n",
      "epoch: 6 step: 738, loss is 0.21943031251430511\n",
      "epoch: 6 step: 739, loss is 0.127751886844635\n",
      "epoch: 6 step: 740, loss is 0.2436840534210205\n",
      "epoch: 6 step: 741, loss is 0.16761353611946106\n",
      "epoch: 6 step: 742, loss is 0.1567915678024292\n",
      "epoch: 6 step: 743, loss is 0.10071936994791031\n",
      "epoch: 6 step: 744, loss is 0.08536706864833832\n",
      "epoch: 6 step: 745, loss is 0.21772174537181854\n",
      "epoch: 6 step: 746, loss is 0.15553773939609528\n",
      "epoch: 6 step: 747, loss is 0.1319698691368103\n",
      "epoch: 6 step: 748, loss is 0.09125643968582153\n",
      "epoch: 6 step: 749, loss is 0.29424071311950684\n",
      "epoch: 6 step: 750, loss is 0.11558429151773453\n",
      "epoch: 6 step: 751, loss is 0.08616411685943604\n",
      "epoch: 6 step: 752, loss is 0.12224596738815308\n",
      "epoch: 6 step: 753, loss is 0.1084584966301918\n",
      "epoch: 6 step: 754, loss is 0.15045486390590668\n",
      "epoch: 6 step: 755, loss is 0.16080215573310852\n",
      "epoch: 6 step: 756, loss is 0.13267020881175995\n",
      "epoch: 6 step: 757, loss is 0.1388222724199295\n",
      "epoch: 6 step: 758, loss is 0.224828839302063\n",
      "epoch: 6 step: 759, loss is 0.1072688177227974\n",
      "epoch: 6 step: 760, loss is 0.1824982613325119\n",
      "epoch: 6 step: 761, loss is 0.22875358164310455\n",
      "epoch: 6 step: 762, loss is 0.19793803989887238\n",
      "epoch: 6 step: 763, loss is 0.05638841539621353\n",
      "epoch: 6 step: 764, loss is 0.08002069592475891\n",
      "epoch: 6 step: 765, loss is 0.09447315335273743\n",
      "epoch: 6 step: 766, loss is 0.15662556886672974\n",
      "epoch: 6 step: 767, loss is 0.13368837535381317\n",
      "epoch: 6 step: 768, loss is 0.14032812416553497\n",
      "epoch: 6 step: 769, loss is 0.25298377871513367\n",
      "epoch: 6 step: 770, loss is 0.12036391347646713\n",
      "epoch: 6 step: 771, loss is 0.22641144692897797\n",
      "epoch: 6 step: 772, loss is 0.1003095880150795\n",
      "epoch: 6 step: 773, loss is 0.20489689707756042\n",
      "epoch: 6 step: 774, loss is 0.09515232592821121\n",
      "epoch: 6 step: 775, loss is 0.256022185087204\n",
      "epoch: 6 step: 776, loss is 0.1047516018152237\n",
      "epoch: 6 step: 777, loss is 0.0905107781291008\n",
      "epoch: 6 step: 778, loss is 0.18433669209480286\n",
      "epoch: 6 step: 779, loss is 0.08918717503547668\n",
      "epoch: 6 step: 780, loss is 0.27286049723625183\n",
      "epoch: 6 step: 781, loss is 0.14016464352607727\n",
      "epoch: 6 step: 782, loss is 0.1410447210073471\n",
      "epoch: 6 step: 783, loss is 0.1343129724264145\n",
      "epoch: 6 step: 784, loss is 0.17550235986709595\n",
      "epoch: 6 step: 785, loss is 0.18070262670516968\n",
      "epoch: 6 step: 786, loss is 0.102986641228199\n",
      "epoch: 6 step: 787, loss is 0.02728952467441559\n",
      "epoch: 6 step: 788, loss is 0.105921171605587\n",
      "epoch: 6 step: 789, loss is 0.23177580535411835\n",
      "epoch: 6 step: 790, loss is 0.09218692779541016\n",
      "epoch: 6 step: 791, loss is 0.09326032549142838\n",
      "epoch: 6 step: 792, loss is 0.06560013443231583\n",
      "epoch: 6 step: 793, loss is 0.16117741167545319\n",
      "epoch: 6 step: 794, loss is 0.09703336656093597\n",
      "epoch: 6 step: 795, loss is 0.051944296807050705\n",
      "epoch: 6 step: 796, loss is 0.2206203043460846\n",
      "epoch: 6 step: 797, loss is 0.09147703647613525\n",
      "epoch: 6 step: 798, loss is 0.13502660393714905\n",
      "epoch: 6 step: 799, loss is 0.10396666079759598\n",
      "epoch: 6 step: 800, loss is 0.27558115124702454\n",
      "epoch: 6 step: 801, loss is 0.20823708176612854\n",
      "epoch: 6 step: 802, loss is 0.19242818653583527\n",
      "epoch: 6 step: 803, loss is 0.15739844739437103\n",
      "epoch: 6 step: 804, loss is 0.10461147129535675\n",
      "epoch: 6 step: 805, loss is 0.11349043995141983\n",
      "epoch: 6 step: 806, loss is 0.10934880375862122\n",
      "epoch: 6 step: 807, loss is 0.3855285942554474\n",
      "epoch: 6 step: 808, loss is 0.11391405761241913\n",
      "epoch: 6 step: 809, loss is 0.05181876942515373\n",
      "epoch: 6 step: 810, loss is 0.1126883253455162\n",
      "epoch: 6 step: 811, loss is 0.13905327022075653\n",
      "epoch: 6 step: 812, loss is 0.22217799723148346\n",
      "epoch: 6 step: 813, loss is 0.15280233323574066\n",
      "epoch: 6 step: 814, loss is 0.18410103023052216\n",
      "epoch: 6 step: 815, loss is 0.12548330426216125\n",
      "epoch: 6 step: 816, loss is 0.12811949849128723\n",
      "epoch: 6 step: 817, loss is 0.08208837360143661\n",
      "epoch: 6 step: 818, loss is 0.08684243261814117\n",
      "epoch: 6 step: 819, loss is 0.05489025637507439\n",
      "epoch: 6 step: 820, loss is 0.1997561752796173\n",
      "epoch: 6 step: 821, loss is 0.384782999753952\n",
      "epoch: 6 step: 822, loss is 0.16659730672836304\n",
      "epoch: 6 step: 823, loss is 0.11902926862239838\n",
      "epoch: 6 step: 824, loss is 0.12119004875421524\n",
      "epoch: 6 step: 825, loss is 0.05978965014219284\n",
      "epoch: 6 step: 826, loss is 0.046620357781648636\n",
      "epoch: 6 step: 827, loss is 0.08915288001298904\n",
      "epoch: 6 step: 828, loss is 0.17441391944885254\n",
      "epoch: 6 step: 829, loss is 0.13823819160461426\n",
      "epoch: 6 step: 830, loss is 0.075813889503479\n",
      "epoch: 6 step: 831, loss is 0.09548784047365189\n",
      "epoch: 6 step: 832, loss is 0.06088683754205704\n",
      "epoch: 6 step: 833, loss is 0.23571208119392395\n",
      "epoch: 6 step: 834, loss is 0.14862409234046936\n",
      "epoch: 6 step: 835, loss is 0.08894702047109604\n",
      "epoch: 6 step: 836, loss is 0.07046028971672058\n",
      "epoch: 6 step: 837, loss is 0.09499045461416245\n",
      "epoch: 6 step: 838, loss is 0.18983317911624908\n",
      "epoch: 6 step: 839, loss is 0.1507492959499359\n",
      "epoch: 6 step: 840, loss is 0.11677859723567963\n",
      "epoch: 6 step: 841, loss is 0.25208503007888794\n",
      "epoch: 6 step: 842, loss is 0.20134305953979492\n",
      "epoch: 6 step: 843, loss is 0.16898854076862335\n",
      "epoch: 6 step: 844, loss is 0.08785562962293625\n",
      "epoch: 6 step: 845, loss is 0.1269315928220749\n",
      "epoch: 6 step: 846, loss is 0.11961153894662857\n",
      "epoch: 6 step: 847, loss is 0.04582234472036362\n",
      "epoch: 6 step: 848, loss is 0.18662285804748535\n",
      "epoch: 6 step: 849, loss is 0.04883934184908867\n",
      "epoch: 6 step: 850, loss is 0.13637800514698029\n",
      "epoch: 6 step: 851, loss is 0.05412416160106659\n",
      "epoch: 6 step: 852, loss is 0.04175771400332451\n",
      "epoch: 6 step: 853, loss is 0.17424042522907257\n",
      "epoch: 6 step: 854, loss is 0.11995838582515717\n",
      "epoch: 6 step: 855, loss is 0.14431314170360565\n",
      "epoch: 6 step: 856, loss is 0.15892015397548676\n",
      "epoch: 6 step: 857, loss is 0.20778144896030426\n",
      "epoch: 6 step: 858, loss is 0.12061915546655655\n",
      "epoch: 6 step: 859, loss is 0.16211245954036713\n",
      "epoch: 6 step: 860, loss is 0.11371950805187225\n",
      "epoch: 6 step: 861, loss is 0.20142154395580292\n",
      "epoch: 6 step: 862, loss is 0.1521666944026947\n",
      "epoch: 6 step: 863, loss is 0.0935872346162796\n",
      "epoch: 6 step: 864, loss is 0.09850544482469559\n",
      "epoch: 6 step: 865, loss is 0.23635387420654297\n",
      "epoch: 6 step: 866, loss is 0.1492246389389038\n",
      "epoch: 6 step: 867, loss is 0.2861512005329132\n",
      "epoch: 6 step: 868, loss is 0.24024106562137604\n",
      "epoch: 6 step: 869, loss is 0.08706404268741608\n",
      "epoch: 6 step: 870, loss is 0.4329219460487366\n",
      "epoch: 6 step: 871, loss is 0.06571487337350845\n",
      "epoch: 6 step: 872, loss is 0.20167414844036102\n",
      "epoch: 6 step: 873, loss is 0.3751089870929718\n",
      "epoch: 6 step: 874, loss is 0.2614411413669586\n",
      "epoch: 6 step: 875, loss is 0.21328936517238617\n",
      "epoch: 6 step: 876, loss is 0.2963457703590393\n",
      "epoch: 6 step: 877, loss is 0.3118462860584259\n",
      "epoch: 6 step: 878, loss is 0.11984580755233765\n",
      "epoch: 6 step: 879, loss is 0.10569369792938232\n",
      "epoch: 6 step: 880, loss is 0.15011653304100037\n",
      "epoch: 6 step: 881, loss is 0.09931660443544388\n",
      "epoch: 6 step: 882, loss is 0.19488143920898438\n",
      "epoch: 6 step: 883, loss is 0.2293698936700821\n",
      "epoch: 6 step: 884, loss is 0.1752592921257019\n",
      "epoch: 6 step: 885, loss is 0.10218662768602371\n",
      "epoch: 6 step: 886, loss is 0.22872065007686615\n",
      "epoch: 6 step: 887, loss is 0.1892736256122589\n",
      "epoch: 6 step: 888, loss is 0.15325960516929626\n",
      "epoch: 6 step: 889, loss is 0.10517662763595581\n",
      "epoch: 6 step: 890, loss is 0.15741820633411407\n",
      "epoch: 6 step: 891, loss is 0.21919338405132294\n",
      "epoch: 6 step: 892, loss is 0.23026782274246216\n",
      "epoch: 6 step: 893, loss is 0.11761444807052612\n",
      "epoch: 6 step: 894, loss is 0.10738474875688553\n",
      "epoch: 6 step: 895, loss is 0.1965043991804123\n",
      "epoch: 6 step: 896, loss is 0.21768388152122498\n",
      "epoch: 6 step: 897, loss is 0.16469816863536835\n",
      "epoch: 6 step: 898, loss is 0.12321749329566956\n",
      "epoch: 6 step: 899, loss is 0.20447710156440735\n",
      "epoch: 6 step: 900, loss is 0.0592709556221962\n",
      "epoch: 6 step: 901, loss is 0.15373806655406952\n",
      "epoch: 6 step: 902, loss is 0.25043052434921265\n",
      "epoch: 6 step: 903, loss is 0.05095468834042549\n",
      "epoch: 6 step: 904, loss is 0.1663699895143509\n",
      "epoch: 6 step: 905, loss is 0.1284765899181366\n",
      "epoch: 6 step: 906, loss is 0.19433337450027466\n",
      "epoch: 6 step: 907, loss is 0.14935435354709625\n",
      "epoch: 6 step: 908, loss is 0.22052845358848572\n",
      "epoch: 6 step: 909, loss is 0.14834807813167572\n",
      "epoch: 6 step: 910, loss is 0.08541760593652725\n",
      "epoch: 6 step: 911, loss is 0.1402333825826645\n",
      "epoch: 6 step: 912, loss is 0.23146353662014008\n",
      "epoch: 6 step: 913, loss is 0.12006474286317825\n",
      "epoch: 6 step: 914, loss is 0.07001958042383194\n",
      "epoch: 6 step: 915, loss is 0.23552758991718292\n",
      "epoch: 6 step: 916, loss is 0.09016799181699753\n",
      "epoch: 6 step: 917, loss is 0.0679413229227066\n",
      "epoch: 6 step: 918, loss is 0.09840965270996094\n",
      "epoch: 6 step: 919, loss is 0.1627669334411621\n",
      "epoch: 6 step: 920, loss is 0.13194583356380463\n",
      "epoch: 6 step: 921, loss is 0.30485808849334717\n",
      "epoch: 6 step: 922, loss is 0.10885000973939896\n",
      "epoch: 6 step: 923, loss is 0.22344578802585602\n",
      "epoch: 6 step: 924, loss is 0.19880713522434235\n",
      "epoch: 6 step: 925, loss is 0.16291910409927368\n",
      "epoch: 6 step: 926, loss is 0.33951783180236816\n",
      "epoch: 6 step: 927, loss is 0.17904196679592133\n",
      "epoch: 6 step: 928, loss is 0.21766437590122223\n",
      "epoch: 6 step: 929, loss is 0.2706905007362366\n",
      "epoch: 6 step: 930, loss is 0.10838543623685837\n",
      "epoch: 6 step: 931, loss is 0.06844303756952286\n",
      "epoch: 6 step: 932, loss is 0.25845450162887573\n",
      "epoch: 6 step: 933, loss is 0.13234160840511322\n",
      "epoch: 6 step: 934, loss is 0.1721177101135254\n",
      "epoch: 6 step: 935, loss is 0.2578701674938202\n",
      "epoch: 6 step: 936, loss is 0.13835594058036804\n",
      "epoch: 6 step: 937, loss is 0.08616626262664795\n",
      "epoch: 7 step: 1, loss is 0.19240179657936096\n",
      "epoch: 7 step: 2, loss is 0.1374630331993103\n",
      "epoch: 7 step: 3, loss is 0.1685606688261032\n",
      "epoch: 7 step: 4, loss is 0.05227990075945854\n",
      "epoch: 7 step: 5, loss is 0.1150011494755745\n",
      "epoch: 7 step: 6, loss is 0.10861499607563019\n",
      "epoch: 7 step: 7, loss is 0.06152566149830818\n",
      "epoch: 7 step: 8, loss is 0.11746034771203995\n",
      "epoch: 7 step: 9, loss is 0.06855032593011856\n",
      "epoch: 7 step: 10, loss is 0.17462582886219025\n",
      "epoch: 7 step: 11, loss is 0.08931171894073486\n",
      "epoch: 7 step: 12, loss is 0.11302078515291214\n",
      "epoch: 7 step: 13, loss is 0.09534607082605362\n",
      "epoch: 7 step: 14, loss is 0.14551714062690735\n",
      "epoch: 7 step: 15, loss is 0.06948783993721008\n",
      "epoch: 7 step: 16, loss is 0.07486999034881592\n",
      "epoch: 7 step: 17, loss is 0.09780123084783554\n",
      "epoch: 7 step: 18, loss is 0.08914975076913834\n",
      "epoch: 7 step: 19, loss is 0.10825135558843613\n",
      "epoch: 7 step: 20, loss is 0.18266622722148895\n",
      "epoch: 7 step: 21, loss is 0.08942624181509018\n",
      "epoch: 7 step: 22, loss is 0.08815149962902069\n",
      "epoch: 7 step: 23, loss is 0.060721203684806824\n",
      "epoch: 7 step: 24, loss is 0.11497866362333298\n",
      "epoch: 7 step: 25, loss is 0.13999246060848236\n",
      "epoch: 7 step: 26, loss is 0.09009765088558197\n",
      "epoch: 7 step: 27, loss is 0.21737390756607056\n",
      "epoch: 7 step: 28, loss is 0.06058649718761444\n",
      "epoch: 7 step: 29, loss is 0.07636316120624542\n",
      "epoch: 7 step: 30, loss is 0.12481914460659027\n",
      "epoch: 7 step: 31, loss is 0.10073272138834\n",
      "epoch: 7 step: 32, loss is 0.15650369226932526\n",
      "epoch: 7 step: 33, loss is 0.16852468252182007\n",
      "epoch: 7 step: 34, loss is 0.06299164891242981\n",
      "epoch: 7 step: 35, loss is 0.08809288591146469\n",
      "epoch: 7 step: 36, loss is 0.12982472777366638\n",
      "epoch: 7 step: 37, loss is 0.057722125202417374\n",
      "epoch: 7 step: 38, loss is 0.11019870638847351\n",
      "epoch: 7 step: 39, loss is 0.07822182029485703\n",
      "epoch: 7 step: 40, loss is 0.03285377845168114\n",
      "epoch: 7 step: 41, loss is 0.09632333368062973\n",
      "epoch: 7 step: 42, loss is 0.07161421328783035\n",
      "epoch: 7 step: 43, loss is 0.05677172541618347\n",
      "epoch: 7 step: 44, loss is 0.017913522198796272\n",
      "epoch: 7 step: 45, loss is 0.1296847015619278\n",
      "epoch: 7 step: 46, loss is 0.1778569370508194\n",
      "epoch: 7 step: 47, loss is 0.15819191932678223\n",
      "epoch: 7 step: 48, loss is 0.21868249773979187\n",
      "epoch: 7 step: 49, loss is 0.0888914167881012\n",
      "epoch: 7 step: 50, loss is 0.14624600112438202\n",
      "epoch: 7 step: 51, loss is 0.1650385707616806\n",
      "epoch: 7 step: 52, loss is 0.12472465634346008\n",
      "epoch: 7 step: 53, loss is 0.09427570551633835\n",
      "epoch: 7 step: 54, loss is 0.23607172071933746\n",
      "epoch: 7 step: 55, loss is 0.09604299813508987\n",
      "epoch: 7 step: 56, loss is 0.095132976770401\n",
      "epoch: 7 step: 57, loss is 0.09328927844762802\n",
      "epoch: 7 step: 58, loss is 0.22785405814647675\n",
      "epoch: 7 step: 59, loss is 0.1696467101573944\n",
      "epoch: 7 step: 60, loss is 0.13928069174289703\n",
      "epoch: 7 step: 61, loss is 0.06815017759799957\n",
      "epoch: 7 step: 62, loss is 0.2254740595817566\n",
      "epoch: 7 step: 63, loss is 0.10920876264572144\n",
      "epoch: 7 step: 64, loss is 0.16676092147827148\n",
      "epoch: 7 step: 65, loss is 0.13999661803245544\n",
      "epoch: 7 step: 66, loss is 0.04089407995343208\n",
      "epoch: 7 step: 67, loss is 0.04849563539028168\n",
      "epoch: 7 step: 68, loss is 0.17125843465328217\n",
      "epoch: 7 step: 69, loss is 0.2134665697813034\n",
      "epoch: 7 step: 70, loss is 0.1805931180715561\n",
      "epoch: 7 step: 71, loss is 0.08161543309688568\n",
      "epoch: 7 step: 72, loss is 0.12607870995998383\n",
      "epoch: 7 step: 73, loss is 0.07877292484045029\n",
      "epoch: 7 step: 74, loss is 0.0414101779460907\n",
      "epoch: 7 step: 75, loss is 0.11638502776622772\n",
      "epoch: 7 step: 76, loss is 0.11245536059141159\n",
      "epoch: 7 step: 77, loss is 0.06997938454151154\n",
      "epoch: 7 step: 78, loss is 0.2032974511384964\n",
      "epoch: 7 step: 79, loss is 0.10383885353803635\n",
      "epoch: 7 step: 80, loss is 0.16856814920902252\n",
      "epoch: 7 step: 81, loss is 0.16536761820316315\n",
      "epoch: 7 step: 82, loss is 0.15397968888282776\n",
      "epoch: 7 step: 83, loss is 0.1597725749015808\n",
      "epoch: 7 step: 84, loss is 0.12025338411331177\n",
      "epoch: 7 step: 85, loss is 0.23348368704319\n",
      "epoch: 7 step: 86, loss is 0.08935320377349854\n",
      "epoch: 7 step: 87, loss is 0.09679906815290451\n",
      "epoch: 7 step: 88, loss is 0.25976744294166565\n",
      "epoch: 7 step: 89, loss is 0.13352766633033752\n",
      "epoch: 7 step: 90, loss is 0.1085907444357872\n",
      "epoch: 7 step: 91, loss is 0.17103570699691772\n",
      "epoch: 7 step: 92, loss is 0.0579284243285656\n",
      "epoch: 7 step: 93, loss is 0.07811427116394043\n",
      "epoch: 7 step: 94, loss is 0.12273597717285156\n",
      "epoch: 7 step: 95, loss is 0.07712594419717789\n",
      "epoch: 7 step: 96, loss is 0.09490636736154556\n",
      "epoch: 7 step: 97, loss is 0.13009460270404816\n",
      "epoch: 7 step: 98, loss is 0.09668247401714325\n",
      "epoch: 7 step: 99, loss is 0.08510120958089828\n",
      "epoch: 7 step: 100, loss is 0.12282689660787582\n",
      "epoch: 7 step: 101, loss is 0.1806001514196396\n",
      "epoch: 7 step: 102, loss is 0.12694895267486572\n",
      "epoch: 7 step: 103, loss is 0.11609227955341339\n",
      "epoch: 7 step: 104, loss is 0.1626199185848236\n",
      "epoch: 7 step: 105, loss is 0.10607266426086426\n",
      "epoch: 7 step: 106, loss is 0.10453063249588013\n",
      "epoch: 7 step: 107, loss is 0.18733227252960205\n",
      "epoch: 7 step: 108, loss is 0.10072972625494003\n",
      "epoch: 7 step: 109, loss is 0.21113964915275574\n",
      "epoch: 7 step: 110, loss is 0.07188187539577484\n",
      "epoch: 7 step: 111, loss is 0.18517614901065826\n",
      "epoch: 7 step: 112, loss is 0.09679167717695236\n",
      "epoch: 7 step: 113, loss is 0.053128935396671295\n",
      "epoch: 7 step: 114, loss is 0.08947131037712097\n",
      "epoch: 7 step: 115, loss is 0.21616455912590027\n",
      "epoch: 7 step: 116, loss is 0.11116442084312439\n",
      "epoch: 7 step: 117, loss is 0.10807343572378159\n",
      "epoch: 7 step: 118, loss is 0.1596120446920395\n",
      "epoch: 7 step: 119, loss is 0.053739745169878006\n",
      "epoch: 7 step: 120, loss is 0.12329255044460297\n",
      "epoch: 7 step: 121, loss is 0.04679777845740318\n",
      "epoch: 7 step: 122, loss is 0.07503088563680649\n",
      "epoch: 7 step: 123, loss is 0.15667393803596497\n",
      "epoch: 7 step: 124, loss is 0.1969137340784073\n",
      "epoch: 7 step: 125, loss is 0.10312513262033463\n",
      "epoch: 7 step: 126, loss is 0.04785894975066185\n",
      "epoch: 7 step: 127, loss is 0.17049971222877502\n",
      "epoch: 7 step: 128, loss is 0.04206189140677452\n",
      "epoch: 7 step: 129, loss is 0.07094582915306091\n",
      "epoch: 7 step: 130, loss is 0.12263734638690948\n",
      "epoch: 7 step: 131, loss is 0.08173450827598572\n",
      "epoch: 7 step: 132, loss is 0.09233658760786057\n",
      "epoch: 7 step: 133, loss is 0.06680623441934586\n",
      "epoch: 7 step: 134, loss is 0.07728248089551926\n",
      "epoch: 7 step: 135, loss is 0.11522959172725677\n",
      "epoch: 7 step: 136, loss is 0.09940347820520401\n",
      "epoch: 7 step: 137, loss is 0.17525312304496765\n",
      "epoch: 7 step: 138, loss is 0.26986443996429443\n",
      "epoch: 7 step: 139, loss is 0.11547888070344925\n",
      "epoch: 7 step: 140, loss is 0.07585599273443222\n",
      "epoch: 7 step: 141, loss is 0.13566459715366364\n",
      "epoch: 7 step: 142, loss is 0.09063928574323654\n",
      "epoch: 7 step: 143, loss is 0.07878252118825912\n",
      "epoch: 7 step: 144, loss is 0.10865219682455063\n",
      "epoch: 7 step: 145, loss is 0.09657149016857147\n",
      "epoch: 7 step: 146, loss is 0.10712133347988129\n",
      "epoch: 7 step: 147, loss is 0.11793170869350433\n",
      "epoch: 7 step: 148, loss is 0.0961550697684288\n",
      "epoch: 7 step: 149, loss is 0.08800068497657776\n",
      "epoch: 7 step: 150, loss is 0.18606480956077576\n",
      "epoch: 7 step: 151, loss is 0.08014287799596786\n",
      "epoch: 7 step: 152, loss is 0.07449784874916077\n",
      "epoch: 7 step: 153, loss is 0.07276773452758789\n",
      "epoch: 7 step: 154, loss is 0.07032300531864166\n",
      "epoch: 7 step: 155, loss is 0.11994335800409317\n",
      "epoch: 7 step: 156, loss is 0.1376347541809082\n",
      "epoch: 7 step: 157, loss is 0.12948863208293915\n",
      "epoch: 7 step: 158, loss is 0.21133586764335632\n",
      "epoch: 7 step: 159, loss is 0.1125798374414444\n",
      "epoch: 7 step: 160, loss is 0.13400588929653168\n",
      "epoch: 7 step: 161, loss is 0.09173087030649185\n",
      "epoch: 7 step: 162, loss is 0.13783928751945496\n",
      "epoch: 7 step: 163, loss is 0.09996581822633743\n",
      "epoch: 7 step: 164, loss is 0.15825094282627106\n",
      "epoch: 7 step: 165, loss is 0.16811108589172363\n",
      "epoch: 7 step: 166, loss is 0.09597433358430862\n",
      "epoch: 7 step: 167, loss is 0.17308713495731354\n",
      "epoch: 7 step: 168, loss is 0.1387132853269577\n",
      "epoch: 7 step: 169, loss is 0.15414856374263763\n",
      "epoch: 7 step: 170, loss is 0.07813169062137604\n",
      "epoch: 7 step: 171, loss is 0.03630593791604042\n",
      "epoch: 7 step: 172, loss is 0.08131343126296997\n",
      "epoch: 7 step: 173, loss is 0.06924152374267578\n",
      "epoch: 7 step: 174, loss is 0.0781606063246727\n",
      "epoch: 7 step: 175, loss is 0.21453766524791718\n",
      "epoch: 7 step: 176, loss is 0.16266681253910065\n",
      "epoch: 7 step: 177, loss is 0.10663314163684845\n",
      "epoch: 7 step: 178, loss is 0.23030871152877808\n",
      "epoch: 7 step: 179, loss is 0.07834609597921371\n",
      "epoch: 7 step: 180, loss is 0.10942281782627106\n",
      "epoch: 7 step: 181, loss is 0.09786879271268845\n",
      "epoch: 7 step: 182, loss is 0.07850906997919083\n",
      "epoch: 7 step: 183, loss is 0.038882799446582794\n",
      "epoch: 7 step: 184, loss is 0.09542657434940338\n",
      "epoch: 7 step: 185, loss is 0.05759703740477562\n",
      "epoch: 7 step: 186, loss is 0.18737265467643738\n",
      "epoch: 7 step: 187, loss is 0.08631312847137451\n",
      "epoch: 7 step: 188, loss is 0.1832471638917923\n",
      "epoch: 7 step: 189, loss is 0.07929182797670364\n",
      "epoch: 7 step: 190, loss is 0.0751868188381195\n",
      "epoch: 7 step: 191, loss is 0.029897047206759453\n",
      "epoch: 7 step: 192, loss is 0.07357830554246902\n",
      "epoch: 7 step: 193, loss is 0.05875968560576439\n",
      "epoch: 7 step: 194, loss is 0.16556203365325928\n",
      "epoch: 7 step: 195, loss is 0.1474633663892746\n",
      "epoch: 7 step: 196, loss is 0.11884904652833939\n",
      "epoch: 7 step: 197, loss is 0.024824388325214386\n",
      "epoch: 7 step: 198, loss is 0.03709923475980759\n",
      "epoch: 7 step: 199, loss is 0.09906017780303955\n",
      "epoch: 7 step: 200, loss is 0.11455322802066803\n",
      "epoch: 7 step: 201, loss is 0.17750941216945648\n",
      "epoch: 7 step: 202, loss is 0.1497143656015396\n",
      "epoch: 7 step: 203, loss is 0.03711505979299545\n",
      "epoch: 7 step: 204, loss is 0.09640845656394958\n",
      "epoch: 7 step: 205, loss is 0.1921474039554596\n",
      "epoch: 7 step: 206, loss is 0.14099794626235962\n",
      "epoch: 7 step: 207, loss is 0.08922760933637619\n",
      "epoch: 7 step: 208, loss is 0.2826138436794281\n",
      "epoch: 7 step: 209, loss is 0.020774591714143753\n",
      "epoch: 7 step: 210, loss is 0.11212053894996643\n",
      "epoch: 7 step: 211, loss is 0.15670697391033173\n",
      "epoch: 7 step: 212, loss is 0.10700853168964386\n",
      "epoch: 7 step: 213, loss is 0.12587182223796844\n",
      "epoch: 7 step: 214, loss is 0.22455719113349915\n",
      "epoch: 7 step: 215, loss is 0.10766386985778809\n",
      "epoch: 7 step: 216, loss is 0.07696722447872162\n",
      "epoch: 7 step: 217, loss is 0.14774832129478455\n",
      "epoch: 7 step: 218, loss is 0.10601276159286499\n",
      "epoch: 7 step: 219, loss is 0.10663675516843796\n",
      "epoch: 7 step: 220, loss is 0.08586007356643677\n",
      "epoch: 7 step: 221, loss is 0.03857219219207764\n",
      "epoch: 7 step: 222, loss is 0.02604205347597599\n",
      "epoch: 7 step: 223, loss is 0.13044048845767975\n",
      "epoch: 7 step: 224, loss is 0.07023411244153976\n",
      "epoch: 7 step: 225, loss is 0.0967223271727562\n",
      "epoch: 7 step: 226, loss is 0.17224133014678955\n",
      "epoch: 7 step: 227, loss is 0.11829578876495361\n",
      "epoch: 7 step: 228, loss is 0.03470248728990555\n",
      "epoch: 7 step: 229, loss is 0.1410580426454544\n",
      "epoch: 7 step: 230, loss is 0.0938497856259346\n",
      "epoch: 7 step: 231, loss is 0.16508518159389496\n",
      "epoch: 7 step: 232, loss is 0.15874838829040527\n",
      "epoch: 7 step: 233, loss is 0.10554318875074387\n",
      "epoch: 7 step: 234, loss is 0.11578524112701416\n",
      "epoch: 7 step: 235, loss is 0.2050890475511551\n",
      "epoch: 7 step: 236, loss is 0.17127695679664612\n",
      "epoch: 7 step: 237, loss is 0.13329769670963287\n",
      "epoch: 7 step: 238, loss is 0.18637004494667053\n",
      "epoch: 7 step: 239, loss is 0.09428005665540695\n",
      "epoch: 7 step: 240, loss is 0.1736641824245453\n",
      "epoch: 7 step: 241, loss is 0.11189226806163788\n",
      "epoch: 7 step: 242, loss is 0.1048395186662674\n",
      "epoch: 7 step: 243, loss is 0.12155815958976746\n",
      "epoch: 7 step: 244, loss is 0.08304201066493988\n",
      "epoch: 7 step: 245, loss is 0.1111351028084755\n",
      "epoch: 7 step: 246, loss is 0.10716141015291214\n",
      "epoch: 7 step: 247, loss is 0.09035302698612213\n",
      "epoch: 7 step: 248, loss is 0.047336168587207794\n",
      "epoch: 7 step: 249, loss is 0.07638733834028244\n",
      "epoch: 7 step: 250, loss is 0.04294803738594055\n",
      "epoch: 7 step: 251, loss is 0.18077117204666138\n",
      "epoch: 7 step: 252, loss is 0.10648617893457413\n",
      "epoch: 7 step: 253, loss is 0.10910337418317795\n",
      "epoch: 7 step: 254, loss is 0.06406863033771515\n",
      "epoch: 7 step: 255, loss is 0.07934483885765076\n",
      "epoch: 7 step: 256, loss is 0.1821131557226181\n",
      "epoch: 7 step: 257, loss is 0.18033312261104584\n",
      "epoch: 7 step: 258, loss is 0.17234055697917938\n",
      "epoch: 7 step: 259, loss is 0.08480705320835114\n",
      "epoch: 7 step: 260, loss is 0.13773801922798157\n",
      "epoch: 7 step: 261, loss is 0.13768403232097626\n",
      "epoch: 7 step: 262, loss is 0.09641937911510468\n",
      "epoch: 7 step: 263, loss is 0.04344554990530014\n",
      "epoch: 7 step: 264, loss is 0.07270576059818268\n",
      "epoch: 7 step: 265, loss is 0.12169455736875534\n",
      "epoch: 7 step: 266, loss is 0.1444392055273056\n",
      "epoch: 7 step: 267, loss is 0.10894785821437836\n",
      "epoch: 7 step: 268, loss is 0.1476852297782898\n",
      "epoch: 7 step: 269, loss is 0.09362000226974487\n",
      "epoch: 7 step: 270, loss is 0.07856706529855728\n",
      "epoch: 7 step: 271, loss is 0.17105889320373535\n",
      "epoch: 7 step: 272, loss is 0.10288814455270767\n",
      "epoch: 7 step: 273, loss is 0.11716365069150925\n",
      "epoch: 7 step: 274, loss is 0.0783151164650917\n",
      "epoch: 7 step: 275, loss is 0.07712668180465698\n",
      "epoch: 7 step: 276, loss is 0.0221917312592268\n",
      "epoch: 7 step: 277, loss is 0.13387711346149445\n",
      "epoch: 7 step: 278, loss is 0.10123642534017563\n",
      "epoch: 7 step: 279, loss is 0.12391850352287292\n",
      "epoch: 7 step: 280, loss is 0.03379875048995018\n",
      "epoch: 7 step: 281, loss is 0.20163564383983612\n",
      "epoch: 7 step: 282, loss is 0.07845655828714371\n",
      "epoch: 7 step: 283, loss is 0.1352209895849228\n",
      "epoch: 7 step: 284, loss is 0.041823193430900574\n",
      "epoch: 7 step: 285, loss is 0.10687851160764694\n",
      "epoch: 7 step: 286, loss is 0.11373867094516754\n",
      "epoch: 7 step: 287, loss is 0.04349372535943985\n",
      "epoch: 7 step: 288, loss is 0.3051553964614868\n",
      "epoch: 7 step: 289, loss is 0.16162310540676117\n",
      "epoch: 7 step: 290, loss is 0.1315322071313858\n",
      "epoch: 7 step: 291, loss is 0.09437467157840729\n",
      "epoch: 7 step: 292, loss is 0.268555223941803\n",
      "epoch: 7 step: 293, loss is 0.0806158035993576\n",
      "epoch: 7 step: 294, loss is 0.08268773555755615\n",
      "epoch: 7 step: 295, loss is 0.17790450155735016\n",
      "epoch: 7 step: 296, loss is 0.04239979758858681\n",
      "epoch: 7 step: 297, loss is 0.058201566338539124\n",
      "epoch: 7 step: 298, loss is 0.09411744773387909\n",
      "epoch: 7 step: 299, loss is 0.08547183871269226\n",
      "epoch: 7 step: 300, loss is 0.17129777371883392\n",
      "epoch: 7 step: 301, loss is 0.10618026554584503\n",
      "epoch: 7 step: 302, loss is 0.1293075978755951\n",
      "epoch: 7 step: 303, loss is 0.07385989278554916\n",
      "epoch: 7 step: 304, loss is 0.061893757432699203\n",
      "epoch: 7 step: 305, loss is 0.2964959740638733\n",
      "epoch: 7 step: 306, loss is 0.092514269053936\n",
      "epoch: 7 step: 307, loss is 0.05431816354393959\n",
      "epoch: 7 step: 308, loss is 0.07264870405197144\n",
      "epoch: 7 step: 309, loss is 0.0679653063416481\n",
      "epoch: 7 step: 310, loss is 0.26880085468292236\n",
      "epoch: 7 step: 311, loss is 0.05957463011145592\n",
      "epoch: 7 step: 312, loss is 0.23890745639801025\n",
      "epoch: 7 step: 313, loss is 0.18154065310955048\n",
      "epoch: 7 step: 314, loss is 0.11583450436592102\n",
      "epoch: 7 step: 315, loss is 0.14813780784606934\n",
      "epoch: 7 step: 316, loss is 0.21003346145153046\n",
      "epoch: 7 step: 317, loss is 0.08590293675661087\n",
      "epoch: 7 step: 318, loss is 0.149827241897583\n",
      "epoch: 7 step: 319, loss is 0.17011000216007233\n",
      "epoch: 7 step: 320, loss is 0.16147048771381378\n",
      "epoch: 7 step: 321, loss is 0.09772250056266785\n",
      "epoch: 7 step: 322, loss is 0.02775825746357441\n",
      "epoch: 7 step: 323, loss is 0.09380701184272766\n",
      "epoch: 7 step: 324, loss is 0.11471700668334961\n",
      "epoch: 7 step: 325, loss is 0.07086170464754105\n",
      "epoch: 7 step: 326, loss is 0.15301698446273804\n",
      "epoch: 7 step: 327, loss is 0.10042665898799896\n",
      "epoch: 7 step: 328, loss is 0.21242010593414307\n",
      "epoch: 7 step: 329, loss is 0.043204694986343384\n",
      "epoch: 7 step: 330, loss is 0.05399329587817192\n",
      "epoch: 7 step: 331, loss is 0.14245642721652985\n",
      "epoch: 7 step: 332, loss is 0.05122166872024536\n",
      "epoch: 7 step: 333, loss is 0.08021287620067596\n",
      "epoch: 7 step: 334, loss is 0.04501471668481827\n",
      "epoch: 7 step: 335, loss is 0.07751977443695068\n",
      "epoch: 7 step: 336, loss is 0.16086719930171967\n",
      "epoch: 7 step: 337, loss is 0.08119335025548935\n",
      "epoch: 7 step: 338, loss is 0.1900791972875595\n",
      "epoch: 7 step: 339, loss is 0.08383987098932266\n",
      "epoch: 7 step: 340, loss is 0.12586772441864014\n",
      "epoch: 7 step: 341, loss is 0.19491249322891235\n",
      "epoch: 7 step: 342, loss is 0.1681516021490097\n",
      "epoch: 7 step: 343, loss is 0.041877321898937225\n",
      "epoch: 7 step: 344, loss is 0.19318720698356628\n",
      "epoch: 7 step: 345, loss is 0.0950191393494606\n",
      "epoch: 7 step: 346, loss is 0.08535702526569366\n",
      "epoch: 7 step: 347, loss is 0.041258662939071655\n",
      "epoch: 7 step: 348, loss is 0.16682100296020508\n",
      "epoch: 7 step: 349, loss is 0.11371539533138275\n",
      "epoch: 7 step: 350, loss is 0.13880203664302826\n",
      "epoch: 7 step: 351, loss is 0.1516270935535431\n",
      "epoch: 7 step: 352, loss is 0.07533222436904907\n",
      "epoch: 7 step: 353, loss is 0.024022340774536133\n",
      "epoch: 7 step: 354, loss is 0.07235794514417648\n",
      "epoch: 7 step: 355, loss is 0.24289727210998535\n",
      "epoch: 7 step: 356, loss is 0.07002683728933334\n",
      "epoch: 7 step: 357, loss is 0.07038521766662598\n",
      "epoch: 7 step: 358, loss is 0.3059668242931366\n",
      "epoch: 7 step: 359, loss is 0.056988149881362915\n",
      "epoch: 7 step: 360, loss is 0.09983325749635696\n",
      "epoch: 7 step: 361, loss is 0.17322920262813568\n",
      "epoch: 7 step: 362, loss is 0.12262734025716782\n",
      "epoch: 7 step: 363, loss is 0.1369291990995407\n",
      "epoch: 7 step: 364, loss is 0.12343638390302658\n",
      "epoch: 7 step: 365, loss is 0.04749608412384987\n",
      "epoch: 7 step: 366, loss is 0.1377246379852295\n",
      "epoch: 7 step: 367, loss is 0.08553534001111984\n",
      "epoch: 7 step: 368, loss is 0.12163779884576797\n",
      "epoch: 7 step: 369, loss is 0.042112793773412704\n",
      "epoch: 7 step: 370, loss is 0.09904825687408447\n",
      "epoch: 7 step: 371, loss is 0.05433991178870201\n",
      "epoch: 7 step: 372, loss is 0.1098688393831253\n",
      "epoch: 7 step: 373, loss is 0.09550341963768005\n",
      "epoch: 7 step: 374, loss is 0.11101537197828293\n",
      "epoch: 7 step: 375, loss is 0.0280033890157938\n",
      "epoch: 7 step: 376, loss is 0.0818435549736023\n",
      "epoch: 7 step: 377, loss is 0.11777213215827942\n",
      "epoch: 7 step: 378, loss is 0.14404185116291046\n",
      "epoch: 7 step: 379, loss is 0.15909242630004883\n",
      "epoch: 7 step: 380, loss is 0.10397199541330338\n",
      "epoch: 7 step: 381, loss is 0.07153346389532089\n",
      "epoch: 7 step: 382, loss is 0.07418450713157654\n",
      "epoch: 7 step: 383, loss is 0.058431800454854965\n",
      "epoch: 7 step: 384, loss is 0.1401563137769699\n",
      "epoch: 7 step: 385, loss is 0.11295349150896072\n",
      "epoch: 7 step: 386, loss is 0.10077201575040817\n",
      "epoch: 7 step: 387, loss is 0.035719096660614014\n",
      "epoch: 7 step: 388, loss is 0.1266675889492035\n",
      "epoch: 7 step: 389, loss is 0.11004438996315002\n",
      "epoch: 7 step: 390, loss is 0.09869234263896942\n",
      "epoch: 7 step: 391, loss is 0.1493680477142334\n",
      "epoch: 7 step: 392, loss is 0.08443751186132431\n",
      "epoch: 7 step: 393, loss is 0.0480392687022686\n",
      "epoch: 7 step: 394, loss is 0.10389987379312515\n",
      "epoch: 7 step: 395, loss is 0.06939416378736496\n",
      "epoch: 7 step: 396, loss is 0.27159595489501953\n",
      "epoch: 7 step: 397, loss is 0.11194328963756561\n",
      "epoch: 7 step: 398, loss is 0.04124240204691887\n",
      "epoch: 7 step: 399, loss is 0.16942670941352844\n",
      "epoch: 7 step: 400, loss is 0.17055845260620117\n",
      "epoch: 7 step: 401, loss is 0.16092707216739655\n",
      "epoch: 7 step: 402, loss is 0.04641268029808998\n",
      "epoch: 7 step: 403, loss is 0.08396150171756744\n",
      "epoch: 7 step: 404, loss is 0.11646337807178497\n",
      "epoch: 7 step: 405, loss is 0.15281586349010468\n",
      "epoch: 7 step: 406, loss is 0.06400851905345917\n",
      "epoch: 7 step: 407, loss is 0.08164390176534653\n",
      "epoch: 7 step: 408, loss is 0.07086469233036041\n",
      "epoch: 7 step: 409, loss is 0.0694543719291687\n",
      "epoch: 7 step: 410, loss is 0.1070890948176384\n",
      "epoch: 7 step: 411, loss is 0.2583928108215332\n",
      "epoch: 7 step: 412, loss is 0.09217654913663864\n",
      "epoch: 7 step: 413, loss is 0.15303637087345123\n",
      "epoch: 7 step: 414, loss is 0.1233893483877182\n",
      "epoch: 7 step: 415, loss is 0.09721556305885315\n",
      "epoch: 7 step: 416, loss is 0.11178857088088989\n",
      "epoch: 7 step: 417, loss is 0.08502429723739624\n",
      "epoch: 7 step: 418, loss is 0.054860569536685944\n",
      "epoch: 7 step: 419, loss is 0.29653480648994446\n",
      "epoch: 7 step: 420, loss is 0.03318013623356819\n",
      "epoch: 7 step: 421, loss is 0.0936310663819313\n",
      "epoch: 7 step: 422, loss is 0.26974713802337646\n",
      "epoch: 7 step: 423, loss is 0.1657719910144806\n",
      "epoch: 7 step: 424, loss is 0.06209627538919449\n",
      "epoch: 7 step: 425, loss is 0.13829350471496582\n",
      "epoch: 7 step: 426, loss is 0.09529387205839157\n",
      "epoch: 7 step: 427, loss is 0.05174160376191139\n",
      "epoch: 7 step: 428, loss is 0.06305605918169022\n",
      "epoch: 7 step: 429, loss is 0.07827373594045639\n",
      "epoch: 7 step: 430, loss is 0.06752844899892807\n",
      "epoch: 7 step: 431, loss is 0.1051861047744751\n",
      "epoch: 7 step: 432, loss is 0.1456926017999649\n",
      "epoch: 7 step: 433, loss is 0.17746171355247498\n",
      "epoch: 7 step: 434, loss is 0.13229705393314362\n",
      "epoch: 7 step: 435, loss is 0.1479700803756714\n",
      "epoch: 7 step: 436, loss is 0.21165211498737335\n",
      "epoch: 7 step: 437, loss is 0.04956677183508873\n",
      "epoch: 7 step: 438, loss is 0.09244207292795181\n",
      "epoch: 7 step: 439, loss is 0.18825466930866241\n",
      "epoch: 7 step: 440, loss is 0.08702528476715088\n",
      "epoch: 7 step: 441, loss is 0.06804684549570084\n",
      "epoch: 7 step: 442, loss is 0.06381699442863464\n",
      "epoch: 7 step: 443, loss is 0.037837497889995575\n",
      "epoch: 7 step: 444, loss is 0.13527551293373108\n",
      "epoch: 7 step: 445, loss is 0.09981811791658401\n",
      "epoch: 7 step: 446, loss is 0.07758445292711258\n",
      "epoch: 7 step: 447, loss is 0.07381568104028702\n",
      "epoch: 7 step: 448, loss is 0.18819455802440643\n",
      "epoch: 7 step: 449, loss is 0.13831348717212677\n",
      "epoch: 7 step: 450, loss is 0.14415790140628815\n",
      "epoch: 7 step: 451, loss is 0.10389410704374313\n",
      "epoch: 7 step: 452, loss is 0.11248882114887238\n",
      "epoch: 7 step: 453, loss is 0.13433240354061127\n",
      "epoch: 7 step: 454, loss is 0.06957819312810898\n",
      "epoch: 7 step: 455, loss is 0.15056589245796204\n",
      "epoch: 7 step: 456, loss is 0.10603762418031693\n",
      "epoch: 7 step: 457, loss is 0.09642986953258514\n",
      "epoch: 7 step: 458, loss is 0.09084367007017136\n",
      "epoch: 7 step: 459, loss is 0.13361133635044098\n",
      "epoch: 7 step: 460, loss is 0.09415777027606964\n",
      "epoch: 7 step: 461, loss is 0.10293851792812347\n",
      "epoch: 7 step: 462, loss is 0.16447946429252625\n",
      "epoch: 7 step: 463, loss is 0.12258348613977432\n",
      "epoch: 7 step: 464, loss is 0.229252427816391\n",
      "epoch: 7 step: 465, loss is 0.08658986538648605\n",
      "epoch: 7 step: 466, loss is 0.11465051770210266\n",
      "epoch: 7 step: 467, loss is 0.204911470413208\n",
      "epoch: 7 step: 468, loss is 0.2251647561788559\n",
      "epoch: 7 step: 469, loss is 0.5024445056915283\n",
      "epoch: 7 step: 470, loss is 0.1144423857331276\n",
      "epoch: 7 step: 471, loss is 0.11790581792593002\n",
      "epoch: 7 step: 472, loss is 0.09293007850646973\n",
      "epoch: 7 step: 473, loss is 0.21358801424503326\n",
      "epoch: 7 step: 474, loss is 0.1556246280670166\n",
      "epoch: 7 step: 475, loss is 0.1274007260799408\n",
      "epoch: 7 step: 476, loss is 0.11570823937654495\n",
      "epoch: 7 step: 477, loss is 0.03344051539897919\n",
      "epoch: 7 step: 478, loss is 0.11566323041915894\n",
      "epoch: 7 step: 479, loss is 0.045490797609090805\n",
      "epoch: 7 step: 480, loss is 0.02956254966557026\n",
      "epoch: 7 step: 481, loss is 0.30747973918914795\n",
      "epoch: 7 step: 482, loss is 0.1968870311975479\n",
      "epoch: 7 step: 483, loss is 0.10360077023506165\n",
      "epoch: 7 step: 484, loss is 0.10577791929244995\n",
      "epoch: 7 step: 485, loss is 0.15171872079372406\n",
      "epoch: 7 step: 486, loss is 0.11016473174095154\n",
      "epoch: 7 step: 487, loss is 0.17629380524158478\n",
      "epoch: 7 step: 488, loss is 0.11792127043008804\n",
      "epoch: 7 step: 489, loss is 0.05420297384262085\n",
      "epoch: 7 step: 490, loss is 0.19314458966255188\n",
      "epoch: 7 step: 491, loss is 0.18845921754837036\n",
      "epoch: 7 step: 492, loss is 0.07369428128004074\n",
      "epoch: 7 step: 493, loss is 0.12176523357629776\n",
      "epoch: 7 step: 494, loss is 0.15683192014694214\n",
      "epoch: 7 step: 495, loss is 0.11389269679784775\n",
      "epoch: 7 step: 496, loss is 0.09540757536888123\n",
      "epoch: 7 step: 497, loss is 0.19578491151332855\n",
      "epoch: 7 step: 498, loss is 0.1189950704574585\n",
      "epoch: 7 step: 499, loss is 0.14768625795841217\n",
      "epoch: 7 step: 500, loss is 0.16885700821876526\n",
      "epoch: 7 step: 501, loss is 0.12080693244934082\n",
      "epoch: 7 step: 502, loss is 0.058605071157217026\n",
      "epoch: 7 step: 503, loss is 0.11674247682094574\n",
      "epoch: 7 step: 504, loss is 0.17384417355060577\n",
      "epoch: 7 step: 505, loss is 0.07585209608078003\n",
      "epoch: 7 step: 506, loss is 0.08832083642482758\n",
      "epoch: 7 step: 507, loss is 0.18166455626487732\n",
      "epoch: 7 step: 508, loss is 0.17612440884113312\n",
      "epoch: 7 step: 509, loss is 0.19103389978408813\n",
      "epoch: 7 step: 510, loss is 0.22388048470020294\n",
      "epoch: 7 step: 511, loss is 0.22967156767845154\n",
      "epoch: 7 step: 512, loss is 0.05359457805752754\n",
      "epoch: 7 step: 513, loss is 0.06961984187364578\n",
      "epoch: 7 step: 514, loss is 0.27946048974990845\n",
      "epoch: 7 step: 515, loss is 0.09057511389255524\n",
      "epoch: 7 step: 516, loss is 0.11937098205089569\n",
      "epoch: 7 step: 517, loss is 0.14689336717128754\n",
      "epoch: 7 step: 518, loss is 0.2586424946784973\n",
      "epoch: 7 step: 519, loss is 0.2797366976737976\n",
      "epoch: 7 step: 520, loss is 0.0922892689704895\n",
      "epoch: 7 step: 521, loss is 0.11494459211826324\n",
      "epoch: 7 step: 522, loss is 0.12313415110111237\n",
      "epoch: 7 step: 523, loss is 0.09732315689325333\n",
      "epoch: 7 step: 524, loss is 0.11229030042886734\n",
      "epoch: 7 step: 525, loss is 0.09858600050210953\n",
      "epoch: 7 step: 526, loss is 0.03391394391655922\n",
      "epoch: 7 step: 527, loss is 0.11219841241836548\n",
      "epoch: 7 step: 528, loss is 0.14765754342079163\n",
      "epoch: 7 step: 529, loss is 0.15456970036029816\n",
      "epoch: 7 step: 530, loss is 0.08717111498117447\n",
      "epoch: 7 step: 531, loss is 0.111610546708107\n",
      "epoch: 7 step: 532, loss is 0.15407323837280273\n",
      "epoch: 7 step: 533, loss is 0.05131562799215317\n",
      "epoch: 7 step: 534, loss is 0.24167543649673462\n",
      "epoch: 7 step: 535, loss is 0.06438278406858444\n",
      "epoch: 7 step: 536, loss is 0.1061476320028305\n",
      "epoch: 7 step: 537, loss is 0.18539728224277496\n",
      "epoch: 7 step: 538, loss is 0.07145998626947403\n",
      "epoch: 7 step: 539, loss is 0.12155510485172272\n",
      "epoch: 7 step: 540, loss is 0.09407975524663925\n",
      "epoch: 7 step: 541, loss is 0.06242995336651802\n",
      "epoch: 7 step: 542, loss is 0.08479991555213928\n",
      "epoch: 7 step: 543, loss is 0.16667987406253815\n",
      "epoch: 7 step: 544, loss is 0.10795541107654572\n",
      "epoch: 7 step: 545, loss is 0.15017028152942657\n",
      "epoch: 7 step: 546, loss is 0.18167291581630707\n",
      "epoch: 7 step: 547, loss is 0.1328134983778\n",
      "epoch: 7 step: 548, loss is 0.06654338538646698\n",
      "epoch: 7 step: 549, loss is 0.10796225816011429\n",
      "epoch: 7 step: 550, loss is 0.0639251321554184\n",
      "epoch: 7 step: 551, loss is 0.09429401159286499\n",
      "epoch: 7 step: 552, loss is 0.06891325861215591\n",
      "epoch: 7 step: 553, loss is 0.1328914314508438\n",
      "epoch: 7 step: 554, loss is 0.22433701157569885\n",
      "epoch: 7 step: 555, loss is 0.15519991517066956\n",
      "epoch: 7 step: 556, loss is 0.1583099216222763\n",
      "epoch: 7 step: 557, loss is 0.05885845422744751\n",
      "epoch: 7 step: 558, loss is 0.10484977811574936\n",
      "epoch: 7 step: 559, loss is 0.10370059311389923\n",
      "epoch: 7 step: 560, loss is 0.08492893725633621\n",
      "epoch: 7 step: 561, loss is 0.11157990247011185\n",
      "epoch: 7 step: 562, loss is 0.17923010885715485\n",
      "epoch: 7 step: 563, loss is 0.08522261679172516\n",
      "epoch: 7 step: 564, loss is 0.13983845710754395\n",
      "epoch: 7 step: 565, loss is 0.1379518061876297\n",
      "epoch: 7 step: 566, loss is 0.20293858647346497\n",
      "epoch: 7 step: 567, loss is 0.09328404068946838\n",
      "epoch: 7 step: 568, loss is 0.07661926746368408\n",
      "epoch: 7 step: 569, loss is 0.13873377442359924\n",
      "epoch: 7 step: 570, loss is 0.0528545044362545\n",
      "epoch: 7 step: 571, loss is 0.05297521874308586\n",
      "epoch: 7 step: 572, loss is 0.13349343836307526\n",
      "epoch: 7 step: 573, loss is 0.0804496705532074\n",
      "epoch: 7 step: 574, loss is 0.0658002644777298\n",
      "epoch: 7 step: 575, loss is 0.08086562901735306\n",
      "epoch: 7 step: 576, loss is 0.11201853305101395\n",
      "epoch: 7 step: 577, loss is 0.13036566972732544\n",
      "epoch: 7 step: 578, loss is 0.1900252401828766\n",
      "epoch: 7 step: 579, loss is 0.0725654736161232\n",
      "epoch: 7 step: 580, loss is 0.07530227303504944\n",
      "epoch: 7 step: 581, loss is 0.09077544510364532\n",
      "epoch: 7 step: 582, loss is 0.11030066013336182\n",
      "epoch: 7 step: 583, loss is 0.1569748818874359\n",
      "epoch: 7 step: 584, loss is 0.06196986883878708\n",
      "epoch: 7 step: 585, loss is 0.24412201344966888\n",
      "epoch: 7 step: 586, loss is 0.07327043265104294\n",
      "epoch: 7 step: 587, loss is 0.23468001186847687\n",
      "epoch: 7 step: 588, loss is 0.20639845728874207\n",
      "epoch: 7 step: 589, loss is 0.16186675429344177\n",
      "epoch: 7 step: 590, loss is 0.0638856291770935\n",
      "epoch: 7 step: 591, loss is 0.15147686004638672\n",
      "epoch: 7 step: 592, loss is 0.08589684218168259\n",
      "epoch: 7 step: 593, loss is 0.10592712461948395\n",
      "epoch: 7 step: 594, loss is 0.12847870588302612\n",
      "epoch: 7 step: 595, loss is 0.05809234455227852\n",
      "epoch: 7 step: 596, loss is 0.12586556375026703\n",
      "epoch: 7 step: 597, loss is 0.11320406198501587\n",
      "epoch: 7 step: 598, loss is 0.06785321235656738\n",
      "epoch: 7 step: 599, loss is 0.11292365193367004\n",
      "epoch: 7 step: 600, loss is 0.0892401710152626\n",
      "epoch: 7 step: 601, loss is 0.21506530046463013\n",
      "epoch: 7 step: 602, loss is 0.12331009656190872\n",
      "epoch: 7 step: 603, loss is 0.018998494371771812\n",
      "epoch: 7 step: 604, loss is 0.031211674213409424\n",
      "epoch: 7 step: 605, loss is 0.17376501858234406\n",
      "epoch: 7 step: 606, loss is 0.14099042117595673\n",
      "epoch: 7 step: 607, loss is 0.14066465198993683\n",
      "epoch: 7 step: 608, loss is 0.1553371548652649\n",
      "epoch: 7 step: 609, loss is 0.2929912209510803\n",
      "epoch: 7 step: 610, loss is 0.11016649007797241\n",
      "epoch: 7 step: 611, loss is 0.14808402955532074\n",
      "epoch: 7 step: 612, loss is 0.11762852221727371\n",
      "epoch: 7 step: 613, loss is 0.02735634706914425\n",
      "epoch: 7 step: 614, loss is 0.06689731031656265\n",
      "epoch: 7 step: 615, loss is 0.07218331843614578\n",
      "epoch: 7 step: 616, loss is 0.18766751885414124\n",
      "epoch: 7 step: 617, loss is 0.1321520060300827\n",
      "epoch: 7 step: 618, loss is 0.18906424939632416\n",
      "epoch: 7 step: 619, loss is 0.08635496348142624\n",
      "epoch: 7 step: 620, loss is 0.2423868179321289\n",
      "epoch: 7 step: 621, loss is 0.12534630298614502\n",
      "epoch: 7 step: 622, loss is 0.12472735345363617\n",
      "epoch: 7 step: 623, loss is 0.0453936941921711\n",
      "epoch: 7 step: 624, loss is 0.12288250774145126\n",
      "epoch: 7 step: 625, loss is 0.09564411640167236\n",
      "epoch: 7 step: 626, loss is 0.055937521159648895\n",
      "epoch: 7 step: 627, loss is 0.10729525983333588\n",
      "epoch: 7 step: 628, loss is 0.09760969877243042\n",
      "epoch: 7 step: 629, loss is 0.19724005460739136\n",
      "epoch: 7 step: 630, loss is 0.1400669664144516\n",
      "epoch: 7 step: 631, loss is 0.2896677255630493\n",
      "epoch: 7 step: 632, loss is 0.11409131437540054\n",
      "epoch: 7 step: 633, loss is 0.08954962342977524\n",
      "epoch: 7 step: 634, loss is 0.20083628594875336\n",
      "epoch: 7 step: 635, loss is 0.1429247260093689\n",
      "epoch: 7 step: 636, loss is 0.14036700129508972\n",
      "epoch: 7 step: 637, loss is 0.16582028567790985\n",
      "epoch: 7 step: 638, loss is 0.18373414874076843\n",
      "epoch: 7 step: 639, loss is 0.10298657417297363\n",
      "epoch: 7 step: 640, loss is 0.1468677669763565\n",
      "epoch: 7 step: 641, loss is 0.2532537877559662\n",
      "epoch: 7 step: 642, loss is 0.12952356040477753\n",
      "epoch: 7 step: 643, loss is 0.14645051956176758\n",
      "epoch: 7 step: 644, loss is 0.1143372654914856\n",
      "epoch: 7 step: 645, loss is 0.12013164907693863\n",
      "epoch: 7 step: 646, loss is 0.27957892417907715\n",
      "epoch: 7 step: 647, loss is 0.192337766289711\n",
      "epoch: 7 step: 648, loss is 0.15881383419036865\n",
      "epoch: 7 step: 649, loss is 0.11854919791221619\n",
      "epoch: 7 step: 650, loss is 0.20148509740829468\n",
      "epoch: 7 step: 651, loss is 0.10314221680164337\n",
      "epoch: 7 step: 652, loss is 0.2584840953350067\n",
      "epoch: 7 step: 653, loss is 0.11307975649833679\n",
      "epoch: 7 step: 654, loss is 0.1580304354429245\n",
      "epoch: 7 step: 655, loss is 0.1444024294614792\n",
      "epoch: 7 step: 656, loss is 0.1487925946712494\n",
      "epoch: 7 step: 657, loss is 0.1307070553302765\n",
      "epoch: 7 step: 658, loss is 0.16963134706020355\n",
      "epoch: 7 step: 659, loss is 0.10436825454235077\n",
      "epoch: 7 step: 660, loss is 0.11430139094591141\n",
      "epoch: 7 step: 661, loss is 0.18641245365142822\n",
      "epoch: 7 step: 662, loss is 0.13119947910308838\n",
      "epoch: 7 step: 663, loss is 0.09179569780826569\n",
      "epoch: 7 step: 664, loss is 0.03428870439529419\n",
      "epoch: 7 step: 665, loss is 0.05564236640930176\n",
      "epoch: 7 step: 666, loss is 0.059736717492341995\n",
      "epoch: 7 step: 667, loss is 0.1521284133195877\n",
      "epoch: 7 step: 668, loss is 0.17287100851535797\n",
      "epoch: 7 step: 669, loss is 0.10916607081890106\n",
      "epoch: 7 step: 670, loss is 0.1330021619796753\n",
      "epoch: 7 step: 671, loss is 0.10276979207992554\n",
      "epoch: 7 step: 672, loss is 0.12603044509887695\n",
      "epoch: 7 step: 673, loss is 0.07409211993217468\n",
      "epoch: 7 step: 674, loss is 0.08323728293180466\n",
      "epoch: 7 step: 675, loss is 0.10037064552307129\n",
      "epoch: 7 step: 676, loss is 0.02672467939555645\n",
      "epoch: 7 step: 677, loss is 0.1410813331604004\n",
      "epoch: 7 step: 678, loss is 0.14334584772586823\n",
      "epoch: 7 step: 679, loss is 0.22482119500637054\n",
      "epoch: 7 step: 680, loss is 0.0690971314907074\n",
      "epoch: 7 step: 681, loss is 0.13744047284126282\n",
      "epoch: 7 step: 682, loss is 0.042954158037900925\n",
      "epoch: 7 step: 683, loss is 0.04159872233867645\n",
      "epoch: 7 step: 684, loss is 0.10525662451982498\n",
      "epoch: 7 step: 685, loss is 0.06218017265200615\n",
      "epoch: 7 step: 686, loss is 0.14760784804821014\n",
      "epoch: 7 step: 687, loss is 0.18128013610839844\n",
      "epoch: 7 step: 688, loss is 0.23492425680160522\n",
      "epoch: 7 step: 689, loss is 0.13129287958145142\n",
      "epoch: 7 step: 690, loss is 0.14515414834022522\n",
      "epoch: 7 step: 691, loss is 0.10950414836406708\n",
      "epoch: 7 step: 692, loss is 0.16531789302825928\n",
      "epoch: 7 step: 693, loss is 0.04665861278772354\n",
      "epoch: 7 step: 694, loss is 0.0756048932671547\n",
      "epoch: 7 step: 695, loss is 0.120644211769104\n",
      "epoch: 7 step: 696, loss is 0.14200831949710846\n",
      "epoch: 7 step: 697, loss is 0.2535519599914551\n",
      "epoch: 7 step: 698, loss is 0.056905027478933334\n",
      "epoch: 7 step: 699, loss is 0.11702420562505722\n",
      "epoch: 7 step: 700, loss is 0.12496497482061386\n",
      "epoch: 7 step: 701, loss is 0.10801900178194046\n",
      "epoch: 7 step: 702, loss is 0.30751729011535645\n",
      "epoch: 7 step: 703, loss is 0.0885479748249054\n",
      "epoch: 7 step: 704, loss is 0.10903798043727875\n",
      "epoch: 7 step: 705, loss is 0.054035484790802\n",
      "epoch: 7 step: 706, loss is 0.17210078239440918\n",
      "epoch: 7 step: 707, loss is 0.09536147117614746\n",
      "epoch: 7 step: 708, loss is 0.07215029746294022\n",
      "epoch: 7 step: 709, loss is 0.14762109518051147\n",
      "epoch: 7 step: 710, loss is 0.13751037418842316\n",
      "epoch: 7 step: 711, loss is 0.08791650086641312\n",
      "epoch: 7 step: 712, loss is 0.1290256530046463\n",
      "epoch: 7 step: 713, loss is 0.03159882873296738\n",
      "epoch: 7 step: 714, loss is 0.1315920054912567\n",
      "epoch: 7 step: 715, loss is 0.09083396941423416\n",
      "epoch: 7 step: 716, loss is 0.18823321163654327\n",
      "epoch: 7 step: 717, loss is 0.07580124586820602\n",
      "epoch: 7 step: 718, loss is 0.09241276979446411\n",
      "epoch: 7 step: 719, loss is 0.06499551236629486\n",
      "epoch: 7 step: 720, loss is 0.23164184391498566\n",
      "epoch: 7 step: 721, loss is 0.14010518789291382\n",
      "epoch: 7 step: 722, loss is 0.0781639888882637\n",
      "epoch: 7 step: 723, loss is 0.14729644358158112\n",
      "epoch: 7 step: 724, loss is 0.007452043239027262\n",
      "epoch: 7 step: 725, loss is 0.12038755416870117\n",
      "epoch: 7 step: 726, loss is 0.06262321770191193\n",
      "epoch: 7 step: 727, loss is 0.09991481900215149\n",
      "epoch: 7 step: 728, loss is 0.14214245975017548\n",
      "epoch: 7 step: 729, loss is 0.04806562885642052\n",
      "epoch: 7 step: 730, loss is 0.200459286570549\n",
      "epoch: 7 step: 731, loss is 0.03090614452958107\n",
      "epoch: 7 step: 732, loss is 0.06741699576377869\n",
      "epoch: 7 step: 733, loss is 0.1285625398159027\n",
      "epoch: 7 step: 734, loss is 0.06609775871038437\n",
      "epoch: 7 step: 735, loss is 0.19010253250598907\n",
      "epoch: 7 step: 736, loss is 0.20984478294849396\n",
      "epoch: 7 step: 737, loss is 0.3232078552246094\n",
      "epoch: 7 step: 738, loss is 0.08487556874752045\n",
      "epoch: 7 step: 739, loss is 0.06301062554121017\n",
      "epoch: 7 step: 740, loss is 0.10853363573551178\n",
      "epoch: 7 step: 741, loss is 0.08298002183437347\n",
      "epoch: 7 step: 742, loss is 0.07870402932167053\n",
      "epoch: 7 step: 743, loss is 0.21376864612102509\n",
      "epoch: 7 step: 744, loss is 0.0807325690984726\n",
      "epoch: 7 step: 745, loss is 0.07075896859169006\n",
      "epoch: 7 step: 746, loss is 0.06310976296663284\n",
      "epoch: 7 step: 747, loss is 0.12866564095020294\n",
      "epoch: 7 step: 748, loss is 0.09336759150028229\n",
      "epoch: 7 step: 749, loss is 0.10770775377750397\n",
      "epoch: 7 step: 750, loss is 0.21344052255153656\n",
      "epoch: 7 step: 751, loss is 0.15092316269874573\n",
      "epoch: 7 step: 752, loss is 0.05467057600617409\n",
      "epoch: 7 step: 753, loss is 0.09639015048742294\n",
      "epoch: 7 step: 754, loss is 0.10092227905988693\n",
      "epoch: 7 step: 755, loss is 0.2127784639596939\n",
      "epoch: 7 step: 756, loss is 0.20234452188014984\n",
      "epoch: 7 step: 757, loss is 0.08539809286594391\n",
      "epoch: 7 step: 758, loss is 0.10056672245264053\n",
      "epoch: 7 step: 759, loss is 0.08046603202819824\n",
      "epoch: 7 step: 760, loss is 0.043370556086301804\n",
      "epoch: 7 step: 761, loss is 0.14359910786151886\n",
      "epoch: 7 step: 762, loss is 0.09516123682260513\n",
      "epoch: 7 step: 763, loss is 0.08448217809200287\n",
      "epoch: 7 step: 764, loss is 0.18780140578746796\n",
      "epoch: 7 step: 765, loss is 0.1574007123708725\n",
      "epoch: 7 step: 766, loss is 0.19958624243736267\n",
      "epoch: 7 step: 767, loss is 0.06071193143725395\n",
      "epoch: 7 step: 768, loss is 0.15057945251464844\n",
      "epoch: 7 step: 769, loss is 0.09406493604183197\n",
      "epoch: 7 step: 770, loss is 0.13228046894073486\n",
      "epoch: 7 step: 771, loss is 0.033836930990219116\n",
      "epoch: 7 step: 772, loss is 0.16643355786800385\n",
      "epoch: 7 step: 773, loss is 0.10256648808717728\n",
      "epoch: 7 step: 774, loss is 0.08247271925210953\n",
      "epoch: 7 step: 775, loss is 0.07387436181306839\n",
      "epoch: 7 step: 776, loss is 0.06568346172571182\n",
      "epoch: 7 step: 777, loss is 0.10506095737218857\n",
      "epoch: 7 step: 778, loss is 0.04662762209773064\n",
      "epoch: 7 step: 779, loss is 0.19318804144859314\n",
      "epoch: 7 step: 780, loss is 0.05193052813410759\n",
      "epoch: 7 step: 781, loss is 0.13378484547138214\n",
      "epoch: 7 step: 782, loss is 0.10595007985830307\n",
      "epoch: 7 step: 783, loss is 0.19605417549610138\n",
      "epoch: 7 step: 784, loss is 0.16151690483093262\n",
      "epoch: 7 step: 785, loss is 0.05990099161863327\n",
      "epoch: 7 step: 786, loss is 0.16926144063472748\n",
      "epoch: 7 step: 787, loss is 0.16792824864387512\n",
      "epoch: 7 step: 788, loss is 0.058137401938438416\n",
      "epoch: 7 step: 789, loss is 0.05001480504870415\n",
      "epoch: 7 step: 790, loss is 0.09349893033504486\n",
      "epoch: 7 step: 791, loss is 0.03998180851340294\n",
      "epoch: 7 step: 792, loss is 0.09432291239500046\n",
      "epoch: 7 step: 793, loss is 0.12091745436191559\n",
      "epoch: 7 step: 794, loss is 0.10371806472539902\n",
      "epoch: 7 step: 795, loss is 0.22896166145801544\n",
      "epoch: 7 step: 796, loss is 0.13579684495925903\n",
      "epoch: 7 step: 797, loss is 0.15788519382476807\n",
      "epoch: 7 step: 798, loss is 0.07989213615655899\n",
      "epoch: 7 step: 799, loss is 0.0907810851931572\n",
      "epoch: 7 step: 800, loss is 0.1210026666522026\n",
      "epoch: 7 step: 801, loss is 0.08437858521938324\n",
      "epoch: 7 step: 802, loss is 0.2017156332731247\n",
      "epoch: 7 step: 803, loss is 0.16904108226299286\n",
      "epoch: 7 step: 804, loss is 0.07124309241771698\n",
      "epoch: 7 step: 805, loss is 0.21080727875232697\n",
      "epoch: 7 step: 806, loss is 0.12477870285511017\n",
      "epoch: 7 step: 807, loss is 0.1962614506483078\n",
      "epoch: 7 step: 808, loss is 0.16121196746826172\n",
      "epoch: 7 step: 809, loss is 0.1459750235080719\n",
      "epoch: 7 step: 810, loss is 0.11696577817201614\n",
      "epoch: 7 step: 811, loss is 0.10784947127103806\n",
      "epoch: 7 step: 812, loss is 0.0779566690325737\n",
      "epoch: 7 step: 813, loss is 0.2040572613477707\n",
      "epoch: 7 step: 814, loss is 0.06755324453115463\n",
      "epoch: 7 step: 815, loss is 0.05447670444846153\n",
      "epoch: 7 step: 816, loss is 0.05054794251918793\n",
      "epoch: 7 step: 817, loss is 0.13130100071430206\n",
      "epoch: 7 step: 818, loss is 0.1018398255109787\n",
      "epoch: 7 step: 819, loss is 0.12593118846416473\n",
      "epoch: 7 step: 820, loss is 0.10234671086072922\n",
      "epoch: 7 step: 821, loss is 0.058663494884967804\n",
      "epoch: 7 step: 822, loss is 0.15025906264781952\n",
      "epoch: 7 step: 823, loss is 0.0515558160841465\n",
      "epoch: 7 step: 824, loss is 0.12762261927127838\n",
      "epoch: 7 step: 825, loss is 0.15961425006389618\n",
      "epoch: 7 step: 826, loss is 0.05556546524167061\n",
      "epoch: 7 step: 827, loss is 0.06517663598060608\n",
      "epoch: 7 step: 828, loss is 0.11519961059093475\n",
      "epoch: 7 step: 829, loss is 0.08779079467058182\n",
      "epoch: 7 step: 830, loss is 0.12086131423711777\n",
      "epoch: 7 step: 831, loss is 0.21022635698318481\n",
      "epoch: 7 step: 832, loss is 0.12592163681983948\n",
      "epoch: 7 step: 833, loss is 0.06580241769552231\n",
      "epoch: 7 step: 834, loss is 0.07655098289251328\n",
      "epoch: 7 step: 835, loss is 0.11812321841716766\n",
      "epoch: 7 step: 836, loss is 0.1527998000383377\n",
      "epoch: 7 step: 837, loss is 0.03463590890169144\n",
      "epoch: 7 step: 838, loss is 0.0835295170545578\n",
      "epoch: 7 step: 839, loss is 0.1262768805027008\n",
      "epoch: 7 step: 840, loss is 0.09095661342144012\n",
      "epoch: 7 step: 841, loss is 0.08528084307909012\n",
      "epoch: 7 step: 842, loss is 0.11802760511636734\n",
      "epoch: 7 step: 843, loss is 0.1089189350605011\n",
      "epoch: 7 step: 844, loss is 0.09742338955402374\n",
      "epoch: 7 step: 845, loss is 0.02027735486626625\n",
      "epoch: 7 step: 846, loss is 0.11701451241970062\n",
      "epoch: 7 step: 847, loss is 0.09217853844165802\n",
      "epoch: 7 step: 848, loss is 0.14526520669460297\n",
      "epoch: 7 step: 849, loss is 0.061912890523672104\n",
      "epoch: 7 step: 850, loss is 0.031022079288959503\n",
      "epoch: 7 step: 851, loss is 0.09589792788028717\n",
      "epoch: 7 step: 852, loss is 0.14688937366008759\n",
      "epoch: 7 step: 853, loss is 0.3676624596118927\n",
      "epoch: 7 step: 854, loss is 0.23827961087226868\n",
      "epoch: 7 step: 855, loss is 0.07369872182607651\n",
      "epoch: 7 step: 856, loss is 0.2208738476037979\n",
      "epoch: 7 step: 857, loss is 0.20681767165660858\n",
      "epoch: 7 step: 858, loss is 0.16387806832790375\n",
      "epoch: 7 step: 859, loss is 0.19434566795825958\n",
      "epoch: 7 step: 860, loss is 0.08874239027500153\n",
      "epoch: 7 step: 861, loss is 0.12667123973369598\n",
      "epoch: 7 step: 862, loss is 0.06722643971443176\n",
      "epoch: 7 step: 863, loss is 0.1253979653120041\n",
      "epoch: 7 step: 864, loss is 0.18415579199790955\n",
      "epoch: 7 step: 865, loss is 0.14585906267166138\n",
      "epoch: 7 step: 866, loss is 0.1482781618833542\n",
      "epoch: 7 step: 867, loss is 0.21303942799568176\n",
      "epoch: 7 step: 868, loss is 0.14219816029071808\n",
      "epoch: 7 step: 869, loss is 0.0958571657538414\n",
      "epoch: 7 step: 870, loss is 0.19842824339866638\n",
      "epoch: 7 step: 871, loss is 0.26848867535591125\n",
      "epoch: 7 step: 872, loss is 0.16400673985481262\n",
      "epoch: 7 step: 873, loss is 0.13732513785362244\n",
      "epoch: 7 step: 874, loss is 0.15707379579544067\n",
      "epoch: 7 step: 875, loss is 0.1468663066625595\n",
      "epoch: 7 step: 876, loss is 0.09026609361171722\n",
      "epoch: 7 step: 877, loss is 0.1005340963602066\n",
      "epoch: 7 step: 878, loss is 0.12408386915922165\n",
      "epoch: 7 step: 879, loss is 0.18206298351287842\n",
      "epoch: 7 step: 880, loss is 0.15925918519496918\n",
      "epoch: 7 step: 881, loss is 0.16828152537345886\n",
      "epoch: 7 step: 882, loss is 0.07044903188943863\n",
      "epoch: 7 step: 883, loss is 0.14211060106754303\n",
      "epoch: 7 step: 884, loss is 0.13620668649673462\n",
      "epoch: 7 step: 885, loss is 0.11656776815652847\n",
      "epoch: 7 step: 886, loss is 0.12620441615581512\n",
      "epoch: 7 step: 887, loss is 0.20211313664913177\n",
      "epoch: 7 step: 888, loss is 0.041797980666160583\n",
      "epoch: 7 step: 889, loss is 0.06185127794742584\n",
      "epoch: 7 step: 890, loss is 0.19905555248260498\n",
      "epoch: 7 step: 891, loss is 0.050861939787864685\n",
      "epoch: 7 step: 892, loss is 0.09147506207227707\n",
      "epoch: 7 step: 893, loss is 0.17727024853229523\n",
      "epoch: 7 step: 894, loss is 0.06509137898683548\n",
      "epoch: 7 step: 895, loss is 0.04932467266917229\n",
      "epoch: 7 step: 896, loss is 0.11239753663539886\n",
      "epoch: 7 step: 897, loss is 0.03787348046898842\n",
      "epoch: 7 step: 898, loss is 0.03169702738523483\n",
      "epoch: 7 step: 899, loss is 0.4033637046813965\n",
      "epoch: 7 step: 900, loss is 0.1519995778799057\n",
      "epoch: 7 step: 901, loss is 0.07327599823474884\n",
      "epoch: 7 step: 902, loss is 0.09267938882112503\n",
      "epoch: 7 step: 903, loss is 0.06547833234071732\n",
      "epoch: 7 step: 904, loss is 0.13176433742046356\n",
      "epoch: 7 step: 905, loss is 0.05409238487482071\n",
      "epoch: 7 step: 906, loss is 0.14591816067695618\n",
      "epoch: 7 step: 907, loss is 0.10373891890048981\n",
      "epoch: 7 step: 908, loss is 0.32387199997901917\n",
      "epoch: 7 step: 909, loss is 0.21879403293132782\n",
      "epoch: 7 step: 910, loss is 0.22969351708889008\n",
      "epoch: 7 step: 911, loss is 0.07923334836959839\n",
      "epoch: 7 step: 912, loss is 0.04610000178217888\n",
      "epoch: 7 step: 913, loss is 0.19238479435443878\n",
      "epoch: 7 step: 914, loss is 0.046197559684515\n",
      "epoch: 7 step: 915, loss is 0.03278119117021561\n",
      "epoch: 7 step: 916, loss is 0.12670572102069855\n",
      "epoch: 7 step: 917, loss is 0.09582943469285965\n",
      "epoch: 7 step: 918, loss is 0.08786050975322723\n",
      "epoch: 7 step: 919, loss is 0.0996052622795105\n",
      "epoch: 7 step: 920, loss is 0.06262435019016266\n",
      "epoch: 7 step: 921, loss is 0.2226656675338745\n",
      "epoch: 7 step: 922, loss is 0.06312080472707748\n",
      "epoch: 7 step: 923, loss is 0.05558805912733078\n",
      "epoch: 7 step: 924, loss is 0.09960353374481201\n",
      "epoch: 7 step: 925, loss is 0.39138472080230713\n",
      "epoch: 7 step: 926, loss is 0.11271298676729202\n",
      "epoch: 7 step: 927, loss is 0.026991279795765877\n",
      "epoch: 7 step: 928, loss is 0.18714313209056854\n",
      "epoch: 7 step: 929, loss is 0.07573182880878448\n",
      "epoch: 7 step: 930, loss is 0.16386257112026215\n",
      "epoch: 7 step: 931, loss is 0.0563327856361866\n",
      "epoch: 7 step: 932, loss is 0.06669063121080399\n",
      "epoch: 7 step: 933, loss is 0.2852764427661896\n",
      "epoch: 7 step: 934, loss is 0.1130421981215477\n",
      "epoch: 7 step: 935, loss is 0.1184176504611969\n",
      "epoch: 7 step: 936, loss is 0.09726870059967041\n",
      "epoch: 7 step: 937, loss is 0.12715698778629303\n",
      "epoch: 8 step: 1, loss is 0.1356470286846161\n",
      "epoch: 8 step: 2, loss is 0.08566311746835709\n",
      "epoch: 8 step: 3, loss is 0.06594190001487732\n",
      "epoch: 8 step: 4, loss is 0.03605832904577255\n",
      "epoch: 8 step: 5, loss is 0.1420600712299347\n",
      "epoch: 8 step: 6, loss is 0.04785657301545143\n",
      "epoch: 8 step: 7, loss is 0.2627508342266083\n",
      "epoch: 8 step: 8, loss is 0.06144516542553902\n",
      "epoch: 8 step: 9, loss is 0.08975487947463989\n",
      "epoch: 8 step: 10, loss is 0.07559096068143845\n",
      "epoch: 8 step: 11, loss is 0.04524500295519829\n",
      "epoch: 8 step: 12, loss is 0.14320409297943115\n",
      "epoch: 8 step: 13, loss is 0.08179283142089844\n",
      "epoch: 8 step: 14, loss is 0.10294238477945328\n",
      "epoch: 8 step: 15, loss is 0.1411195993423462\n",
      "epoch: 8 step: 16, loss is 0.0535423681139946\n",
      "epoch: 8 step: 17, loss is 0.05302995815873146\n",
      "epoch: 8 step: 18, loss is 0.10741576552391052\n",
      "epoch: 8 step: 19, loss is 0.19030264019966125\n",
      "epoch: 8 step: 20, loss is 0.07653798162937164\n",
      "epoch: 8 step: 21, loss is 0.050190556794404984\n",
      "epoch: 8 step: 22, loss is 0.07084737718105316\n",
      "epoch: 8 step: 23, loss is 0.1444796621799469\n",
      "epoch: 8 step: 24, loss is 0.11106409132480621\n",
      "epoch: 8 step: 25, loss is 0.09081806242465973\n",
      "epoch: 8 step: 26, loss is 0.09574448317289352\n",
      "epoch: 8 step: 27, loss is 0.06290549784898758\n",
      "epoch: 8 step: 28, loss is 0.08647897094488144\n",
      "epoch: 8 step: 29, loss is 0.16306863725185394\n",
      "epoch: 8 step: 30, loss is 0.07817397266626358\n",
      "epoch: 8 step: 31, loss is 0.017883500084280968\n",
      "epoch: 8 step: 32, loss is 0.0666939839720726\n",
      "epoch: 8 step: 33, loss is 0.09768805652856827\n",
      "epoch: 8 step: 34, loss is 0.09288237243890762\n",
      "epoch: 8 step: 35, loss is 0.07749079912900925\n",
      "epoch: 8 step: 36, loss is 0.042001873254776\n",
      "epoch: 8 step: 37, loss is 0.059563472867012024\n",
      "epoch: 8 step: 38, loss is 0.12112511694431305\n",
      "epoch: 8 step: 39, loss is 0.042316827923059464\n",
      "epoch: 8 step: 40, loss is 0.10196539759635925\n",
      "epoch: 8 step: 41, loss is 0.08222923427820206\n",
      "epoch: 8 step: 42, loss is 0.19252857565879822\n",
      "epoch: 8 step: 43, loss is 0.10648512840270996\n",
      "epoch: 8 step: 44, loss is 0.2012912482023239\n",
      "epoch: 8 step: 45, loss is 0.0598614476621151\n",
      "epoch: 8 step: 46, loss is 0.1667061150074005\n",
      "epoch: 8 step: 47, loss is 0.27162718772888184\n",
      "epoch: 8 step: 48, loss is 0.10593032091856003\n",
      "epoch: 8 step: 49, loss is 0.03413804620504379\n",
      "epoch: 8 step: 50, loss is 0.07475689053535461\n",
      "epoch: 8 step: 51, loss is 0.05588199198246002\n",
      "epoch: 8 step: 52, loss is 0.05962638556957245\n",
      "epoch: 8 step: 53, loss is 0.23128457367420197\n",
      "epoch: 8 step: 54, loss is 0.1456293761730194\n",
      "epoch: 8 step: 55, loss is 0.06672059744596481\n",
      "epoch: 8 step: 56, loss is 0.14583773910999298\n",
      "epoch: 8 step: 57, loss is 0.048894744366407394\n",
      "epoch: 8 step: 58, loss is 0.0608016662299633\n",
      "epoch: 8 step: 59, loss is 0.02146170474588871\n",
      "epoch: 8 step: 60, loss is 0.1215139776468277\n",
      "epoch: 8 step: 61, loss is 0.13904713094234467\n",
      "epoch: 8 step: 62, loss is 0.13190385699272156\n",
      "epoch: 8 step: 63, loss is 0.021156640723347664\n",
      "epoch: 8 step: 64, loss is 0.08064229786396027\n",
      "epoch: 8 step: 65, loss is 0.1437166929244995\n",
      "epoch: 8 step: 66, loss is 0.07453739643096924\n",
      "epoch: 8 step: 67, loss is 0.04852579906582832\n",
      "epoch: 8 step: 68, loss is 0.030262868851423264\n",
      "epoch: 8 step: 69, loss is 0.03447924554347992\n",
      "epoch: 8 step: 70, loss is 0.09617995470762253\n",
      "epoch: 8 step: 71, loss is 0.07365719974040985\n",
      "epoch: 8 step: 72, loss is 0.04676595702767372\n",
      "epoch: 8 step: 73, loss is 0.0557025745511055\n",
      "epoch: 8 step: 74, loss is 0.0616537444293499\n",
      "epoch: 8 step: 75, loss is 0.17184585332870483\n",
      "epoch: 8 step: 76, loss is 0.046616118401288986\n",
      "epoch: 8 step: 77, loss is 0.04945679381489754\n",
      "epoch: 8 step: 78, loss is 0.044047266244888306\n",
      "epoch: 8 step: 79, loss is 0.043479450047016144\n",
      "epoch: 8 step: 80, loss is 0.15389227867126465\n",
      "epoch: 8 step: 81, loss is 0.08936130255460739\n",
      "epoch: 8 step: 82, loss is 0.08854227513074875\n",
      "epoch: 8 step: 83, loss is 0.1335601508617401\n",
      "epoch: 8 step: 84, loss is 0.02422706037759781\n",
      "epoch: 8 step: 85, loss is 0.0513945147395134\n",
      "epoch: 8 step: 86, loss is 0.05959619954228401\n",
      "epoch: 8 step: 87, loss is 0.03596024587750435\n",
      "epoch: 8 step: 88, loss is 0.09818512201309204\n",
      "epoch: 8 step: 89, loss is 0.04864920675754547\n",
      "epoch: 8 step: 90, loss is 0.036950208246707916\n",
      "epoch: 8 step: 91, loss is 0.03377130255103111\n",
      "epoch: 8 step: 92, loss is 0.0513027161359787\n",
      "epoch: 8 step: 93, loss is 0.162230983376503\n",
      "epoch: 8 step: 94, loss is 0.10055645555257797\n",
      "epoch: 8 step: 95, loss is 0.02756594493985176\n",
      "epoch: 8 step: 96, loss is 0.11403089761734009\n",
      "epoch: 8 step: 97, loss is 0.14814026653766632\n",
      "epoch: 8 step: 98, loss is 0.02406826801598072\n",
      "epoch: 8 step: 99, loss is 0.04453685134649277\n",
      "epoch: 8 step: 100, loss is 0.09047075361013412\n",
      "epoch: 8 step: 101, loss is 0.07309367507696152\n",
      "epoch: 8 step: 102, loss is 0.040675099939107895\n",
      "epoch: 8 step: 103, loss is 0.11154444515705109\n",
      "epoch: 8 step: 104, loss is 0.10752837359905243\n",
      "epoch: 8 step: 105, loss is 0.091595359146595\n",
      "epoch: 8 step: 106, loss is 0.1664181351661682\n",
      "epoch: 8 step: 107, loss is 0.05335351452231407\n",
      "epoch: 8 step: 108, loss is 0.16410881280899048\n",
      "epoch: 8 step: 109, loss is 0.06722603738307953\n",
      "epoch: 8 step: 110, loss is 0.15862149000167847\n",
      "epoch: 8 step: 111, loss is 0.13079625368118286\n",
      "epoch: 8 step: 112, loss is 0.10601314157247543\n",
      "epoch: 8 step: 113, loss is 0.18663449585437775\n",
      "epoch: 8 step: 114, loss is 0.03222958371043205\n",
      "epoch: 8 step: 115, loss is 0.11130119115114212\n",
      "epoch: 8 step: 116, loss is 0.05690421164035797\n",
      "epoch: 8 step: 117, loss is 0.0680028572678566\n",
      "epoch: 8 step: 118, loss is 0.029966088011860847\n",
      "epoch: 8 step: 119, loss is 0.15712782740592957\n",
      "epoch: 8 step: 120, loss is 0.1226300299167633\n",
      "epoch: 8 step: 121, loss is 0.09055230766534805\n",
      "epoch: 8 step: 122, loss is 0.0390995591878891\n",
      "epoch: 8 step: 123, loss is 0.10118254274129868\n",
      "epoch: 8 step: 124, loss is 0.06387164443731308\n",
      "epoch: 8 step: 125, loss is 0.08977246284484863\n",
      "epoch: 8 step: 126, loss is 0.16150499880313873\n",
      "epoch: 8 step: 127, loss is 0.07766737788915634\n",
      "epoch: 8 step: 128, loss is 0.15794724225997925\n",
      "epoch: 8 step: 129, loss is 0.12498757243156433\n",
      "epoch: 8 step: 130, loss is 0.12319763004779816\n",
      "epoch: 8 step: 131, loss is 0.050833847373723984\n",
      "epoch: 8 step: 132, loss is 0.036992259323596954\n",
      "epoch: 8 step: 133, loss is 0.023643285036087036\n",
      "epoch: 8 step: 134, loss is 0.05523543804883957\n",
      "epoch: 8 step: 135, loss is 0.035332728177309036\n",
      "epoch: 8 step: 136, loss is 0.08295777440071106\n",
      "epoch: 8 step: 137, loss is 0.06660256534814835\n",
      "epoch: 8 step: 138, loss is 0.06450904160737991\n",
      "epoch: 8 step: 139, loss is 0.09439264982938766\n",
      "epoch: 8 step: 140, loss is 0.1011720597743988\n",
      "epoch: 8 step: 141, loss is 0.03494006022810936\n",
      "epoch: 8 step: 142, loss is 0.034903835505247116\n",
      "epoch: 8 step: 143, loss is 0.04320473596453667\n",
      "epoch: 8 step: 144, loss is 0.04258003830909729\n",
      "epoch: 8 step: 145, loss is 0.06154695525765419\n",
      "epoch: 8 step: 146, loss is 0.08703988045454025\n",
      "epoch: 8 step: 147, loss is 0.07166077196598053\n",
      "epoch: 8 step: 148, loss is 0.03693534806370735\n",
      "epoch: 8 step: 149, loss is 0.10456471145153046\n",
      "epoch: 8 step: 150, loss is 0.18307267129421234\n",
      "epoch: 8 step: 151, loss is 0.05813322961330414\n",
      "epoch: 8 step: 152, loss is 0.10507197678089142\n",
      "epoch: 8 step: 153, loss is 0.09846914559602737\n",
      "epoch: 8 step: 154, loss is 0.08330965042114258\n",
      "epoch: 8 step: 155, loss is 0.07456830888986588\n",
      "epoch: 8 step: 156, loss is 0.05778694152832031\n",
      "epoch: 8 step: 157, loss is 0.013916649855673313\n",
      "epoch: 8 step: 158, loss is 0.0719112977385521\n",
      "epoch: 8 step: 159, loss is 0.0981893390417099\n",
      "epoch: 8 step: 160, loss is 0.16403988003730774\n",
      "epoch: 8 step: 161, loss is 0.10455556958913803\n",
      "epoch: 8 step: 162, loss is 0.14801062643527985\n",
      "epoch: 8 step: 163, loss is 0.0975186675786972\n",
      "epoch: 8 step: 164, loss is 0.08621663600206375\n",
      "epoch: 8 step: 165, loss is 0.02805229276418686\n",
      "epoch: 8 step: 166, loss is 0.10176843404769897\n",
      "epoch: 8 step: 167, loss is 0.1131083220243454\n",
      "epoch: 8 step: 168, loss is 0.005834996234625578\n",
      "epoch: 8 step: 169, loss is 0.036329664289951324\n",
      "epoch: 8 step: 170, loss is 0.1406191885471344\n",
      "epoch: 8 step: 171, loss is 0.22038359940052032\n",
      "epoch: 8 step: 172, loss is 0.09484703093767166\n",
      "epoch: 8 step: 173, loss is 0.02613007090985775\n",
      "epoch: 8 step: 174, loss is 0.11588862538337708\n",
      "epoch: 8 step: 175, loss is 0.07197421789169312\n",
      "epoch: 8 step: 176, loss is 0.19386599957942963\n",
      "epoch: 8 step: 177, loss is 0.1891520768404007\n",
      "epoch: 8 step: 178, loss is 0.09086421132087708\n",
      "epoch: 8 step: 179, loss is 0.06003977730870247\n",
      "epoch: 8 step: 180, loss is 0.07146009802818298\n",
      "epoch: 8 step: 181, loss is 0.05053539201617241\n",
      "epoch: 8 step: 182, loss is 0.11947149783372879\n",
      "epoch: 8 step: 183, loss is 0.09406045079231262\n",
      "epoch: 8 step: 184, loss is 0.05459068343043327\n",
      "epoch: 8 step: 185, loss is 0.17793332040309906\n",
      "epoch: 8 step: 186, loss is 0.029855670407414436\n",
      "epoch: 8 step: 187, loss is 0.10360297560691833\n",
      "epoch: 8 step: 188, loss is 0.06654626876115799\n",
      "epoch: 8 step: 189, loss is 0.05050378292798996\n",
      "epoch: 8 step: 190, loss is 0.09543855488300323\n",
      "epoch: 8 step: 191, loss is 0.06084582954645157\n",
      "epoch: 8 step: 192, loss is 0.015599681995809078\n",
      "epoch: 8 step: 193, loss is 0.044865187257528305\n",
      "epoch: 8 step: 194, loss is 0.04724469780921936\n",
      "epoch: 8 step: 195, loss is 0.07245582342147827\n",
      "epoch: 8 step: 196, loss is 0.10405740141868591\n",
      "epoch: 8 step: 197, loss is 0.18208925426006317\n",
      "epoch: 8 step: 198, loss is 0.07723359763622284\n",
      "epoch: 8 step: 199, loss is 0.16585887968540192\n",
      "epoch: 8 step: 200, loss is 0.1831381916999817\n",
      "epoch: 8 step: 201, loss is 0.07240644097328186\n",
      "epoch: 8 step: 202, loss is 0.1821393072605133\n",
      "epoch: 8 step: 203, loss is 0.1671883761882782\n",
      "epoch: 8 step: 204, loss is 0.008009890094399452\n",
      "epoch: 8 step: 205, loss is 0.1142037883400917\n",
      "epoch: 8 step: 206, loss is 0.08534318953752518\n",
      "epoch: 8 step: 207, loss is 0.10197975486516953\n",
      "epoch: 8 step: 208, loss is 0.07303177565336227\n",
      "epoch: 8 step: 209, loss is 0.06924949586391449\n",
      "epoch: 8 step: 210, loss is 0.1889270544052124\n",
      "epoch: 8 step: 211, loss is 0.11069785058498383\n",
      "epoch: 8 step: 212, loss is 0.0384928435087204\n",
      "epoch: 8 step: 213, loss is 0.054305702447891235\n",
      "epoch: 8 step: 214, loss is 0.06511752307415009\n",
      "epoch: 8 step: 215, loss is 0.1408890038728714\n",
      "epoch: 8 step: 216, loss is 0.0959053635597229\n",
      "epoch: 8 step: 217, loss is 0.07512715458869934\n",
      "epoch: 8 step: 218, loss is 0.11502426117658615\n",
      "epoch: 8 step: 219, loss is 0.13108646869659424\n",
      "epoch: 8 step: 220, loss is 0.04252064600586891\n",
      "epoch: 8 step: 221, loss is 0.17340758442878723\n",
      "epoch: 8 step: 222, loss is 0.03849109634757042\n",
      "epoch: 8 step: 223, loss is 0.15307503938674927\n",
      "epoch: 8 step: 224, loss is 0.09189379215240479\n",
      "epoch: 8 step: 225, loss is 0.06058519706130028\n",
      "epoch: 8 step: 226, loss is 0.03818074241280556\n",
      "epoch: 8 step: 227, loss is 0.07169836014509201\n",
      "epoch: 8 step: 228, loss is 0.10752174258232117\n",
      "epoch: 8 step: 229, loss is 0.09092618525028229\n",
      "epoch: 8 step: 230, loss is 0.13936403393745422\n",
      "epoch: 8 step: 231, loss is 0.07359718531370163\n",
      "epoch: 8 step: 232, loss is 0.1612652838230133\n",
      "epoch: 8 step: 233, loss is 0.09534407407045364\n",
      "epoch: 8 step: 234, loss is 0.08166517317295074\n",
      "epoch: 8 step: 235, loss is 0.12036510556936264\n",
      "epoch: 8 step: 236, loss is 0.03506764769554138\n",
      "epoch: 8 step: 237, loss is 0.068619504570961\n",
      "epoch: 8 step: 238, loss is 0.06624876707792282\n",
      "epoch: 8 step: 239, loss is 0.04658589884638786\n",
      "epoch: 8 step: 240, loss is 0.06755086779594421\n",
      "epoch: 8 step: 241, loss is 0.014379746280610561\n",
      "epoch: 8 step: 242, loss is 0.046201396733522415\n",
      "epoch: 8 step: 243, loss is 0.04089052975177765\n",
      "epoch: 8 step: 244, loss is 0.08407644182443619\n",
      "epoch: 8 step: 245, loss is 0.041614633053541183\n",
      "epoch: 8 step: 246, loss is 0.0418989472091198\n",
      "epoch: 8 step: 247, loss is 0.06560804694890976\n",
      "epoch: 8 step: 248, loss is 0.04821576550602913\n",
      "epoch: 8 step: 249, loss is 0.0805889293551445\n",
      "epoch: 8 step: 250, loss is 0.03265487402677536\n",
      "epoch: 8 step: 251, loss is 0.012549275532364845\n",
      "epoch: 8 step: 252, loss is 0.14884646236896515\n",
      "epoch: 8 step: 253, loss is 0.03159219026565552\n",
      "epoch: 8 step: 254, loss is 0.06581024825572968\n",
      "epoch: 8 step: 255, loss is 0.10270565748214722\n",
      "epoch: 8 step: 256, loss is 0.12243472039699554\n",
      "epoch: 8 step: 257, loss is 0.034788310527801514\n",
      "epoch: 8 step: 258, loss is 0.1071406826376915\n",
      "epoch: 8 step: 259, loss is 0.09212270379066467\n",
      "epoch: 8 step: 260, loss is 0.06541731208562851\n",
      "epoch: 8 step: 261, loss is 0.10793153196573257\n",
      "epoch: 8 step: 262, loss is 0.12214421480894089\n",
      "epoch: 8 step: 263, loss is 0.1545100063085556\n",
      "epoch: 8 step: 264, loss is 0.02727952040731907\n",
      "epoch: 8 step: 265, loss is 0.028265122324228287\n",
      "epoch: 8 step: 266, loss is 0.024566175416111946\n",
      "epoch: 8 step: 267, loss is 0.16362185776233673\n",
      "epoch: 8 step: 268, loss is 0.08185810595750809\n",
      "epoch: 8 step: 269, loss is 0.00796875637024641\n",
      "epoch: 8 step: 270, loss is 0.07568359375\n",
      "epoch: 8 step: 271, loss is 0.03536830097436905\n",
      "epoch: 8 step: 272, loss is 0.1366858035326004\n",
      "epoch: 8 step: 273, loss is 0.03688475489616394\n",
      "epoch: 8 step: 274, loss is 0.07804093509912491\n",
      "epoch: 8 step: 275, loss is 0.05610056594014168\n",
      "epoch: 8 step: 276, loss is 0.07296240329742432\n",
      "epoch: 8 step: 277, loss is 0.05320940911769867\n",
      "epoch: 8 step: 278, loss is 0.07110954076051712\n",
      "epoch: 8 step: 279, loss is 0.105967216193676\n",
      "epoch: 8 step: 280, loss is 0.09872700273990631\n",
      "epoch: 8 step: 281, loss is 0.07218558341264725\n",
      "epoch: 8 step: 282, loss is 0.11105071753263474\n",
      "epoch: 8 step: 283, loss is 0.07638515532016754\n",
      "epoch: 8 step: 284, loss is 0.13101719319820404\n",
      "epoch: 8 step: 285, loss is 0.04901004955172539\n",
      "epoch: 8 step: 286, loss is 0.05940663442015648\n",
      "epoch: 8 step: 287, loss is 0.04299677535891533\n",
      "epoch: 8 step: 288, loss is 0.03372926637530327\n",
      "epoch: 8 step: 289, loss is 0.07122461497783661\n",
      "epoch: 8 step: 290, loss is 0.05683808773756027\n",
      "epoch: 8 step: 291, loss is 0.1352558732032776\n",
      "epoch: 8 step: 292, loss is 0.019562378525733948\n",
      "epoch: 8 step: 293, loss is 0.10410800576210022\n",
      "epoch: 8 step: 294, loss is 0.07803209871053696\n",
      "epoch: 8 step: 295, loss is 0.10395396500825882\n",
      "epoch: 8 step: 296, loss is 0.06280096620321274\n",
      "epoch: 8 step: 297, loss is 0.12531451880931854\n",
      "epoch: 8 step: 298, loss is 0.0628104954957962\n",
      "epoch: 8 step: 299, loss is 0.083980493247509\n",
      "epoch: 8 step: 300, loss is 0.15036197006702423\n",
      "epoch: 8 step: 301, loss is 0.03917747735977173\n",
      "epoch: 8 step: 302, loss is 0.2592587172985077\n",
      "epoch: 8 step: 303, loss is 0.09289795905351639\n",
      "epoch: 8 step: 304, loss is 0.19186392426490784\n",
      "epoch: 8 step: 305, loss is 0.10181556642055511\n",
      "epoch: 8 step: 306, loss is 0.08002457022666931\n",
      "epoch: 8 step: 307, loss is 0.15139804780483246\n",
      "epoch: 8 step: 308, loss is 0.059446968138217926\n",
      "epoch: 8 step: 309, loss is 0.12583138048648834\n",
      "epoch: 8 step: 310, loss is 0.10935444384813309\n",
      "epoch: 8 step: 311, loss is 0.08792151510715485\n",
      "epoch: 8 step: 312, loss is 0.10284990072250366\n",
      "epoch: 8 step: 313, loss is 0.13367219269275665\n",
      "epoch: 8 step: 314, loss is 0.06670863181352615\n",
      "epoch: 8 step: 315, loss is 0.08294008672237396\n",
      "epoch: 8 step: 316, loss is 0.1515282839536667\n",
      "epoch: 8 step: 317, loss is 0.03305505961179733\n",
      "epoch: 8 step: 318, loss is 0.07151827216148376\n",
      "epoch: 8 step: 319, loss is 0.10105860233306885\n",
      "epoch: 8 step: 320, loss is 0.10944770276546478\n",
      "epoch: 8 step: 321, loss is 0.06516635417938232\n",
      "epoch: 8 step: 322, loss is 0.03065354935824871\n",
      "epoch: 8 step: 323, loss is 0.022185681387782097\n",
      "epoch: 8 step: 324, loss is 0.10272887349128723\n",
      "epoch: 8 step: 325, loss is 0.17889592051506042\n",
      "epoch: 8 step: 326, loss is 0.07012186199426651\n",
      "epoch: 8 step: 327, loss is 0.07928156107664108\n",
      "epoch: 8 step: 328, loss is 0.1065249815583229\n",
      "epoch: 8 step: 329, loss is 0.13670027256011963\n",
      "epoch: 8 step: 330, loss is 0.07402453571557999\n",
      "epoch: 8 step: 331, loss is 0.08770483732223511\n",
      "epoch: 8 step: 332, loss is 0.11861405521631241\n",
      "epoch: 8 step: 333, loss is 0.14935149252414703\n",
      "epoch: 8 step: 334, loss is 0.12723693251609802\n",
      "epoch: 8 step: 335, loss is 0.0272970087826252\n",
      "epoch: 8 step: 336, loss is 0.042865507304668427\n",
      "epoch: 8 step: 337, loss is 0.1280798316001892\n",
      "epoch: 8 step: 338, loss is 0.07257916778326035\n",
      "epoch: 8 step: 339, loss is 0.13843442499637604\n",
      "epoch: 8 step: 340, loss is 0.08638158440589905\n",
      "epoch: 8 step: 341, loss is 0.07165279239416122\n",
      "epoch: 8 step: 342, loss is 0.16025331616401672\n",
      "epoch: 8 step: 343, loss is 0.05108996108174324\n",
      "epoch: 8 step: 344, loss is 0.03219030424952507\n",
      "epoch: 8 step: 345, loss is 0.06566424667835236\n",
      "epoch: 8 step: 346, loss is 0.10391692072153091\n",
      "epoch: 8 step: 347, loss is 0.09741896390914917\n",
      "epoch: 8 step: 348, loss is 0.08646930009126663\n",
      "epoch: 8 step: 349, loss is 0.25563785433769226\n",
      "epoch: 8 step: 350, loss is 0.0983370915055275\n",
      "epoch: 8 step: 351, loss is 0.07628914713859558\n",
      "epoch: 8 step: 352, loss is 0.16224868595600128\n",
      "epoch: 8 step: 353, loss is 0.08291351050138474\n",
      "epoch: 8 step: 354, loss is 0.10104457288980484\n",
      "epoch: 8 step: 355, loss is 0.046528082340955734\n",
      "epoch: 8 step: 356, loss is 0.06534720212221146\n",
      "epoch: 8 step: 357, loss is 0.04225459322333336\n",
      "epoch: 8 step: 358, loss is 0.13161838054656982\n",
      "epoch: 8 step: 359, loss is 0.15099012851715088\n",
      "epoch: 8 step: 360, loss is 0.07385795563459396\n",
      "epoch: 8 step: 361, loss is 0.12063062191009521\n",
      "epoch: 8 step: 362, loss is 0.05745457485318184\n",
      "epoch: 8 step: 363, loss is 0.3055994212627411\n",
      "epoch: 8 step: 364, loss is 0.033071015030145645\n",
      "epoch: 8 step: 365, loss is 0.03998960554599762\n",
      "epoch: 8 step: 366, loss is 0.032844994217157364\n",
      "epoch: 8 step: 367, loss is 0.043037209659814835\n",
      "epoch: 8 step: 368, loss is 0.2278519719839096\n",
      "epoch: 8 step: 369, loss is 0.08667806535959244\n",
      "epoch: 8 step: 370, loss is 0.12467774003744125\n",
      "epoch: 8 step: 371, loss is 0.12237359583377838\n",
      "epoch: 8 step: 372, loss is 0.18051254749298096\n",
      "epoch: 8 step: 373, loss is 0.03153542801737785\n",
      "epoch: 8 step: 374, loss is 0.14998920261859894\n",
      "epoch: 8 step: 375, loss is 0.09943242371082306\n",
      "epoch: 8 step: 376, loss is 0.032583992928266525\n",
      "epoch: 8 step: 377, loss is 0.07933209836483002\n",
      "epoch: 8 step: 378, loss is 0.11780718713998795\n",
      "epoch: 8 step: 379, loss is 0.049129631370306015\n",
      "epoch: 8 step: 380, loss is 0.08803120255470276\n",
      "epoch: 8 step: 381, loss is 0.13254790008068085\n",
      "epoch: 8 step: 382, loss is 0.1297360062599182\n",
      "epoch: 8 step: 383, loss is 0.0589594691991806\n",
      "epoch: 8 step: 384, loss is 0.2013615369796753\n",
      "epoch: 8 step: 385, loss is 0.06517361849546432\n",
      "epoch: 8 step: 386, loss is 0.13443006575107574\n",
      "epoch: 8 step: 387, loss is 0.1364036500453949\n",
      "epoch: 8 step: 388, loss is 0.06472614407539368\n",
      "epoch: 8 step: 389, loss is 0.10375029593706131\n",
      "epoch: 8 step: 390, loss is 0.04481285810470581\n",
      "epoch: 8 step: 391, loss is 0.07453273981809616\n",
      "epoch: 8 step: 392, loss is 0.040666863322257996\n",
      "epoch: 8 step: 393, loss is 0.08775258809328079\n",
      "epoch: 8 step: 394, loss is 0.011742868460714817\n",
      "epoch: 8 step: 395, loss is 0.11467454582452774\n",
      "epoch: 8 step: 396, loss is 0.05822261795401573\n",
      "epoch: 8 step: 397, loss is 0.055111292749643326\n",
      "epoch: 8 step: 398, loss is 0.07010581344366074\n",
      "epoch: 8 step: 399, loss is 0.1466558426618576\n",
      "epoch: 8 step: 400, loss is 0.0672549158334732\n",
      "epoch: 8 step: 401, loss is 0.0818614810705185\n",
      "epoch: 8 step: 402, loss is 0.07923863083124161\n",
      "epoch: 8 step: 403, loss is 0.07407833635807037\n",
      "epoch: 8 step: 404, loss is 0.10725860297679901\n",
      "epoch: 8 step: 405, loss is 0.043696317821741104\n",
      "epoch: 8 step: 406, loss is 0.1715535670518875\n",
      "epoch: 8 step: 407, loss is 0.16613687574863434\n",
      "epoch: 8 step: 408, loss is 0.09979905188083649\n",
      "epoch: 8 step: 409, loss is 0.24382591247558594\n",
      "epoch: 8 step: 410, loss is 0.24712108075618744\n",
      "epoch: 8 step: 411, loss is 0.1386771947145462\n",
      "epoch: 8 step: 412, loss is 0.10178627073764801\n",
      "epoch: 8 step: 413, loss is 0.04388628527522087\n",
      "epoch: 8 step: 414, loss is 0.10142186284065247\n",
      "epoch: 8 step: 415, loss is 0.03309769183397293\n",
      "epoch: 8 step: 416, loss is 0.0953807532787323\n",
      "epoch: 8 step: 417, loss is 0.13426481187343597\n",
      "epoch: 8 step: 418, loss is 0.08886745572090149\n",
      "epoch: 8 step: 419, loss is 0.0415988489985466\n",
      "epoch: 8 step: 420, loss is 0.0727800726890564\n",
      "epoch: 8 step: 421, loss is 0.07600504159927368\n",
      "epoch: 8 step: 422, loss is 0.08825060725212097\n",
      "epoch: 8 step: 423, loss is 0.12524454295635223\n",
      "epoch: 8 step: 424, loss is 0.03822433948516846\n",
      "epoch: 8 step: 425, loss is 0.25761285424232483\n",
      "epoch: 8 step: 426, loss is 0.054520133882761\n",
      "epoch: 8 step: 427, loss is 0.1073671281337738\n",
      "epoch: 8 step: 428, loss is 0.11664275825023651\n",
      "epoch: 8 step: 429, loss is 0.1447991132736206\n",
      "epoch: 8 step: 430, loss is 0.10164869576692581\n",
      "epoch: 8 step: 431, loss is 0.09410129487514496\n",
      "epoch: 8 step: 432, loss is 0.06498924642801285\n",
      "epoch: 8 step: 433, loss is 0.044846970587968826\n",
      "epoch: 8 step: 434, loss is 0.11570674926042557\n",
      "epoch: 8 step: 435, loss is 0.08630100637674332\n",
      "epoch: 8 step: 436, loss is 0.14326028525829315\n",
      "epoch: 8 step: 437, loss is 0.03919888287782669\n",
      "epoch: 8 step: 438, loss is 0.0907478928565979\n",
      "epoch: 8 step: 439, loss is 0.05368097871541977\n",
      "epoch: 8 step: 440, loss is 0.07633687555789948\n",
      "epoch: 8 step: 441, loss is 0.08760124444961548\n",
      "epoch: 8 step: 442, loss is 0.12798097729682922\n",
      "epoch: 8 step: 443, loss is 0.04363209009170532\n",
      "epoch: 8 step: 444, loss is 0.04855600371956825\n",
      "epoch: 8 step: 445, loss is 0.09531837701797485\n",
      "epoch: 8 step: 446, loss is 0.1347503811120987\n",
      "epoch: 8 step: 447, loss is 0.03042110800743103\n",
      "epoch: 8 step: 448, loss is 0.04330174997448921\n",
      "epoch: 8 step: 449, loss is 0.13432298600673676\n",
      "epoch: 8 step: 450, loss is 0.08518031984567642\n",
      "epoch: 8 step: 451, loss is 0.10597725957632065\n",
      "epoch: 8 step: 452, loss is 0.049213163554668427\n",
      "epoch: 8 step: 453, loss is 0.06938126683235168\n",
      "epoch: 8 step: 454, loss is 0.10952844470739365\n",
      "epoch: 8 step: 455, loss is 0.08915024250745773\n",
      "epoch: 8 step: 456, loss is 0.0988508015871048\n",
      "epoch: 8 step: 457, loss is 0.03061063587665558\n",
      "epoch: 8 step: 458, loss is 0.03405344858765602\n",
      "epoch: 8 step: 459, loss is 0.09867336601018906\n",
      "epoch: 8 step: 460, loss is 0.2329777479171753\n",
      "epoch: 8 step: 461, loss is 0.03617846220731735\n",
      "epoch: 8 step: 462, loss is 0.04863309487700462\n",
      "epoch: 8 step: 463, loss is 0.04347711801528931\n",
      "epoch: 8 step: 464, loss is 0.14386627078056335\n",
      "epoch: 8 step: 465, loss is 0.03543660789728165\n",
      "epoch: 8 step: 466, loss is 0.031151656061410904\n",
      "epoch: 8 step: 467, loss is 0.0995911955833435\n",
      "epoch: 8 step: 468, loss is 0.07842123508453369\n",
      "epoch: 8 step: 469, loss is 0.06566860526800156\n",
      "epoch: 8 step: 470, loss is 0.022477103397250175\n",
      "epoch: 8 step: 471, loss is 0.2260088175535202\n",
      "epoch: 8 step: 472, loss is 0.04172639548778534\n",
      "epoch: 8 step: 473, loss is 0.13117846846580505\n",
      "epoch: 8 step: 474, loss is 0.062176842242479324\n",
      "epoch: 8 step: 475, loss is 0.11127793788909912\n",
      "epoch: 8 step: 476, loss is 0.08252763748168945\n",
      "epoch: 8 step: 477, loss is 0.07090204954147339\n",
      "epoch: 8 step: 478, loss is 0.07717138528823853\n",
      "epoch: 8 step: 479, loss is 0.0303204245865345\n",
      "epoch: 8 step: 480, loss is 0.08639375865459442\n",
      "epoch: 8 step: 481, loss is 0.051773007959127426\n",
      "epoch: 8 step: 482, loss is 0.11531862616539001\n",
      "epoch: 8 step: 483, loss is 0.054146140813827515\n",
      "epoch: 8 step: 484, loss is 0.13376471400260925\n",
      "epoch: 8 step: 485, loss is 0.014733431860804558\n",
      "epoch: 8 step: 486, loss is 0.21636614203453064\n",
      "epoch: 8 step: 487, loss is 0.04701242223381996\n",
      "epoch: 8 step: 488, loss is 0.31567084789276123\n",
      "epoch: 8 step: 489, loss is 0.1028863862156868\n",
      "epoch: 8 step: 490, loss is 0.06551273167133331\n",
      "epoch: 8 step: 491, loss is 0.0320611409842968\n",
      "epoch: 8 step: 492, loss is 0.1977401077747345\n",
      "epoch: 8 step: 493, loss is 0.14248374104499817\n",
      "epoch: 8 step: 494, loss is 0.054234828799963\n",
      "epoch: 8 step: 495, loss is 0.12386351823806763\n",
      "epoch: 8 step: 496, loss is 0.0827842503786087\n",
      "epoch: 8 step: 497, loss is 0.14737315475940704\n",
      "epoch: 8 step: 498, loss is 0.057623058557510376\n",
      "epoch: 8 step: 499, loss is 0.046572212129831314\n",
      "epoch: 8 step: 500, loss is 0.08120885491371155\n",
      "epoch: 8 step: 501, loss is 0.12551479041576385\n",
      "epoch: 8 step: 502, loss is 0.11423948407173157\n",
      "epoch: 8 step: 503, loss is 0.07994738221168518\n",
      "epoch: 8 step: 504, loss is 0.16898547112941742\n",
      "epoch: 8 step: 505, loss is 0.05786876380443573\n",
      "epoch: 8 step: 506, loss is 0.07838182896375656\n",
      "epoch: 8 step: 507, loss is 0.11928382515907288\n",
      "epoch: 8 step: 508, loss is 0.07260457426309586\n",
      "epoch: 8 step: 509, loss is 0.12553530931472778\n",
      "epoch: 8 step: 510, loss is 0.06222653016448021\n",
      "epoch: 8 step: 511, loss is 0.15877798199653625\n",
      "epoch: 8 step: 512, loss is 0.0680430456995964\n",
      "epoch: 8 step: 513, loss is 0.14137637615203857\n",
      "epoch: 8 step: 514, loss is 0.12553465366363525\n",
      "epoch: 8 step: 515, loss is 0.14133933186531067\n",
      "epoch: 8 step: 516, loss is 0.08508722484111786\n",
      "epoch: 8 step: 517, loss is 0.14667096734046936\n",
      "epoch: 8 step: 518, loss is 0.11571651697158813\n",
      "epoch: 8 step: 519, loss is 0.045209918171167374\n",
      "epoch: 8 step: 520, loss is 0.09963735938072205\n",
      "epoch: 8 step: 521, loss is 0.06020988151431084\n",
      "epoch: 8 step: 522, loss is 0.035625047981739044\n",
      "epoch: 8 step: 523, loss is 0.0828457772731781\n",
      "epoch: 8 step: 524, loss is 0.06240241602063179\n",
      "epoch: 8 step: 525, loss is 0.05673965439200401\n",
      "epoch: 8 step: 526, loss is 0.13695837557315826\n",
      "epoch: 8 step: 527, loss is 0.472419410943985\n",
      "epoch: 8 step: 528, loss is 0.15630172193050385\n",
      "epoch: 8 step: 529, loss is 0.15609221160411835\n",
      "epoch: 8 step: 530, loss is 0.05104929953813553\n",
      "epoch: 8 step: 531, loss is 0.037003204226493835\n",
      "epoch: 8 step: 532, loss is 0.09483708441257477\n",
      "epoch: 8 step: 533, loss is 0.1291753053665161\n",
      "epoch: 8 step: 534, loss is 0.015105192549526691\n",
      "epoch: 8 step: 535, loss is 0.24544712901115417\n",
      "epoch: 8 step: 536, loss is 0.1755271852016449\n",
      "epoch: 8 step: 537, loss is 0.1085238978266716\n",
      "epoch: 8 step: 538, loss is 0.047295548021793365\n",
      "epoch: 8 step: 539, loss is 0.028084754943847656\n",
      "epoch: 8 step: 540, loss is 0.0702706053853035\n",
      "epoch: 8 step: 541, loss is 0.09382087737321854\n",
      "epoch: 8 step: 542, loss is 0.08524180203676224\n",
      "epoch: 8 step: 543, loss is 0.05232693627476692\n",
      "epoch: 8 step: 544, loss is 0.05727815255522728\n",
      "epoch: 8 step: 545, loss is 0.20161747932434082\n",
      "epoch: 8 step: 546, loss is 0.06829111278057098\n",
      "epoch: 8 step: 547, loss is 0.11425206810235977\n",
      "epoch: 8 step: 548, loss is 0.09367835521697998\n",
      "epoch: 8 step: 549, loss is 0.09558558464050293\n",
      "epoch: 8 step: 550, loss is 0.17361630499362946\n",
      "epoch: 8 step: 551, loss is 0.08378994464874268\n",
      "epoch: 8 step: 552, loss is 0.16451846063137054\n",
      "epoch: 8 step: 553, loss is 0.2804683446884155\n",
      "epoch: 8 step: 554, loss is 0.11861875653266907\n",
      "epoch: 8 step: 555, loss is 0.08616074174642563\n",
      "epoch: 8 step: 556, loss is 0.17282111942768097\n",
      "epoch: 8 step: 557, loss is 0.1345699578523636\n",
      "epoch: 8 step: 558, loss is 0.15057292580604553\n",
      "epoch: 8 step: 559, loss is 0.10316513478755951\n",
      "epoch: 8 step: 560, loss is 0.13902708888053894\n",
      "epoch: 8 step: 561, loss is 0.15597915649414062\n",
      "epoch: 8 step: 562, loss is 0.13201457262039185\n",
      "epoch: 8 step: 563, loss is 0.08333270996809006\n",
      "epoch: 8 step: 564, loss is 0.03189380466938019\n",
      "epoch: 8 step: 565, loss is 0.07130517810583115\n",
      "epoch: 8 step: 566, loss is 0.08963519334793091\n",
      "epoch: 8 step: 567, loss is 0.27701592445373535\n",
      "epoch: 8 step: 568, loss is 0.14873746037483215\n",
      "epoch: 8 step: 569, loss is 0.13535217940807343\n",
      "epoch: 8 step: 570, loss is 0.1481422632932663\n",
      "epoch: 8 step: 571, loss is 0.1110261082649231\n",
      "epoch: 8 step: 572, loss is 0.03628525510430336\n",
      "epoch: 8 step: 573, loss is 0.10578200221061707\n",
      "epoch: 8 step: 574, loss is 0.13432830572128296\n",
      "epoch: 8 step: 575, loss is 0.07906561344861984\n",
      "epoch: 8 step: 576, loss is 0.08569364249706268\n",
      "epoch: 8 step: 577, loss is 0.039507292211055756\n",
      "epoch: 8 step: 578, loss is 0.1302049160003662\n",
      "epoch: 8 step: 579, loss is 0.08561287075281143\n",
      "epoch: 8 step: 580, loss is 0.06725779175758362\n",
      "epoch: 8 step: 581, loss is 0.09587022662162781\n",
      "epoch: 8 step: 582, loss is 0.0860535204410553\n",
      "epoch: 8 step: 583, loss is 0.10008618235588074\n",
      "epoch: 8 step: 584, loss is 0.07687445729970932\n",
      "epoch: 8 step: 585, loss is 0.13118278980255127\n",
      "epoch: 8 step: 586, loss is 0.033516060560941696\n",
      "epoch: 8 step: 587, loss is 0.12420615553855896\n",
      "epoch: 8 step: 588, loss is 0.08914659172296524\n",
      "epoch: 8 step: 589, loss is 0.14754003286361694\n",
      "epoch: 8 step: 590, loss is 0.07726496458053589\n",
      "epoch: 8 step: 591, loss is 0.14345291256904602\n",
      "epoch: 8 step: 592, loss is 0.1330139935016632\n",
      "epoch: 8 step: 593, loss is 0.1386564075946808\n",
      "epoch: 8 step: 594, loss is 0.06584034115076065\n",
      "epoch: 8 step: 595, loss is 0.1033722460269928\n",
      "epoch: 8 step: 596, loss is 0.09155868738889694\n",
      "epoch: 8 step: 597, loss is 0.16398465633392334\n",
      "epoch: 8 step: 598, loss is 0.0905757024884224\n",
      "epoch: 8 step: 599, loss is 0.11563574522733688\n",
      "epoch: 8 step: 600, loss is 0.0970996543765068\n",
      "epoch: 8 step: 601, loss is 0.0604996532201767\n",
      "epoch: 8 step: 602, loss is 0.1253952831029892\n",
      "epoch: 8 step: 603, loss is 0.11304385960102081\n",
      "epoch: 8 step: 604, loss is 0.14734578132629395\n",
      "epoch: 8 step: 605, loss is 0.026137467473745346\n",
      "epoch: 8 step: 606, loss is 0.08644772320985794\n",
      "epoch: 8 step: 607, loss is 0.14520768821239471\n",
      "epoch: 8 step: 608, loss is 0.12930810451507568\n",
      "epoch: 8 step: 609, loss is 0.10815003514289856\n",
      "epoch: 8 step: 610, loss is 0.22789371013641357\n",
      "epoch: 8 step: 611, loss is 0.19365330040454865\n",
      "epoch: 8 step: 612, loss is 0.04940703511238098\n",
      "epoch: 8 step: 613, loss is 0.10893221944570541\n",
      "epoch: 8 step: 614, loss is 0.05432765930891037\n",
      "epoch: 8 step: 615, loss is 0.06195328012108803\n",
      "epoch: 8 step: 616, loss is 0.14197438955307007\n",
      "epoch: 8 step: 617, loss is 0.11357412487268448\n",
      "epoch: 8 step: 618, loss is 0.15116509795188904\n",
      "epoch: 8 step: 619, loss is 0.15056413412094116\n",
      "epoch: 8 step: 620, loss is 0.06727766245603561\n",
      "epoch: 8 step: 621, loss is 0.08290816098451614\n",
      "epoch: 8 step: 622, loss is 0.10087127983570099\n",
      "epoch: 8 step: 623, loss is 0.1794697642326355\n",
      "epoch: 8 step: 624, loss is 0.1202014833688736\n",
      "epoch: 8 step: 625, loss is 0.12053324282169342\n",
      "epoch: 8 step: 626, loss is 0.10070700198411942\n",
      "epoch: 8 step: 627, loss is 0.11847946792840958\n",
      "epoch: 8 step: 628, loss is 0.14651601016521454\n",
      "epoch: 8 step: 629, loss is 0.028218014165759087\n",
      "epoch: 8 step: 630, loss is 0.08310160040855408\n",
      "epoch: 8 step: 631, loss is 0.1182011067867279\n",
      "epoch: 8 step: 632, loss is 0.10902340710163116\n",
      "epoch: 8 step: 633, loss is 0.09159751981496811\n",
      "epoch: 8 step: 634, loss is 0.15593339502811432\n",
      "epoch: 8 step: 635, loss is 0.11643674969673157\n",
      "epoch: 8 step: 636, loss is 0.09727828204631805\n",
      "epoch: 8 step: 637, loss is 0.014434201642870903\n",
      "epoch: 8 step: 638, loss is 0.11210854351520538\n",
      "epoch: 8 step: 639, loss is 0.03739234432578087\n",
      "epoch: 8 step: 640, loss is 0.038323868066072464\n",
      "epoch: 8 step: 641, loss is 0.03734469786286354\n",
      "epoch: 8 step: 642, loss is 0.08071672916412354\n",
      "epoch: 8 step: 643, loss is 0.13363324105739594\n",
      "epoch: 8 step: 644, loss is 0.10924109071493149\n",
      "epoch: 8 step: 645, loss is 0.1613536924123764\n",
      "epoch: 8 step: 646, loss is 0.03670398145914078\n",
      "epoch: 8 step: 647, loss is 0.06125381961464882\n",
      "epoch: 8 step: 648, loss is 0.056271910667419434\n",
      "epoch: 8 step: 649, loss is 0.056843068450689316\n",
      "epoch: 8 step: 650, loss is 0.07158343493938446\n",
      "epoch: 8 step: 651, loss is 0.06784754246473312\n",
      "epoch: 8 step: 652, loss is 0.038228150457143784\n",
      "epoch: 8 step: 653, loss is 0.05505523458123207\n",
      "epoch: 8 step: 654, loss is 0.12792450189590454\n",
      "epoch: 8 step: 655, loss is 0.06325317919254303\n",
      "epoch: 8 step: 656, loss is 0.06631596386432648\n",
      "epoch: 8 step: 657, loss is 0.13044172525405884\n",
      "epoch: 8 step: 658, loss is 0.12731249630451202\n",
      "epoch: 8 step: 659, loss is 0.08895184099674225\n",
      "epoch: 8 step: 660, loss is 0.053265612572431564\n",
      "epoch: 8 step: 661, loss is 0.06023627147078514\n",
      "epoch: 8 step: 662, loss is 0.12001829594373703\n",
      "epoch: 8 step: 663, loss is 0.12139859795570374\n",
      "epoch: 8 step: 664, loss is 0.11606018990278244\n",
      "epoch: 8 step: 665, loss is 0.13694538176059723\n",
      "epoch: 8 step: 666, loss is 0.12484820187091827\n",
      "epoch: 8 step: 667, loss is 0.12035181373357773\n",
      "epoch: 8 step: 668, loss is 0.13598208129405975\n",
      "epoch: 8 step: 669, loss is 0.10640212893486023\n",
      "epoch: 8 step: 670, loss is 0.10538728535175323\n",
      "epoch: 8 step: 671, loss is 0.053448379039764404\n",
      "epoch: 8 step: 672, loss is 0.0967710092663765\n",
      "epoch: 8 step: 673, loss is 0.1020164042711258\n",
      "epoch: 8 step: 674, loss is 0.13517220318317413\n",
      "epoch: 8 step: 675, loss is 0.0984657034277916\n",
      "epoch: 8 step: 676, loss is 0.15485267341136932\n",
      "epoch: 8 step: 677, loss is 0.07686658948659897\n",
      "epoch: 8 step: 678, loss is 0.10403653979301453\n",
      "epoch: 8 step: 679, loss is 0.08118897676467896\n",
      "epoch: 8 step: 680, loss is 0.06092635914683342\n",
      "epoch: 8 step: 681, loss is 0.13247063755989075\n",
      "epoch: 8 step: 682, loss is 0.18167836964130402\n",
      "epoch: 8 step: 683, loss is 0.029537992551922798\n",
      "epoch: 8 step: 684, loss is 0.31879299879074097\n",
      "epoch: 8 step: 685, loss is 0.09746599197387695\n",
      "epoch: 8 step: 686, loss is 0.1055489182472229\n",
      "epoch: 8 step: 687, loss is 0.10059807449579239\n",
      "epoch: 8 step: 688, loss is 0.13616728782653809\n",
      "epoch: 8 step: 689, loss is 0.04887213557958603\n",
      "epoch: 8 step: 690, loss is 0.04241704195737839\n",
      "epoch: 8 step: 691, loss is 0.06241465359926224\n",
      "epoch: 8 step: 692, loss is 0.049501579254865646\n",
      "epoch: 8 step: 693, loss is 0.0868312194943428\n",
      "epoch: 8 step: 694, loss is 0.13501209020614624\n",
      "epoch: 8 step: 695, loss is 0.10642841458320618\n",
      "epoch: 8 step: 696, loss is 0.10101272910833359\n",
      "epoch: 8 step: 697, loss is 0.06050000339746475\n",
      "epoch: 8 step: 698, loss is 0.05923853814601898\n",
      "epoch: 8 step: 699, loss is 0.11154698580503464\n",
      "epoch: 8 step: 700, loss is 0.13647675514221191\n",
      "epoch: 8 step: 701, loss is 0.09042205661535263\n",
      "epoch: 8 step: 702, loss is 0.14617496728897095\n",
      "epoch: 8 step: 703, loss is 0.10358526557683945\n",
      "epoch: 8 step: 704, loss is 0.12569265067577362\n",
      "epoch: 8 step: 705, loss is 0.02339853160083294\n",
      "epoch: 8 step: 706, loss is 0.0716099664568901\n",
      "epoch: 8 step: 707, loss is 0.12060651928186417\n",
      "epoch: 8 step: 708, loss is 0.06361372768878937\n",
      "epoch: 8 step: 709, loss is 0.14829882979393005\n",
      "epoch: 8 step: 710, loss is 0.026261979714035988\n",
      "epoch: 8 step: 711, loss is 0.12263517081737518\n",
      "epoch: 8 step: 712, loss is 0.08547311276197433\n",
      "epoch: 8 step: 713, loss is 0.18988579511642456\n",
      "epoch: 8 step: 714, loss is 0.13083043694496155\n",
      "epoch: 8 step: 715, loss is 0.1118365004658699\n",
      "epoch: 8 step: 716, loss is 0.06595633924007416\n",
      "epoch: 8 step: 717, loss is 0.09479928016662598\n",
      "epoch: 8 step: 718, loss is 0.1417071372270584\n",
      "epoch: 8 step: 719, loss is 0.12804952263832092\n",
      "epoch: 8 step: 720, loss is 0.06075419485569\n",
      "epoch: 8 step: 721, loss is 0.0704941600561142\n",
      "epoch: 8 step: 722, loss is 0.1504802405834198\n",
      "epoch: 8 step: 723, loss is 0.03907998278737068\n",
      "epoch: 8 step: 724, loss is 0.24546097218990326\n",
      "epoch: 8 step: 725, loss is 0.021271318197250366\n",
      "epoch: 8 step: 726, loss is 0.091584712266922\n",
      "epoch: 8 step: 727, loss is 0.10327231138944626\n",
      "epoch: 8 step: 728, loss is 0.10348786413669586\n",
      "epoch: 8 step: 729, loss is 0.13083332777023315\n",
      "epoch: 8 step: 730, loss is 0.11719520390033722\n",
      "epoch: 8 step: 731, loss is 0.03202403709292412\n",
      "epoch: 8 step: 732, loss is 0.10761281847953796\n",
      "epoch: 8 step: 733, loss is 0.055851660668849945\n",
      "epoch: 8 step: 734, loss is 0.08718128502368927\n",
      "epoch: 8 step: 735, loss is 0.11645615100860596\n",
      "epoch: 8 step: 736, loss is 0.10487260669469833\n",
      "epoch: 8 step: 737, loss is 0.21533866226673126\n",
      "epoch: 8 step: 738, loss is 0.15022628009319305\n",
      "epoch: 8 step: 739, loss is 0.06623095273971558\n",
      "epoch: 8 step: 740, loss is 0.018776683136820793\n",
      "epoch: 8 step: 741, loss is 0.02156243845820427\n",
      "epoch: 8 step: 742, loss is 0.05979936197400093\n",
      "epoch: 8 step: 743, loss is 0.10338776558637619\n",
      "epoch: 8 step: 744, loss is 0.19266366958618164\n",
      "epoch: 8 step: 745, loss is 0.12684844434261322\n",
      "epoch: 8 step: 746, loss is 0.09672629088163376\n",
      "epoch: 8 step: 747, loss is 0.03221214935183525\n",
      "epoch: 8 step: 748, loss is 0.07095222175121307\n",
      "epoch: 8 step: 749, loss is 0.2272273153066635\n",
      "epoch: 8 step: 750, loss is 0.021552596241235733\n",
      "epoch: 8 step: 751, loss is 0.1126413494348526\n",
      "epoch: 8 step: 752, loss is 0.09204260259866714\n",
      "epoch: 8 step: 753, loss is 0.12127451598644257\n",
      "epoch: 8 step: 754, loss is 0.17374293506145477\n",
      "epoch: 8 step: 755, loss is 0.08756711333990097\n",
      "epoch: 8 step: 756, loss is 0.09809934347867966\n",
      "epoch: 8 step: 757, loss is 0.23244482278823853\n",
      "epoch: 8 step: 758, loss is 0.11797752976417542\n",
      "epoch: 8 step: 759, loss is 0.024190841242671013\n",
      "epoch: 8 step: 760, loss is 0.09909426420927048\n",
      "epoch: 8 step: 761, loss is 0.07829905301332474\n",
      "epoch: 8 step: 762, loss is 0.08044778555631638\n",
      "epoch: 8 step: 763, loss is 0.11841688305139542\n",
      "epoch: 8 step: 764, loss is 0.028865156695246696\n",
      "epoch: 8 step: 765, loss is 0.15207698941230774\n",
      "epoch: 8 step: 766, loss is 0.13691304624080658\n",
      "epoch: 8 step: 767, loss is 0.03823554515838623\n",
      "epoch: 8 step: 768, loss is 0.09249486029148102\n",
      "epoch: 8 step: 769, loss is 0.03261534124612808\n",
      "epoch: 8 step: 770, loss is 0.09795286506414413\n",
      "epoch: 8 step: 771, loss is 0.04302205145359039\n",
      "epoch: 8 step: 772, loss is 0.05996875837445259\n",
      "epoch: 8 step: 773, loss is 0.04850770905613899\n",
      "epoch: 8 step: 774, loss is 0.09898167848587036\n",
      "epoch: 8 step: 775, loss is 0.044475048780441284\n",
      "epoch: 8 step: 776, loss is 0.09458380937576294\n",
      "epoch: 8 step: 777, loss is 0.07487256079912186\n",
      "epoch: 8 step: 778, loss is 0.06692150235176086\n",
      "epoch: 8 step: 779, loss is 0.07554440200328827\n",
      "epoch: 8 step: 780, loss is 0.03976575285196304\n",
      "epoch: 8 step: 781, loss is 0.11237500607967377\n",
      "epoch: 8 step: 782, loss is 0.06486114859580994\n",
      "epoch: 8 step: 783, loss is 0.1725355088710785\n",
      "epoch: 8 step: 784, loss is 0.08496586978435516\n",
      "epoch: 8 step: 785, loss is 0.14513680338859558\n",
      "epoch: 8 step: 786, loss is 0.05446397140622139\n",
      "epoch: 8 step: 787, loss is 0.18887542188167572\n",
      "epoch: 8 step: 788, loss is 0.07218056172132492\n",
      "epoch: 8 step: 789, loss is 0.11711621284484863\n",
      "epoch: 8 step: 790, loss is 0.19228433072566986\n",
      "epoch: 8 step: 791, loss is 0.13423220813274384\n",
      "epoch: 8 step: 792, loss is 0.024280205368995667\n",
      "epoch: 8 step: 793, loss is 0.07156357169151306\n",
      "epoch: 8 step: 794, loss is 0.033174384385347366\n",
      "epoch: 8 step: 795, loss is 0.0955439880490303\n",
      "epoch: 8 step: 796, loss is 0.059658050537109375\n",
      "epoch: 8 step: 797, loss is 0.048342060297727585\n",
      "epoch: 8 step: 798, loss is 0.11278045922517776\n",
      "epoch: 8 step: 799, loss is 0.0668753832578659\n",
      "epoch: 8 step: 800, loss is 0.24886290729045868\n",
      "epoch: 8 step: 801, loss is 0.125187486410141\n",
      "epoch: 8 step: 802, loss is 0.09290667623281479\n",
      "epoch: 8 step: 803, loss is 0.09628524631261826\n",
      "epoch: 8 step: 804, loss is 0.11874169856309891\n",
      "epoch: 8 step: 805, loss is 0.03805740922689438\n",
      "epoch: 8 step: 806, loss is 0.29771560430526733\n",
      "epoch: 8 step: 807, loss is 0.14473575353622437\n",
      "epoch: 8 step: 808, loss is 0.1674623191356659\n",
      "epoch: 8 step: 809, loss is 0.04924231022596359\n",
      "epoch: 8 step: 810, loss is 0.028302913531661034\n",
      "epoch: 8 step: 811, loss is 0.10590542107820511\n",
      "epoch: 8 step: 812, loss is 0.042754434049129486\n",
      "epoch: 8 step: 813, loss is 0.14992564916610718\n",
      "epoch: 8 step: 814, loss is 0.08258719742298126\n",
      "epoch: 8 step: 815, loss is 0.06777132302522659\n",
      "epoch: 8 step: 816, loss is 0.10845820605754852\n",
      "epoch: 8 step: 817, loss is 0.12706376612186432\n",
      "epoch: 8 step: 818, loss is 0.024698320776224136\n",
      "epoch: 8 step: 819, loss is 0.044003698974847794\n",
      "epoch: 8 step: 820, loss is 0.09698907285928726\n",
      "epoch: 8 step: 821, loss is 0.045110952109098434\n",
      "epoch: 8 step: 822, loss is 0.1691296100616455\n",
      "epoch: 8 step: 823, loss is 0.1502493917942047\n",
      "epoch: 8 step: 824, loss is 0.14979226887226105\n",
      "epoch: 8 step: 825, loss is 0.1027410551905632\n",
      "epoch: 8 step: 826, loss is 0.2772740125656128\n",
      "epoch: 8 step: 827, loss is 0.10224796831607819\n",
      "epoch: 8 step: 828, loss is 0.09743983298540115\n",
      "epoch: 8 step: 829, loss is 0.24980120360851288\n",
      "epoch: 8 step: 830, loss is 0.11588636040687561\n",
      "epoch: 8 step: 831, loss is 0.10229222476482391\n",
      "epoch: 8 step: 832, loss is 0.03961057960987091\n",
      "epoch: 8 step: 833, loss is 0.07223343104124069\n",
      "epoch: 8 step: 834, loss is 0.10472282022237778\n",
      "epoch: 8 step: 835, loss is 0.1518203467130661\n",
      "epoch: 8 step: 836, loss is 0.12727344036102295\n",
      "epoch: 8 step: 837, loss is 0.10028819739818573\n",
      "epoch: 8 step: 838, loss is 0.046669960021972656\n",
      "epoch: 8 step: 839, loss is 0.07177867740392685\n",
      "epoch: 8 step: 840, loss is 0.11038226634263992\n",
      "epoch: 8 step: 841, loss is 0.09394710510969162\n",
      "epoch: 8 step: 842, loss is 0.13184987008571625\n",
      "epoch: 8 step: 843, loss is 0.07532075047492981\n",
      "epoch: 8 step: 844, loss is 0.17019414901733398\n",
      "epoch: 8 step: 845, loss is 0.19144131243228912\n",
      "epoch: 8 step: 846, loss is 0.17753560841083527\n",
      "epoch: 8 step: 847, loss is 0.09815160930156708\n",
      "epoch: 8 step: 848, loss is 0.33780863881111145\n",
      "epoch: 8 step: 849, loss is 0.039000969380140305\n",
      "epoch: 8 step: 850, loss is 0.1974039077758789\n",
      "epoch: 8 step: 851, loss is 0.11162850260734558\n",
      "epoch: 8 step: 852, loss is 0.18128207325935364\n",
      "epoch: 8 step: 853, loss is 0.19868095219135284\n",
      "epoch: 8 step: 854, loss is 0.057649970054626465\n",
      "epoch: 8 step: 855, loss is 0.14169155061244965\n",
      "epoch: 8 step: 856, loss is 0.14266104996204376\n",
      "epoch: 8 step: 857, loss is 0.10113789886236191\n",
      "epoch: 8 step: 858, loss is 0.03625920042395592\n",
      "epoch: 8 step: 859, loss is 0.04814685881137848\n",
      "epoch: 8 step: 860, loss is 0.037013813853263855\n",
      "epoch: 8 step: 861, loss is 0.044284671545028687\n",
      "epoch: 8 step: 862, loss is 0.09954726696014404\n",
      "epoch: 8 step: 863, loss is 0.11884995549917221\n",
      "epoch: 8 step: 864, loss is 0.16835565865039825\n",
      "epoch: 8 step: 865, loss is 0.031354956328868866\n",
      "epoch: 8 step: 866, loss is 0.08093235641717911\n",
      "epoch: 8 step: 867, loss is 0.17578253149986267\n",
      "epoch: 8 step: 868, loss is 0.08157516270875931\n",
      "epoch: 8 step: 869, loss is 0.10407918691635132\n",
      "epoch: 8 step: 870, loss is 0.1952783167362213\n",
      "epoch: 8 step: 871, loss is 0.19199137389659882\n",
      "epoch: 8 step: 872, loss is 0.12463294714689255\n",
      "epoch: 8 step: 873, loss is 0.08373408764600754\n",
      "epoch: 8 step: 874, loss is 0.12727119028568268\n",
      "epoch: 8 step: 875, loss is 0.07556036859750748\n",
      "epoch: 8 step: 876, loss is 0.08753781020641327\n",
      "epoch: 8 step: 877, loss is 0.16441266238689423\n",
      "epoch: 8 step: 878, loss is 0.1412334144115448\n",
      "epoch: 8 step: 879, loss is 0.06781645119190216\n",
      "epoch: 8 step: 880, loss is 0.10097746551036835\n",
      "epoch: 8 step: 881, loss is 0.06850945949554443\n",
      "epoch: 8 step: 882, loss is 0.04388198256492615\n",
      "epoch: 8 step: 883, loss is 0.24141448736190796\n",
      "epoch: 8 step: 884, loss is 0.1164325475692749\n",
      "epoch: 8 step: 885, loss is 0.10440327972173691\n",
      "epoch: 8 step: 886, loss is 0.07417134940624237\n",
      "epoch: 8 step: 887, loss is 0.06156177818775177\n",
      "epoch: 8 step: 888, loss is 0.07519367337226868\n",
      "epoch: 8 step: 889, loss is 0.1916627585887909\n",
      "epoch: 8 step: 890, loss is 0.09905655682086945\n",
      "epoch: 8 step: 891, loss is 0.19472183287143707\n",
      "epoch: 8 step: 892, loss is 0.15800179541110992\n",
      "epoch: 8 step: 893, loss is 0.09810511022806168\n",
      "epoch: 8 step: 894, loss is 0.1267901510000229\n",
      "epoch: 8 step: 895, loss is 0.08879110962152481\n",
      "epoch: 8 step: 896, loss is 0.05668983981013298\n",
      "epoch: 8 step: 897, loss is 0.1131746768951416\n",
      "epoch: 8 step: 898, loss is 0.06813099980354309\n",
      "epoch: 8 step: 899, loss is 0.077200748026371\n",
      "epoch: 8 step: 900, loss is 0.167078897356987\n",
      "epoch: 8 step: 901, loss is 0.05478352680802345\n",
      "epoch: 8 step: 902, loss is 0.10674989223480225\n",
      "epoch: 8 step: 903, loss is 0.06490783393383026\n",
      "epoch: 8 step: 904, loss is 0.09594668447971344\n",
      "epoch: 8 step: 905, loss is 0.05278652906417847\n",
      "epoch: 8 step: 906, loss is 0.07706780731678009\n",
      "epoch: 8 step: 907, loss is 0.1019863560795784\n",
      "epoch: 8 step: 908, loss is 0.07941976189613342\n",
      "epoch: 8 step: 909, loss is 0.12990939617156982\n",
      "epoch: 8 step: 910, loss is 0.235559344291687\n",
      "epoch: 8 step: 911, loss is 0.06504689902067184\n",
      "epoch: 8 step: 912, loss is 0.08338373899459839\n",
      "epoch: 8 step: 913, loss is 0.02415534481406212\n",
      "epoch: 8 step: 914, loss is 0.16273510456085205\n",
      "epoch: 8 step: 915, loss is 0.14429976046085358\n",
      "epoch: 8 step: 916, loss is 0.10706613212823868\n",
      "epoch: 8 step: 917, loss is 0.046611517667770386\n",
      "epoch: 8 step: 918, loss is 0.13589325547218323\n",
      "epoch: 8 step: 919, loss is 0.14536981284618378\n",
      "epoch: 8 step: 920, loss is 0.14426176249980927\n",
      "epoch: 8 step: 921, loss is 0.11181961745023727\n",
      "epoch: 8 step: 922, loss is 0.08844874799251556\n",
      "epoch: 8 step: 923, loss is 0.050313811749219894\n",
      "epoch: 8 step: 924, loss is 0.04370110109448433\n",
      "epoch: 8 step: 925, loss is 0.09389613568782806\n",
      "epoch: 8 step: 926, loss is 0.14459675550460815\n",
      "epoch: 8 step: 927, loss is 0.11337058991193771\n",
      "epoch: 8 step: 928, loss is 0.17306457459926605\n",
      "epoch: 8 step: 929, loss is 0.07967689633369446\n",
      "epoch: 8 step: 930, loss is 0.17895182967185974\n",
      "epoch: 8 step: 931, loss is 0.06326670944690704\n",
      "epoch: 8 step: 932, loss is 0.08258078247308731\n",
      "epoch: 8 step: 933, loss is 0.06736289709806442\n",
      "epoch: 8 step: 934, loss is 0.1290278136730194\n",
      "epoch: 8 step: 935, loss is 0.041364360600709915\n",
      "epoch: 8 step: 936, loss is 0.05692275986075401\n",
      "epoch: 8 step: 937, loss is 0.11135362088680267\n",
      "epoch: 9 step: 1, loss is 0.06531819701194763\n",
      "epoch: 9 step: 2, loss is 0.07117357105016708\n",
      "epoch: 9 step: 3, loss is 0.056127749383449554\n",
      "epoch: 9 step: 4, loss is 0.09194235503673553\n",
      "epoch: 9 step: 5, loss is 0.020320294424891472\n",
      "epoch: 9 step: 6, loss is 0.05846776068210602\n",
      "epoch: 9 step: 7, loss is 0.04387618601322174\n",
      "epoch: 9 step: 8, loss is 0.05043956637382507\n",
      "epoch: 9 step: 9, loss is 0.0670049861073494\n",
      "epoch: 9 step: 10, loss is 0.14329230785369873\n",
      "epoch: 9 step: 11, loss is 0.08264699578285217\n",
      "epoch: 9 step: 12, loss is 0.039086394011974335\n",
      "epoch: 9 step: 13, loss is 0.08046988397836685\n",
      "epoch: 9 step: 14, loss is 0.05637241527438164\n",
      "epoch: 9 step: 15, loss is 0.05202139914035797\n",
      "epoch: 9 step: 16, loss is 0.019786203280091286\n",
      "epoch: 9 step: 17, loss is 0.07789930701255798\n",
      "epoch: 9 step: 18, loss is 0.04552805423736572\n",
      "epoch: 9 step: 19, loss is 0.07891064882278442\n",
      "epoch: 9 step: 20, loss is 0.06151643022894859\n",
      "epoch: 9 step: 21, loss is 0.11122377216815948\n",
      "epoch: 9 step: 22, loss is 0.08357293903827667\n",
      "epoch: 9 step: 23, loss is 0.15446841716766357\n",
      "epoch: 9 step: 24, loss is 0.044848598539829254\n",
      "epoch: 9 step: 25, loss is 0.024743521586060524\n",
      "epoch: 9 step: 26, loss is 0.038007140159606934\n",
      "epoch: 9 step: 27, loss is 0.04534492269158363\n",
      "epoch: 9 step: 28, loss is 0.06393241137266159\n",
      "epoch: 9 step: 29, loss is 0.03862213343381882\n",
      "epoch: 9 step: 30, loss is 0.05675183981657028\n",
      "epoch: 9 step: 31, loss is 0.06938231736421585\n",
      "epoch: 9 step: 32, loss is 0.1507919430732727\n",
      "epoch: 9 step: 33, loss is 0.024695605039596558\n",
      "epoch: 9 step: 34, loss is 0.07570569962263107\n",
      "epoch: 9 step: 35, loss is 0.033526767045259476\n",
      "epoch: 9 step: 36, loss is 0.05986038222908974\n",
      "epoch: 9 step: 37, loss is 0.031539734452962875\n",
      "epoch: 9 step: 38, loss is 0.02413102798163891\n",
      "epoch: 9 step: 39, loss is 0.010952040553092957\n",
      "epoch: 9 step: 40, loss is 0.13829004764556885\n",
      "epoch: 9 step: 41, loss is 0.0539347305893898\n",
      "epoch: 9 step: 42, loss is 0.04227561876177788\n",
      "epoch: 9 step: 43, loss is 0.03302790969610214\n",
      "epoch: 9 step: 44, loss is 0.07044750452041626\n",
      "epoch: 9 step: 45, loss is 0.045905742794275284\n",
      "epoch: 9 step: 46, loss is 0.08463724702596664\n",
      "epoch: 9 step: 47, loss is 0.06366917490959167\n",
      "epoch: 9 step: 48, loss is 0.020387539640069008\n",
      "epoch: 9 step: 49, loss is 0.034866832196712494\n",
      "epoch: 9 step: 50, loss is 0.09510671347379684\n",
      "epoch: 9 step: 51, loss is 0.041175808757543564\n",
      "epoch: 9 step: 52, loss is 0.08054829388856888\n",
      "epoch: 9 step: 53, loss is 0.026582572609186172\n",
      "epoch: 9 step: 54, loss is 0.062180012464523315\n",
      "epoch: 9 step: 55, loss is 0.04234478995203972\n",
      "epoch: 9 step: 56, loss is 0.01609090343117714\n",
      "epoch: 9 step: 57, loss is 0.10988201946020126\n",
      "epoch: 9 step: 58, loss is 0.05701502412557602\n",
      "epoch: 9 step: 59, loss is 0.09588945657014847\n",
      "epoch: 9 step: 60, loss is 0.08884544670581818\n",
      "epoch: 9 step: 61, loss is 0.058294106274843216\n",
      "epoch: 9 step: 62, loss is 0.07012711465358734\n",
      "epoch: 9 step: 63, loss is 0.03272226080298424\n",
      "epoch: 9 step: 64, loss is 0.04290709272027016\n",
      "epoch: 9 step: 65, loss is 0.01950761489570141\n",
      "epoch: 9 step: 66, loss is 0.019891798496246338\n",
      "epoch: 9 step: 67, loss is 0.06739068031311035\n",
      "epoch: 9 step: 68, loss is 0.11961940675973892\n",
      "epoch: 9 step: 69, loss is 0.030596762895584106\n",
      "epoch: 9 step: 70, loss is 0.13748808205127716\n",
      "epoch: 9 step: 71, loss is 0.022471852600574493\n",
      "epoch: 9 step: 72, loss is 0.0477653406560421\n",
      "epoch: 9 step: 73, loss is 0.0556398369371891\n",
      "epoch: 9 step: 74, loss is 0.07185353338718414\n",
      "epoch: 9 step: 75, loss is 0.029428647831082344\n",
      "epoch: 9 step: 76, loss is 0.04940216615796089\n",
      "epoch: 9 step: 77, loss is 0.026493648067116737\n",
      "epoch: 9 step: 78, loss is 0.05358026549220085\n",
      "epoch: 9 step: 79, loss is 0.08664588630199432\n",
      "epoch: 9 step: 80, loss is 0.028432300314307213\n",
      "epoch: 9 step: 81, loss is 0.1367262303829193\n",
      "epoch: 9 step: 82, loss is 0.06740563362836838\n",
      "epoch: 9 step: 83, loss is 0.029253337532281876\n",
      "epoch: 9 step: 84, loss is 0.04915278032422066\n",
      "epoch: 9 step: 85, loss is 0.03580416738986969\n",
      "epoch: 9 step: 86, loss is 0.06115085259079933\n",
      "epoch: 9 step: 87, loss is 0.05646368861198425\n",
      "epoch: 9 step: 88, loss is 0.04356291890144348\n",
      "epoch: 9 step: 89, loss is 0.03557545691728592\n",
      "epoch: 9 step: 90, loss is 0.03194694593548775\n",
      "epoch: 9 step: 91, loss is 0.037182457745075226\n",
      "epoch: 9 step: 92, loss is 0.06270712614059448\n",
      "epoch: 9 step: 93, loss is 0.06758465617895126\n",
      "epoch: 9 step: 94, loss is 0.06408005207777023\n",
      "epoch: 9 step: 95, loss is 0.05172591283917427\n",
      "epoch: 9 step: 96, loss is 0.05313919484615326\n",
      "epoch: 9 step: 97, loss is 0.07517236471176147\n",
      "epoch: 9 step: 98, loss is 0.09882810711860657\n",
      "epoch: 9 step: 99, loss is 0.07959754765033722\n",
      "epoch: 9 step: 100, loss is 0.024091126397252083\n",
      "epoch: 9 step: 101, loss is 0.09406157582998276\n",
      "epoch: 9 step: 102, loss is 0.05681033805012703\n",
      "epoch: 9 step: 103, loss is 0.09383415430784225\n",
      "epoch: 9 step: 104, loss is 0.01785937324166298\n",
      "epoch: 9 step: 105, loss is 0.07402556389570236\n",
      "epoch: 9 step: 106, loss is 0.0786517933011055\n",
      "epoch: 9 step: 107, loss is 0.08043865114450455\n",
      "epoch: 9 step: 108, loss is 0.05211266875267029\n",
      "epoch: 9 step: 109, loss is 0.031224878504872322\n",
      "epoch: 9 step: 110, loss is 0.1360996961593628\n",
      "epoch: 9 step: 111, loss is 0.032194457948207855\n",
      "epoch: 9 step: 112, loss is 0.013910450972616673\n",
      "epoch: 9 step: 113, loss is 0.18002666532993317\n",
      "epoch: 9 step: 114, loss is 0.055086467415094376\n",
      "epoch: 9 step: 115, loss is 0.07498622685670853\n",
      "epoch: 9 step: 116, loss is 0.10901464521884918\n",
      "epoch: 9 step: 117, loss is 0.036238837987184525\n",
      "epoch: 9 step: 118, loss is 0.0697440505027771\n",
      "epoch: 9 step: 119, loss is 0.030016200616955757\n",
      "epoch: 9 step: 120, loss is 0.08989906311035156\n",
      "epoch: 9 step: 121, loss is 0.08898536115884781\n",
      "epoch: 9 step: 122, loss is 0.019012657925486565\n",
      "epoch: 9 step: 123, loss is 0.2772582173347473\n",
      "epoch: 9 step: 124, loss is 0.045233145356178284\n",
      "epoch: 9 step: 125, loss is 0.03148510307073593\n",
      "epoch: 9 step: 126, loss is 0.09852971136569977\n",
      "epoch: 9 step: 127, loss is 0.024075424298644066\n",
      "epoch: 9 step: 128, loss is 0.05030271038413048\n",
      "epoch: 9 step: 129, loss is 0.09553886204957962\n",
      "epoch: 9 step: 130, loss is 0.03980998694896698\n",
      "epoch: 9 step: 131, loss is 0.04903825744986534\n",
      "epoch: 9 step: 132, loss is 0.009291674010455608\n",
      "epoch: 9 step: 133, loss is 0.06691011786460876\n",
      "epoch: 9 step: 134, loss is 0.12309172004461288\n",
      "epoch: 9 step: 135, loss is 0.01979704573750496\n",
      "epoch: 9 step: 136, loss is 0.04809487238526344\n",
      "epoch: 9 step: 137, loss is 0.060399871319532394\n",
      "epoch: 9 step: 138, loss is 0.03821469843387604\n",
      "epoch: 9 step: 139, loss is 0.06980117410421371\n",
      "epoch: 9 step: 140, loss is 0.031103981658816338\n",
      "epoch: 9 step: 141, loss is 0.15168032050132751\n",
      "epoch: 9 step: 142, loss is 0.0749388039112091\n",
      "epoch: 9 step: 143, loss is 0.047149959951639175\n",
      "epoch: 9 step: 144, loss is 0.020160388201475143\n",
      "epoch: 9 step: 145, loss is 0.1482129991054535\n",
      "epoch: 9 step: 146, loss is 0.009261009283363819\n",
      "epoch: 9 step: 147, loss is 0.04700218141078949\n",
      "epoch: 9 step: 148, loss is 0.11028306186199188\n",
      "epoch: 9 step: 149, loss is 0.04088475555181503\n",
      "epoch: 9 step: 150, loss is 0.12004918605089188\n",
      "epoch: 9 step: 151, loss is 0.05250059440732002\n",
      "epoch: 9 step: 152, loss is 0.04883043095469475\n",
      "epoch: 9 step: 153, loss is 0.13722988963127136\n",
      "epoch: 9 step: 154, loss is 0.06363759189844131\n",
      "epoch: 9 step: 155, loss is 0.043220896273851395\n",
      "epoch: 9 step: 156, loss is 0.3058462142944336\n",
      "epoch: 9 step: 157, loss is 0.1587456613779068\n",
      "epoch: 9 step: 158, loss is 0.0516425296664238\n",
      "epoch: 9 step: 159, loss is 0.05612729489803314\n",
      "epoch: 9 step: 160, loss is 0.11017541587352753\n",
      "epoch: 9 step: 161, loss is 0.06023552641272545\n",
      "epoch: 9 step: 162, loss is 0.0701262578368187\n",
      "epoch: 9 step: 163, loss is 0.06250007450580597\n",
      "epoch: 9 step: 164, loss is 0.07275231182575226\n",
      "epoch: 9 step: 165, loss is 0.11127563565969467\n",
      "epoch: 9 step: 166, loss is 0.039125241339206696\n",
      "epoch: 9 step: 167, loss is 0.0400785356760025\n",
      "epoch: 9 step: 168, loss is 0.04113477095961571\n",
      "epoch: 9 step: 169, loss is 0.1657934933900833\n",
      "epoch: 9 step: 170, loss is 0.13608680665493011\n",
      "epoch: 9 step: 171, loss is 0.06615159660577774\n",
      "epoch: 9 step: 172, loss is 0.09635245054960251\n",
      "epoch: 9 step: 173, loss is 0.05663065239787102\n",
      "epoch: 9 step: 174, loss is 0.18061096966266632\n",
      "epoch: 9 step: 175, loss is 0.0313199982047081\n",
      "epoch: 9 step: 176, loss is 0.028367018327116966\n",
      "epoch: 9 step: 177, loss is 0.11930709332227707\n",
      "epoch: 9 step: 178, loss is 0.07407271862030029\n",
      "epoch: 9 step: 179, loss is 0.035805411636829376\n",
      "epoch: 9 step: 180, loss is 0.0429241769015789\n",
      "epoch: 9 step: 181, loss is 0.05019526556134224\n",
      "epoch: 9 step: 182, loss is 0.10633822530508041\n",
      "epoch: 9 step: 183, loss is 0.033231671899557114\n",
      "epoch: 9 step: 184, loss is 0.05795343592762947\n",
      "epoch: 9 step: 185, loss is 0.07803051173686981\n",
      "epoch: 9 step: 186, loss is 0.10549616068601608\n",
      "epoch: 9 step: 187, loss is 0.04236241802573204\n",
      "epoch: 9 step: 188, loss is 0.035266414284706116\n",
      "epoch: 9 step: 189, loss is 0.14082245528697968\n",
      "epoch: 9 step: 190, loss is 0.025994516909122467\n",
      "epoch: 9 step: 191, loss is 0.1041804626584053\n",
      "epoch: 9 step: 192, loss is 0.07582531124353409\n",
      "epoch: 9 step: 193, loss is 0.07883395254611969\n",
      "epoch: 9 step: 194, loss is 0.03785407170653343\n",
      "epoch: 9 step: 195, loss is 0.1823762059211731\n",
      "epoch: 9 step: 196, loss is 0.04946523159742355\n",
      "epoch: 9 step: 197, loss is 0.06534431874752045\n",
      "epoch: 9 step: 198, loss is 0.18212981522083282\n",
      "epoch: 9 step: 199, loss is 0.04107484593987465\n",
      "epoch: 9 step: 200, loss is 0.052429161965847015\n",
      "epoch: 9 step: 201, loss is 0.04781945049762726\n",
      "epoch: 9 step: 202, loss is 0.062115903943777084\n",
      "epoch: 9 step: 203, loss is 0.053453654050827026\n",
      "epoch: 9 step: 204, loss is 0.13690246641635895\n",
      "epoch: 9 step: 205, loss is 0.07410421222448349\n",
      "epoch: 9 step: 206, loss is 0.1534937024116516\n",
      "epoch: 9 step: 207, loss is 0.06589365005493164\n",
      "epoch: 9 step: 208, loss is 0.20664380490779877\n",
      "epoch: 9 step: 209, loss is 0.15536925196647644\n",
      "epoch: 9 step: 210, loss is 0.08490227907896042\n",
      "epoch: 9 step: 211, loss is 0.10260700434446335\n",
      "epoch: 9 step: 212, loss is 0.08802362531423569\n",
      "epoch: 9 step: 213, loss is 0.053080614656209946\n",
      "epoch: 9 step: 214, loss is 0.022068988531827927\n",
      "epoch: 9 step: 215, loss is 0.043105557560920715\n",
      "epoch: 9 step: 216, loss is 0.2080610990524292\n",
      "epoch: 9 step: 217, loss is 0.09825542569160461\n",
      "epoch: 9 step: 218, loss is 0.08960995823144913\n",
      "epoch: 9 step: 219, loss is 0.09949476271867752\n",
      "epoch: 9 step: 220, loss is 0.05256323888897896\n",
      "epoch: 9 step: 221, loss is 0.019579440355300903\n",
      "epoch: 9 step: 222, loss is 0.049506526440382004\n",
      "epoch: 9 step: 223, loss is 0.07248733937740326\n",
      "epoch: 9 step: 224, loss is 0.1437341421842575\n",
      "epoch: 9 step: 225, loss is 0.08227504044771194\n",
      "epoch: 9 step: 226, loss is 0.10356082767248154\n",
      "epoch: 9 step: 227, loss is 0.14805464446544647\n",
      "epoch: 9 step: 228, loss is 0.08319833129644394\n",
      "epoch: 9 step: 229, loss is 0.05437411367893219\n",
      "epoch: 9 step: 230, loss is 0.1794682890176773\n",
      "epoch: 9 step: 231, loss is 0.07577625662088394\n",
      "epoch: 9 step: 232, loss is 0.10142005234956741\n",
      "epoch: 9 step: 233, loss is 0.058166757225990295\n",
      "epoch: 9 step: 234, loss is 0.1303943693637848\n",
      "epoch: 9 step: 235, loss is 0.045411884784698486\n",
      "epoch: 9 step: 236, loss is 0.032535091042518616\n",
      "epoch: 9 step: 237, loss is 0.1451159417629242\n",
      "epoch: 9 step: 238, loss is 0.044204168021678925\n",
      "epoch: 9 step: 239, loss is 0.2201194316148758\n",
      "epoch: 9 step: 240, loss is 0.03482413664460182\n",
      "epoch: 9 step: 241, loss is 0.029766160994768143\n",
      "epoch: 9 step: 242, loss is 0.12478715926408768\n",
      "epoch: 9 step: 243, loss is 0.04654192179441452\n",
      "epoch: 9 step: 244, loss is 0.08557527512311935\n",
      "epoch: 9 step: 245, loss is 0.08673497289419174\n",
      "epoch: 9 step: 246, loss is 0.03380860015749931\n",
      "epoch: 9 step: 247, loss is 0.08179370313882828\n",
      "epoch: 9 step: 248, loss is 0.05890679731965065\n",
      "epoch: 9 step: 249, loss is 0.06790297478437424\n",
      "epoch: 9 step: 250, loss is 0.07256428897380829\n",
      "epoch: 9 step: 251, loss is 0.0769551619887352\n",
      "epoch: 9 step: 252, loss is 0.03953831270337105\n",
      "epoch: 9 step: 253, loss is 0.030618412420153618\n",
      "epoch: 9 step: 254, loss is 0.05442897975444794\n",
      "epoch: 9 step: 255, loss is 0.09531930088996887\n",
      "epoch: 9 step: 256, loss is 0.04129920154809952\n",
      "epoch: 9 step: 257, loss is 0.06633025407791138\n",
      "epoch: 9 step: 258, loss is 0.033307138830423355\n",
      "epoch: 9 step: 259, loss is 0.05757375434041023\n",
      "epoch: 9 step: 260, loss is 0.16388088464736938\n",
      "epoch: 9 step: 261, loss is 0.037877194583415985\n",
      "epoch: 9 step: 262, loss is 0.0510118268430233\n",
      "epoch: 9 step: 263, loss is 0.0792495384812355\n",
      "epoch: 9 step: 264, loss is 0.04312114790081978\n",
      "epoch: 9 step: 265, loss is 0.0753626823425293\n",
      "epoch: 9 step: 266, loss is 0.15916836261749268\n",
      "epoch: 9 step: 267, loss is 0.06867749989032745\n",
      "epoch: 9 step: 268, loss is 0.028711453080177307\n",
      "epoch: 9 step: 269, loss is 0.037960898131132126\n",
      "epoch: 9 step: 270, loss is 0.045795176178216934\n",
      "epoch: 9 step: 271, loss is 0.1434973031282425\n",
      "epoch: 9 step: 272, loss is 0.2605323791503906\n",
      "epoch: 9 step: 273, loss is 0.06772445887327194\n",
      "epoch: 9 step: 274, loss is 0.2520199120044708\n",
      "epoch: 9 step: 275, loss is 0.11606469005346298\n",
      "epoch: 9 step: 276, loss is 0.08343184739351273\n",
      "epoch: 9 step: 277, loss is 0.09265323728322983\n",
      "epoch: 9 step: 278, loss is 0.03776078298687935\n",
      "epoch: 9 step: 279, loss is 0.07627975940704346\n",
      "epoch: 9 step: 280, loss is 0.05521419271826744\n",
      "epoch: 9 step: 281, loss is 0.008200372569262981\n",
      "epoch: 9 step: 282, loss is 0.10114062577486038\n",
      "epoch: 9 step: 283, loss is 0.11013028025627136\n",
      "epoch: 9 step: 284, loss is 0.14563731849193573\n",
      "epoch: 9 step: 285, loss is 0.1443483978509903\n",
      "epoch: 9 step: 286, loss is 0.0938655361533165\n",
      "epoch: 9 step: 287, loss is 0.06262001395225525\n",
      "epoch: 9 step: 288, loss is 0.0734056681394577\n",
      "epoch: 9 step: 289, loss is 0.02380499430000782\n",
      "epoch: 9 step: 290, loss is 0.01872752234339714\n",
      "epoch: 9 step: 291, loss is 0.010209358297288418\n",
      "epoch: 9 step: 292, loss is 0.09335678070783615\n",
      "epoch: 9 step: 293, loss is 0.08627963066101074\n",
      "epoch: 9 step: 294, loss is 0.03453492745757103\n",
      "epoch: 9 step: 295, loss is 0.09458404034376144\n",
      "epoch: 9 step: 296, loss is 0.13082671165466309\n",
      "epoch: 9 step: 297, loss is 0.039244260638952255\n",
      "epoch: 9 step: 298, loss is 0.09532895684242249\n",
      "epoch: 9 step: 299, loss is 0.041784219443798065\n",
      "epoch: 9 step: 300, loss is 0.03947756811976433\n",
      "epoch: 9 step: 301, loss is 0.0847567692399025\n",
      "epoch: 9 step: 302, loss is 0.05918429046869278\n",
      "epoch: 9 step: 303, loss is 0.11332201957702637\n",
      "epoch: 9 step: 304, loss is 0.047786369919776917\n",
      "epoch: 9 step: 305, loss is 0.1152598187327385\n",
      "epoch: 9 step: 306, loss is 0.04123552888631821\n",
      "epoch: 9 step: 307, loss is 0.05670742318034172\n",
      "epoch: 9 step: 308, loss is 0.18282078206539154\n",
      "epoch: 9 step: 309, loss is 0.0486471988260746\n",
      "epoch: 9 step: 310, loss is 0.019550958648324013\n",
      "epoch: 9 step: 311, loss is 0.023145902901887894\n",
      "epoch: 9 step: 312, loss is 0.04238990321755409\n",
      "epoch: 9 step: 313, loss is 0.0930202379822731\n",
      "epoch: 9 step: 314, loss is 0.08391974121332169\n",
      "epoch: 9 step: 315, loss is 0.03508470207452774\n",
      "epoch: 9 step: 316, loss is 0.0357634536921978\n",
      "epoch: 9 step: 317, loss is 0.08634012192487717\n",
      "epoch: 9 step: 318, loss is 0.11421003192663193\n",
      "epoch: 9 step: 319, loss is 0.044248323887586594\n",
      "epoch: 9 step: 320, loss is 0.16768157482147217\n",
      "epoch: 9 step: 321, loss is 0.0798015147447586\n",
      "epoch: 9 step: 322, loss is 0.0952589362859726\n",
      "epoch: 9 step: 323, loss is 0.044143978506326675\n",
      "epoch: 9 step: 324, loss is 0.019345121458172798\n",
      "epoch: 9 step: 325, loss is 0.03953925520181656\n",
      "epoch: 9 step: 326, loss is 0.02333802357316017\n",
      "epoch: 9 step: 327, loss is 0.1405397653579712\n",
      "epoch: 9 step: 328, loss is 0.03715168312191963\n",
      "epoch: 9 step: 329, loss is 0.039473555982112885\n",
      "epoch: 9 step: 330, loss is 0.0733191967010498\n",
      "epoch: 9 step: 331, loss is 0.03245093673467636\n",
      "epoch: 9 step: 332, loss is 0.12340220808982849\n",
      "epoch: 9 step: 333, loss is 0.11533141881227493\n",
      "epoch: 9 step: 334, loss is 0.04701363295316696\n",
      "epoch: 9 step: 335, loss is 0.03293358162045479\n",
      "epoch: 9 step: 336, loss is 0.04942138120532036\n",
      "epoch: 9 step: 337, loss is 0.03381883725523949\n",
      "epoch: 9 step: 338, loss is 0.19396185874938965\n",
      "epoch: 9 step: 339, loss is 0.07479235529899597\n",
      "epoch: 9 step: 340, loss is 0.0870744064450264\n",
      "epoch: 9 step: 341, loss is 0.0987536832690239\n",
      "epoch: 9 step: 342, loss is 0.05687783285975456\n",
      "epoch: 9 step: 343, loss is 0.1359778493642807\n",
      "epoch: 9 step: 344, loss is 0.032756634056568146\n",
      "epoch: 9 step: 345, loss is 0.11841181665658951\n",
      "epoch: 9 step: 346, loss is 0.06500783562660217\n",
      "epoch: 9 step: 347, loss is 0.0436074323952198\n",
      "epoch: 9 step: 348, loss is 0.060166504234075546\n",
      "epoch: 9 step: 349, loss is 0.025788553059101105\n",
      "epoch: 9 step: 350, loss is 0.1063576191663742\n",
      "epoch: 9 step: 351, loss is 0.053205132484436035\n",
      "epoch: 9 step: 352, loss is 0.12228275090456009\n",
      "epoch: 9 step: 353, loss is 0.07654105871915817\n",
      "epoch: 9 step: 354, loss is 0.08647159487009048\n",
      "epoch: 9 step: 355, loss is 0.03606170788407326\n",
      "epoch: 9 step: 356, loss is 0.0664234384894371\n",
      "epoch: 9 step: 357, loss is 0.03001127950847149\n",
      "epoch: 9 step: 358, loss is 0.0944366529583931\n",
      "epoch: 9 step: 359, loss is 0.03146625682711601\n",
      "epoch: 9 step: 360, loss is 0.028067072853446007\n",
      "epoch: 9 step: 361, loss is 0.07533569633960724\n",
      "epoch: 9 step: 362, loss is 0.035953350365161896\n",
      "epoch: 9 step: 363, loss is 0.055048663169145584\n",
      "epoch: 9 step: 364, loss is 0.10892191529273987\n",
      "epoch: 9 step: 365, loss is 0.044697485864162445\n",
      "epoch: 9 step: 366, loss is 0.07195364683866501\n",
      "epoch: 9 step: 367, loss is 0.10730297118425369\n",
      "epoch: 9 step: 368, loss is 0.07362008839845657\n",
      "epoch: 9 step: 369, loss is 0.026238620281219482\n",
      "epoch: 9 step: 370, loss is 0.0462697297334671\n",
      "epoch: 9 step: 371, loss is 0.06485351175069809\n",
      "epoch: 9 step: 372, loss is 0.03664081543684006\n",
      "epoch: 9 step: 373, loss is 0.06487836688756943\n",
      "epoch: 9 step: 374, loss is 0.08824338763952255\n",
      "epoch: 9 step: 375, loss is 0.016155244782567024\n",
      "epoch: 9 step: 376, loss is 0.11119581758975983\n",
      "epoch: 9 step: 377, loss is 0.045243263244628906\n",
      "epoch: 9 step: 378, loss is 0.11848752945661545\n",
      "epoch: 9 step: 379, loss is 0.10468835383653641\n",
      "epoch: 9 step: 380, loss is 0.12699094414710999\n",
      "epoch: 9 step: 381, loss is 0.14066530764102936\n",
      "epoch: 9 step: 382, loss is 0.05688506364822388\n",
      "epoch: 9 step: 383, loss is 0.24194949865341187\n",
      "epoch: 9 step: 384, loss is 0.19552427530288696\n",
      "epoch: 9 step: 385, loss is 0.038398850709199905\n",
      "epoch: 9 step: 386, loss is 0.050531912595033646\n",
      "epoch: 9 step: 387, loss is 0.029235444962978363\n",
      "epoch: 9 step: 388, loss is 0.01939762756228447\n",
      "epoch: 9 step: 389, loss is 0.04964058846235275\n",
      "epoch: 9 step: 390, loss is 0.06788259744644165\n",
      "epoch: 9 step: 391, loss is 0.06574324518442154\n",
      "epoch: 9 step: 392, loss is 0.13329435884952545\n",
      "epoch: 9 step: 393, loss is 0.013672705739736557\n",
      "epoch: 9 step: 394, loss is 0.09444040805101395\n",
      "epoch: 9 step: 395, loss is 0.04282582923769951\n",
      "epoch: 9 step: 396, loss is 0.13987359404563904\n",
      "epoch: 9 step: 397, loss is 0.07227512449026108\n",
      "epoch: 9 step: 398, loss is 0.0793052539229393\n",
      "epoch: 9 step: 399, loss is 0.048390794545412064\n",
      "epoch: 9 step: 400, loss is 0.059708379209041595\n",
      "epoch: 9 step: 401, loss is 0.04889660328626633\n",
      "epoch: 9 step: 402, loss is 0.20030595362186432\n",
      "epoch: 9 step: 403, loss is 0.038118164986371994\n",
      "epoch: 9 step: 404, loss is 0.047332651913166046\n",
      "epoch: 9 step: 405, loss is 0.08313613384962082\n",
      "epoch: 9 step: 406, loss is 0.012278915382921696\n",
      "epoch: 9 step: 407, loss is 0.0724586471915245\n",
      "epoch: 9 step: 408, loss is 0.07208090275526047\n",
      "epoch: 9 step: 409, loss is 0.043684571981430054\n",
      "epoch: 9 step: 410, loss is 0.023176761344075203\n",
      "epoch: 9 step: 411, loss is 0.08938814699649811\n",
      "epoch: 9 step: 412, loss is 0.03361480310559273\n",
      "epoch: 9 step: 413, loss is 0.03682254999876022\n",
      "epoch: 9 step: 414, loss is 0.036265384405851364\n",
      "epoch: 9 step: 415, loss is 0.17149412631988525\n",
      "epoch: 9 step: 416, loss is 0.034423746168613434\n",
      "epoch: 9 step: 417, loss is 0.04471220076084137\n",
      "epoch: 9 step: 418, loss is 0.1951880156993866\n",
      "epoch: 9 step: 419, loss is 0.022648561745882034\n",
      "epoch: 9 step: 420, loss is 0.01265950407832861\n",
      "epoch: 9 step: 421, loss is 0.0612279549241066\n",
      "epoch: 9 step: 422, loss is 0.052189674228429794\n",
      "epoch: 9 step: 423, loss is 0.09231701493263245\n",
      "epoch: 9 step: 424, loss is 0.01933431625366211\n",
      "epoch: 9 step: 425, loss is 0.04186221584677696\n",
      "epoch: 9 step: 426, loss is 0.10857351869344711\n",
      "epoch: 9 step: 427, loss is 0.06069721654057503\n",
      "epoch: 9 step: 428, loss is 0.027882834896445274\n",
      "epoch: 9 step: 429, loss is 0.045135390013456345\n",
      "epoch: 9 step: 430, loss is 0.06743793934583664\n",
      "epoch: 9 step: 431, loss is 0.12332279980182648\n",
      "epoch: 9 step: 432, loss is 0.04049023613333702\n",
      "epoch: 9 step: 433, loss is 0.05536060035228729\n",
      "epoch: 9 step: 434, loss is 0.03999914973974228\n",
      "epoch: 9 step: 435, loss is 0.05299212783575058\n",
      "epoch: 9 step: 436, loss is 0.06397939473390579\n",
      "epoch: 9 step: 437, loss is 0.045570723712444305\n",
      "epoch: 9 step: 438, loss is 0.05462583154439926\n",
      "epoch: 9 step: 439, loss is 0.14182120561599731\n",
      "epoch: 9 step: 440, loss is 0.06899364292621613\n",
      "epoch: 9 step: 441, loss is 0.09667375683784485\n",
      "epoch: 9 step: 442, loss is 0.05441015586256981\n",
      "epoch: 9 step: 443, loss is 0.050453003495931625\n",
      "epoch: 9 step: 444, loss is 0.016207074746489525\n",
      "epoch: 9 step: 445, loss is 0.14350153505802155\n",
      "epoch: 9 step: 446, loss is 0.06988664716482162\n",
      "epoch: 9 step: 447, loss is 0.0785924419760704\n",
      "epoch: 9 step: 448, loss is 0.043809108436107635\n",
      "epoch: 9 step: 449, loss is 0.16896897554397583\n",
      "epoch: 9 step: 450, loss is 0.1420406550168991\n",
      "epoch: 9 step: 451, loss is 0.11603331565856934\n",
      "epoch: 9 step: 452, loss is 0.05495665222406387\n",
      "epoch: 9 step: 453, loss is 0.09873111546039581\n",
      "epoch: 9 step: 454, loss is 0.08581197261810303\n",
      "epoch: 9 step: 455, loss is 0.025679845362901688\n",
      "epoch: 9 step: 456, loss is 0.15065762400627136\n",
      "epoch: 9 step: 457, loss is 0.11101958900690079\n",
      "epoch: 9 step: 458, loss is 0.04654127359390259\n",
      "epoch: 9 step: 459, loss is 0.14267995953559875\n",
      "epoch: 9 step: 460, loss is 0.10408303886651993\n",
      "epoch: 9 step: 461, loss is 0.07687615603208542\n",
      "epoch: 9 step: 462, loss is 0.11187303811311722\n",
      "epoch: 9 step: 463, loss is 0.13781778514385223\n",
      "epoch: 9 step: 464, loss is 0.051020216196775436\n",
      "epoch: 9 step: 465, loss is 0.02522255852818489\n",
      "epoch: 9 step: 466, loss is 0.19257307052612305\n",
      "epoch: 9 step: 467, loss is 0.035656243562698364\n",
      "epoch: 9 step: 468, loss is 0.0411626398563385\n",
      "epoch: 9 step: 469, loss is 0.010453296825289726\n",
      "epoch: 9 step: 470, loss is 0.061244938522577286\n",
      "epoch: 9 step: 471, loss is 0.23593942821025848\n",
      "epoch: 9 step: 472, loss is 0.11744193732738495\n",
      "epoch: 9 step: 473, loss is 0.01883765682578087\n",
      "epoch: 9 step: 474, loss is 0.08560442924499512\n",
      "epoch: 9 step: 475, loss is 0.07018238306045532\n",
      "epoch: 9 step: 476, loss is 0.07693950086832047\n",
      "epoch: 9 step: 477, loss is 0.0793345496058464\n",
      "epoch: 9 step: 478, loss is 0.06960932165384293\n",
      "epoch: 9 step: 479, loss is 0.12699444591999054\n",
      "epoch: 9 step: 480, loss is 0.06315821409225464\n",
      "epoch: 9 step: 481, loss is 0.07222134619951248\n",
      "epoch: 9 step: 482, loss is 0.03932930901646614\n",
      "epoch: 9 step: 483, loss is 0.09302595257759094\n",
      "epoch: 9 step: 484, loss is 0.01707548275589943\n",
      "epoch: 9 step: 485, loss is 0.15178267657756805\n",
      "epoch: 9 step: 486, loss is 0.06704273074865341\n",
      "epoch: 9 step: 487, loss is 0.05013863742351532\n",
      "epoch: 9 step: 488, loss is 0.037570416927337646\n",
      "epoch: 9 step: 489, loss is 0.06662508845329285\n",
      "epoch: 9 step: 490, loss is 0.04370937496423721\n",
      "epoch: 9 step: 491, loss is 0.02717757783830166\n",
      "epoch: 9 step: 492, loss is 0.039339449256658554\n",
      "epoch: 9 step: 493, loss is 0.055400945246219635\n",
      "epoch: 9 step: 494, loss is 0.015884622931480408\n",
      "epoch: 9 step: 495, loss is 0.026051755994558334\n",
      "epoch: 9 step: 496, loss is 0.08405519276857376\n",
      "epoch: 9 step: 497, loss is 0.03883770853281021\n",
      "epoch: 9 step: 498, loss is 0.03294416889548302\n",
      "epoch: 9 step: 499, loss is 0.10141532868146896\n",
      "epoch: 9 step: 500, loss is 0.10562305897474289\n",
      "epoch: 9 step: 501, loss is 0.17419368028640747\n",
      "epoch: 9 step: 502, loss is 0.04065634310245514\n",
      "epoch: 9 step: 503, loss is 0.051468461751937866\n",
      "epoch: 9 step: 504, loss is 0.03361223638057709\n",
      "epoch: 9 step: 505, loss is 0.03773827850818634\n",
      "epoch: 9 step: 506, loss is 0.05225442722439766\n",
      "epoch: 9 step: 507, loss is 0.03910750150680542\n",
      "epoch: 9 step: 508, loss is 0.08995670825242996\n",
      "epoch: 9 step: 509, loss is 0.18759936094284058\n",
      "epoch: 9 step: 510, loss is 0.08022165298461914\n",
      "epoch: 9 step: 511, loss is 0.0204206183552742\n",
      "epoch: 9 step: 512, loss is 0.10050266236066818\n",
      "epoch: 9 step: 513, loss is 0.08280148357152939\n",
      "epoch: 9 step: 514, loss is 0.08361601829528809\n",
      "epoch: 9 step: 515, loss is 0.05017861723899841\n",
      "epoch: 9 step: 516, loss is 0.1121130958199501\n",
      "epoch: 9 step: 517, loss is 0.05239070951938629\n",
      "epoch: 9 step: 518, loss is 0.16991247236728668\n",
      "epoch: 9 step: 519, loss is 0.16870097815990448\n",
      "epoch: 9 step: 520, loss is 0.02856479585170746\n",
      "epoch: 9 step: 521, loss is 0.02665272355079651\n",
      "epoch: 9 step: 522, loss is 0.05714929476380348\n",
      "epoch: 9 step: 523, loss is 0.1072022020816803\n",
      "epoch: 9 step: 524, loss is 0.22803980112075806\n",
      "epoch: 9 step: 525, loss is 0.08039914816617966\n",
      "epoch: 9 step: 526, loss is 0.04976598545908928\n",
      "epoch: 9 step: 527, loss is 0.06498301029205322\n",
      "epoch: 9 step: 528, loss is 0.05800745263695717\n",
      "epoch: 9 step: 529, loss is 0.044434696435928345\n",
      "epoch: 9 step: 530, loss is 0.09323491156101227\n",
      "epoch: 9 step: 531, loss is 0.06619716435670853\n",
      "epoch: 9 step: 532, loss is 0.05349372327327728\n",
      "epoch: 9 step: 533, loss is 0.05712633207440376\n",
      "epoch: 9 step: 534, loss is 0.14647994935512543\n",
      "epoch: 9 step: 535, loss is 0.06622910499572754\n",
      "epoch: 9 step: 536, loss is 0.04966840520501137\n",
      "epoch: 9 step: 537, loss is 0.01552891731262207\n",
      "epoch: 9 step: 538, loss is 0.07980871200561523\n",
      "epoch: 9 step: 539, loss is 0.10204169154167175\n",
      "epoch: 9 step: 540, loss is 0.09916198253631592\n",
      "epoch: 9 step: 541, loss is 0.06784661114215851\n",
      "epoch: 9 step: 542, loss is 0.10494270920753479\n",
      "epoch: 9 step: 543, loss is 0.022673225030303\n",
      "epoch: 9 step: 544, loss is 0.12207470089197159\n",
      "epoch: 9 step: 545, loss is 0.03328153118491173\n",
      "epoch: 9 step: 546, loss is 0.04827359318733215\n",
      "epoch: 9 step: 547, loss is 0.07346530258655548\n",
      "epoch: 9 step: 548, loss is 0.03761855512857437\n",
      "epoch: 9 step: 549, loss is 0.05120743811130524\n",
      "epoch: 9 step: 550, loss is 0.09805268049240112\n",
      "epoch: 9 step: 551, loss is 0.0594463013112545\n",
      "epoch: 9 step: 552, loss is 0.08383315801620483\n",
      "epoch: 9 step: 553, loss is 0.045024823397397995\n",
      "epoch: 9 step: 554, loss is 0.027568282559514046\n",
      "epoch: 9 step: 555, loss is 0.034936219453811646\n",
      "epoch: 9 step: 556, loss is 0.02469346672296524\n",
      "epoch: 9 step: 557, loss is 0.042045678943395615\n",
      "epoch: 9 step: 558, loss is 0.12119032442569733\n",
      "epoch: 9 step: 559, loss is 0.05497805029153824\n",
      "epoch: 9 step: 560, loss is 0.05744769424200058\n",
      "epoch: 9 step: 561, loss is 0.03538725897669792\n",
      "epoch: 9 step: 562, loss is 0.036058828234672546\n",
      "epoch: 9 step: 563, loss is 0.04488677531480789\n",
      "epoch: 9 step: 564, loss is 0.07119467109441757\n",
      "epoch: 9 step: 565, loss is 0.07748426496982574\n",
      "epoch: 9 step: 566, loss is 0.12921856343746185\n",
      "epoch: 9 step: 567, loss is 0.0182090662419796\n",
      "epoch: 9 step: 568, loss is 0.13047173619270325\n",
      "epoch: 9 step: 569, loss is 0.08135394006967545\n",
      "epoch: 9 step: 570, loss is 0.028986502438783646\n",
      "epoch: 9 step: 571, loss is 0.07836180925369263\n",
      "epoch: 9 step: 572, loss is 0.047704216092824936\n",
      "epoch: 9 step: 573, loss is 0.04444301873445511\n",
      "epoch: 9 step: 574, loss is 0.16419556736946106\n",
      "epoch: 9 step: 575, loss is 0.08056911081075668\n",
      "epoch: 9 step: 576, loss is 0.005354732275009155\n",
      "epoch: 9 step: 577, loss is 0.06746016442775726\n",
      "epoch: 9 step: 578, loss is 0.05682851001620293\n",
      "epoch: 9 step: 579, loss is 0.11488518118858337\n",
      "epoch: 9 step: 580, loss is 0.06361889094114304\n",
      "epoch: 9 step: 581, loss is 0.07701168954372406\n",
      "epoch: 9 step: 582, loss is 0.03891444206237793\n",
      "epoch: 9 step: 583, loss is 0.07307656854391098\n",
      "epoch: 9 step: 584, loss is 0.020097212865948677\n",
      "epoch: 9 step: 585, loss is 0.033302534371614456\n",
      "epoch: 9 step: 586, loss is 0.05272408947348595\n",
      "epoch: 9 step: 587, loss is 0.03778083622455597\n",
      "epoch: 9 step: 588, loss is 0.03281186893582344\n",
      "epoch: 9 step: 589, loss is 0.06243489310145378\n",
      "epoch: 9 step: 590, loss is 0.11311361193656921\n",
      "epoch: 9 step: 591, loss is 0.06790143251419067\n",
      "epoch: 9 step: 592, loss is 0.11651229113340378\n",
      "epoch: 9 step: 593, loss is 0.04437854886054993\n",
      "epoch: 9 step: 594, loss is 0.16139958798885345\n",
      "epoch: 9 step: 595, loss is 0.05242219567298889\n",
      "epoch: 9 step: 596, loss is 0.04967783764004707\n",
      "epoch: 9 step: 597, loss is 0.07664193212985992\n",
      "epoch: 9 step: 598, loss is 0.12304279208183289\n",
      "epoch: 9 step: 599, loss is 0.10656442493200302\n",
      "epoch: 9 step: 600, loss is 0.1328091323375702\n",
      "epoch: 9 step: 601, loss is 0.08980215340852737\n",
      "epoch: 9 step: 602, loss is 0.07143107056617737\n",
      "epoch: 9 step: 603, loss is 0.04550987854599953\n",
      "epoch: 9 step: 604, loss is 0.13423454761505127\n",
      "epoch: 9 step: 605, loss is 0.1495814323425293\n",
      "epoch: 9 step: 606, loss is 0.10320544242858887\n",
      "epoch: 9 step: 607, loss is 0.06875130534172058\n",
      "epoch: 9 step: 608, loss is 0.13484838604927063\n",
      "epoch: 9 step: 609, loss is 0.13956774771213531\n",
      "epoch: 9 step: 610, loss is 0.07331041991710663\n",
      "epoch: 9 step: 611, loss is 0.10165005177259445\n",
      "epoch: 9 step: 612, loss is 0.16320747137069702\n",
      "epoch: 9 step: 613, loss is 0.11268922686576843\n",
      "epoch: 9 step: 614, loss is 0.03988882526755333\n",
      "epoch: 9 step: 615, loss is 0.04687127098441124\n",
      "epoch: 9 step: 616, loss is 0.13595837354660034\n",
      "epoch: 9 step: 617, loss is 0.02689778432250023\n",
      "epoch: 9 step: 618, loss is 0.02680770494043827\n",
      "epoch: 9 step: 619, loss is 0.11617257446050644\n",
      "epoch: 9 step: 620, loss is 0.16802024841308594\n",
      "epoch: 9 step: 621, loss is 0.1386677324771881\n",
      "epoch: 9 step: 622, loss is 0.10679454356431961\n",
      "epoch: 9 step: 623, loss is 0.052605677396059036\n",
      "epoch: 9 step: 624, loss is 0.19702568650245667\n",
      "epoch: 9 step: 625, loss is 0.08077043294906616\n",
      "epoch: 9 step: 626, loss is 0.03836345672607422\n",
      "epoch: 9 step: 627, loss is 0.04140012338757515\n",
      "epoch: 9 step: 628, loss is 0.02598663978278637\n",
      "epoch: 9 step: 629, loss is 0.08718673884868622\n",
      "epoch: 9 step: 630, loss is 0.21132522821426392\n",
      "epoch: 9 step: 631, loss is 0.10975459218025208\n",
      "epoch: 9 step: 632, loss is 0.18868622183799744\n",
      "epoch: 9 step: 633, loss is 0.06556617468595505\n",
      "epoch: 9 step: 634, loss is 0.15152810513973236\n",
      "epoch: 9 step: 635, loss is 0.07081305980682373\n",
      "epoch: 9 step: 636, loss is 0.15551599860191345\n",
      "epoch: 9 step: 637, loss is 0.04799802601337433\n",
      "epoch: 9 step: 638, loss is 0.08705979585647583\n",
      "epoch: 9 step: 639, loss is 0.01907925121486187\n",
      "epoch: 9 step: 640, loss is 0.1505368947982788\n",
      "epoch: 9 step: 641, loss is 0.041263509541749954\n",
      "epoch: 9 step: 642, loss is 0.037308987230062485\n",
      "epoch: 9 step: 643, loss is 0.04772462323307991\n",
      "epoch: 9 step: 644, loss is 0.024854624643921852\n",
      "epoch: 9 step: 645, loss is 0.04072214290499687\n",
      "epoch: 9 step: 646, loss is 0.05964002385735512\n",
      "epoch: 9 step: 647, loss is 0.03749798983335495\n",
      "epoch: 9 step: 648, loss is 0.10729213058948517\n",
      "epoch: 9 step: 649, loss is 0.08784320205450058\n",
      "epoch: 9 step: 650, loss is 0.01074173767119646\n",
      "epoch: 9 step: 651, loss is 0.11240548640489578\n",
      "epoch: 9 step: 652, loss is 0.11426763236522675\n",
      "epoch: 9 step: 653, loss is 0.06044695898890495\n",
      "epoch: 9 step: 654, loss is 0.009390667080879211\n",
      "epoch: 9 step: 655, loss is 0.1667817384004593\n",
      "epoch: 9 step: 656, loss is 0.07960740476846695\n",
      "epoch: 9 step: 657, loss is 0.11522068083286285\n",
      "epoch: 9 step: 658, loss is 0.022260669618844986\n",
      "epoch: 9 step: 659, loss is 0.09475068002939224\n",
      "epoch: 9 step: 660, loss is 0.07313510775566101\n",
      "epoch: 9 step: 661, loss is 0.1177048459649086\n",
      "epoch: 9 step: 662, loss is 0.21175460517406464\n",
      "epoch: 9 step: 663, loss is 0.13025173544883728\n",
      "epoch: 9 step: 664, loss is 0.02953389845788479\n",
      "epoch: 9 step: 665, loss is 0.106410451233387\n",
      "epoch: 9 step: 666, loss is 0.08264704793691635\n",
      "epoch: 9 step: 667, loss is 0.10690291225910187\n",
      "epoch: 9 step: 668, loss is 0.0861615538597107\n",
      "epoch: 9 step: 669, loss is 0.05835120379924774\n",
      "epoch: 9 step: 670, loss is 0.10493620485067368\n",
      "epoch: 9 step: 671, loss is 0.0661027729511261\n",
      "epoch: 9 step: 672, loss is 0.08689512312412262\n",
      "epoch: 9 step: 673, loss is 0.053864479064941406\n",
      "epoch: 9 step: 674, loss is 0.05547716096043587\n",
      "epoch: 9 step: 675, loss is 0.051180146634578705\n",
      "epoch: 9 step: 676, loss is 0.05599626898765564\n",
      "epoch: 9 step: 677, loss is 0.045195695012807846\n",
      "epoch: 9 step: 678, loss is 0.06014281138777733\n",
      "epoch: 9 step: 679, loss is 0.06622505933046341\n",
      "epoch: 9 step: 680, loss is 0.03555087372660637\n",
      "epoch: 9 step: 681, loss is 0.11565843224525452\n",
      "epoch: 9 step: 682, loss is 0.11165966838598251\n",
      "epoch: 9 step: 683, loss is 0.09483161568641663\n",
      "epoch: 9 step: 684, loss is 0.12662911415100098\n",
      "epoch: 9 step: 685, loss is 0.06607069075107574\n",
      "epoch: 9 step: 686, loss is 0.03061196208000183\n",
      "epoch: 9 step: 687, loss is 0.13244403898715973\n",
      "epoch: 9 step: 688, loss is 0.06480038166046143\n",
      "epoch: 9 step: 689, loss is 0.0482294037938118\n",
      "epoch: 9 step: 690, loss is 0.050780680030584335\n",
      "epoch: 9 step: 691, loss is 0.0489899143576622\n",
      "epoch: 9 step: 692, loss is 0.0754624679684639\n",
      "epoch: 9 step: 693, loss is 0.026843378320336342\n",
      "epoch: 9 step: 694, loss is 0.26163700222969055\n",
      "epoch: 9 step: 695, loss is 0.09298061579465866\n",
      "epoch: 9 step: 696, loss is 0.055464327335357666\n",
      "epoch: 9 step: 697, loss is 0.19920511543750763\n",
      "epoch: 9 step: 698, loss is 0.06404252350330353\n",
      "epoch: 9 step: 699, loss is 0.110994353890419\n",
      "epoch: 9 step: 700, loss is 0.11125820875167847\n",
      "epoch: 9 step: 701, loss is 0.05084194242954254\n",
      "epoch: 9 step: 702, loss is 0.05108035355806351\n",
      "epoch: 9 step: 703, loss is 0.05714244022965431\n",
      "epoch: 9 step: 704, loss is 0.059279490262269974\n",
      "epoch: 9 step: 705, loss is 0.042579181492328644\n",
      "epoch: 9 step: 706, loss is 0.1613280177116394\n",
      "epoch: 9 step: 707, loss is 0.03687839210033417\n",
      "epoch: 9 step: 708, loss is 0.08842059224843979\n",
      "epoch: 9 step: 709, loss is 0.08937432616949081\n",
      "epoch: 9 step: 710, loss is 0.12754949927330017\n",
      "epoch: 9 step: 711, loss is 0.07010627537965775\n",
      "epoch: 9 step: 712, loss is 0.049466732889413834\n",
      "epoch: 9 step: 713, loss is 0.0404278002679348\n",
      "epoch: 9 step: 714, loss is 0.11992652714252472\n",
      "epoch: 9 step: 715, loss is 0.03204292431473732\n",
      "epoch: 9 step: 716, loss is 0.08466271311044693\n",
      "epoch: 9 step: 717, loss is 0.034965064376592636\n",
      "epoch: 9 step: 718, loss is 0.02312815934419632\n",
      "epoch: 9 step: 719, loss is 0.03724825009703636\n",
      "epoch: 9 step: 720, loss is 0.06210322305560112\n",
      "epoch: 9 step: 721, loss is 0.10394741594791412\n",
      "epoch: 9 step: 722, loss is 0.028946511447429657\n",
      "epoch: 9 step: 723, loss is 0.035124968737363815\n",
      "epoch: 9 step: 724, loss is 0.0902208760380745\n",
      "epoch: 9 step: 725, loss is 0.02478831447660923\n",
      "epoch: 9 step: 726, loss is 0.13613243401050568\n",
      "epoch: 9 step: 727, loss is 0.12929236888885498\n",
      "epoch: 9 step: 728, loss is 0.06048277020454407\n",
      "epoch: 9 step: 729, loss is 0.16619952023029327\n",
      "epoch: 9 step: 730, loss is 0.18962694704532623\n",
      "epoch: 9 step: 731, loss is 0.19797173142433167\n",
      "epoch: 9 step: 732, loss is 0.1547466665506363\n",
      "epoch: 9 step: 733, loss is 0.037036534398794174\n",
      "epoch: 9 step: 734, loss is 0.016612697392702103\n",
      "epoch: 9 step: 735, loss is 0.0763140618801117\n",
      "epoch: 9 step: 736, loss is 0.10778802633285522\n",
      "epoch: 9 step: 737, loss is 0.08099334686994553\n",
      "epoch: 9 step: 738, loss is 0.21848542988300323\n",
      "epoch: 9 step: 739, loss is 0.08162310719490051\n",
      "epoch: 9 step: 740, loss is 0.1853611171245575\n",
      "epoch: 9 step: 741, loss is 0.0740785002708435\n",
      "epoch: 9 step: 742, loss is 0.0667811781167984\n",
      "epoch: 9 step: 743, loss is 0.1501173973083496\n",
      "epoch: 9 step: 744, loss is 0.059834737330675125\n",
      "epoch: 9 step: 745, loss is 0.08244168013334274\n",
      "epoch: 9 step: 746, loss is 0.07273709028959274\n",
      "epoch: 9 step: 747, loss is 0.07765352725982666\n",
      "epoch: 9 step: 748, loss is 0.07296186685562134\n",
      "epoch: 9 step: 749, loss is 0.02688528224825859\n",
      "epoch: 9 step: 750, loss is 0.09174324572086334\n",
      "epoch: 9 step: 751, loss is 0.11055516451597214\n",
      "epoch: 9 step: 752, loss is 0.05565265566110611\n",
      "epoch: 9 step: 753, loss is 0.03703593462705612\n",
      "epoch: 9 step: 754, loss is 0.08989308029413223\n",
      "epoch: 9 step: 755, loss is 0.019462954252958298\n",
      "epoch: 9 step: 756, loss is 0.06330390274524689\n",
      "epoch: 9 step: 757, loss is 0.040607061237096786\n",
      "epoch: 9 step: 758, loss is 0.19114038348197937\n",
      "epoch: 9 step: 759, loss is 0.06337877362966537\n",
      "epoch: 9 step: 760, loss is 0.07232734560966492\n",
      "epoch: 9 step: 761, loss is 0.07726917415857315\n",
      "epoch: 9 step: 762, loss is 0.10313419252634048\n",
      "epoch: 9 step: 763, loss is 0.11377736926078796\n",
      "epoch: 9 step: 764, loss is 0.0787985622882843\n",
      "epoch: 9 step: 765, loss is 0.1107725203037262\n",
      "epoch: 9 step: 766, loss is 0.02155386470258236\n",
      "epoch: 9 step: 767, loss is 0.08865824341773987\n",
      "epoch: 9 step: 768, loss is 0.1275714486837387\n",
      "epoch: 9 step: 769, loss is 0.07937172055244446\n",
      "epoch: 9 step: 770, loss is 0.06537973880767822\n",
      "epoch: 9 step: 771, loss is 0.03145691752433777\n",
      "epoch: 9 step: 772, loss is 0.054946307092905045\n",
      "epoch: 9 step: 773, loss is 0.2165469080209732\n",
      "epoch: 9 step: 774, loss is 0.10901868343353271\n",
      "epoch: 9 step: 775, loss is 0.07949379831552505\n",
      "epoch: 9 step: 776, loss is 0.07772582024335861\n",
      "epoch: 9 step: 777, loss is 0.050304580479860306\n",
      "epoch: 9 step: 778, loss is 0.1625337451696396\n",
      "epoch: 9 step: 779, loss is 0.050763729959726334\n",
      "epoch: 9 step: 780, loss is 0.08422838151454926\n",
      "epoch: 9 step: 781, loss is 0.08986997604370117\n",
      "epoch: 9 step: 782, loss is 0.016443857923150063\n",
      "epoch: 9 step: 783, loss is 0.16995003819465637\n",
      "epoch: 9 step: 784, loss is 0.02827809751033783\n",
      "epoch: 9 step: 785, loss is 0.14140813052654266\n",
      "epoch: 9 step: 786, loss is 0.0450618714094162\n",
      "epoch: 9 step: 787, loss is 0.09201452136039734\n",
      "epoch: 9 step: 788, loss is 0.028026964515447617\n",
      "epoch: 9 step: 789, loss is 0.03125515952706337\n",
      "epoch: 9 step: 790, loss is 0.10910002887248993\n",
      "epoch: 9 step: 791, loss is 0.0770948976278305\n",
      "epoch: 9 step: 792, loss is 0.19771181046962738\n",
      "epoch: 9 step: 793, loss is 0.06441576778888702\n",
      "epoch: 9 step: 794, loss is 0.06535477191209793\n",
      "epoch: 9 step: 795, loss is 0.10053063184022903\n",
      "epoch: 9 step: 796, loss is 0.06342168897390366\n",
      "epoch: 9 step: 797, loss is 0.12545974552631378\n",
      "epoch: 9 step: 798, loss is 0.06504081189632416\n",
      "epoch: 9 step: 799, loss is 0.049657151103019714\n",
      "epoch: 9 step: 800, loss is 0.037625595927238464\n",
      "epoch: 9 step: 801, loss is 0.03824981302022934\n",
      "epoch: 9 step: 802, loss is 0.12917156517505646\n",
      "epoch: 9 step: 803, loss is 0.1274372935295105\n",
      "epoch: 9 step: 804, loss is 0.08993543684482574\n",
      "epoch: 9 step: 805, loss is 0.05903797969222069\n",
      "epoch: 9 step: 806, loss is 0.08581807464361191\n",
      "epoch: 9 step: 807, loss is 0.07152479887008667\n",
      "epoch: 9 step: 808, loss is 0.09732678532600403\n",
      "epoch: 9 step: 809, loss is 0.05138847604393959\n",
      "epoch: 9 step: 810, loss is 0.12554199993610382\n",
      "epoch: 9 step: 811, loss is 0.05435299500823021\n",
      "epoch: 9 step: 812, loss is 0.04281952977180481\n",
      "epoch: 9 step: 813, loss is 0.04541857913136482\n",
      "epoch: 9 step: 814, loss is 0.2141657918691635\n",
      "epoch: 9 step: 815, loss is 0.08520378172397614\n",
      "epoch: 9 step: 816, loss is 0.06755539029836655\n",
      "epoch: 9 step: 817, loss is 0.13235580921173096\n",
      "epoch: 9 step: 818, loss is 0.08961424231529236\n",
      "epoch: 9 step: 819, loss is 0.07760229706764221\n",
      "epoch: 9 step: 820, loss is 0.06189608946442604\n",
      "epoch: 9 step: 821, loss is 0.11784698069095612\n",
      "epoch: 9 step: 822, loss is 0.11096853017807007\n",
      "epoch: 9 step: 823, loss is 0.041363757103681564\n",
      "epoch: 9 step: 824, loss is 0.03636467829346657\n",
      "epoch: 9 step: 825, loss is 0.05644926428794861\n",
      "epoch: 9 step: 826, loss is 0.08207795023918152\n",
      "epoch: 9 step: 827, loss is 0.0692756325006485\n",
      "epoch: 9 step: 828, loss is 0.016612255945801735\n",
      "epoch: 9 step: 829, loss is 0.05117766931653023\n",
      "epoch: 9 step: 830, loss is 0.06363311409950256\n",
      "epoch: 9 step: 831, loss is 0.03667685016989708\n",
      "epoch: 9 step: 832, loss is 0.05661972612142563\n",
      "epoch: 9 step: 833, loss is 0.2132122963666916\n",
      "epoch: 9 step: 834, loss is 0.050814300775527954\n",
      "epoch: 9 step: 835, loss is 0.13291053473949432\n",
      "epoch: 9 step: 836, loss is 0.09792155772447586\n",
      "epoch: 9 step: 837, loss is 0.11214462667703629\n",
      "epoch: 9 step: 838, loss is 0.15837040543556213\n",
      "epoch: 9 step: 839, loss is 0.19336341321468353\n",
      "epoch: 9 step: 840, loss is 0.03682875260710716\n",
      "epoch: 9 step: 841, loss is 0.027127917855978012\n",
      "epoch: 9 step: 842, loss is 0.06570979952812195\n",
      "epoch: 9 step: 843, loss is 0.029718076810240746\n",
      "epoch: 9 step: 844, loss is 0.03743748366832733\n",
      "epoch: 9 step: 845, loss is 0.15382961928844452\n",
      "epoch: 9 step: 846, loss is 0.044874150305986404\n",
      "epoch: 9 step: 847, loss is 0.18028536438941956\n",
      "epoch: 9 step: 848, loss is 0.03992660343647003\n",
      "epoch: 9 step: 849, loss is 0.013401618227362633\n",
      "epoch: 9 step: 850, loss is 0.14565959572792053\n",
      "epoch: 9 step: 851, loss is 0.03286797180771828\n",
      "epoch: 9 step: 852, loss is 0.1146664023399353\n",
      "epoch: 9 step: 853, loss is 0.0580509677529335\n",
      "epoch: 9 step: 854, loss is 0.15455615520477295\n",
      "epoch: 9 step: 855, loss is 0.05086042359471321\n",
      "epoch: 9 step: 856, loss is 0.052328966557979584\n",
      "epoch: 9 step: 857, loss is 0.041837427765131\n",
      "epoch: 9 step: 858, loss is 0.08877046406269073\n",
      "epoch: 9 step: 859, loss is 0.02311505191028118\n",
      "epoch: 9 step: 860, loss is 0.06226251646876335\n",
      "epoch: 9 step: 861, loss is 0.08775676041841507\n",
      "epoch: 9 step: 862, loss is 0.05984840542078018\n",
      "epoch: 9 step: 863, loss is 0.14313389360904694\n",
      "epoch: 9 step: 864, loss is 0.11255095899105072\n",
      "epoch: 9 step: 865, loss is 0.07350108027458191\n",
      "epoch: 9 step: 866, loss is 0.04981261491775513\n",
      "epoch: 9 step: 867, loss is 0.07561640441417694\n",
      "epoch: 9 step: 868, loss is 0.10799345374107361\n",
      "epoch: 9 step: 869, loss is 0.020860984921455383\n",
      "epoch: 9 step: 870, loss is 0.07228897511959076\n",
      "epoch: 9 step: 871, loss is 0.052371472120285034\n",
      "epoch: 9 step: 872, loss is 0.08294638246297836\n",
      "epoch: 9 step: 873, loss is 0.014794424176216125\n",
      "epoch: 9 step: 874, loss is 0.043768059462308884\n",
      "epoch: 9 step: 875, loss is 0.01018302608281374\n",
      "epoch: 9 step: 876, loss is 0.09031454473733902\n",
      "epoch: 9 step: 877, loss is 0.08122117072343826\n",
      "epoch: 9 step: 878, loss is 0.06226738914847374\n",
      "epoch: 9 step: 879, loss is 0.019052162766456604\n",
      "epoch: 9 step: 880, loss is 0.06830565631389618\n",
      "epoch: 9 step: 881, loss is 0.060035571455955505\n",
      "epoch: 9 step: 882, loss is 0.054922569543123245\n",
      "epoch: 9 step: 883, loss is 0.06872281432151794\n",
      "epoch: 9 step: 884, loss is 0.11428293585777283\n",
      "epoch: 9 step: 885, loss is 0.10319370031356812\n",
      "epoch: 9 step: 886, loss is 0.13815899193286896\n",
      "epoch: 9 step: 887, loss is 0.09421991556882858\n",
      "epoch: 9 step: 888, loss is 0.19922314584255219\n",
      "epoch: 9 step: 889, loss is 0.05697406828403473\n",
      "epoch: 9 step: 890, loss is 0.08267289400100708\n",
      "epoch: 9 step: 891, loss is 0.08473094552755356\n",
      "epoch: 9 step: 892, loss is 0.009846578352153301\n",
      "epoch: 9 step: 893, loss is 0.048511628061532974\n",
      "epoch: 9 step: 894, loss is 0.23211023211479187\n",
      "epoch: 9 step: 895, loss is 0.09019339829683304\n",
      "epoch: 9 step: 896, loss is 0.11353036761283875\n",
      "epoch: 9 step: 897, loss is 0.08958398550748825\n",
      "epoch: 9 step: 898, loss is 0.09187546372413635\n",
      "epoch: 9 step: 899, loss is 0.10623177140951157\n",
      "epoch: 9 step: 900, loss is 0.12396874278783798\n",
      "epoch: 9 step: 901, loss is 0.06559672951698303\n",
      "epoch: 9 step: 902, loss is 0.01811356469988823\n",
      "epoch: 9 step: 903, loss is 0.03855513781309128\n",
      "epoch: 9 step: 904, loss is 0.1372954100370407\n",
      "epoch: 9 step: 905, loss is 0.08826418966054916\n",
      "epoch: 9 step: 906, loss is 0.030428001657128334\n",
      "epoch: 9 step: 907, loss is 0.01954054646193981\n",
      "epoch: 9 step: 908, loss is 0.09503346681594849\n",
      "epoch: 9 step: 909, loss is 0.12992991507053375\n",
      "epoch: 9 step: 910, loss is 0.021761566400527954\n",
      "epoch: 9 step: 911, loss is 0.09104444086551666\n",
      "epoch: 9 step: 912, loss is 0.027418477460741997\n",
      "epoch: 9 step: 913, loss is 0.11293608695268631\n",
      "epoch: 9 step: 914, loss is 0.036021262407302856\n",
      "epoch: 9 step: 915, loss is 0.04015454649925232\n",
      "epoch: 9 step: 916, loss is 0.07030703872442245\n",
      "epoch: 9 step: 917, loss is 0.043902281671762466\n",
      "epoch: 9 step: 918, loss is 0.051682598888874054\n",
      "epoch: 9 step: 919, loss is 0.1411631554365158\n",
      "epoch: 9 step: 920, loss is 0.08870968222618103\n",
      "epoch: 9 step: 921, loss is 0.046207934617996216\n",
      "epoch: 9 step: 922, loss is 0.109369195997715\n",
      "epoch: 9 step: 923, loss is 0.07424286752939224\n",
      "epoch: 9 step: 924, loss is 0.08874309808015823\n",
      "epoch: 9 step: 925, loss is 0.08127716183662415\n",
      "epoch: 9 step: 926, loss is 0.13575345277786255\n",
      "epoch: 9 step: 927, loss is 0.11058070510625839\n",
      "epoch: 9 step: 928, loss is 0.05614465847611427\n",
      "epoch: 9 step: 929, loss is 0.05999268218874931\n",
      "epoch: 9 step: 930, loss is 0.2229379415512085\n",
      "epoch: 9 step: 931, loss is 0.09858392924070358\n",
      "epoch: 9 step: 932, loss is 0.04405280202627182\n",
      "epoch: 9 step: 933, loss is 0.07449743151664734\n",
      "epoch: 9 step: 934, loss is 0.05129053443670273\n",
      "epoch: 9 step: 935, loss is 0.03433689475059509\n",
      "epoch: 9 step: 936, loss is 0.03449492156505585\n",
      "epoch: 9 step: 937, loss is 0.05895572528243065\n",
      "epoch: 10 step: 1, loss is 0.07304833084344864\n",
      "epoch: 10 step: 2, loss is 0.07294831424951553\n",
      "epoch: 10 step: 3, loss is 0.09318768233060837\n",
      "epoch: 10 step: 4, loss is 0.02716095559298992\n",
      "epoch: 10 step: 5, loss is 0.04892430081963539\n",
      "epoch: 10 step: 6, loss is 0.037458308041095734\n",
      "epoch: 10 step: 7, loss is 0.02465745061635971\n",
      "epoch: 10 step: 8, loss is 0.0666465163230896\n",
      "epoch: 10 step: 9, loss is 0.086640864610672\n",
      "epoch: 10 step: 10, loss is 0.03135840222239494\n",
      "epoch: 10 step: 11, loss is 0.02969372272491455\n",
      "epoch: 10 step: 12, loss is 0.010941857472062111\n",
      "epoch: 10 step: 13, loss is 0.03214367479085922\n",
      "epoch: 10 step: 14, loss is 0.12797114253044128\n",
      "epoch: 10 step: 15, loss is 0.06014971435070038\n",
      "epoch: 10 step: 16, loss is 0.07357636094093323\n",
      "epoch: 10 step: 17, loss is 0.016133859753608704\n",
      "epoch: 10 step: 18, loss is 0.00758660351857543\n",
      "epoch: 10 step: 19, loss is 0.03831489011645317\n",
      "epoch: 10 step: 20, loss is 0.038924992084503174\n",
      "epoch: 10 step: 21, loss is 0.14220695197582245\n",
      "epoch: 10 step: 22, loss is 0.015471762977540493\n",
      "epoch: 10 step: 23, loss is 0.06627558916807175\n",
      "epoch: 10 step: 24, loss is 0.1384797990322113\n",
      "epoch: 10 step: 25, loss is 0.06131616607308388\n",
      "epoch: 10 step: 26, loss is 0.08489131182432175\n",
      "epoch: 10 step: 27, loss is 0.0720229223370552\n",
      "epoch: 10 step: 28, loss is 0.06536716222763062\n",
      "epoch: 10 step: 29, loss is 0.06516503542661667\n",
      "epoch: 10 step: 30, loss is 0.04092484340071678\n",
      "epoch: 10 step: 31, loss is 0.02002732641994953\n",
      "epoch: 10 step: 32, loss is 0.09705124795436859\n",
      "epoch: 10 step: 33, loss is 0.09003984928131104\n",
      "epoch: 10 step: 34, loss is 0.06968364864587784\n",
      "epoch: 10 step: 35, loss is 0.01702597737312317\n",
      "epoch: 10 step: 36, loss is 0.03576836362481117\n",
      "epoch: 10 step: 37, loss is 0.056158311665058136\n",
      "epoch: 10 step: 38, loss is 0.04614870250225067\n",
      "epoch: 10 step: 39, loss is 0.06372667849063873\n",
      "epoch: 10 step: 40, loss is 0.023111145943403244\n",
      "epoch: 10 step: 41, loss is 0.018522964790463448\n",
      "epoch: 10 step: 42, loss is 0.05386858060956001\n",
      "epoch: 10 step: 43, loss is 0.02354830503463745\n",
      "epoch: 10 step: 44, loss is 0.054363880306482315\n",
      "epoch: 10 step: 45, loss is 0.018045304343104362\n",
      "epoch: 10 step: 46, loss is 0.06448698788881302\n",
      "epoch: 10 step: 47, loss is 0.02762344852089882\n",
      "epoch: 10 step: 48, loss is 0.018562689423561096\n",
      "epoch: 10 step: 49, loss is 0.026395486667752266\n",
      "epoch: 10 step: 50, loss is 0.027222931385040283\n",
      "epoch: 10 step: 51, loss is 0.018663929775357246\n",
      "epoch: 10 step: 52, loss is 0.07441435009241104\n",
      "epoch: 10 step: 53, loss is 0.05921564996242523\n",
      "epoch: 10 step: 54, loss is 0.07692215591669083\n",
      "epoch: 10 step: 55, loss is 0.09152209758758545\n",
      "epoch: 10 step: 56, loss is 0.05226779729127884\n",
      "epoch: 10 step: 57, loss is 0.029608143493533134\n",
      "epoch: 10 step: 58, loss is 0.03005320020020008\n",
      "epoch: 10 step: 59, loss is 0.05433933064341545\n",
      "epoch: 10 step: 60, loss is 0.026752840727567673\n",
      "epoch: 10 step: 61, loss is 0.052108101546764374\n",
      "epoch: 10 step: 62, loss is 0.01503337174654007\n",
      "epoch: 10 step: 63, loss is 0.04116415977478027\n",
      "epoch: 10 step: 64, loss is 0.022544315084815025\n",
      "epoch: 10 step: 65, loss is 0.021762114018201828\n",
      "epoch: 10 step: 66, loss is 0.015710480511188507\n",
      "epoch: 10 step: 67, loss is 0.04707632213830948\n",
      "epoch: 10 step: 68, loss is 0.04479214921593666\n",
      "epoch: 10 step: 69, loss is 0.07274024188518524\n",
      "epoch: 10 step: 70, loss is 0.0723370835185051\n",
      "epoch: 10 step: 71, loss is 0.025713445618748665\n",
      "epoch: 10 step: 72, loss is 0.010354946367442608\n",
      "epoch: 10 step: 73, loss is 0.16471759974956512\n",
      "epoch: 10 step: 74, loss is 0.05388730764389038\n",
      "epoch: 10 step: 75, loss is 0.021296652033925056\n",
      "epoch: 10 step: 76, loss is 0.0712192952632904\n",
      "epoch: 10 step: 77, loss is 0.012225464917719364\n",
      "epoch: 10 step: 78, loss is 0.026178212836384773\n",
      "epoch: 10 step: 79, loss is 0.05508580803871155\n",
      "epoch: 10 step: 80, loss is 0.04490114748477936\n",
      "epoch: 10 step: 81, loss is 0.0573895201086998\n",
      "epoch: 10 step: 82, loss is 0.065680131316185\n",
      "epoch: 10 step: 83, loss is 0.007117251865565777\n",
      "epoch: 10 step: 84, loss is 0.1453220546245575\n",
      "epoch: 10 step: 85, loss is 0.037038158625364304\n",
      "epoch: 10 step: 86, loss is 0.04731908440589905\n",
      "epoch: 10 step: 87, loss is 0.05600816011428833\n",
      "epoch: 10 step: 88, loss is 0.06295306235551834\n",
      "epoch: 10 step: 89, loss is 0.036420729011297226\n",
      "epoch: 10 step: 90, loss is 0.03197662904858589\n",
      "epoch: 10 step: 91, loss is 0.005119836423546076\n",
      "epoch: 10 step: 92, loss is 0.02836962603032589\n",
      "epoch: 10 step: 93, loss is 0.005978831090033054\n",
      "epoch: 10 step: 94, loss is 0.03445664793252945\n",
      "epoch: 10 step: 95, loss is 0.09971795976161957\n",
      "epoch: 10 step: 96, loss is 0.0035838282201439142\n",
      "epoch: 10 step: 97, loss is 0.04972916841506958\n",
      "epoch: 10 step: 98, loss is 0.051141154021024704\n",
      "epoch: 10 step: 99, loss is 0.0819629356265068\n",
      "epoch: 10 step: 100, loss is 0.05910101905465126\n",
      "epoch: 10 step: 101, loss is 0.06042870134115219\n",
      "epoch: 10 step: 102, loss is 0.01240517757833004\n",
      "epoch: 10 step: 103, loss is 0.09827885776758194\n",
      "epoch: 10 step: 104, loss is 0.1207268089056015\n",
      "epoch: 10 step: 105, loss is 0.07912709563970566\n",
      "epoch: 10 step: 106, loss is 0.017065221443772316\n",
      "epoch: 10 step: 107, loss is 0.13553304970264435\n",
      "epoch: 10 step: 108, loss is 0.09299109131097794\n",
      "epoch: 10 step: 109, loss is 0.018290601670742035\n",
      "epoch: 10 step: 110, loss is 0.025861402973532677\n",
      "epoch: 10 step: 111, loss is 0.013701170682907104\n",
      "epoch: 10 step: 112, loss is 0.06130307540297508\n",
      "epoch: 10 step: 113, loss is 0.21022385358810425\n",
      "epoch: 10 step: 114, loss is 0.011841642670333385\n",
      "epoch: 10 step: 115, loss is 0.056337691843509674\n",
      "epoch: 10 step: 116, loss is 0.04761698469519615\n",
      "epoch: 10 step: 117, loss is 0.12072610855102539\n",
      "epoch: 10 step: 118, loss is 0.13246934115886688\n",
      "epoch: 10 step: 119, loss is 0.09150364249944687\n",
      "epoch: 10 step: 120, loss is 0.029993006959557533\n",
      "epoch: 10 step: 121, loss is 0.06744001805782318\n",
      "epoch: 10 step: 122, loss is 0.012154885567724705\n",
      "epoch: 10 step: 123, loss is 0.08218713104724884\n",
      "epoch: 10 step: 124, loss is 0.04605909064412117\n",
      "epoch: 10 step: 125, loss is 0.04803667217493057\n",
      "epoch: 10 step: 126, loss is 0.04371954873204231\n",
      "epoch: 10 step: 127, loss is 0.08079284429550171\n",
      "epoch: 10 step: 128, loss is 0.1077004224061966\n",
      "epoch: 10 step: 129, loss is 0.050900474190711975\n",
      "epoch: 10 step: 130, loss is 0.0212874598801136\n",
      "epoch: 10 step: 131, loss is 0.04364308714866638\n",
      "epoch: 10 step: 132, loss is 0.011000759899616241\n",
      "epoch: 10 step: 133, loss is 0.13611048460006714\n",
      "epoch: 10 step: 134, loss is 0.029891936108469963\n",
      "epoch: 10 step: 135, loss is 0.037913087755441666\n",
      "epoch: 10 step: 136, loss is 0.028290018439292908\n",
      "epoch: 10 step: 137, loss is 0.09546779841184616\n",
      "epoch: 10 step: 138, loss is 0.037669796496629715\n",
      "epoch: 10 step: 139, loss is 0.008952783420681953\n",
      "epoch: 10 step: 140, loss is 0.09262534976005554\n",
      "epoch: 10 step: 141, loss is 0.03768513724207878\n",
      "epoch: 10 step: 142, loss is 0.04553953558206558\n",
      "epoch: 10 step: 143, loss is 0.08091118931770325\n",
      "epoch: 10 step: 144, loss is 0.01156599074602127\n",
      "epoch: 10 step: 145, loss is 0.052222367376089096\n",
      "epoch: 10 step: 146, loss is 0.018876217305660248\n",
      "epoch: 10 step: 147, loss is 0.03826765716075897\n",
      "epoch: 10 step: 148, loss is 0.034873466938734055\n",
      "epoch: 10 step: 149, loss is 0.11090179532766342\n",
      "epoch: 10 step: 150, loss is 0.02816985920071602\n",
      "epoch: 10 step: 151, loss is 0.16746512055397034\n",
      "epoch: 10 step: 152, loss is 0.16083824634552002\n",
      "epoch: 10 step: 153, loss is 0.05693268030881882\n",
      "epoch: 10 step: 154, loss is 0.08544959127902985\n",
      "epoch: 10 step: 155, loss is 0.008394736796617508\n",
      "epoch: 10 step: 156, loss is 0.017429808154702187\n",
      "epoch: 10 step: 157, loss is 0.06536313146352768\n",
      "epoch: 10 step: 158, loss is 0.16819486021995544\n",
      "epoch: 10 step: 159, loss is 0.04923539608716965\n",
      "epoch: 10 step: 160, loss is 0.10774946212768555\n",
      "epoch: 10 step: 161, loss is 0.04718378931283951\n",
      "epoch: 10 step: 162, loss is 0.13122723996639252\n",
      "epoch: 10 step: 163, loss is 0.03987104073166847\n",
      "epoch: 10 step: 164, loss is 0.04855037108063698\n",
      "epoch: 10 step: 165, loss is 0.039828840643167496\n",
      "epoch: 10 step: 166, loss is 0.13869191706180573\n",
      "epoch: 10 step: 167, loss is 0.09281426668167114\n",
      "epoch: 10 step: 168, loss is 0.013798536732792854\n",
      "epoch: 10 step: 169, loss is 0.06339417397975922\n",
      "epoch: 10 step: 170, loss is 0.09568927437067032\n",
      "epoch: 10 step: 171, loss is 0.026374638080596924\n",
      "epoch: 10 step: 172, loss is 0.04332275316119194\n",
      "epoch: 10 step: 173, loss is 0.061520013958215714\n",
      "epoch: 10 step: 174, loss is 0.012925141490995884\n",
      "epoch: 10 step: 175, loss is 0.060673583298921585\n",
      "epoch: 10 step: 176, loss is 0.04354652389883995\n",
      "epoch: 10 step: 177, loss is 0.03705199062824249\n",
      "epoch: 10 step: 178, loss is 0.06684686988592148\n",
      "epoch: 10 step: 179, loss is 0.06640582531690598\n",
      "epoch: 10 step: 180, loss is 0.10298476368188858\n",
      "epoch: 10 step: 181, loss is 0.04224085435271263\n",
      "epoch: 10 step: 182, loss is 0.1267547309398651\n",
      "epoch: 10 step: 183, loss is 0.06820689886808395\n",
      "epoch: 10 step: 184, loss is 0.03308708220720291\n",
      "epoch: 10 step: 185, loss is 0.13832804560661316\n",
      "epoch: 10 step: 186, loss is 0.0660586878657341\n",
      "epoch: 10 step: 187, loss is 0.01941104792058468\n",
      "epoch: 10 step: 188, loss is 0.009295808151364326\n",
      "epoch: 10 step: 189, loss is 0.03383250534534454\n",
      "epoch: 10 step: 190, loss is 0.03016527183353901\n",
      "epoch: 10 step: 191, loss is 0.02346934750676155\n",
      "epoch: 10 step: 192, loss is 0.07095780968666077\n",
      "epoch: 10 step: 193, loss is 0.03208063542842865\n",
      "epoch: 10 step: 194, loss is 0.07161201536655426\n",
      "epoch: 10 step: 195, loss is 0.07693222165107727\n",
      "epoch: 10 step: 196, loss is 0.04597959294915199\n",
      "epoch: 10 step: 197, loss is 0.11509425938129425\n",
      "epoch: 10 step: 198, loss is 0.02770792692899704\n",
      "epoch: 10 step: 199, loss is 0.02013784646987915\n",
      "epoch: 10 step: 200, loss is 0.0644860789179802\n",
      "epoch: 10 step: 201, loss is 0.04000680893659592\n",
      "epoch: 10 step: 202, loss is 0.03410850465297699\n",
      "epoch: 10 step: 203, loss is 0.02459791488945484\n",
      "epoch: 10 step: 204, loss is 0.034813761711120605\n",
      "epoch: 10 step: 205, loss is 0.019653337076306343\n",
      "epoch: 10 step: 206, loss is 0.04566878080368042\n",
      "epoch: 10 step: 207, loss is 0.05463724583387375\n",
      "epoch: 10 step: 208, loss is 0.03879028186202049\n",
      "epoch: 10 step: 209, loss is 0.06729790568351746\n",
      "epoch: 10 step: 210, loss is 0.0766000896692276\n",
      "epoch: 10 step: 211, loss is 0.0472334660589695\n",
      "epoch: 10 step: 212, loss is 0.006770490203052759\n",
      "epoch: 10 step: 213, loss is 0.07052526623010635\n",
      "epoch: 10 step: 214, loss is 0.07268780469894409\n",
      "epoch: 10 step: 215, loss is 0.06267894059419632\n",
      "epoch: 10 step: 216, loss is 0.1536010503768921\n",
      "epoch: 10 step: 217, loss is 0.05142946168780327\n",
      "epoch: 10 step: 218, loss is 0.04149070754647255\n",
      "epoch: 10 step: 219, loss is 0.06852018088102341\n",
      "epoch: 10 step: 220, loss is 0.06283488124608994\n",
      "epoch: 10 step: 221, loss is 0.09164796024560928\n",
      "epoch: 10 step: 222, loss is 0.011960627511143684\n",
      "epoch: 10 step: 223, loss is 0.015856370329856873\n",
      "epoch: 10 step: 224, loss is 0.00767897954210639\n",
      "epoch: 10 step: 225, loss is 0.06877408176660538\n",
      "epoch: 10 step: 226, loss is 0.14218644797801971\n",
      "epoch: 10 step: 227, loss is 0.02205500565469265\n",
      "epoch: 10 step: 228, loss is 0.08487236499786377\n",
      "epoch: 10 step: 229, loss is 0.05977509543299675\n",
      "epoch: 10 step: 230, loss is 0.019293837249279022\n",
      "epoch: 10 step: 231, loss is 0.03967144340276718\n",
      "epoch: 10 step: 232, loss is 0.059281494468450546\n",
      "epoch: 10 step: 233, loss is 0.03194659948348999\n",
      "epoch: 10 step: 234, loss is 0.046424154192209244\n",
      "epoch: 10 step: 235, loss is 0.018908575177192688\n",
      "epoch: 10 step: 236, loss is 0.13574939966201782\n",
      "epoch: 10 step: 237, loss is 0.039019446820020676\n",
      "epoch: 10 step: 238, loss is 0.009638185612857342\n",
      "epoch: 10 step: 239, loss is 0.04216005280613899\n",
      "epoch: 10 step: 240, loss is 0.05243502929806709\n",
      "epoch: 10 step: 241, loss is 0.008716033771634102\n",
      "epoch: 10 step: 242, loss is 0.11381563544273376\n",
      "epoch: 10 step: 243, loss is 0.02047639712691307\n",
      "epoch: 10 step: 244, loss is 0.013599425554275513\n",
      "epoch: 10 step: 245, loss is 0.019531454890966415\n",
      "epoch: 10 step: 246, loss is 0.05627560243010521\n",
      "epoch: 10 step: 247, loss is 0.022043440490961075\n",
      "epoch: 10 step: 248, loss is 0.018742863088846207\n",
      "epoch: 10 step: 249, loss is 0.06096585839986801\n",
      "epoch: 10 step: 250, loss is 0.031759653240442276\n",
      "epoch: 10 step: 251, loss is 0.04357483610510826\n",
      "epoch: 10 step: 252, loss is 0.025038983672857285\n",
      "epoch: 10 step: 253, loss is 0.08090685307979584\n",
      "epoch: 10 step: 254, loss is 0.035584643483161926\n",
      "epoch: 10 step: 255, loss is 0.044502872973680496\n",
      "epoch: 10 step: 256, loss is 0.014050832949578762\n",
      "epoch: 10 step: 257, loss is 0.09732948988676071\n",
      "epoch: 10 step: 258, loss is 0.032435499131679535\n",
      "epoch: 10 step: 259, loss is 0.18807893991470337\n",
      "epoch: 10 step: 260, loss is 0.042558617889881134\n",
      "epoch: 10 step: 261, loss is 0.08369161188602448\n",
      "epoch: 10 step: 262, loss is 0.03739762678742409\n",
      "epoch: 10 step: 263, loss is 0.014333338476717472\n",
      "epoch: 10 step: 264, loss is 0.04503249377012253\n",
      "epoch: 10 step: 265, loss is 0.06335144490003586\n",
      "epoch: 10 step: 266, loss is 0.03265826031565666\n",
      "epoch: 10 step: 267, loss is 0.09195686131715775\n",
      "epoch: 10 step: 268, loss is 0.0417850986123085\n",
      "epoch: 10 step: 269, loss is 0.011614971794188023\n",
      "epoch: 10 step: 270, loss is 0.07255788147449493\n",
      "epoch: 10 step: 271, loss is 0.0652935728430748\n",
      "epoch: 10 step: 272, loss is 0.07636499404907227\n",
      "epoch: 10 step: 273, loss is 0.030673373490571976\n",
      "epoch: 10 step: 274, loss is 0.101820208132267\n",
      "epoch: 10 step: 275, loss is 0.0597568117082119\n",
      "epoch: 10 step: 276, loss is 0.09600488096475601\n",
      "epoch: 10 step: 277, loss is 0.04062600061297417\n",
      "epoch: 10 step: 278, loss is 0.03396392613649368\n",
      "epoch: 10 step: 279, loss is 0.09789302945137024\n",
      "epoch: 10 step: 280, loss is 0.034796666353940964\n",
      "epoch: 10 step: 281, loss is 0.1169031485915184\n",
      "epoch: 10 step: 282, loss is 0.07044267654418945\n",
      "epoch: 10 step: 283, loss is 0.08051164448261261\n",
      "epoch: 10 step: 284, loss is 0.016219310462474823\n",
      "epoch: 10 step: 285, loss is 0.09115952998399734\n",
      "epoch: 10 step: 286, loss is 0.09597249329090118\n",
      "epoch: 10 step: 287, loss is 0.04583774879574776\n",
      "epoch: 10 step: 288, loss is 0.04854309931397438\n",
      "epoch: 10 step: 289, loss is 0.07189223915338516\n",
      "epoch: 10 step: 290, loss is 0.06458685547113419\n",
      "epoch: 10 step: 291, loss is 0.10307974368333817\n",
      "epoch: 10 step: 292, loss is 0.07521958649158478\n",
      "epoch: 10 step: 293, loss is 0.1343054324388504\n",
      "epoch: 10 step: 294, loss is 0.08460282534360886\n",
      "epoch: 10 step: 295, loss is 0.10274792462587357\n",
      "epoch: 10 step: 296, loss is 0.04256303235888481\n",
      "epoch: 10 step: 297, loss is 0.11186946928501129\n",
      "epoch: 10 step: 298, loss is 0.10160043835639954\n",
      "epoch: 10 step: 299, loss is 0.05278089642524719\n",
      "epoch: 10 step: 300, loss is 0.02071836031973362\n",
      "epoch: 10 step: 301, loss is 0.02333979308605194\n",
      "epoch: 10 step: 302, loss is 0.031064443290233612\n",
      "epoch: 10 step: 303, loss is 0.018273018300533295\n",
      "epoch: 10 step: 304, loss is 0.00874814111739397\n",
      "epoch: 10 step: 305, loss is 0.04659586772322655\n",
      "epoch: 10 step: 306, loss is 0.10732472687959671\n",
      "epoch: 10 step: 307, loss is 0.08510385453701019\n",
      "epoch: 10 step: 308, loss is 0.03793986514210701\n",
      "epoch: 10 step: 309, loss is 0.07601919025182724\n",
      "epoch: 10 step: 310, loss is 0.07013940811157227\n",
      "epoch: 10 step: 311, loss is 0.019054096192121506\n",
      "epoch: 10 step: 312, loss is 0.022425943985581398\n",
      "epoch: 10 step: 313, loss is 0.005325132980942726\n",
      "epoch: 10 step: 314, loss is 0.034749001264572144\n",
      "epoch: 10 step: 315, loss is 0.11122706532478333\n",
      "epoch: 10 step: 316, loss is 0.027587030082941055\n",
      "epoch: 10 step: 317, loss is 0.030700281262397766\n",
      "epoch: 10 step: 318, loss is 0.07541811466217041\n",
      "epoch: 10 step: 319, loss is 0.0542500875890255\n",
      "epoch: 10 step: 320, loss is 0.013571199029684067\n",
      "epoch: 10 step: 321, loss is 0.05111943930387497\n",
      "epoch: 10 step: 322, loss is 0.05080030858516693\n",
      "epoch: 10 step: 323, loss is 0.039877839386463165\n",
      "epoch: 10 step: 324, loss is 0.045260947197675705\n",
      "epoch: 10 step: 325, loss is 0.048991911113262177\n",
      "epoch: 10 step: 326, loss is 0.07226404547691345\n",
      "epoch: 10 step: 327, loss is 0.019636377692222595\n",
      "epoch: 10 step: 328, loss is 0.03979426994919777\n",
      "epoch: 10 step: 329, loss is 0.10219869017601013\n",
      "epoch: 10 step: 330, loss is 0.04717471823096275\n",
      "epoch: 10 step: 331, loss is 0.16075095534324646\n",
      "epoch: 10 step: 332, loss is 0.038384635001420975\n",
      "epoch: 10 step: 333, loss is 0.06764332205057144\n",
      "epoch: 10 step: 334, loss is 0.03225371986627579\n",
      "epoch: 10 step: 335, loss is 0.027843890711665154\n",
      "epoch: 10 step: 336, loss is 0.07184717804193497\n",
      "epoch: 10 step: 337, loss is 0.13241033256053925\n",
      "epoch: 10 step: 338, loss is 0.13691851496696472\n",
      "epoch: 10 step: 339, loss is 0.026459259912371635\n",
      "epoch: 10 step: 340, loss is 0.035302095115184784\n",
      "epoch: 10 step: 341, loss is 0.07705464214086533\n",
      "epoch: 10 step: 342, loss is 0.026924937963485718\n",
      "epoch: 10 step: 343, loss is 0.020794164389371872\n",
      "epoch: 10 step: 344, loss is 0.06766431033611298\n",
      "epoch: 10 step: 345, loss is 0.05935193598270416\n",
      "epoch: 10 step: 346, loss is 0.10204511135816574\n",
      "epoch: 10 step: 347, loss is 0.11264972388744354\n",
      "epoch: 10 step: 348, loss is 0.13044579327106476\n",
      "epoch: 10 step: 349, loss is 0.022578474134206772\n",
      "epoch: 10 step: 350, loss is 0.07820463925600052\n",
      "epoch: 10 step: 351, loss is 0.07053789496421814\n",
      "epoch: 10 step: 352, loss is 0.018640808761119843\n",
      "epoch: 10 step: 353, loss is 0.046837929636240005\n",
      "epoch: 10 step: 354, loss is 0.015075745061039925\n",
      "epoch: 10 step: 355, loss is 0.10397481918334961\n",
      "epoch: 10 step: 356, loss is 0.03999220207333565\n",
      "epoch: 10 step: 357, loss is 0.04033035412430763\n",
      "epoch: 10 step: 358, loss is 0.029762908816337585\n",
      "epoch: 10 step: 359, loss is 0.1526450663805008\n",
      "epoch: 10 step: 360, loss is 0.050271667540073395\n",
      "epoch: 10 step: 361, loss is 0.05089369788765907\n",
      "epoch: 10 step: 362, loss is 0.10449378937482834\n",
      "epoch: 10 step: 363, loss is 0.02537843957543373\n",
      "epoch: 10 step: 364, loss is 0.0901111513376236\n",
      "epoch: 10 step: 365, loss is 0.10633070021867752\n",
      "epoch: 10 step: 366, loss is 0.04831581562757492\n",
      "epoch: 10 step: 367, loss is 0.07310067117214203\n",
      "epoch: 10 step: 368, loss is 0.05818817764520645\n",
      "epoch: 10 step: 369, loss is 0.0072148386389017105\n",
      "epoch: 10 step: 370, loss is 0.04098579287528992\n",
      "epoch: 10 step: 371, loss is 0.0864364430308342\n",
      "epoch: 10 step: 372, loss is 0.008638540282845497\n",
      "epoch: 10 step: 373, loss is 0.055059097707271576\n",
      "epoch: 10 step: 374, loss is 0.010211866348981857\n",
      "epoch: 10 step: 375, loss is 0.07072856277227402\n",
      "epoch: 10 step: 376, loss is 0.05065318942070007\n",
      "epoch: 10 step: 377, loss is 0.03617046773433685\n",
      "epoch: 10 step: 378, loss is 0.04945004731416702\n",
      "epoch: 10 step: 379, loss is 0.0816849023103714\n",
      "epoch: 10 step: 380, loss is 0.03845047205686569\n",
      "epoch: 10 step: 381, loss is 0.0693124309182167\n",
      "epoch: 10 step: 382, loss is 0.021875346079468727\n",
      "epoch: 10 step: 383, loss is 0.022877220064401627\n",
      "epoch: 10 step: 384, loss is 0.02891005016863346\n",
      "epoch: 10 step: 385, loss is 0.32191914319992065\n",
      "epoch: 10 step: 386, loss is 0.04612070694565773\n",
      "epoch: 10 step: 387, loss is 0.16773271560668945\n",
      "epoch: 10 step: 388, loss is 0.023210931569337845\n",
      "epoch: 10 step: 389, loss is 0.07271643728017807\n",
      "epoch: 10 step: 390, loss is 0.04880589619278908\n",
      "epoch: 10 step: 391, loss is 0.05249021202325821\n",
      "epoch: 10 step: 392, loss is 0.028817281126976013\n",
      "epoch: 10 step: 393, loss is 0.06174631044268608\n",
      "epoch: 10 step: 394, loss is 0.032994408160448074\n",
      "epoch: 10 step: 395, loss is 0.05843708664178848\n",
      "epoch: 10 step: 396, loss is 0.11640254408121109\n",
      "epoch: 10 step: 397, loss is 0.07415195554494858\n",
      "epoch: 10 step: 398, loss is 0.06873854249715805\n",
      "epoch: 10 step: 399, loss is 0.023795029148459435\n",
      "epoch: 10 step: 400, loss is 0.024232758209109306\n",
      "epoch: 10 step: 401, loss is 0.07145152240991592\n",
      "epoch: 10 step: 402, loss is 0.019405821338295937\n",
      "epoch: 10 step: 403, loss is 0.08773854374885559\n",
      "epoch: 10 step: 404, loss is 0.05779089033603668\n",
      "epoch: 10 step: 405, loss is 0.11367523670196533\n",
      "epoch: 10 step: 406, loss is 0.03648925945162773\n",
      "epoch: 10 step: 407, loss is 0.004879315849393606\n",
      "epoch: 10 step: 408, loss is 0.05774860456585884\n",
      "epoch: 10 step: 409, loss is 0.055800911039114\n",
      "epoch: 10 step: 410, loss is 0.14729151129722595\n",
      "epoch: 10 step: 411, loss is 0.09742927551269531\n",
      "epoch: 10 step: 412, loss is 0.036272402852773666\n",
      "epoch: 10 step: 413, loss is 0.11223383992910385\n",
      "epoch: 10 step: 414, loss is 0.036049697548151016\n",
      "epoch: 10 step: 415, loss is 0.13324443995952606\n",
      "epoch: 10 step: 416, loss is 0.01211949996650219\n",
      "epoch: 10 step: 417, loss is 0.06173088774085045\n",
      "epoch: 10 step: 418, loss is 0.08305380493402481\n",
      "epoch: 10 step: 419, loss is 0.11072522401809692\n",
      "epoch: 10 step: 420, loss is 0.03420741483569145\n",
      "epoch: 10 step: 421, loss is 0.02940613403916359\n",
      "epoch: 10 step: 422, loss is 0.1361912339925766\n",
      "epoch: 10 step: 423, loss is 0.12096042931079865\n",
      "epoch: 10 step: 424, loss is 0.05434712395071983\n",
      "epoch: 10 step: 425, loss is 0.08335019648075104\n",
      "epoch: 10 step: 426, loss is 0.018610311672091484\n",
      "epoch: 10 step: 427, loss is 0.006589533295482397\n",
      "epoch: 10 step: 428, loss is 0.1157265454530716\n",
      "epoch: 10 step: 429, loss is 0.08597268909215927\n",
      "epoch: 10 step: 430, loss is 0.16573551297187805\n",
      "epoch: 10 step: 431, loss is 0.13672637939453125\n",
      "epoch: 10 step: 432, loss is 0.08364897966384888\n",
      "epoch: 10 step: 433, loss is 0.024744348600506783\n",
      "epoch: 10 step: 434, loss is 0.10227355360984802\n",
      "epoch: 10 step: 435, loss is 0.05149460583925247\n",
      "epoch: 10 step: 436, loss is 0.08882378786802292\n",
      "epoch: 10 step: 437, loss is 0.010944096371531487\n",
      "epoch: 10 step: 438, loss is 0.06051287055015564\n",
      "epoch: 10 step: 439, loss is 0.09164207428693771\n",
      "epoch: 10 step: 440, loss is 0.05959529057145119\n",
      "epoch: 10 step: 441, loss is 0.13478130102157593\n",
      "epoch: 10 step: 442, loss is 0.04380836710333824\n",
      "epoch: 10 step: 443, loss is 0.0568329356610775\n",
      "epoch: 10 step: 444, loss is 0.09142062067985535\n",
      "epoch: 10 step: 445, loss is 0.038976818323135376\n",
      "epoch: 10 step: 446, loss is 0.08063402771949768\n",
      "epoch: 10 step: 447, loss is 0.08869984745979309\n",
      "epoch: 10 step: 448, loss is 0.056525517255067825\n",
      "epoch: 10 step: 449, loss is 0.15289340913295746\n",
      "epoch: 10 step: 450, loss is 0.1085112914443016\n",
      "epoch: 10 step: 451, loss is 0.030110836029052734\n",
      "epoch: 10 step: 452, loss is 0.06418874859809875\n",
      "epoch: 10 step: 453, loss is 0.08956408500671387\n",
      "epoch: 10 step: 454, loss is 0.03324389085173607\n",
      "epoch: 10 step: 455, loss is 0.048719629645347595\n",
      "epoch: 10 step: 456, loss is 0.10927014797925949\n",
      "epoch: 10 step: 457, loss is 0.1339433193206787\n",
      "epoch: 10 step: 458, loss is 0.11690718680620193\n",
      "epoch: 10 step: 459, loss is 0.12501224875450134\n",
      "epoch: 10 step: 460, loss is 0.0569523461163044\n",
      "epoch: 10 step: 461, loss is 0.013316033408045769\n",
      "epoch: 10 step: 462, loss is 0.062580406665802\n",
      "epoch: 10 step: 463, loss is 0.057348690927028656\n",
      "epoch: 10 step: 464, loss is 0.06916256994009018\n",
      "epoch: 10 step: 465, loss is 0.04509606212377548\n",
      "epoch: 10 step: 466, loss is 0.07343874871730804\n",
      "epoch: 10 step: 467, loss is 0.044439591467380524\n",
      "epoch: 10 step: 468, loss is 0.06799198687076569\n",
      "epoch: 10 step: 469, loss is 0.051474761217832565\n",
      "epoch: 10 step: 470, loss is 0.05079248920083046\n",
      "epoch: 10 step: 471, loss is 0.07270527631044388\n",
      "epoch: 10 step: 472, loss is 0.04580901563167572\n",
      "epoch: 10 step: 473, loss is 0.03759544715285301\n",
      "epoch: 10 step: 474, loss is 0.1328326165676117\n",
      "epoch: 10 step: 475, loss is 0.07875942438840866\n",
      "epoch: 10 step: 476, loss is 0.06555129587650299\n",
      "epoch: 10 step: 477, loss is 0.044630762189626694\n",
      "epoch: 10 step: 478, loss is 0.03262148052453995\n",
      "epoch: 10 step: 479, loss is 0.14308492839336395\n",
      "epoch: 10 step: 480, loss is 0.034494780004024506\n",
      "epoch: 10 step: 481, loss is 0.09345989674329758\n",
      "epoch: 10 step: 482, loss is 0.017422856763005257\n",
      "epoch: 10 step: 483, loss is 0.14142610132694244\n",
      "epoch: 10 step: 484, loss is 0.03592519089579582\n",
      "epoch: 10 step: 485, loss is 0.1411256343126297\n",
      "epoch: 10 step: 486, loss is 0.04977014660835266\n",
      "epoch: 10 step: 487, loss is 0.043464772403240204\n",
      "epoch: 10 step: 488, loss is 0.01593908481299877\n",
      "epoch: 10 step: 489, loss is 0.013688764534890652\n",
      "epoch: 10 step: 490, loss is 0.17354485392570496\n",
      "epoch: 10 step: 491, loss is 0.04118027165532112\n",
      "epoch: 10 step: 492, loss is 0.08189576864242554\n",
      "epoch: 10 step: 493, loss is 0.045967359095811844\n",
      "epoch: 10 step: 494, loss is 0.030523139983415604\n",
      "epoch: 10 step: 495, loss is 0.013933283276855946\n",
      "epoch: 10 step: 496, loss is 0.03769182786345482\n",
      "epoch: 10 step: 497, loss is 0.07183194905519485\n",
      "epoch: 10 step: 498, loss is 0.029868587851524353\n",
      "epoch: 10 step: 499, loss is 0.1173316091299057\n",
      "epoch: 10 step: 500, loss is 0.07991402596235275\n",
      "epoch: 10 step: 501, loss is 0.12478569895029068\n",
      "epoch: 10 step: 502, loss is 0.07067745178937912\n",
      "epoch: 10 step: 503, loss is 0.12988527119159698\n",
      "epoch: 10 step: 504, loss is 0.03919906169176102\n",
      "epoch: 10 step: 505, loss is 0.054699696600437164\n",
      "epoch: 10 step: 506, loss is 0.04055716097354889\n",
      "epoch: 10 step: 507, loss is 0.02020341530442238\n",
      "epoch: 10 step: 508, loss is 0.0400664322078228\n",
      "epoch: 10 step: 509, loss is 0.044565774500370026\n",
      "epoch: 10 step: 510, loss is 0.09532743692398071\n",
      "epoch: 10 step: 511, loss is 0.06297515332698822\n",
      "epoch: 10 step: 512, loss is 0.03172365948557854\n",
      "epoch: 10 step: 513, loss is 0.01787164807319641\n",
      "epoch: 10 step: 514, loss is 0.11054860800504684\n",
      "epoch: 10 step: 515, loss is 0.09695906192064285\n",
      "epoch: 10 step: 516, loss is 0.02818634733557701\n",
      "epoch: 10 step: 517, loss is 0.07384517043828964\n",
      "epoch: 10 step: 518, loss is 0.05048181489109993\n",
      "epoch: 10 step: 519, loss is 0.06581473350524902\n",
      "epoch: 10 step: 520, loss is 0.06389375030994415\n",
      "epoch: 10 step: 521, loss is 0.06218227744102478\n",
      "epoch: 10 step: 522, loss is 0.09122303873300552\n",
      "epoch: 10 step: 523, loss is 0.053166262805461884\n",
      "epoch: 10 step: 524, loss is 0.06546784937381744\n",
      "epoch: 10 step: 525, loss is 0.004108925350010395\n",
      "epoch: 10 step: 526, loss is 0.04546687379479408\n",
      "epoch: 10 step: 527, loss is 0.028318896889686584\n",
      "epoch: 10 step: 528, loss is 0.09490030258893967\n",
      "epoch: 10 step: 529, loss is 0.04043541103601456\n",
      "epoch: 10 step: 530, loss is 0.15703880786895752\n",
      "epoch: 10 step: 531, loss is 0.009567652828991413\n",
      "epoch: 10 step: 532, loss is 0.09742757678031921\n",
      "epoch: 10 step: 533, loss is 0.18072041869163513\n",
      "epoch: 10 step: 534, loss is 0.052485205233097076\n",
      "epoch: 10 step: 535, loss is 0.0314144566655159\n",
      "epoch: 10 step: 536, loss is 0.08004886656999588\n",
      "epoch: 10 step: 537, loss is 0.01990007609128952\n",
      "epoch: 10 step: 538, loss is 0.08238521963357925\n",
      "epoch: 10 step: 539, loss is 0.014778407290577888\n",
      "epoch: 10 step: 540, loss is 0.020882900804281235\n",
      "epoch: 10 step: 541, loss is 0.053555525839328766\n",
      "epoch: 10 step: 542, loss is 0.027656055986881256\n",
      "epoch: 10 step: 543, loss is 0.021043963730335236\n",
      "epoch: 10 step: 544, loss is 0.06106854975223541\n",
      "epoch: 10 step: 545, loss is 0.06675413250923157\n",
      "epoch: 10 step: 546, loss is 0.07499425113201141\n",
      "epoch: 10 step: 547, loss is 0.04060696065425873\n",
      "epoch: 10 step: 548, loss is 0.10138276219367981\n",
      "epoch: 10 step: 549, loss is 0.10404627025127411\n",
      "epoch: 10 step: 550, loss is 0.030762318521738052\n",
      "epoch: 10 step: 551, loss is 0.0592670701444149\n",
      "epoch: 10 step: 552, loss is 0.08322275429964066\n",
      "epoch: 10 step: 553, loss is 0.021807491779327393\n",
      "epoch: 10 step: 554, loss is 0.02624082751572132\n",
      "epoch: 10 step: 555, loss is 0.1898195594549179\n",
      "epoch: 10 step: 556, loss is 0.04045572504401207\n",
      "epoch: 10 step: 557, loss is 0.04026813432574272\n",
      "epoch: 10 step: 558, loss is 0.1318967640399933\n",
      "epoch: 10 step: 559, loss is 0.04765968397259712\n",
      "epoch: 10 step: 560, loss is 0.02803795412182808\n",
      "epoch: 10 step: 561, loss is 0.07557984441518784\n",
      "epoch: 10 step: 562, loss is 0.028157686814665794\n",
      "epoch: 10 step: 563, loss is 0.11268787086009979\n",
      "epoch: 10 step: 564, loss is 0.02911701239645481\n",
      "epoch: 10 step: 565, loss is 0.03396284207701683\n",
      "epoch: 10 step: 566, loss is 0.048787184059619904\n",
      "epoch: 10 step: 567, loss is 0.07506145536899567\n",
      "epoch: 10 step: 568, loss is 0.08509784191846848\n",
      "epoch: 10 step: 569, loss is 0.06384021788835526\n",
      "epoch: 10 step: 570, loss is 0.12913191318511963\n",
      "epoch: 10 step: 571, loss is 0.02603233978152275\n",
      "epoch: 10 step: 572, loss is 0.04880724474787712\n",
      "epoch: 10 step: 573, loss is 0.031820617616176605\n",
      "epoch: 10 step: 574, loss is 0.01328528393059969\n",
      "epoch: 10 step: 575, loss is 0.114048071205616\n",
      "epoch: 10 step: 576, loss is 0.05835995450615883\n",
      "epoch: 10 step: 577, loss is 0.08968254178762436\n",
      "epoch: 10 step: 578, loss is 0.01813112013041973\n",
      "epoch: 10 step: 579, loss is 0.02447020821273327\n",
      "epoch: 10 step: 580, loss is 0.060820285230875015\n",
      "epoch: 10 step: 581, loss is 0.023412220180034637\n",
      "epoch: 10 step: 582, loss is 0.13501685857772827\n",
      "epoch: 10 step: 583, loss is 0.14264337718486786\n",
      "epoch: 10 step: 584, loss is 0.02627457119524479\n",
      "epoch: 10 step: 585, loss is 0.12077789753675461\n",
      "epoch: 10 step: 586, loss is 0.028909744694828987\n",
      "epoch: 10 step: 587, loss is 0.00923208985477686\n",
      "epoch: 10 step: 588, loss is 0.03024466335773468\n",
      "epoch: 10 step: 589, loss is 0.20344828069210052\n",
      "epoch: 10 step: 590, loss is 0.08056359738111496\n",
      "epoch: 10 step: 591, loss is 0.03357569873332977\n",
      "epoch: 10 step: 592, loss is 0.011345760896801949\n",
      "epoch: 10 step: 593, loss is 0.04979797080159187\n",
      "epoch: 10 step: 594, loss is 0.06637046486139297\n",
      "epoch: 10 step: 595, loss is 0.03390224277973175\n",
      "epoch: 10 step: 596, loss is 0.02638092264533043\n",
      "epoch: 10 step: 597, loss is 0.023171834647655487\n",
      "epoch: 10 step: 598, loss is 0.010971182957291603\n",
      "epoch: 10 step: 599, loss is 0.0654384046792984\n",
      "epoch: 10 step: 600, loss is 0.05292566120624542\n",
      "epoch: 10 step: 601, loss is 0.07729209959506989\n",
      "epoch: 10 step: 602, loss is 0.03651764988899231\n",
      "epoch: 10 step: 603, loss is 0.08469226211309433\n",
      "epoch: 10 step: 604, loss is 0.017253916710615158\n",
      "epoch: 10 step: 605, loss is 0.015680599957704544\n",
      "epoch: 10 step: 606, loss is 0.07122930884361267\n",
      "epoch: 10 step: 607, loss is 0.0463375449180603\n",
      "epoch: 10 step: 608, loss is 0.07337471097707748\n",
      "epoch: 10 step: 609, loss is 0.026614584028720856\n",
      "epoch: 10 step: 610, loss is 0.008559939451515675\n",
      "epoch: 10 step: 611, loss is 0.04362323880195618\n",
      "epoch: 10 step: 612, loss is 0.04645683616399765\n",
      "epoch: 10 step: 613, loss is 0.02601914294064045\n",
      "epoch: 10 step: 614, loss is 0.12538860738277435\n",
      "epoch: 10 step: 615, loss is 0.10435809940099716\n",
      "epoch: 10 step: 616, loss is 0.12624987959861755\n",
      "epoch: 10 step: 617, loss is 0.14829352498054504\n",
      "epoch: 10 step: 618, loss is 0.01584249548614025\n",
      "epoch: 10 step: 619, loss is 0.07254298776388168\n",
      "epoch: 10 step: 620, loss is 0.08454640954732895\n",
      "epoch: 10 step: 621, loss is 0.12341655790805817\n",
      "epoch: 10 step: 622, loss is 0.09965179860591888\n",
      "epoch: 10 step: 623, loss is 0.258558452129364\n",
      "epoch: 10 step: 624, loss is 0.027599485591053963\n",
      "epoch: 10 step: 625, loss is 0.014374288730323315\n",
      "epoch: 10 step: 626, loss is 0.01582808792591095\n",
      "epoch: 10 step: 627, loss is 0.09401208162307739\n",
      "epoch: 10 step: 628, loss is 0.08698657155036926\n",
      "epoch: 10 step: 629, loss is 0.05715232715010643\n",
      "epoch: 10 step: 630, loss is 0.06842895597219467\n",
      "epoch: 10 step: 631, loss is 0.14688946306705475\n",
      "epoch: 10 step: 632, loss is 0.049627773463726044\n",
      "epoch: 10 step: 633, loss is 0.14247633516788483\n",
      "epoch: 10 step: 634, loss is 0.04724929481744766\n",
      "epoch: 10 step: 635, loss is 0.1420958936214447\n",
      "epoch: 10 step: 636, loss is 0.02622952312231064\n",
      "epoch: 10 step: 637, loss is 0.024795152246952057\n",
      "epoch: 10 step: 638, loss is 0.051649272441864014\n",
      "epoch: 10 step: 639, loss is 0.022466082125902176\n",
      "epoch: 10 step: 640, loss is 0.06989651918411255\n",
      "epoch: 10 step: 641, loss is 0.1677139401435852\n",
      "epoch: 10 step: 642, loss is 0.09138229489326477\n",
      "epoch: 10 step: 643, loss is 0.0788983628153801\n",
      "epoch: 10 step: 644, loss is 0.19010281562805176\n",
      "epoch: 10 step: 645, loss is 0.04790337011218071\n",
      "epoch: 10 step: 646, loss is 0.07038410007953644\n",
      "epoch: 10 step: 647, loss is 0.07551725208759308\n",
      "epoch: 10 step: 648, loss is 0.15045179426670074\n",
      "epoch: 10 step: 649, loss is 0.11941444128751755\n",
      "epoch: 10 step: 650, loss is 0.033642787486314774\n",
      "epoch: 10 step: 651, loss is 0.05875759944319725\n",
      "epoch: 10 step: 652, loss is 0.06996773183345795\n",
      "epoch: 10 step: 653, loss is 0.07368484884500504\n",
      "epoch: 10 step: 654, loss is 0.1017652377486229\n",
      "epoch: 10 step: 655, loss is 0.018020497635006905\n",
      "epoch: 10 step: 656, loss is 0.04785889387130737\n",
      "epoch: 10 step: 657, loss is 0.11374356597661972\n",
      "epoch: 10 step: 658, loss is 0.07150977104902267\n",
      "epoch: 10 step: 659, loss is 0.054558128118515015\n",
      "epoch: 10 step: 660, loss is 0.0405169241130352\n",
      "epoch: 10 step: 661, loss is 0.026232346892356873\n",
      "epoch: 10 step: 662, loss is 0.041858162730932236\n",
      "epoch: 10 step: 663, loss is 0.044179972261190414\n",
      "epoch: 10 step: 664, loss is 0.06513426452875137\n",
      "epoch: 10 step: 665, loss is 0.01979788951575756\n",
      "epoch: 10 step: 666, loss is 0.09603824466466904\n",
      "epoch: 10 step: 667, loss is 0.1534111052751541\n",
      "epoch: 10 step: 668, loss is 0.20756468176841736\n",
      "epoch: 10 step: 669, loss is 0.08339794725179672\n",
      "epoch: 10 step: 670, loss is 0.04520846903324127\n",
      "epoch: 10 step: 671, loss is 0.07672367990016937\n",
      "epoch: 10 step: 672, loss is 0.023613056167960167\n",
      "epoch: 10 step: 673, loss is 0.12443294376134872\n",
      "epoch: 10 step: 674, loss is 0.03995818644762039\n",
      "epoch: 10 step: 675, loss is 0.04709615185856819\n",
      "epoch: 10 step: 676, loss is 0.02059076726436615\n",
      "epoch: 10 step: 677, loss is 0.08421605825424194\n",
      "epoch: 10 step: 678, loss is 0.05537303164601326\n",
      "epoch: 10 step: 679, loss is 0.050313375890254974\n",
      "epoch: 10 step: 680, loss is 0.05868620425462723\n",
      "epoch: 10 step: 681, loss is 0.027374589815735817\n",
      "epoch: 10 step: 682, loss is 0.12654152512550354\n",
      "epoch: 10 step: 683, loss is 0.048209983855485916\n",
      "epoch: 10 step: 684, loss is 0.0310447309166193\n",
      "epoch: 10 step: 685, loss is 0.03783459961414337\n",
      "epoch: 10 step: 686, loss is 0.04433707147836685\n",
      "epoch: 10 step: 687, loss is 0.04849210008978844\n",
      "epoch: 10 step: 688, loss is 0.02554730325937271\n",
      "epoch: 10 step: 689, loss is 0.06261377781629562\n",
      "epoch: 10 step: 690, loss is 0.029279902577400208\n",
      "epoch: 10 step: 691, loss is 0.08324087411165237\n",
      "epoch: 10 step: 692, loss is 0.05869371443986893\n",
      "epoch: 10 step: 693, loss is 0.08158201724290848\n",
      "epoch: 10 step: 694, loss is 0.06894092261791229\n",
      "epoch: 10 step: 695, loss is 0.1072961837053299\n",
      "epoch: 10 step: 696, loss is 0.06438891589641571\n",
      "epoch: 10 step: 697, loss is 0.07037696987390518\n",
      "epoch: 10 step: 698, loss is 0.14185790717601776\n",
      "epoch: 10 step: 699, loss is 0.04133709892630577\n",
      "epoch: 10 step: 700, loss is 0.041950523853302\n",
      "epoch: 10 step: 701, loss is 0.06250162422657013\n",
      "epoch: 10 step: 702, loss is 0.09695295244455338\n",
      "epoch: 10 step: 703, loss is 0.07160824537277222\n",
      "epoch: 10 step: 704, loss is 0.04822566360235214\n",
      "epoch: 10 step: 705, loss is 0.1539454609155655\n",
      "epoch: 10 step: 706, loss is 0.015348718501627445\n",
      "epoch: 10 step: 707, loss is 0.08157821744680405\n",
      "epoch: 10 step: 708, loss is 0.06739409267902374\n",
      "epoch: 10 step: 709, loss is 0.02390529401600361\n",
      "epoch: 10 step: 710, loss is 0.015094611793756485\n",
      "epoch: 10 step: 711, loss is 0.07428222894668579\n",
      "epoch: 10 step: 712, loss is 0.03576631844043732\n",
      "epoch: 10 step: 713, loss is 0.021988049149513245\n",
      "epoch: 10 step: 714, loss is 0.023616842925548553\n",
      "epoch: 10 step: 715, loss is 0.17336152493953705\n",
      "epoch: 10 step: 716, loss is 0.04092074930667877\n",
      "epoch: 10 step: 717, loss is 0.045923519879579544\n",
      "epoch: 10 step: 718, loss is 0.10224850475788116\n",
      "epoch: 10 step: 719, loss is 0.05144210904836655\n",
      "epoch: 10 step: 720, loss is 0.06520341336727142\n",
      "epoch: 10 step: 721, loss is 0.14843258261680603\n",
      "epoch: 10 step: 722, loss is 0.06394349038600922\n",
      "epoch: 10 step: 723, loss is 0.09734069555997849\n",
      "epoch: 10 step: 724, loss is 0.10107806324958801\n",
      "epoch: 10 step: 725, loss is 0.12540893256664276\n",
      "epoch: 10 step: 726, loss is 0.04230931028723717\n",
      "epoch: 10 step: 727, loss is 0.12922228872776031\n",
      "epoch: 10 step: 728, loss is 0.08017268776893616\n",
      "epoch: 10 step: 729, loss is 0.05777132138609886\n",
      "epoch: 10 step: 730, loss is 0.07229471951723099\n",
      "epoch: 10 step: 731, loss is 0.06727860867977142\n",
      "epoch: 10 step: 732, loss is 0.025585293769836426\n",
      "epoch: 10 step: 733, loss is 0.06347651779651642\n",
      "epoch: 10 step: 734, loss is 0.025589074939489365\n",
      "epoch: 10 step: 735, loss is 0.05956145375967026\n",
      "epoch: 10 step: 736, loss is 0.04721074923872948\n",
      "epoch: 10 step: 737, loss is 0.048289887607097626\n",
      "epoch: 10 step: 738, loss is 0.016078535467386246\n",
      "epoch: 10 step: 739, loss is 0.027458813041448593\n",
      "epoch: 10 step: 740, loss is 0.025803785771131516\n",
      "epoch: 10 step: 741, loss is 0.03169762343168259\n",
      "epoch: 10 step: 742, loss is 0.013838495127856731\n",
      "epoch: 10 step: 743, loss is 0.040536750108003616\n",
      "epoch: 10 step: 744, loss is 0.09465239197015762\n",
      "epoch: 10 step: 745, loss is 0.08115413039922714\n",
      "epoch: 10 step: 746, loss is 0.0050549753941595554\n",
      "epoch: 10 step: 747, loss is 0.01704317145049572\n",
      "epoch: 10 step: 748, loss is 0.022839553654193878\n",
      "epoch: 10 step: 749, loss is 0.031686313450336456\n",
      "epoch: 10 step: 750, loss is 0.20455537736415863\n",
      "epoch: 10 step: 751, loss is 0.027392534539103508\n",
      "epoch: 10 step: 752, loss is 0.10962233692407608\n",
      "epoch: 10 step: 753, loss is 0.03269250690937042\n",
      "epoch: 10 step: 754, loss is 0.013554311357438564\n",
      "epoch: 10 step: 755, loss is 0.020032238215208054\n",
      "epoch: 10 step: 756, loss is 0.05110793933272362\n",
      "epoch: 10 step: 757, loss is 0.07047322392463684\n",
      "epoch: 10 step: 758, loss is 0.08522526919841766\n",
      "epoch: 10 step: 759, loss is 0.03912169486284256\n",
      "epoch: 10 step: 760, loss is 0.22086751461029053\n",
      "epoch: 10 step: 761, loss is 0.06401041150093079\n",
      "epoch: 10 step: 762, loss is 0.0935930386185646\n",
      "epoch: 10 step: 763, loss is 0.03402341529726982\n",
      "epoch: 10 step: 764, loss is 0.028709039092063904\n",
      "epoch: 10 step: 765, loss is 0.03382648527622223\n",
      "epoch: 10 step: 766, loss is 0.09879516810178757\n",
      "epoch: 10 step: 767, loss is 0.013279005885124207\n",
      "epoch: 10 step: 768, loss is 0.0643313080072403\n",
      "epoch: 10 step: 769, loss is 0.04487600177526474\n",
      "epoch: 10 step: 770, loss is 0.10137511789798737\n",
      "epoch: 10 step: 771, loss is 0.04225481301546097\n",
      "epoch: 10 step: 772, loss is 0.08280230313539505\n",
      "epoch: 10 step: 773, loss is 0.2449522316455841\n",
      "epoch: 10 step: 774, loss is 0.04685509204864502\n",
      "epoch: 10 step: 775, loss is 0.07281073927879333\n",
      "epoch: 10 step: 776, loss is 0.08699693530797958\n",
      "epoch: 10 step: 777, loss is 0.08702753484249115\n",
      "epoch: 10 step: 778, loss is 0.07623838633298874\n",
      "epoch: 10 step: 779, loss is 0.02378729172050953\n",
      "epoch: 10 step: 780, loss is 0.016813449561595917\n",
      "epoch: 10 step: 781, loss is 0.14092780649662018\n",
      "epoch: 10 step: 782, loss is 0.050811078399419785\n",
      "epoch: 10 step: 783, loss is 0.01408638246357441\n",
      "epoch: 10 step: 784, loss is 0.07400694489479065\n",
      "epoch: 10 step: 785, loss is 0.13526856899261475\n",
      "epoch: 10 step: 786, loss is 0.04901580512523651\n",
      "epoch: 10 step: 787, loss is 0.027378469705581665\n",
      "epoch: 10 step: 788, loss is 0.09981537610292435\n",
      "epoch: 10 step: 789, loss is 0.07051315903663635\n",
      "epoch: 10 step: 790, loss is 0.018516046926379204\n",
      "epoch: 10 step: 791, loss is 0.016076885163784027\n",
      "epoch: 10 step: 792, loss is 0.06757786870002747\n",
      "epoch: 10 step: 793, loss is 0.11717116832733154\n",
      "epoch: 10 step: 794, loss is 0.035273220390081406\n",
      "epoch: 10 step: 795, loss is 0.06490675359964371\n",
      "epoch: 10 step: 796, loss is 0.13495928049087524\n",
      "epoch: 10 step: 797, loss is 0.1644667387008667\n",
      "epoch: 10 step: 798, loss is 0.2249082624912262\n",
      "epoch: 10 step: 799, loss is 0.06374628841876984\n",
      "epoch: 10 step: 800, loss is 0.0962512269616127\n",
      "epoch: 10 step: 801, loss is 0.0941765159368515\n",
      "epoch: 10 step: 802, loss is 0.07923121750354767\n",
      "epoch: 10 step: 803, loss is 0.0778844878077507\n",
      "epoch: 10 step: 804, loss is 0.03522643819451332\n",
      "epoch: 10 step: 805, loss is 0.08403778821229935\n",
      "epoch: 10 step: 806, loss is 0.03331037610769272\n",
      "epoch: 10 step: 807, loss is 0.05221940204501152\n",
      "epoch: 10 step: 808, loss is 0.06219954416155815\n",
      "epoch: 10 step: 809, loss is 0.056528836488723755\n",
      "epoch: 10 step: 810, loss is 0.12789307534694672\n",
      "epoch: 10 step: 811, loss is 0.13287289440631866\n",
      "epoch: 10 step: 812, loss is 0.06198931112885475\n",
      "epoch: 10 step: 813, loss is 0.09403159469366074\n",
      "epoch: 10 step: 814, loss is 0.14547811448574066\n",
      "epoch: 10 step: 815, loss is 0.05865625664591789\n",
      "epoch: 10 step: 816, loss is 0.03939317911863327\n",
      "epoch: 10 step: 817, loss is 0.1231827586889267\n",
      "epoch: 10 step: 818, loss is 0.06536159664392471\n",
      "epoch: 10 step: 819, loss is 0.10108796507120132\n",
      "epoch: 10 step: 820, loss is 0.06103454530239105\n",
      "epoch: 10 step: 821, loss is 0.0728001669049263\n",
      "epoch: 10 step: 822, loss is 0.13243521749973297\n",
      "epoch: 10 step: 823, loss is 0.20346853137016296\n",
      "epoch: 10 step: 824, loss is 0.10367865115404129\n",
      "epoch: 10 step: 825, loss is 0.048840831965208054\n",
      "epoch: 10 step: 826, loss is 0.05330969765782356\n",
      "epoch: 10 step: 827, loss is 0.024610716849565506\n",
      "epoch: 10 step: 828, loss is 0.09688808768987656\n",
      "epoch: 10 step: 829, loss is 0.006057268008589745\n",
      "epoch: 10 step: 830, loss is 0.042394790798425674\n",
      "epoch: 10 step: 831, loss is 0.014880367554724216\n",
      "epoch: 10 step: 832, loss is 0.02417043410241604\n",
      "epoch: 10 step: 833, loss is 0.05467524006962776\n",
      "epoch: 10 step: 834, loss is 0.03224366903305054\n",
      "epoch: 10 step: 835, loss is 0.09920264780521393\n",
      "epoch: 10 step: 836, loss is 0.03831346705555916\n",
      "epoch: 10 step: 837, loss is 0.039015691727399826\n",
      "epoch: 10 step: 838, loss is 0.10089565068483353\n",
      "epoch: 10 step: 839, loss is 0.08407919108867645\n",
      "epoch: 10 step: 840, loss is 0.0656467080116272\n",
      "epoch: 10 step: 841, loss is 0.0227818563580513\n",
      "epoch: 10 step: 842, loss is 0.05187254771590233\n",
      "epoch: 10 step: 843, loss is 0.01827285625040531\n",
      "epoch: 10 step: 844, loss is 0.07953856885433197\n",
      "epoch: 10 step: 845, loss is 0.0581308975815773\n",
      "epoch: 10 step: 846, loss is 0.012042205780744553\n",
      "epoch: 10 step: 847, loss is 0.0371890589594841\n",
      "epoch: 10 step: 848, loss is 0.044864457100629807\n",
      "epoch: 10 step: 849, loss is 0.046319302171468735\n",
      "epoch: 10 step: 850, loss is 0.1613687425851822\n",
      "epoch: 10 step: 851, loss is 0.07198838144540787\n",
      "epoch: 10 step: 852, loss is 0.05073975399136543\n",
      "epoch: 10 step: 853, loss is 0.009883075021207333\n",
      "epoch: 10 step: 854, loss is 0.02951337955892086\n",
      "epoch: 10 step: 855, loss is 0.032990965992212296\n",
      "epoch: 10 step: 856, loss is 0.06088724732398987\n",
      "epoch: 10 step: 857, loss is 0.022110087797045708\n",
      "epoch: 10 step: 858, loss is 0.0803423821926117\n",
      "epoch: 10 step: 859, loss is 0.06357288360595703\n",
      "epoch: 10 step: 860, loss is 0.07004575431346893\n",
      "epoch: 10 step: 861, loss is 0.03813008591532707\n",
      "epoch: 10 step: 862, loss is 0.03352774679660797\n",
      "epoch: 10 step: 863, loss is 0.11428169906139374\n",
      "epoch: 10 step: 864, loss is 0.05308156833052635\n",
      "epoch: 10 step: 865, loss is 0.09918786585330963\n",
      "epoch: 10 step: 866, loss is 0.016087310388684273\n",
      "epoch: 10 step: 867, loss is 0.09291108697652817\n",
      "epoch: 10 step: 868, loss is 0.0459083616733551\n",
      "epoch: 10 step: 869, loss is 0.018592890352010727\n",
      "epoch: 10 step: 870, loss is 0.06293598562479019\n",
      "epoch: 10 step: 871, loss is 0.11453632265329361\n",
      "epoch: 10 step: 872, loss is 0.04526253044605255\n",
      "epoch: 10 step: 873, loss is 0.009409784339368343\n",
      "epoch: 10 step: 874, loss is 0.0872872844338417\n",
      "epoch: 10 step: 875, loss is 0.01334682758897543\n",
      "epoch: 10 step: 876, loss is 0.0900144949555397\n",
      "epoch: 10 step: 877, loss is 0.010655254125595093\n",
      "epoch: 10 step: 878, loss is 0.04488968849182129\n",
      "epoch: 10 step: 879, loss is 0.0807291641831398\n",
      "epoch: 10 step: 880, loss is 0.017136944457888603\n",
      "epoch: 10 step: 881, loss is 0.07692974805831909\n",
      "epoch: 10 step: 882, loss is 0.07309789210557938\n",
      "epoch: 10 step: 883, loss is 0.03455003350973129\n",
      "epoch: 10 step: 884, loss is 0.08574997633695602\n",
      "epoch: 10 step: 885, loss is 0.047536496073007584\n",
      "epoch: 10 step: 886, loss is 0.02798878774046898\n",
      "epoch: 10 step: 887, loss is 0.03014441579580307\n",
      "epoch: 10 step: 888, loss is 0.02851027622818947\n",
      "epoch: 10 step: 889, loss is 0.03774423152208328\n",
      "epoch: 10 step: 890, loss is 0.07087401300668716\n",
      "epoch: 10 step: 891, loss is 0.06277558952569962\n",
      "epoch: 10 step: 892, loss is 0.04859612509608269\n",
      "epoch: 10 step: 893, loss is 0.05621492490172386\n",
      "epoch: 10 step: 894, loss is 0.020241480320692062\n",
      "epoch: 10 step: 895, loss is 0.030360767617821693\n",
      "epoch: 10 step: 896, loss is 0.012066798284649849\n",
      "epoch: 10 step: 897, loss is 0.06987261027097702\n",
      "epoch: 10 step: 898, loss is 0.08236493170261383\n",
      "epoch: 10 step: 899, loss is 0.02153193950653076\n",
      "epoch: 10 step: 900, loss is 0.12676241993904114\n",
      "epoch: 10 step: 901, loss is 0.09195080399513245\n",
      "epoch: 10 step: 902, loss is 0.03294831141829491\n",
      "epoch: 10 step: 903, loss is 0.012636198662221432\n",
      "epoch: 10 step: 904, loss is 0.05329592525959015\n",
      "epoch: 10 step: 905, loss is 0.027364008128643036\n",
      "epoch: 10 step: 906, loss is 0.01330231036990881\n",
      "epoch: 10 step: 907, loss is 0.011054137721657753\n",
      "epoch: 10 step: 908, loss is 0.028957238420844078\n",
      "epoch: 10 step: 909, loss is 0.04776329919695854\n",
      "epoch: 10 step: 910, loss is 0.07584841549396515\n",
      "epoch: 10 step: 911, loss is 0.0785563662648201\n",
      "epoch: 10 step: 912, loss is 0.10002268105745316\n",
      "epoch: 10 step: 913, loss is 0.05814104154706001\n",
      "epoch: 10 step: 914, loss is 0.004882026929408312\n",
      "epoch: 10 step: 915, loss is 0.04763881489634514\n",
      "epoch: 10 step: 916, loss is 0.09207796305418015\n",
      "epoch: 10 step: 917, loss is 0.04120975360274315\n",
      "epoch: 10 step: 918, loss is 0.012353555299341679\n",
      "epoch: 10 step: 919, loss is 0.17587728798389435\n",
      "epoch: 10 step: 920, loss is 0.040421679615974426\n",
      "epoch: 10 step: 921, loss is 0.05024000257253647\n",
      "epoch: 10 step: 922, loss is 0.10297057032585144\n",
      "epoch: 10 step: 923, loss is 0.022899195551872253\n",
      "epoch: 10 step: 924, loss is 0.06539727747440338\n",
      "epoch: 10 step: 925, loss is 0.03820822015404701\n",
      "epoch: 10 step: 926, loss is 0.2349693924188614\n",
      "epoch: 10 step: 927, loss is 0.09094748646020889\n",
      "epoch: 10 step: 928, loss is 0.21408960223197937\n",
      "epoch: 10 step: 929, loss is 0.08921243995428085\n",
      "epoch: 10 step: 930, loss is 0.035963162779808044\n",
      "epoch: 10 step: 931, loss is 0.027224408462643623\n",
      "epoch: 10 step: 932, loss is 0.07293880730867386\n",
      "epoch: 10 step: 933, loss is 0.0070382519625127316\n",
      "epoch: 10 step: 934, loss is 0.06795872002840042\n",
      "epoch: 10 step: 935, loss is 0.05618217587471008\n",
      "epoch: 10 step: 936, loss is 0.05566033720970154\n",
      "epoch: 10 step: 937, loss is 0.033013779670000076\n",
      "epoch: 11 step: 1, loss is 0.057615235447883606\n",
      "epoch: 11 step: 2, loss is 0.005816382355988026\n",
      "epoch: 11 step: 3, loss is 0.06323584914207458\n",
      "epoch: 11 step: 4, loss is 0.004492185078561306\n",
      "epoch: 11 step: 5, loss is 0.028270825743675232\n",
      "epoch: 11 step: 6, loss is 0.007169206626713276\n",
      "epoch: 11 step: 7, loss is 0.04847756028175354\n",
      "epoch: 11 step: 8, loss is 0.04969567432999611\n",
      "epoch: 11 step: 9, loss is 0.09386955946683884\n",
      "epoch: 11 step: 10, loss is 0.18566684424877167\n",
      "epoch: 11 step: 11, loss is 0.028128592297434807\n",
      "epoch: 11 step: 12, loss is 0.017420094460248947\n",
      "epoch: 11 step: 13, loss is 0.032875485718250275\n",
      "epoch: 11 step: 14, loss is 0.074310801923275\n",
      "epoch: 11 step: 15, loss is 0.04449530318379402\n",
      "epoch: 11 step: 16, loss is 0.007768554147332907\n",
      "epoch: 11 step: 17, loss is 0.02930501475930214\n",
      "epoch: 11 step: 18, loss is 0.01571316085755825\n",
      "epoch: 11 step: 19, loss is 0.026610778644680977\n",
      "epoch: 11 step: 20, loss is 0.02556433342397213\n",
      "epoch: 11 step: 21, loss is 0.05375966802239418\n",
      "epoch: 11 step: 22, loss is 0.09384798258543015\n",
      "epoch: 11 step: 23, loss is 0.06600375473499298\n",
      "epoch: 11 step: 24, loss is 0.07192159444093704\n",
      "epoch: 11 step: 25, loss is 0.02084004320204258\n",
      "epoch: 11 step: 26, loss is 0.019869187846779823\n",
      "epoch: 11 step: 27, loss is 0.015269163995981216\n",
      "epoch: 11 step: 28, loss is 0.01331550907343626\n",
      "epoch: 11 step: 29, loss is 0.035761091858148575\n",
      "epoch: 11 step: 30, loss is 0.032390475273132324\n",
      "epoch: 11 step: 31, loss is 0.01000626478344202\n",
      "epoch: 11 step: 32, loss is 0.19058139622211456\n",
      "epoch: 11 step: 33, loss is 0.05036472529172897\n",
      "epoch: 11 step: 34, loss is 0.034958675503730774\n",
      "epoch: 11 step: 35, loss is 0.06481629610061646\n",
      "epoch: 11 step: 36, loss is 0.041180603206157684\n",
      "epoch: 11 step: 37, loss is 0.045781802386045456\n",
      "epoch: 11 step: 38, loss is 0.010675214231014252\n",
      "epoch: 11 step: 39, loss is 0.01860174909234047\n",
      "epoch: 11 step: 40, loss is 0.037587933242321014\n",
      "epoch: 11 step: 41, loss is 0.023249754682183266\n",
      "epoch: 11 step: 42, loss is 0.12279004603624344\n",
      "epoch: 11 step: 43, loss is 0.031783752143383026\n",
      "epoch: 11 step: 44, loss is 0.004973378498107195\n",
      "epoch: 11 step: 45, loss is 0.033614594489336014\n",
      "epoch: 11 step: 46, loss is 0.030451856553554535\n",
      "epoch: 11 step: 47, loss is 0.024282600730657578\n",
      "epoch: 11 step: 48, loss is 0.012369408272206783\n",
      "epoch: 11 step: 49, loss is 0.023010708391666412\n",
      "epoch: 11 step: 50, loss is 0.050433509051799774\n",
      "epoch: 11 step: 51, loss is 0.019534830003976822\n",
      "epoch: 11 step: 52, loss is 0.053683437407016754\n",
      "epoch: 11 step: 53, loss is 0.017314640805125237\n",
      "epoch: 11 step: 54, loss is 0.00825402420014143\n",
      "epoch: 11 step: 55, loss is 0.011349400505423546\n",
      "epoch: 11 step: 56, loss is 0.017446715384721756\n",
      "epoch: 11 step: 57, loss is 0.014162743464112282\n",
      "epoch: 11 step: 58, loss is 0.027602648362517357\n",
      "epoch: 11 step: 59, loss is 0.062233470380306244\n",
      "epoch: 11 step: 60, loss is 0.11907728016376495\n",
      "epoch: 11 step: 61, loss is 0.02839401550590992\n",
      "epoch: 11 step: 62, loss is 0.07324643433094025\n",
      "epoch: 11 step: 63, loss is 0.03203517198562622\n",
      "epoch: 11 step: 64, loss is 0.032311517745256424\n",
      "epoch: 11 step: 65, loss is 0.016627343371510506\n",
      "epoch: 11 step: 66, loss is 0.018405618146061897\n",
      "epoch: 11 step: 67, loss is 0.006562404800206423\n",
      "epoch: 11 step: 68, loss is 0.0935758426785469\n",
      "epoch: 11 step: 69, loss is 0.030665500089526176\n",
      "epoch: 11 step: 70, loss is 0.04348251223564148\n",
      "epoch: 11 step: 71, loss is 0.0504334531724453\n",
      "epoch: 11 step: 72, loss is 0.011146477423608303\n",
      "epoch: 11 step: 73, loss is 0.0604337714612484\n",
      "epoch: 11 step: 74, loss is 0.026498809456825256\n",
      "epoch: 11 step: 75, loss is 0.005333905573934317\n",
      "epoch: 11 step: 76, loss is 0.01175065990537405\n",
      "epoch: 11 step: 77, loss is 0.04030204564332962\n",
      "epoch: 11 step: 78, loss is 0.009711100719869137\n",
      "epoch: 11 step: 79, loss is 0.14257782697677612\n",
      "epoch: 11 step: 80, loss is 0.015487220138311386\n",
      "epoch: 11 step: 81, loss is 0.037928350269794464\n",
      "epoch: 11 step: 82, loss is 0.04300233721733093\n",
      "epoch: 11 step: 83, loss is 0.002234288025647402\n",
      "epoch: 11 step: 84, loss is 0.014609579928219318\n",
      "epoch: 11 step: 85, loss is 0.0081867640838027\n",
      "epoch: 11 step: 86, loss is 0.020961927250027657\n",
      "epoch: 11 step: 87, loss is 0.05125819519162178\n",
      "epoch: 11 step: 88, loss is 0.02520773559808731\n",
      "epoch: 11 step: 89, loss is 0.11685634404420853\n",
      "epoch: 11 step: 90, loss is 0.02118489518761635\n",
      "epoch: 11 step: 91, loss is 0.03153587505221367\n",
      "epoch: 11 step: 92, loss is 0.02694031037390232\n",
      "epoch: 11 step: 93, loss is 0.014294937252998352\n",
      "epoch: 11 step: 94, loss is 0.06651601195335388\n",
      "epoch: 11 step: 95, loss is 0.028752503916621208\n",
      "epoch: 11 step: 96, loss is 0.007389754522591829\n",
      "epoch: 11 step: 97, loss is 0.07662924379110336\n",
      "epoch: 11 step: 98, loss is 0.0647435188293457\n",
      "epoch: 11 step: 99, loss is 0.09946448355913162\n",
      "epoch: 11 step: 100, loss is 0.00681701023131609\n",
      "epoch: 11 step: 101, loss is 0.02629191055893898\n",
      "epoch: 11 step: 102, loss is 0.12107374519109726\n",
      "epoch: 11 step: 103, loss is 0.0024908410850912333\n",
      "epoch: 11 step: 104, loss is 0.025296365842223167\n",
      "epoch: 11 step: 105, loss is 0.004273140337318182\n",
      "epoch: 11 step: 106, loss is 0.07427379488945007\n",
      "epoch: 11 step: 107, loss is 0.01013947557657957\n",
      "epoch: 11 step: 108, loss is 0.06999260932207108\n",
      "epoch: 11 step: 109, loss is 0.019899217411875725\n",
      "epoch: 11 step: 110, loss is 0.020632551982998848\n",
      "epoch: 11 step: 111, loss is 0.04275920242071152\n",
      "epoch: 11 step: 112, loss is 0.040688466280698776\n",
      "epoch: 11 step: 113, loss is 0.013652866706252098\n",
      "epoch: 11 step: 114, loss is 0.04026331752538681\n",
      "epoch: 11 step: 115, loss is 0.040059104561805725\n",
      "epoch: 11 step: 116, loss is 0.027111902832984924\n",
      "epoch: 11 step: 117, loss is 0.06252995133399963\n",
      "epoch: 11 step: 118, loss is 0.02672339417040348\n",
      "epoch: 11 step: 119, loss is 0.0618738979101181\n",
      "epoch: 11 step: 120, loss is 0.03654199466109276\n",
      "epoch: 11 step: 121, loss is 0.09715791791677475\n",
      "epoch: 11 step: 122, loss is 0.01227983832359314\n",
      "epoch: 11 step: 123, loss is 0.007071885745972395\n",
      "epoch: 11 step: 124, loss is 0.028875017538666725\n",
      "epoch: 11 step: 125, loss is 0.028762010857462883\n",
      "epoch: 11 step: 126, loss is 0.07818540185689926\n",
      "epoch: 11 step: 127, loss is 0.02355770207941532\n",
      "epoch: 11 step: 128, loss is 0.014387883245944977\n",
      "epoch: 11 step: 129, loss is 0.07149863243103027\n",
      "epoch: 11 step: 130, loss is 0.04810573160648346\n",
      "epoch: 11 step: 131, loss is 0.0268552228808403\n",
      "epoch: 11 step: 132, loss is 0.01626061089336872\n",
      "epoch: 11 step: 133, loss is 0.07402631640434265\n",
      "epoch: 11 step: 134, loss is 0.060709547251462936\n",
      "epoch: 11 step: 135, loss is 0.009005329571664333\n",
      "epoch: 11 step: 136, loss is 0.08114976435899734\n",
      "epoch: 11 step: 137, loss is 0.06366975605487823\n",
      "epoch: 11 step: 138, loss is 0.0085806455463171\n",
      "epoch: 11 step: 139, loss is 0.03390486538410187\n",
      "epoch: 11 step: 140, loss is 0.020268639549613\n",
      "epoch: 11 step: 141, loss is 0.01916022039949894\n",
      "epoch: 11 step: 142, loss is 0.041852157562971115\n",
      "epoch: 11 step: 143, loss is 0.030371127650141716\n",
      "epoch: 11 step: 144, loss is 0.08153551071882248\n",
      "epoch: 11 step: 145, loss is 0.017869584262371063\n",
      "epoch: 11 step: 146, loss is 0.019986432045698166\n",
      "epoch: 11 step: 147, loss is 0.03281858563423157\n",
      "epoch: 11 step: 148, loss is 0.022480741143226624\n",
      "epoch: 11 step: 149, loss is 0.009507261216640472\n",
      "epoch: 11 step: 150, loss is 0.0033276258036494255\n",
      "epoch: 11 step: 151, loss is 0.031317003071308136\n",
      "epoch: 11 step: 152, loss is 0.02888820692896843\n",
      "epoch: 11 step: 153, loss is 0.03885031118988991\n",
      "epoch: 11 step: 154, loss is 0.028907084837555885\n",
      "epoch: 11 step: 155, loss is 0.06685923039913177\n",
      "epoch: 11 step: 156, loss is 0.01981503702700138\n",
      "epoch: 11 step: 157, loss is 0.12211132049560547\n",
      "epoch: 11 step: 158, loss is 0.059330105781555176\n",
      "epoch: 11 step: 159, loss is 0.01352198701351881\n",
      "epoch: 11 step: 160, loss is 0.09642583131790161\n",
      "epoch: 11 step: 161, loss is 0.029181748628616333\n",
      "epoch: 11 step: 162, loss is 0.036816421896219254\n",
      "epoch: 11 step: 163, loss is 0.026345422491431236\n",
      "epoch: 11 step: 164, loss is 0.02022949606180191\n",
      "epoch: 11 step: 165, loss is 0.03716752678155899\n",
      "epoch: 11 step: 166, loss is 0.008647150360047817\n",
      "epoch: 11 step: 167, loss is 0.03321995586156845\n",
      "epoch: 11 step: 168, loss is 0.01839117333292961\n",
      "epoch: 11 step: 169, loss is 0.007298329845070839\n",
      "epoch: 11 step: 170, loss is 0.012192322872579098\n",
      "epoch: 11 step: 171, loss is 0.014405484311282635\n",
      "epoch: 11 step: 172, loss is 0.028575381264090538\n",
      "epoch: 11 step: 173, loss is 0.004172461573034525\n",
      "epoch: 11 step: 174, loss is 0.06546252220869064\n",
      "epoch: 11 step: 175, loss is 0.052519477903842926\n",
      "epoch: 11 step: 176, loss is 0.008888189680874348\n",
      "epoch: 11 step: 177, loss is 0.027825381606817245\n",
      "epoch: 11 step: 178, loss is 0.05573144555091858\n",
      "epoch: 11 step: 179, loss is 0.0014488567830994725\n",
      "epoch: 11 step: 180, loss is 0.08308234810829163\n",
      "epoch: 11 step: 181, loss is 0.06863319128751755\n",
      "epoch: 11 step: 182, loss is 0.06697551161050797\n",
      "epoch: 11 step: 183, loss is 0.00694700563326478\n",
      "epoch: 11 step: 184, loss is 0.012520591728389263\n",
      "epoch: 11 step: 185, loss is 0.08814610540866852\n",
      "epoch: 11 step: 186, loss is 0.04335661605000496\n",
      "epoch: 11 step: 187, loss is 0.09482322633266449\n",
      "epoch: 11 step: 188, loss is 0.03272378817200661\n",
      "epoch: 11 step: 189, loss is 0.11439560353755951\n",
      "epoch: 11 step: 190, loss is 0.09858568757772446\n",
      "epoch: 11 step: 191, loss is 0.03601396083831787\n",
      "epoch: 11 step: 192, loss is 0.06723865866661072\n",
      "epoch: 11 step: 193, loss is 0.013635863550007343\n",
      "epoch: 11 step: 194, loss is 0.03486516699194908\n",
      "epoch: 11 step: 195, loss is 0.04015633091330528\n",
      "epoch: 11 step: 196, loss is 0.024968676269054413\n",
      "epoch: 11 step: 197, loss is 0.08410274982452393\n",
      "epoch: 11 step: 198, loss is 0.005458384286612272\n",
      "epoch: 11 step: 199, loss is 0.08143755048513412\n",
      "epoch: 11 step: 200, loss is 0.07519753277301788\n",
      "epoch: 11 step: 201, loss is 0.019529758021235466\n",
      "epoch: 11 step: 202, loss is 0.007272140588611364\n",
      "epoch: 11 step: 203, loss is 0.06698738038539886\n",
      "epoch: 11 step: 204, loss is 0.045804087072610855\n",
      "epoch: 11 step: 205, loss is 0.04293825104832649\n",
      "epoch: 11 step: 206, loss is 0.037851158529520035\n",
      "epoch: 11 step: 207, loss is 0.04796251282095909\n",
      "epoch: 11 step: 208, loss is 0.005552233196794987\n",
      "epoch: 11 step: 209, loss is 0.13851450383663177\n",
      "epoch: 11 step: 210, loss is 0.011492536403238773\n",
      "epoch: 11 step: 211, loss is 0.018458819016814232\n",
      "epoch: 11 step: 212, loss is 0.007632171269506216\n",
      "epoch: 11 step: 213, loss is 0.019964594393968582\n",
      "epoch: 11 step: 214, loss is 0.03751743957400322\n",
      "epoch: 11 step: 215, loss is 0.0116129070520401\n",
      "epoch: 11 step: 216, loss is 0.04290083423256874\n",
      "epoch: 11 step: 217, loss is 0.04673384130001068\n",
      "epoch: 11 step: 218, loss is 0.005034739151597023\n",
      "epoch: 11 step: 219, loss is 0.016685206443071365\n",
      "epoch: 11 step: 220, loss is 0.059880953282117844\n",
      "epoch: 11 step: 221, loss is 0.05256287008523941\n",
      "epoch: 11 step: 222, loss is 0.01653168350458145\n",
      "epoch: 11 step: 223, loss is 0.0428701750934124\n",
      "epoch: 11 step: 224, loss is 0.018692852929234505\n",
      "epoch: 11 step: 225, loss is 0.01837703213095665\n",
      "epoch: 11 step: 226, loss is 0.05686510354280472\n",
      "epoch: 11 step: 227, loss is 0.046516213566064835\n",
      "epoch: 11 step: 228, loss is 0.03094233199954033\n",
      "epoch: 11 step: 229, loss is 0.11339377611875534\n",
      "epoch: 11 step: 230, loss is 0.02433895133435726\n",
      "epoch: 11 step: 231, loss is 0.01855628751218319\n",
      "epoch: 11 step: 232, loss is 0.02805682271718979\n",
      "epoch: 11 step: 233, loss is 0.025585325434803963\n",
      "epoch: 11 step: 234, loss is 0.020964153110980988\n",
      "epoch: 11 step: 235, loss is 0.0780566930770874\n",
      "epoch: 11 step: 236, loss is 0.027562778443098068\n",
      "epoch: 11 step: 237, loss is 0.10007353127002716\n",
      "epoch: 11 step: 238, loss is 0.02860178053379059\n",
      "epoch: 11 step: 239, loss is 0.2159947156906128\n",
      "epoch: 11 step: 240, loss is 0.021392827853560448\n",
      "epoch: 11 step: 241, loss is 0.07376255095005035\n",
      "epoch: 11 step: 242, loss is 0.010271378792822361\n",
      "epoch: 11 step: 243, loss is 0.03068930096924305\n",
      "epoch: 11 step: 244, loss is 0.05097789689898491\n",
      "epoch: 11 step: 245, loss is 0.04415544494986534\n",
      "epoch: 11 step: 246, loss is 0.03397257998585701\n",
      "epoch: 11 step: 247, loss is 0.012994466349482536\n",
      "epoch: 11 step: 248, loss is 0.06459885090589523\n",
      "epoch: 11 step: 249, loss is 0.03274409472942352\n",
      "epoch: 11 step: 250, loss is 0.10623393952846527\n",
      "epoch: 11 step: 251, loss is 0.01350477710366249\n",
      "epoch: 11 step: 252, loss is 0.07282290607690811\n",
      "epoch: 11 step: 253, loss is 0.027705080807209015\n",
      "epoch: 11 step: 254, loss is 0.00930363591760397\n",
      "epoch: 11 step: 255, loss is 0.05640785023570061\n",
      "epoch: 11 step: 256, loss is 0.006004052236676216\n",
      "epoch: 11 step: 257, loss is 0.08649098128080368\n",
      "epoch: 11 step: 258, loss is 0.04082028940320015\n",
      "epoch: 11 step: 259, loss is 0.005634527187794447\n",
      "epoch: 11 step: 260, loss is 0.016423022374510765\n",
      "epoch: 11 step: 261, loss is 0.05959344655275345\n",
      "epoch: 11 step: 262, loss is 0.036270081996917725\n",
      "epoch: 11 step: 263, loss is 0.0999581515789032\n",
      "epoch: 11 step: 264, loss is 0.012869151309132576\n",
      "epoch: 11 step: 265, loss is 0.041365448385477066\n",
      "epoch: 11 step: 266, loss is 0.024702582508325577\n",
      "epoch: 11 step: 267, loss is 0.0767073780298233\n",
      "epoch: 11 step: 268, loss is 0.013634946197271347\n",
      "epoch: 11 step: 269, loss is 0.03826574608683586\n",
      "epoch: 11 step: 270, loss is 0.012639680877327919\n",
      "epoch: 11 step: 271, loss is 0.10836165398359299\n",
      "epoch: 11 step: 272, loss is 0.08311493694782257\n",
      "epoch: 11 step: 273, loss is 0.05038747936487198\n",
      "epoch: 11 step: 274, loss is 0.0850985124707222\n",
      "epoch: 11 step: 275, loss is 0.08125443756580353\n",
      "epoch: 11 step: 276, loss is 0.027001801878213882\n",
      "epoch: 11 step: 277, loss is 0.05525797978043556\n",
      "epoch: 11 step: 278, loss is 0.05144709348678589\n",
      "epoch: 11 step: 279, loss is 0.07246744632720947\n",
      "epoch: 11 step: 280, loss is 0.04616355895996094\n",
      "epoch: 11 step: 281, loss is 0.0204172283411026\n",
      "epoch: 11 step: 282, loss is 0.06838010996580124\n",
      "epoch: 11 step: 283, loss is 0.041369929909706116\n",
      "epoch: 11 step: 284, loss is 0.10246609896421432\n",
      "epoch: 11 step: 285, loss is 0.043417397886514664\n",
      "epoch: 11 step: 286, loss is 0.1516363024711609\n",
      "epoch: 11 step: 287, loss is 0.01645134948194027\n",
      "epoch: 11 step: 288, loss is 0.017638003453612328\n",
      "epoch: 11 step: 289, loss is 0.15152740478515625\n",
      "epoch: 11 step: 290, loss is 0.15833519399166107\n",
      "epoch: 11 step: 291, loss is 0.06356839835643768\n",
      "epoch: 11 step: 292, loss is 0.03365086019039154\n",
      "epoch: 11 step: 293, loss is 0.028377976268529892\n",
      "epoch: 11 step: 294, loss is 0.1848558485507965\n",
      "epoch: 11 step: 295, loss is 0.09296863526105881\n",
      "epoch: 11 step: 296, loss is 0.10032143443822861\n",
      "epoch: 11 step: 297, loss is 0.07830106467008591\n",
      "epoch: 11 step: 298, loss is 0.06862026453018188\n",
      "epoch: 11 step: 299, loss is 0.07475616037845612\n",
      "epoch: 11 step: 300, loss is 0.033155299723148346\n",
      "epoch: 11 step: 301, loss is 0.024511436000466347\n",
      "epoch: 11 step: 302, loss is 0.08263295888900757\n",
      "epoch: 11 step: 303, loss is 0.02497030422091484\n",
      "epoch: 11 step: 304, loss is 0.12732526659965515\n",
      "epoch: 11 step: 305, loss is 0.11089473962783813\n",
      "epoch: 11 step: 306, loss is 0.149532750248909\n",
      "epoch: 11 step: 307, loss is 0.01554801780730486\n",
      "epoch: 11 step: 308, loss is 0.13379015028476715\n",
      "epoch: 11 step: 309, loss is 0.05436623841524124\n",
      "epoch: 11 step: 310, loss is 0.12516342103481293\n",
      "epoch: 11 step: 311, loss is 0.011202570050954819\n",
      "epoch: 11 step: 312, loss is 0.031561002135276794\n",
      "epoch: 11 step: 313, loss is 0.007608093786984682\n",
      "epoch: 11 step: 314, loss is 0.05899421498179436\n",
      "epoch: 11 step: 315, loss is 0.01638156548142433\n",
      "epoch: 11 step: 316, loss is 0.015676921233534813\n",
      "epoch: 11 step: 317, loss is 0.06181054189801216\n",
      "epoch: 11 step: 318, loss is 0.07710109651088715\n",
      "epoch: 11 step: 319, loss is 0.1041971817612648\n",
      "epoch: 11 step: 320, loss is 0.05608563870191574\n",
      "epoch: 11 step: 321, loss is 0.02372855134308338\n",
      "epoch: 11 step: 322, loss is 0.0381179116666317\n",
      "epoch: 11 step: 323, loss is 0.04755976423621178\n",
      "epoch: 11 step: 324, loss is 0.03576694056391716\n",
      "epoch: 11 step: 325, loss is 0.053938060998916626\n",
      "epoch: 11 step: 326, loss is 0.007586665917187929\n",
      "epoch: 11 step: 327, loss is 0.028809525072574615\n",
      "epoch: 11 step: 328, loss is 0.06666822731494904\n",
      "epoch: 11 step: 329, loss is 0.12274982035160065\n",
      "epoch: 11 step: 330, loss is 0.048648715019226074\n",
      "epoch: 11 step: 331, loss is 0.01753508299589157\n",
      "epoch: 11 step: 332, loss is 0.06073673069477081\n",
      "epoch: 11 step: 333, loss is 0.016856323927640915\n",
      "epoch: 11 step: 334, loss is 0.013874775730073452\n",
      "epoch: 11 step: 335, loss is 0.19356173276901245\n",
      "epoch: 11 step: 336, loss is 0.023763298988342285\n",
      "epoch: 11 step: 337, loss is 0.017356939613819122\n",
      "epoch: 11 step: 338, loss is 0.040994103997945786\n",
      "epoch: 11 step: 339, loss is 0.11385732889175415\n",
      "epoch: 11 step: 340, loss is 0.01316024363040924\n",
      "epoch: 11 step: 341, loss is 0.08546344190835953\n",
      "epoch: 11 step: 342, loss is 0.0592288002371788\n",
      "epoch: 11 step: 343, loss is 0.08241201937198639\n",
      "epoch: 11 step: 344, loss is 0.03935171291232109\n",
      "epoch: 11 step: 345, loss is 0.1212487518787384\n",
      "epoch: 11 step: 346, loss is 0.08224502950906754\n",
      "epoch: 11 step: 347, loss is 0.11044704169034958\n",
      "epoch: 11 step: 348, loss is 0.05081465467810631\n",
      "epoch: 11 step: 349, loss is 0.09992848336696625\n",
      "epoch: 11 step: 350, loss is 0.019125279039144516\n",
      "epoch: 11 step: 351, loss is 0.012887191027402878\n",
      "epoch: 11 step: 352, loss is 0.012292378582060337\n",
      "epoch: 11 step: 353, loss is 0.07620187103748322\n",
      "epoch: 11 step: 354, loss is 0.05564454197883606\n",
      "epoch: 11 step: 355, loss is 0.0890091210603714\n",
      "epoch: 11 step: 356, loss is 0.13274239003658295\n",
      "epoch: 11 step: 357, loss is 0.05392216518521309\n",
      "epoch: 11 step: 358, loss is 0.015009964816272259\n",
      "epoch: 11 step: 359, loss is 0.04128342866897583\n",
      "epoch: 11 step: 360, loss is 0.0311689805239439\n",
      "epoch: 11 step: 361, loss is 0.033498555421829224\n",
      "epoch: 11 step: 362, loss is 0.07386601716279984\n",
      "epoch: 11 step: 363, loss is 0.021727904677391052\n",
      "epoch: 11 step: 364, loss is 0.019099444150924683\n",
      "epoch: 11 step: 365, loss is 0.023094816133379936\n",
      "epoch: 11 step: 366, loss is 0.029430555179715157\n",
      "epoch: 11 step: 367, loss is 0.025540022179484367\n",
      "epoch: 11 step: 368, loss is 0.04843238368630409\n",
      "epoch: 11 step: 369, loss is 0.0775005891919136\n",
      "epoch: 11 step: 370, loss is 0.04833531007170677\n",
      "epoch: 11 step: 371, loss is 0.03585302084684372\n",
      "epoch: 11 step: 372, loss is 0.041882310062646866\n",
      "epoch: 11 step: 373, loss is 0.02393209934234619\n",
      "epoch: 11 step: 374, loss is 0.008570680394768715\n",
      "epoch: 11 step: 375, loss is 0.021878505125641823\n",
      "epoch: 11 step: 376, loss is 0.01608658954501152\n",
      "epoch: 11 step: 377, loss is 0.05465128272771835\n",
      "epoch: 11 step: 378, loss is 0.02924908511340618\n",
      "epoch: 11 step: 379, loss is 0.10032416135072708\n",
      "epoch: 11 step: 380, loss is 0.09790651500225067\n",
      "epoch: 11 step: 381, loss is 0.04897366464138031\n",
      "epoch: 11 step: 382, loss is 0.07127713412046432\n",
      "epoch: 11 step: 383, loss is 0.07881789654493332\n",
      "epoch: 11 step: 384, loss is 0.017977993935346603\n",
      "epoch: 11 step: 385, loss is 0.019294263795018196\n",
      "epoch: 11 step: 386, loss is 0.12435051798820496\n",
      "epoch: 11 step: 387, loss is 0.0026629187632352114\n",
      "epoch: 11 step: 388, loss is 0.12856994569301605\n",
      "epoch: 11 step: 389, loss is 0.011885643005371094\n",
      "epoch: 11 step: 390, loss is 0.003931939136236906\n",
      "epoch: 11 step: 391, loss is 0.09681756794452667\n",
      "epoch: 11 step: 392, loss is 0.02302590198814869\n",
      "epoch: 11 step: 393, loss is 0.01772381365299225\n",
      "epoch: 11 step: 394, loss is 0.03498777002096176\n",
      "epoch: 11 step: 395, loss is 0.030184077098965645\n",
      "epoch: 11 step: 396, loss is 0.033529311418533325\n",
      "epoch: 11 step: 397, loss is 0.05039757117629051\n",
      "epoch: 11 step: 398, loss is 0.021308621391654015\n",
      "epoch: 11 step: 399, loss is 0.009133828803896904\n",
      "epoch: 11 step: 400, loss is 0.09881641715765\n",
      "epoch: 11 step: 401, loss is 0.02719518542289734\n",
      "epoch: 11 step: 402, loss is 0.21409258246421814\n",
      "epoch: 11 step: 403, loss is 0.19035333395004272\n",
      "epoch: 11 step: 404, loss is 0.09288483113050461\n",
      "epoch: 11 step: 405, loss is 0.11147205531597137\n",
      "epoch: 11 step: 406, loss is 0.04584135860204697\n",
      "epoch: 11 step: 407, loss is 0.08336254209280014\n",
      "epoch: 11 step: 408, loss is 0.1133115291595459\n",
      "epoch: 11 step: 409, loss is 0.12157562375068665\n",
      "epoch: 11 step: 410, loss is 0.0047003463841974735\n",
      "epoch: 11 step: 411, loss is 0.046088382601737976\n",
      "epoch: 11 step: 412, loss is 0.07416428625583649\n",
      "epoch: 11 step: 413, loss is 0.09146500378847122\n",
      "epoch: 11 step: 414, loss is 0.05131339281797409\n",
      "epoch: 11 step: 415, loss is 0.11432953178882599\n",
      "epoch: 11 step: 416, loss is 0.08685394376516342\n",
      "epoch: 11 step: 417, loss is 0.09645355492830276\n",
      "epoch: 11 step: 418, loss is 0.07762247323989868\n",
      "epoch: 11 step: 419, loss is 0.023393377661705017\n",
      "epoch: 11 step: 420, loss is 0.09920147061347961\n",
      "epoch: 11 step: 421, loss is 0.024937575682997704\n",
      "epoch: 11 step: 422, loss is 0.07551309466362\n",
      "epoch: 11 step: 423, loss is 0.019929884001612663\n",
      "epoch: 11 step: 424, loss is 0.017890565097332\n",
      "epoch: 11 step: 425, loss is 0.06596465408802032\n",
      "epoch: 11 step: 426, loss is 0.03637540340423584\n",
      "epoch: 11 step: 427, loss is 0.027506917715072632\n",
      "epoch: 11 step: 428, loss is 0.04854462668299675\n",
      "epoch: 11 step: 429, loss is 0.07820101082324982\n",
      "epoch: 11 step: 430, loss is 0.026849450543522835\n",
      "epoch: 11 step: 431, loss is 0.06878761202096939\n",
      "epoch: 11 step: 432, loss is 0.051427409052848816\n",
      "epoch: 11 step: 433, loss is 0.07248952984809875\n",
      "epoch: 11 step: 434, loss is 0.09997721016407013\n",
      "epoch: 11 step: 435, loss is 0.025903241708874702\n",
      "epoch: 11 step: 436, loss is 0.09955626726150513\n",
      "epoch: 11 step: 437, loss is 0.09768111258745193\n",
      "epoch: 11 step: 438, loss is 0.08504736423492432\n",
      "epoch: 11 step: 439, loss is 0.08077750355005264\n",
      "epoch: 11 step: 440, loss is 0.028068210929632187\n",
      "epoch: 11 step: 441, loss is 0.08949188143014908\n",
      "epoch: 11 step: 442, loss is 0.04614084213972092\n",
      "epoch: 11 step: 443, loss is 0.16515150666236877\n",
      "epoch: 11 step: 444, loss is 0.06814753264188766\n",
      "epoch: 11 step: 445, loss is 0.05412374809384346\n",
      "epoch: 11 step: 446, loss is 0.07487364858388901\n",
      "epoch: 11 step: 447, loss is 0.024551255628466606\n",
      "epoch: 11 step: 448, loss is 0.06286784261465073\n",
      "epoch: 11 step: 449, loss is 0.06602102518081665\n",
      "epoch: 11 step: 450, loss is 0.05138786882162094\n",
      "epoch: 11 step: 451, loss is 0.10020788013935089\n",
      "epoch: 11 step: 452, loss is 0.021507831290364265\n",
      "epoch: 11 step: 453, loss is 0.02128293365240097\n",
      "epoch: 11 step: 454, loss is 0.11409100145101547\n",
      "epoch: 11 step: 455, loss is 0.059899669140577316\n",
      "epoch: 11 step: 456, loss is 0.0474771112203598\n",
      "epoch: 11 step: 457, loss is 0.027411306276917458\n",
      "epoch: 11 step: 458, loss is 0.047154851257801056\n",
      "epoch: 11 step: 459, loss is 0.029378224164247513\n",
      "epoch: 11 step: 460, loss is 0.09498992562294006\n",
      "epoch: 11 step: 461, loss is 0.04182865843176842\n",
      "epoch: 11 step: 462, loss is 0.011600352823734283\n",
      "epoch: 11 step: 463, loss is 0.16482670605182648\n",
      "epoch: 11 step: 464, loss is 0.004933937918394804\n",
      "epoch: 11 step: 465, loss is 0.09246178716421127\n",
      "epoch: 11 step: 466, loss is 0.01908455230295658\n",
      "epoch: 11 step: 467, loss is 0.012518662959337234\n",
      "epoch: 11 step: 468, loss is 0.004862089175730944\n",
      "epoch: 11 step: 469, loss is 0.028137173503637314\n",
      "epoch: 11 step: 470, loss is 0.014981315471231937\n",
      "epoch: 11 step: 471, loss is 0.006420438177883625\n",
      "epoch: 11 step: 472, loss is 0.05983119457960129\n",
      "epoch: 11 step: 473, loss is 0.03677763044834137\n",
      "epoch: 11 step: 474, loss is 0.009807485155761242\n",
      "epoch: 11 step: 475, loss is 0.08633814007043839\n",
      "epoch: 11 step: 476, loss is 0.022111451253294945\n",
      "epoch: 11 step: 477, loss is 0.14690156280994415\n",
      "epoch: 11 step: 478, loss is 0.05567434802651405\n",
      "epoch: 11 step: 479, loss is 0.06099314242601395\n",
      "epoch: 11 step: 480, loss is 0.04186052083969116\n",
      "epoch: 11 step: 481, loss is 0.025016218423843384\n",
      "epoch: 11 step: 482, loss is 0.017845727503299713\n",
      "epoch: 11 step: 483, loss is 0.0147708710283041\n",
      "epoch: 11 step: 484, loss is 0.055952124297618866\n",
      "epoch: 11 step: 485, loss is 0.09412603080272675\n",
      "epoch: 11 step: 486, loss is 0.07279873639345169\n",
      "epoch: 11 step: 487, loss is 0.02555151656270027\n",
      "epoch: 11 step: 488, loss is 0.013874241150915623\n",
      "epoch: 11 step: 489, loss is 0.04375464469194412\n",
      "epoch: 11 step: 490, loss is 0.050692975521087646\n",
      "epoch: 11 step: 491, loss is 0.10580026358366013\n",
      "epoch: 11 step: 492, loss is 0.029515286907553673\n",
      "epoch: 11 step: 493, loss is 0.01292214822024107\n",
      "epoch: 11 step: 494, loss is 0.08427385985851288\n",
      "epoch: 11 step: 495, loss is 0.007987985387444496\n",
      "epoch: 11 step: 496, loss is 0.09847776591777802\n",
      "epoch: 11 step: 497, loss is 0.023254385218024254\n",
      "epoch: 11 step: 498, loss is 0.03304992616176605\n",
      "epoch: 11 step: 499, loss is 0.01712535135447979\n",
      "epoch: 11 step: 500, loss is 0.0778815895318985\n",
      "epoch: 11 step: 501, loss is 0.052072275429964066\n",
      "epoch: 11 step: 502, loss is 0.09024223685264587\n",
      "epoch: 11 step: 503, loss is 0.02556021697819233\n",
      "epoch: 11 step: 504, loss is 0.11907481402158737\n",
      "epoch: 11 step: 505, loss is 0.08075734227895737\n",
      "epoch: 11 step: 506, loss is 0.00859323050826788\n",
      "epoch: 11 step: 507, loss is 0.06190924346446991\n",
      "epoch: 11 step: 508, loss is 0.0384703166782856\n",
      "epoch: 11 step: 509, loss is 0.03210257366299629\n",
      "epoch: 11 step: 510, loss is 0.05900353938341141\n",
      "epoch: 11 step: 511, loss is 0.06404732912778854\n",
      "epoch: 11 step: 512, loss is 0.029008204117417336\n",
      "epoch: 11 step: 513, loss is 0.1616101861000061\n",
      "epoch: 11 step: 514, loss is 0.03587586432695389\n",
      "epoch: 11 step: 515, loss is 0.026890743523836136\n",
      "epoch: 11 step: 516, loss is 0.040722571313381195\n",
      "epoch: 11 step: 517, loss is 0.06212335452437401\n",
      "epoch: 11 step: 518, loss is 0.06746674329042435\n",
      "epoch: 11 step: 519, loss is 0.012982594780623913\n",
      "epoch: 11 step: 520, loss is 0.10568077117204666\n",
      "epoch: 11 step: 521, loss is 0.01994294300675392\n",
      "epoch: 11 step: 522, loss is 0.1274227350950241\n",
      "epoch: 11 step: 523, loss is 0.06848825514316559\n",
      "epoch: 11 step: 524, loss is 0.09165334701538086\n",
      "epoch: 11 step: 525, loss is 0.02661166526377201\n",
      "epoch: 11 step: 526, loss is 0.0194269847124815\n",
      "epoch: 11 step: 527, loss is 0.05982738360762596\n",
      "epoch: 11 step: 528, loss is 0.01412135735154152\n",
      "epoch: 11 step: 529, loss is 0.05449434369802475\n",
      "epoch: 11 step: 530, loss is 0.016142524778842926\n",
      "epoch: 11 step: 531, loss is 0.01862664893269539\n",
      "epoch: 11 step: 532, loss is 0.0561705157160759\n",
      "epoch: 11 step: 533, loss is 0.01504517812281847\n",
      "epoch: 11 step: 534, loss is 0.03297458216547966\n",
      "epoch: 11 step: 535, loss is 0.011098027229309082\n",
      "epoch: 11 step: 536, loss is 0.02468237467110157\n",
      "epoch: 11 step: 537, loss is 0.0963033065199852\n",
      "epoch: 11 step: 538, loss is 0.04520808160305023\n",
      "epoch: 11 step: 539, loss is 0.018150901421904564\n",
      "epoch: 11 step: 540, loss is 0.013507138006389141\n",
      "epoch: 11 step: 541, loss is 0.08203967660665512\n",
      "epoch: 11 step: 542, loss is 0.08512696623802185\n",
      "epoch: 11 step: 543, loss is 0.015275586396455765\n",
      "epoch: 11 step: 544, loss is 0.03745735436677933\n",
      "epoch: 11 step: 545, loss is 0.048287905752658844\n",
      "epoch: 11 step: 546, loss is 0.0596684105694294\n",
      "epoch: 11 step: 547, loss is 0.06484340876340866\n",
      "epoch: 11 step: 548, loss is 0.0023844996467232704\n",
      "epoch: 11 step: 549, loss is 0.14605063199996948\n",
      "epoch: 11 step: 550, loss is 0.02034923806786537\n",
      "epoch: 11 step: 551, loss is 0.07155637443065643\n",
      "epoch: 11 step: 552, loss is 0.0503009594976902\n",
      "epoch: 11 step: 553, loss is 0.09961839020252228\n",
      "epoch: 11 step: 554, loss is 0.07428854703903198\n",
      "epoch: 11 step: 555, loss is 0.012026683427393436\n",
      "epoch: 11 step: 556, loss is 0.018464958295226097\n",
      "epoch: 11 step: 557, loss is 0.10381221026182175\n",
      "epoch: 11 step: 558, loss is 0.11018994450569153\n",
      "epoch: 11 step: 559, loss is 0.013109857216477394\n",
      "epoch: 11 step: 560, loss is 0.10088279843330383\n",
      "epoch: 11 step: 561, loss is 0.005242574959993362\n",
      "epoch: 11 step: 562, loss is 0.07172537595033646\n",
      "epoch: 11 step: 563, loss is 0.046001262962818146\n",
      "epoch: 11 step: 564, loss is 0.07418562471866608\n",
      "epoch: 11 step: 565, loss is 0.007412148639559746\n",
      "epoch: 11 step: 566, loss is 0.03871043026447296\n",
      "epoch: 11 step: 567, loss is 0.11087960749864578\n",
      "epoch: 11 step: 568, loss is 0.04386032000184059\n",
      "epoch: 11 step: 569, loss is 0.05427594110369682\n",
      "epoch: 11 step: 570, loss is 0.05242258682847023\n",
      "epoch: 11 step: 571, loss is 0.005073725711554289\n",
      "epoch: 11 step: 572, loss is 0.02637317217886448\n",
      "epoch: 11 step: 573, loss is 0.018946148455142975\n",
      "epoch: 11 step: 574, loss is 0.020596163347363472\n",
      "epoch: 11 step: 575, loss is 0.02579546719789505\n",
      "epoch: 11 step: 576, loss is 0.07143513858318329\n",
      "epoch: 11 step: 577, loss is 0.10142078995704651\n",
      "epoch: 11 step: 578, loss is 0.09574645012617111\n",
      "epoch: 11 step: 579, loss is 0.035885151475667953\n",
      "epoch: 11 step: 580, loss is 0.047843221575021744\n",
      "epoch: 11 step: 581, loss is 0.029340993613004684\n",
      "epoch: 11 step: 582, loss is 0.005983294919133186\n",
      "epoch: 11 step: 583, loss is 0.009547104127705097\n",
      "epoch: 11 step: 584, loss is 0.05376854166388512\n",
      "epoch: 11 step: 585, loss is 0.03236217424273491\n",
      "epoch: 11 step: 586, loss is 0.03525713458657265\n",
      "epoch: 11 step: 587, loss is 0.015993712469935417\n",
      "epoch: 11 step: 588, loss is 0.018628032878041267\n",
      "epoch: 11 step: 589, loss is 0.006638404913246632\n",
      "epoch: 11 step: 590, loss is 0.0481254979968071\n",
      "epoch: 11 step: 591, loss is 0.07133273780345917\n",
      "epoch: 11 step: 592, loss is 0.08264670521020889\n",
      "epoch: 11 step: 593, loss is 0.0454651303589344\n",
      "epoch: 11 step: 594, loss is 0.06078966706991196\n",
      "epoch: 11 step: 595, loss is 0.0658486858010292\n",
      "epoch: 11 step: 596, loss is 0.051347535103559494\n",
      "epoch: 11 step: 597, loss is 0.054861146956682205\n",
      "epoch: 11 step: 598, loss is 0.046312469989061356\n",
      "epoch: 11 step: 599, loss is 0.15210825204849243\n",
      "epoch: 11 step: 600, loss is 0.10343477874994278\n",
      "epoch: 11 step: 601, loss is 0.0293123722076416\n",
      "epoch: 11 step: 602, loss is 0.016707012429833412\n",
      "epoch: 11 step: 603, loss is 0.050650231540203094\n",
      "epoch: 11 step: 604, loss is 0.024355603381991386\n",
      "epoch: 11 step: 605, loss is 0.11202637851238251\n",
      "epoch: 11 step: 606, loss is 0.013176788575947285\n",
      "epoch: 11 step: 607, loss is 0.03513219952583313\n",
      "epoch: 11 step: 608, loss is 0.12164779007434845\n",
      "epoch: 11 step: 609, loss is 0.060126565396785736\n",
      "epoch: 11 step: 610, loss is 0.03376104310154915\n",
      "epoch: 11 step: 611, loss is 0.0927719995379448\n",
      "epoch: 11 step: 612, loss is 0.03259227052330971\n",
      "epoch: 11 step: 613, loss is 0.08735457807779312\n",
      "epoch: 11 step: 614, loss is 0.08144943416118622\n",
      "epoch: 11 step: 615, loss is 0.024342793971300125\n",
      "epoch: 11 step: 616, loss is 0.034773699939250946\n",
      "epoch: 11 step: 617, loss is 0.03315892815589905\n",
      "epoch: 11 step: 618, loss is 0.018312914296984673\n",
      "epoch: 11 step: 619, loss is 0.045072779059410095\n",
      "epoch: 11 step: 620, loss is 0.04255735129117966\n",
      "epoch: 11 step: 621, loss is 0.1416284143924713\n",
      "epoch: 11 step: 622, loss is 0.09384261816740036\n",
      "epoch: 11 step: 623, loss is 0.0572051927447319\n",
      "epoch: 11 step: 624, loss is 0.026926036924123764\n",
      "epoch: 11 step: 625, loss is 0.03231576830148697\n",
      "epoch: 11 step: 626, loss is 0.027570726349949837\n",
      "epoch: 11 step: 627, loss is 0.05437229201197624\n",
      "epoch: 11 step: 628, loss is 0.10796023905277252\n",
      "epoch: 11 step: 629, loss is 0.0367046557366848\n",
      "epoch: 11 step: 630, loss is 0.0026165952440351248\n",
      "epoch: 11 step: 631, loss is 0.0754506066441536\n",
      "epoch: 11 step: 632, loss is 0.056010376662015915\n",
      "epoch: 11 step: 633, loss is 0.03685641661286354\n",
      "epoch: 11 step: 634, loss is 0.014536071568727493\n",
      "epoch: 11 step: 635, loss is 0.1448822170495987\n",
      "epoch: 11 step: 636, loss is 0.010802091099321842\n",
      "epoch: 11 step: 637, loss is 0.04935150220990181\n",
      "epoch: 11 step: 638, loss is 0.043289005756378174\n",
      "epoch: 11 step: 639, loss is 0.13660357892513275\n",
      "epoch: 11 step: 640, loss is 0.08400282263755798\n",
      "epoch: 11 step: 641, loss is 0.03843054920434952\n",
      "epoch: 11 step: 642, loss is 0.04709111899137497\n",
      "epoch: 11 step: 643, loss is 0.09514611959457397\n",
      "epoch: 11 step: 644, loss is 0.06144247576594353\n",
      "epoch: 11 step: 645, loss is 0.04192422330379486\n",
      "epoch: 11 step: 646, loss is 0.0506851002573967\n",
      "epoch: 11 step: 647, loss is 0.027336249127984047\n",
      "epoch: 11 step: 648, loss is 0.05782394856214523\n",
      "epoch: 11 step: 649, loss is 0.11244208365678787\n",
      "epoch: 11 step: 650, loss is 0.06844325363636017\n",
      "epoch: 11 step: 651, loss is 0.07777813076972961\n",
      "epoch: 11 step: 652, loss is 0.06061115488409996\n",
      "epoch: 11 step: 653, loss is 0.07030308991670609\n",
      "epoch: 11 step: 654, loss is 0.06749210506677628\n",
      "epoch: 11 step: 655, loss is 0.02205694653093815\n",
      "epoch: 11 step: 656, loss is 0.037200067192316055\n",
      "epoch: 11 step: 657, loss is 0.08745744079351425\n",
      "epoch: 11 step: 658, loss is 0.030428780242800713\n",
      "epoch: 11 step: 659, loss is 0.02572857216000557\n",
      "epoch: 11 step: 660, loss is 0.04677191749215126\n",
      "epoch: 11 step: 661, loss is 0.12965112924575806\n",
      "epoch: 11 step: 662, loss is 0.10865755379199982\n",
      "epoch: 11 step: 663, loss is 0.0344574861228466\n",
      "epoch: 11 step: 664, loss is 0.07295656204223633\n",
      "epoch: 11 step: 665, loss is 0.01971016824245453\n",
      "epoch: 11 step: 666, loss is 0.0835174098610878\n",
      "epoch: 11 step: 667, loss is 0.051829852163791656\n",
      "epoch: 11 step: 668, loss is 0.059911638498306274\n",
      "epoch: 11 step: 669, loss is 0.07165315747261047\n",
      "epoch: 11 step: 670, loss is 0.04120614379644394\n",
      "epoch: 11 step: 671, loss is 0.03464008495211601\n",
      "epoch: 11 step: 672, loss is 0.008328876458108425\n",
      "epoch: 11 step: 673, loss is 0.08968882262706757\n",
      "epoch: 11 step: 674, loss is 0.13501863181591034\n",
      "epoch: 11 step: 675, loss is 0.03675714507699013\n",
      "epoch: 11 step: 676, loss is 0.07505661994218826\n",
      "epoch: 11 step: 677, loss is 0.039201293140649796\n",
      "epoch: 11 step: 678, loss is 0.020614424720406532\n",
      "epoch: 11 step: 679, loss is 0.0693708285689354\n",
      "epoch: 11 step: 680, loss is 0.07635542005300522\n",
      "epoch: 11 step: 681, loss is 0.1310594528913498\n",
      "epoch: 11 step: 682, loss is 0.038381218910217285\n",
      "epoch: 11 step: 683, loss is 0.04559028148651123\n",
      "epoch: 11 step: 684, loss is 0.07494676858186722\n",
      "epoch: 11 step: 685, loss is 0.09766539186239243\n",
      "epoch: 11 step: 686, loss is 0.00668296916410327\n",
      "epoch: 11 step: 687, loss is 0.06389564275741577\n",
      "epoch: 11 step: 688, loss is 0.05381779000163078\n",
      "epoch: 11 step: 689, loss is 0.04918145760893822\n",
      "epoch: 11 step: 690, loss is 0.04946596175432205\n",
      "epoch: 11 step: 691, loss is 0.06162542104721069\n",
      "epoch: 11 step: 692, loss is 0.03759784251451492\n",
      "epoch: 11 step: 693, loss is 0.03763266280293465\n",
      "epoch: 11 step: 694, loss is 0.07069568336009979\n",
      "epoch: 11 step: 695, loss is 0.04781216382980347\n",
      "epoch: 11 step: 696, loss is 0.07000886648893356\n",
      "epoch: 11 step: 697, loss is 0.03690949082374573\n",
      "epoch: 11 step: 698, loss is 0.067070871591568\n",
      "epoch: 11 step: 699, loss is 0.05529455468058586\n",
      "epoch: 11 step: 700, loss is 0.01590540260076523\n",
      "epoch: 11 step: 701, loss is 0.020237993448972702\n",
      "epoch: 11 step: 702, loss is 0.027640627697110176\n",
      "epoch: 11 step: 703, loss is 0.0407516174018383\n",
      "epoch: 11 step: 704, loss is 0.06351200491189957\n",
      "epoch: 11 step: 705, loss is 0.07414814829826355\n",
      "epoch: 11 step: 706, loss is 0.10212909430265427\n",
      "epoch: 11 step: 707, loss is 0.06081270053982735\n",
      "epoch: 11 step: 708, loss is 0.023449702188372612\n",
      "epoch: 11 step: 709, loss is 0.10089486092329025\n",
      "epoch: 11 step: 710, loss is 0.04571060836315155\n",
      "epoch: 11 step: 711, loss is 0.02577432245016098\n",
      "epoch: 11 step: 712, loss is 0.08942218124866486\n",
      "epoch: 11 step: 713, loss is 0.036891769617795944\n",
      "epoch: 11 step: 714, loss is 0.1687135398387909\n",
      "epoch: 11 step: 715, loss is 0.03657902404665947\n",
      "epoch: 11 step: 716, loss is 0.04729105532169342\n",
      "epoch: 11 step: 717, loss is 0.021581105887889862\n",
      "epoch: 11 step: 718, loss is 0.05544322356581688\n",
      "epoch: 11 step: 719, loss is 0.01583641581237316\n",
      "epoch: 11 step: 720, loss is 0.017310868948698044\n",
      "epoch: 11 step: 721, loss is 0.022108854725956917\n",
      "epoch: 11 step: 722, loss is 0.09899356961250305\n",
      "epoch: 11 step: 723, loss is 0.008008018136024475\n",
      "epoch: 11 step: 724, loss is 0.051731474697589874\n",
      "epoch: 11 step: 725, loss is 0.02063440904021263\n",
      "epoch: 11 step: 726, loss is 0.10290650278329849\n",
      "epoch: 11 step: 727, loss is 0.023075629025697708\n",
      "epoch: 11 step: 728, loss is 0.0677560493350029\n",
      "epoch: 11 step: 729, loss is 0.05470781773328781\n",
      "epoch: 11 step: 730, loss is 0.03234421834349632\n",
      "epoch: 11 step: 731, loss is 0.09945382177829742\n",
      "epoch: 11 step: 732, loss is 0.031927481293678284\n",
      "epoch: 11 step: 733, loss is 0.1723029464483261\n",
      "epoch: 11 step: 734, loss is 0.04021703079342842\n",
      "epoch: 11 step: 735, loss is 0.0380641371011734\n",
      "epoch: 11 step: 736, loss is 0.11650343239307404\n",
      "epoch: 11 step: 737, loss is 0.024850772693753242\n",
      "epoch: 11 step: 738, loss is 0.041821762919425964\n",
      "epoch: 11 step: 739, loss is 0.024537241086363792\n",
      "epoch: 11 step: 740, loss is 0.035260360687971115\n",
      "epoch: 11 step: 741, loss is 0.04234969988465309\n",
      "epoch: 11 step: 742, loss is 0.09081283956766129\n",
      "epoch: 11 step: 743, loss is 0.021719077602028847\n",
      "epoch: 11 step: 744, loss is 0.02313380315899849\n",
      "epoch: 11 step: 745, loss is 0.04310553893446922\n",
      "epoch: 11 step: 746, loss is 0.03893384337425232\n",
      "epoch: 11 step: 747, loss is 0.05039717257022858\n",
      "epoch: 11 step: 748, loss is 0.0827467292547226\n",
      "epoch: 11 step: 749, loss is 0.0715123862028122\n",
      "epoch: 11 step: 750, loss is 0.014827191829681396\n",
      "epoch: 11 step: 751, loss is 0.00630307337269187\n",
      "epoch: 11 step: 752, loss is 0.022898951545357704\n",
      "epoch: 11 step: 753, loss is 0.05123990774154663\n",
      "epoch: 11 step: 754, loss is 0.16201604902744293\n",
      "epoch: 11 step: 755, loss is 0.029076548293232918\n",
      "epoch: 11 step: 756, loss is 0.09714245051145554\n",
      "epoch: 11 step: 757, loss is 0.056269269436597824\n",
      "epoch: 11 step: 758, loss is 0.04219510778784752\n",
      "epoch: 11 step: 759, loss is 0.012489931657910347\n",
      "epoch: 11 step: 760, loss is 0.0026007757987827063\n",
      "epoch: 11 step: 761, loss is 0.018825506791472435\n",
      "epoch: 11 step: 762, loss is 0.11572564393281937\n",
      "epoch: 11 step: 763, loss is 0.015599866397678852\n",
      "epoch: 11 step: 764, loss is 0.019658004865050316\n",
      "epoch: 11 step: 765, loss is 0.047945600003004074\n",
      "epoch: 11 step: 766, loss is 0.060215674340724945\n",
      "epoch: 11 step: 767, loss is 0.1400628238916397\n",
      "epoch: 11 step: 768, loss is 0.012654297985136509\n",
      "epoch: 11 step: 769, loss is 0.01337195374071598\n",
      "epoch: 11 step: 770, loss is 0.058895498514175415\n",
      "epoch: 11 step: 771, loss is 0.3693329095840454\n",
      "epoch: 11 step: 772, loss is 0.1278918981552124\n",
      "epoch: 11 step: 773, loss is 0.018747227266430855\n",
      "epoch: 11 step: 774, loss is 0.059163857251405716\n",
      "epoch: 11 step: 775, loss is 0.023276306688785553\n",
      "epoch: 11 step: 776, loss is 0.04887092858552933\n",
      "epoch: 11 step: 777, loss is 0.02361958473920822\n",
      "epoch: 11 step: 778, loss is 0.004421668127179146\n",
      "epoch: 11 step: 779, loss is 0.018215224146842957\n",
      "epoch: 11 step: 780, loss is 0.060885537415742874\n",
      "epoch: 11 step: 781, loss is 0.006477611605077982\n",
      "epoch: 11 step: 782, loss is 0.01630544662475586\n",
      "epoch: 11 step: 783, loss is 0.02838710881769657\n",
      "epoch: 11 step: 784, loss is 0.0700494572520256\n",
      "epoch: 11 step: 785, loss is 0.12688595056533813\n",
      "epoch: 11 step: 786, loss is 0.06257972121238708\n",
      "epoch: 11 step: 787, loss is 0.03075377643108368\n",
      "epoch: 11 step: 788, loss is 0.01947936788201332\n",
      "epoch: 11 step: 789, loss is 0.09072310477495193\n",
      "epoch: 11 step: 790, loss is 0.09711205959320068\n",
      "epoch: 11 step: 791, loss is 0.05216994136571884\n",
      "epoch: 11 step: 792, loss is 0.04519809037446976\n",
      "epoch: 11 step: 793, loss is 0.01987726427614689\n",
      "epoch: 11 step: 794, loss is 0.0809035450220108\n",
      "epoch: 11 step: 795, loss is 0.10381092131137848\n",
      "epoch: 11 step: 796, loss is 0.10301626473665237\n",
      "epoch: 11 step: 797, loss is 0.03330473229289055\n",
      "epoch: 11 step: 798, loss is 0.007108229212462902\n",
      "epoch: 11 step: 799, loss is 0.02025662176311016\n",
      "epoch: 11 step: 800, loss is 0.04186132550239563\n",
      "epoch: 11 step: 801, loss is 0.032327573746442795\n",
      "epoch: 11 step: 802, loss is 0.06324984133243561\n",
      "epoch: 11 step: 803, loss is 0.05293995514512062\n",
      "epoch: 11 step: 804, loss is 0.0312977209687233\n",
      "epoch: 11 step: 805, loss is 0.04278089851140976\n",
      "epoch: 11 step: 806, loss is 0.07832049578428268\n",
      "epoch: 11 step: 807, loss is 0.0645613893866539\n",
      "epoch: 11 step: 808, loss is 0.02459079958498478\n",
      "epoch: 11 step: 809, loss is 0.1025247648358345\n",
      "epoch: 11 step: 810, loss is 0.058105044066905975\n",
      "epoch: 11 step: 811, loss is 0.04543738439679146\n",
      "epoch: 11 step: 812, loss is 0.08783519268035889\n",
      "epoch: 11 step: 813, loss is 0.022278953343629837\n",
      "epoch: 11 step: 814, loss is 0.0529043972492218\n",
      "epoch: 11 step: 815, loss is 0.029319241642951965\n",
      "epoch: 11 step: 816, loss is 0.0278061144053936\n",
      "epoch: 11 step: 817, loss is 0.06324764341115952\n",
      "epoch: 11 step: 818, loss is 0.02100203186273575\n",
      "epoch: 11 step: 819, loss is 0.07359710335731506\n",
      "epoch: 11 step: 820, loss is 0.03637482970952988\n",
      "epoch: 11 step: 821, loss is 0.06714034080505371\n",
      "epoch: 11 step: 822, loss is 0.03389083594083786\n",
      "epoch: 11 step: 823, loss is 0.0420842319726944\n",
      "epoch: 11 step: 824, loss is 0.04723099246621132\n",
      "epoch: 11 step: 825, loss is 0.03043827973306179\n",
      "epoch: 11 step: 826, loss is 0.07323051244020462\n",
      "epoch: 11 step: 827, loss is 0.09918664395809174\n",
      "epoch: 11 step: 828, loss is 0.05615410581231117\n",
      "epoch: 11 step: 829, loss is 0.0854761078953743\n",
      "epoch: 11 step: 830, loss is 0.024392670020461082\n",
      "epoch: 11 step: 831, loss is 0.04273435100913048\n",
      "epoch: 11 step: 832, loss is 0.06835346668958664\n",
      "epoch: 11 step: 833, loss is 0.3295397162437439\n",
      "epoch: 11 step: 834, loss is 0.07041981816291809\n",
      "epoch: 11 step: 835, loss is 0.13385649025440216\n",
      "epoch: 11 step: 836, loss is 0.020210081711411476\n",
      "epoch: 11 step: 837, loss is 0.03009253740310669\n",
      "epoch: 11 step: 838, loss is 0.20402659475803375\n",
      "epoch: 11 step: 839, loss is 0.040384553372859955\n",
      "epoch: 11 step: 840, loss is 0.026076337322592735\n",
      "epoch: 11 step: 841, loss is 0.03249865770339966\n",
      "epoch: 11 step: 842, loss is 0.03834529221057892\n",
      "epoch: 11 step: 843, loss is 0.05076306313276291\n",
      "epoch: 11 step: 844, loss is 0.1842837929725647\n",
      "epoch: 11 step: 845, loss is 0.07203054428100586\n",
      "epoch: 11 step: 846, loss is 0.015062730759382248\n",
      "epoch: 11 step: 847, loss is 0.07066889852285385\n",
      "epoch: 11 step: 848, loss is 0.033299509435892105\n",
      "epoch: 11 step: 849, loss is 0.011613359674811363\n",
      "epoch: 11 step: 850, loss is 0.037899766117334366\n",
      "epoch: 11 step: 851, loss is 0.13989131152629852\n",
      "epoch: 11 step: 852, loss is 0.08541607111692429\n",
      "epoch: 11 step: 853, loss is 0.10323521494865417\n",
      "epoch: 11 step: 854, loss is 0.10417471826076508\n",
      "epoch: 11 step: 855, loss is 0.07802516222000122\n",
      "epoch: 11 step: 856, loss is 0.08883751183748245\n",
      "epoch: 11 step: 857, loss is 0.04227568581700325\n",
      "epoch: 11 step: 858, loss is 0.10078069567680359\n",
      "epoch: 11 step: 859, loss is 0.04184494540095329\n",
      "epoch: 11 step: 860, loss is 0.10121896117925644\n",
      "epoch: 11 step: 861, loss is 0.030499113723635674\n",
      "epoch: 11 step: 862, loss is 0.06255020201206207\n",
      "epoch: 11 step: 863, loss is 0.05923211947083473\n",
      "epoch: 11 step: 864, loss is 0.03189627826213837\n",
      "epoch: 11 step: 865, loss is 0.11767987906932831\n",
      "epoch: 11 step: 866, loss is 0.039666060358285904\n",
      "epoch: 11 step: 867, loss is 0.07588227093219757\n",
      "epoch: 11 step: 868, loss is 0.056381579488515854\n",
      "epoch: 11 step: 869, loss is 0.05910565331578255\n",
      "epoch: 11 step: 870, loss is 0.1444644182920456\n",
      "epoch: 11 step: 871, loss is 0.02353392355144024\n",
      "epoch: 11 step: 872, loss is 0.02408538945019245\n",
      "epoch: 11 step: 873, loss is 0.08901261538267136\n",
      "epoch: 11 step: 874, loss is 0.08866515010595322\n",
      "epoch: 11 step: 875, loss is 0.09073840081691742\n",
      "epoch: 11 step: 876, loss is 0.10578662157058716\n",
      "epoch: 11 step: 877, loss is 0.026897164061665535\n",
      "epoch: 11 step: 878, loss is 0.018124228343367577\n",
      "epoch: 11 step: 879, loss is 0.005732693709433079\n",
      "epoch: 11 step: 880, loss is 0.1642015427350998\n",
      "epoch: 11 step: 881, loss is 0.05648777633905411\n",
      "epoch: 11 step: 882, loss is 0.07365480810403824\n",
      "epoch: 11 step: 883, loss is 0.08725623786449432\n",
      "epoch: 11 step: 884, loss is 0.06445779651403427\n",
      "epoch: 11 step: 885, loss is 0.07918605953454971\n",
      "epoch: 11 step: 886, loss is 0.071762315928936\n",
      "epoch: 11 step: 887, loss is 0.03156382218003273\n",
      "epoch: 11 step: 888, loss is 0.016196168959140778\n",
      "epoch: 11 step: 889, loss is 0.040473368018865585\n",
      "epoch: 11 step: 890, loss is 0.022933557629585266\n",
      "epoch: 11 step: 891, loss is 0.0560452863574028\n",
      "epoch: 11 step: 892, loss is 0.0817541629076004\n",
      "epoch: 11 step: 893, loss is 0.1564086526632309\n",
      "epoch: 11 step: 894, loss is 0.02640487253665924\n",
      "epoch: 11 step: 895, loss is 0.043382663279771805\n",
      "epoch: 11 step: 896, loss is 0.030630037188529968\n",
      "epoch: 11 step: 897, loss is 0.22032681107521057\n",
      "epoch: 11 step: 898, loss is 0.136957049369812\n",
      "epoch: 11 step: 899, loss is 0.07960277795791626\n",
      "epoch: 11 step: 900, loss is 0.043820157647132874\n",
      "epoch: 11 step: 901, loss is 0.049283917993307114\n",
      "epoch: 11 step: 902, loss is 0.22921475768089294\n",
      "epoch: 11 step: 903, loss is 0.107379250228405\n",
      "epoch: 11 step: 904, loss is 0.02133394591510296\n",
      "epoch: 11 step: 905, loss is 0.04075900837779045\n",
      "epoch: 11 step: 906, loss is 0.04071718826889992\n",
      "epoch: 11 step: 907, loss is 0.033153653144836426\n",
      "epoch: 11 step: 908, loss is 0.02381201647222042\n",
      "epoch: 11 step: 909, loss is 0.3136526346206665\n",
      "epoch: 11 step: 910, loss is 0.04893111810088158\n",
      "epoch: 11 step: 911, loss is 0.04669446498155594\n",
      "epoch: 11 step: 912, loss is 0.04380807653069496\n",
      "epoch: 11 step: 913, loss is 0.03407328948378563\n",
      "epoch: 11 step: 914, loss is 0.035141363739967346\n",
      "epoch: 11 step: 915, loss is 0.042114533483982086\n",
      "epoch: 11 step: 916, loss is 0.04294285923242569\n",
      "epoch: 11 step: 917, loss is 0.04049171507358551\n",
      "epoch: 11 step: 918, loss is 0.02436191588640213\n",
      "epoch: 11 step: 919, loss is 0.0654456689953804\n",
      "epoch: 11 step: 920, loss is 0.02627454698085785\n",
      "epoch: 11 step: 921, loss is 0.056460753083229065\n",
      "epoch: 11 step: 922, loss is 0.06322769820690155\n",
      "epoch: 11 step: 923, loss is 0.03565271571278572\n",
      "epoch: 11 step: 924, loss is 0.0343029648065567\n",
      "epoch: 11 step: 925, loss is 0.08766691386699677\n",
      "epoch: 11 step: 926, loss is 0.0047075930051505566\n",
      "epoch: 11 step: 927, loss is 0.09942303597927094\n",
      "epoch: 11 step: 928, loss is 0.053958117961883545\n",
      "epoch: 11 step: 929, loss is 0.08706746995449066\n",
      "epoch: 11 step: 930, loss is 0.015079693868756294\n",
      "epoch: 11 step: 931, loss is 0.055926769971847534\n",
      "epoch: 11 step: 932, loss is 0.07913385331630707\n",
      "epoch: 11 step: 933, loss is 0.0950513631105423\n",
      "epoch: 11 step: 934, loss is 0.09644228219985962\n",
      "epoch: 11 step: 935, loss is 0.10929998010396957\n",
      "epoch: 11 step: 936, loss is 0.09311513602733612\n",
      "epoch: 11 step: 937, loss is 0.0964851975440979\n",
      "epoch: 12 step: 1, loss is 0.04474842548370361\n",
      "epoch: 12 step: 2, loss is 0.06058866158127785\n",
      "epoch: 12 step: 3, loss is 0.046028755605220795\n",
      "epoch: 12 step: 4, loss is 0.057850006967782974\n",
      "epoch: 12 step: 5, loss is 0.00946697685867548\n",
      "epoch: 12 step: 6, loss is 0.022775257006287575\n",
      "epoch: 12 step: 7, loss is 0.018816979601979256\n",
      "epoch: 12 step: 8, loss is 0.035795655101537704\n",
      "epoch: 12 step: 9, loss is 0.01669636182487011\n",
      "epoch: 12 step: 10, loss is 0.08861241489648819\n",
      "epoch: 12 step: 11, loss is 0.012196002528071404\n",
      "epoch: 12 step: 12, loss is 0.054136790335178375\n",
      "epoch: 12 step: 13, loss is 0.02927599661052227\n",
      "epoch: 12 step: 14, loss is 0.010934583842754364\n",
      "epoch: 12 step: 15, loss is 0.007435489911586046\n",
      "epoch: 12 step: 16, loss is 0.006377988960593939\n",
      "epoch: 12 step: 17, loss is 0.007409228011965752\n",
      "epoch: 12 step: 18, loss is 0.0076103853061795235\n",
      "epoch: 12 step: 19, loss is 0.07769438624382019\n",
      "epoch: 12 step: 20, loss is 0.01533777266740799\n",
      "epoch: 12 step: 21, loss is 0.08172000199556351\n",
      "epoch: 12 step: 22, loss is 0.016879402101039886\n",
      "epoch: 12 step: 23, loss is 0.0736299604177475\n",
      "epoch: 12 step: 24, loss is 0.04414818063378334\n",
      "epoch: 12 step: 25, loss is 0.12734800577163696\n",
      "epoch: 12 step: 26, loss is 0.0548076406121254\n",
      "epoch: 12 step: 27, loss is 0.04939637333154678\n",
      "epoch: 12 step: 28, loss is 0.0729847103357315\n",
      "epoch: 12 step: 29, loss is 0.03864360600709915\n",
      "epoch: 12 step: 30, loss is 0.03751614689826965\n",
      "epoch: 12 step: 31, loss is 0.18510165810585022\n",
      "epoch: 12 step: 32, loss is 0.026791390031576157\n",
      "epoch: 12 step: 33, loss is 0.06041696295142174\n",
      "epoch: 12 step: 34, loss is 0.05367569997906685\n",
      "epoch: 12 step: 35, loss is 0.07980551570653915\n",
      "epoch: 12 step: 36, loss is 0.0390242338180542\n",
      "epoch: 12 step: 37, loss is 0.0071526640094816685\n",
      "epoch: 12 step: 38, loss is 0.01737413927912712\n",
      "epoch: 12 step: 39, loss is 0.010180392302572727\n",
      "epoch: 12 step: 40, loss is 0.059455402195453644\n",
      "epoch: 12 step: 41, loss is 0.030323131009936333\n",
      "epoch: 12 step: 42, loss is 0.004281884524971247\n",
      "epoch: 12 step: 43, loss is 0.09533765912055969\n",
      "epoch: 12 step: 44, loss is 0.02842644974589348\n",
      "epoch: 12 step: 45, loss is 0.014865964651107788\n",
      "epoch: 12 step: 46, loss is 0.13962510228157043\n",
      "epoch: 12 step: 47, loss is 0.010600137524306774\n",
      "epoch: 12 step: 48, loss is 0.040569934993982315\n",
      "epoch: 12 step: 49, loss is 0.03680974990129471\n",
      "epoch: 12 step: 50, loss is 0.09567515552043915\n",
      "epoch: 12 step: 51, loss is 0.06325143575668335\n",
      "epoch: 12 step: 52, loss is 0.018505200743675232\n",
      "epoch: 12 step: 53, loss is 0.012320540845394135\n",
      "epoch: 12 step: 54, loss is 0.02699260413646698\n",
      "epoch: 12 step: 55, loss is 0.030547140166163445\n",
      "epoch: 12 step: 56, loss is 0.012928210198879242\n",
      "epoch: 12 step: 57, loss is 0.016136595979332924\n",
      "epoch: 12 step: 58, loss is 0.011671430431306362\n",
      "epoch: 12 step: 59, loss is 0.007517711725085974\n",
      "epoch: 12 step: 60, loss is 0.02860969677567482\n",
      "epoch: 12 step: 61, loss is 0.01933000050485134\n",
      "epoch: 12 step: 62, loss is 0.0409097746014595\n",
      "epoch: 12 step: 63, loss is 0.010467028245329857\n",
      "epoch: 12 step: 64, loss is 0.008934073150157928\n",
      "epoch: 12 step: 65, loss is 0.03740239515900612\n",
      "epoch: 12 step: 66, loss is 0.02443370409309864\n",
      "epoch: 12 step: 67, loss is 0.0801255851984024\n",
      "epoch: 12 step: 68, loss is 0.029568267986178398\n",
      "epoch: 12 step: 69, loss is 0.05369733273983002\n",
      "epoch: 12 step: 70, loss is 0.006609918549656868\n",
      "epoch: 12 step: 71, loss is 0.022053690627217293\n",
      "epoch: 12 step: 72, loss is 0.03660593926906586\n",
      "epoch: 12 step: 73, loss is 0.06149085983633995\n",
      "epoch: 12 step: 74, loss is 0.019858554005622864\n",
      "epoch: 12 step: 75, loss is 0.05481467768549919\n",
      "epoch: 12 step: 76, loss is 0.040782392024993896\n",
      "epoch: 12 step: 77, loss is 0.04614878445863724\n",
      "epoch: 12 step: 78, loss is 0.01586230657994747\n",
      "epoch: 12 step: 79, loss is 0.01734018512070179\n",
      "epoch: 12 step: 80, loss is 0.020181946456432343\n",
      "epoch: 12 step: 81, loss is 0.021262552589178085\n",
      "epoch: 12 step: 82, loss is 0.015361221507191658\n",
      "epoch: 12 step: 83, loss is 0.031649112701416016\n",
      "epoch: 12 step: 84, loss is 0.06289210915565491\n",
      "epoch: 12 step: 85, loss is 0.07032450288534164\n",
      "epoch: 12 step: 86, loss is 0.01756894960999489\n",
      "epoch: 12 step: 87, loss is 0.07441592216491699\n",
      "epoch: 12 step: 88, loss is 0.028959089890122414\n",
      "epoch: 12 step: 89, loss is 0.04555149003863335\n",
      "epoch: 12 step: 90, loss is 0.031013470143079758\n",
      "epoch: 12 step: 91, loss is 0.02654840610921383\n",
      "epoch: 12 step: 92, loss is 0.00817686878144741\n",
      "epoch: 12 step: 93, loss is 0.027462009340524673\n",
      "epoch: 12 step: 94, loss is 0.03119516745209694\n",
      "epoch: 12 step: 95, loss is 0.05269475281238556\n",
      "epoch: 12 step: 96, loss is 0.023196404799818993\n",
      "epoch: 12 step: 97, loss is 0.017089607194066048\n",
      "epoch: 12 step: 98, loss is 0.024448707699775696\n",
      "epoch: 12 step: 99, loss is 0.07436539232730865\n",
      "epoch: 12 step: 100, loss is 0.04706073924899101\n",
      "epoch: 12 step: 101, loss is 0.01275707595050335\n",
      "epoch: 12 step: 102, loss is 0.02086581476032734\n",
      "epoch: 12 step: 103, loss is 0.012992078438401222\n",
      "epoch: 12 step: 104, loss is 0.05103866383433342\n",
      "epoch: 12 step: 105, loss is 0.012505067512392998\n",
      "epoch: 12 step: 106, loss is 0.08413003385066986\n",
      "epoch: 12 step: 107, loss is 0.06288831681013107\n",
      "epoch: 12 step: 108, loss is 0.11226272583007812\n",
      "epoch: 12 step: 109, loss is 0.0307846050709486\n",
      "epoch: 12 step: 110, loss is 0.07240788638591766\n",
      "epoch: 12 step: 111, loss is 0.03667773678898811\n",
      "epoch: 12 step: 112, loss is 0.07234358787536621\n",
      "epoch: 12 step: 113, loss is 0.02654433064162731\n",
      "epoch: 12 step: 114, loss is 0.016431085765361786\n",
      "epoch: 12 step: 115, loss is 0.03531205654144287\n",
      "epoch: 12 step: 116, loss is 0.003846143838018179\n",
      "epoch: 12 step: 117, loss is 0.07684779167175293\n",
      "epoch: 12 step: 118, loss is 0.00025335626560263336\n",
      "epoch: 12 step: 119, loss is 0.02748764678835869\n",
      "epoch: 12 step: 120, loss is 0.05233018100261688\n",
      "epoch: 12 step: 121, loss is 0.0031236230861395597\n",
      "epoch: 12 step: 122, loss is 0.03455306217074394\n",
      "epoch: 12 step: 123, loss is 0.03499428182840347\n",
      "epoch: 12 step: 124, loss is 0.028926871716976166\n",
      "epoch: 12 step: 125, loss is 0.0023026028648018837\n",
      "epoch: 12 step: 126, loss is 0.035861194133758545\n",
      "epoch: 12 step: 127, loss is 0.04587605223059654\n",
      "epoch: 12 step: 128, loss is 0.01040554791688919\n",
      "epoch: 12 step: 129, loss is 0.024526983499526978\n",
      "epoch: 12 step: 130, loss is 0.08630518615245819\n",
      "epoch: 12 step: 131, loss is 0.05062787979841232\n",
      "epoch: 12 step: 132, loss is 0.003781957319006324\n",
      "epoch: 12 step: 133, loss is 0.0020982036367058754\n",
      "epoch: 12 step: 134, loss is 0.014948498457670212\n",
      "epoch: 12 step: 135, loss is 0.05724843218922615\n",
      "epoch: 12 step: 136, loss is 0.010321625508368015\n",
      "epoch: 12 step: 137, loss is 0.02230505459010601\n",
      "epoch: 12 step: 138, loss is 0.022365331649780273\n",
      "epoch: 12 step: 139, loss is 0.0043713185004889965\n",
      "epoch: 12 step: 140, loss is 0.020635107532143593\n",
      "epoch: 12 step: 141, loss is 0.0043524811044335365\n",
      "epoch: 12 step: 142, loss is 0.026841478422284126\n",
      "epoch: 12 step: 143, loss is 0.04067584499716759\n",
      "epoch: 12 step: 144, loss is 0.008100838400423527\n",
      "epoch: 12 step: 145, loss is 0.05927099660038948\n",
      "epoch: 12 step: 146, loss is 0.14225827157497406\n",
      "epoch: 12 step: 147, loss is 0.01235557347536087\n",
      "epoch: 12 step: 148, loss is 0.015511810779571533\n",
      "epoch: 12 step: 149, loss is 0.025587107986211777\n",
      "epoch: 12 step: 150, loss is 0.044077616184949875\n",
      "epoch: 12 step: 151, loss is 0.0035921670496463776\n",
      "epoch: 12 step: 152, loss is 0.03965379670262337\n",
      "epoch: 12 step: 153, loss is 0.12451152503490448\n",
      "epoch: 12 step: 154, loss is 0.026580793783068657\n",
      "epoch: 12 step: 155, loss is 0.011036932468414307\n",
      "epoch: 12 step: 156, loss is 0.039921849966049194\n",
      "epoch: 12 step: 157, loss is 0.020168429240584373\n",
      "epoch: 12 step: 158, loss is 0.09625134617090225\n",
      "epoch: 12 step: 159, loss is 0.009158545173704624\n",
      "epoch: 12 step: 160, loss is 0.015794482082128525\n",
      "epoch: 12 step: 161, loss is 0.027807703241705894\n",
      "epoch: 12 step: 162, loss is 0.06368479132652283\n",
      "epoch: 12 step: 163, loss is 0.024445369839668274\n",
      "epoch: 12 step: 164, loss is 0.009838415309786797\n",
      "epoch: 12 step: 165, loss is 0.030144724994897842\n",
      "epoch: 12 step: 166, loss is 0.013462085276842117\n",
      "epoch: 12 step: 167, loss is 0.010703225620090961\n",
      "epoch: 12 step: 168, loss is 0.011388354934751987\n",
      "epoch: 12 step: 169, loss is 0.022746769711375237\n",
      "epoch: 12 step: 170, loss is 0.007860325276851654\n",
      "epoch: 12 step: 171, loss is 0.05866045877337456\n",
      "epoch: 12 step: 172, loss is 0.12264156341552734\n",
      "epoch: 12 step: 173, loss is 0.05783092975616455\n",
      "epoch: 12 step: 174, loss is 0.0073228757828474045\n",
      "epoch: 12 step: 175, loss is 0.01820240542292595\n",
      "epoch: 12 step: 176, loss is 0.02422448620200157\n",
      "epoch: 12 step: 177, loss is 0.08560417592525482\n",
      "epoch: 12 step: 178, loss is 0.029019273817539215\n",
      "epoch: 12 step: 179, loss is 0.08604715019464493\n",
      "epoch: 12 step: 180, loss is 0.02868322841823101\n",
      "epoch: 12 step: 181, loss is 0.03738861903548241\n",
      "epoch: 12 step: 182, loss is 0.027642833068966866\n",
      "epoch: 12 step: 183, loss is 0.0035909023135900497\n",
      "epoch: 12 step: 184, loss is 0.037118397653102875\n",
      "epoch: 12 step: 185, loss is 0.01125225331634283\n",
      "epoch: 12 step: 186, loss is 0.03955257683992386\n",
      "epoch: 12 step: 187, loss is 0.05406899377703667\n",
      "epoch: 12 step: 188, loss is 0.07524735480546951\n",
      "epoch: 12 step: 189, loss is 0.01397849153727293\n",
      "epoch: 12 step: 190, loss is 0.006123232189565897\n",
      "epoch: 12 step: 191, loss is 0.1091202050447464\n",
      "epoch: 12 step: 192, loss is 0.028441745787858963\n",
      "epoch: 12 step: 193, loss is 0.016176870092749596\n",
      "epoch: 12 step: 194, loss is 0.06138133257627487\n",
      "epoch: 12 step: 195, loss is 0.07933370769023895\n",
      "epoch: 12 step: 196, loss is 0.061951182782649994\n",
      "epoch: 12 step: 197, loss is 0.04261156916618347\n",
      "epoch: 12 step: 198, loss is 0.03462851420044899\n",
      "epoch: 12 step: 199, loss is 0.01812143810093403\n",
      "epoch: 12 step: 200, loss is 0.0163198821246624\n",
      "epoch: 12 step: 201, loss is 0.10022146254777908\n",
      "epoch: 12 step: 202, loss is 0.06248551234602928\n",
      "epoch: 12 step: 203, loss is 0.035212092101573944\n",
      "epoch: 12 step: 204, loss is 0.06653721630573273\n",
      "epoch: 12 step: 205, loss is 0.01751687005162239\n",
      "epoch: 12 step: 206, loss is 0.03334309905767441\n",
      "epoch: 12 step: 207, loss is 0.015407741069793701\n",
      "epoch: 12 step: 208, loss is 0.020306702703237534\n",
      "epoch: 12 step: 209, loss is 0.015021661296486855\n",
      "epoch: 12 step: 210, loss is 0.04612267389893532\n",
      "epoch: 12 step: 211, loss is 0.021163297817111015\n",
      "epoch: 12 step: 212, loss is 0.011462986469268799\n",
      "epoch: 12 step: 213, loss is 0.0448908768594265\n",
      "epoch: 12 step: 214, loss is 0.04365967586636543\n",
      "epoch: 12 step: 215, loss is 0.02849002368748188\n",
      "epoch: 12 step: 216, loss is 0.04638053849339485\n",
      "epoch: 12 step: 217, loss is 0.0605325810611248\n",
      "epoch: 12 step: 218, loss is 0.030992118641734123\n",
      "epoch: 12 step: 219, loss is 0.04751606285572052\n",
      "epoch: 12 step: 220, loss is 0.002771885134279728\n",
      "epoch: 12 step: 221, loss is 0.059272538870573044\n",
      "epoch: 12 step: 222, loss is 0.020110078155994415\n",
      "epoch: 12 step: 223, loss is 0.06422807276248932\n",
      "epoch: 12 step: 224, loss is 0.02651815488934517\n",
      "epoch: 12 step: 225, loss is 0.02861582115292549\n",
      "epoch: 12 step: 226, loss is 0.07831872254610062\n",
      "epoch: 12 step: 227, loss is 0.011195131577551365\n",
      "epoch: 12 step: 228, loss is 0.01699954830110073\n",
      "epoch: 12 step: 229, loss is 0.04696273058652878\n",
      "epoch: 12 step: 230, loss is 0.016481492668390274\n",
      "epoch: 12 step: 231, loss is 0.011532395146787167\n",
      "epoch: 12 step: 232, loss is 0.029712237417697906\n",
      "epoch: 12 step: 233, loss is 0.05221559479832649\n",
      "epoch: 12 step: 234, loss is 0.005008228123188019\n",
      "epoch: 12 step: 235, loss is 0.187080979347229\n",
      "epoch: 12 step: 236, loss is 0.035906169563531876\n",
      "epoch: 12 step: 237, loss is 0.018842054530978203\n",
      "epoch: 12 step: 238, loss is 0.052773766219615936\n",
      "epoch: 12 step: 239, loss is 0.03910506144165993\n",
      "epoch: 12 step: 240, loss is 0.020174719393253326\n",
      "epoch: 12 step: 241, loss is 0.005219282116740942\n",
      "epoch: 12 step: 242, loss is 0.015044483356177807\n",
      "epoch: 12 step: 243, loss is 0.09156117588281631\n",
      "epoch: 12 step: 244, loss is 0.03668766841292381\n",
      "epoch: 12 step: 245, loss is 0.0571695938706398\n",
      "epoch: 12 step: 246, loss is 0.12170752882957458\n",
      "epoch: 12 step: 247, loss is 0.0693032369017601\n",
      "epoch: 12 step: 248, loss is 0.03205125406384468\n",
      "epoch: 12 step: 249, loss is 0.008269913494586945\n",
      "epoch: 12 step: 250, loss is 0.019383694976568222\n",
      "epoch: 12 step: 251, loss is 0.03544091433286667\n",
      "epoch: 12 step: 252, loss is 0.11417388916015625\n",
      "epoch: 12 step: 253, loss is 0.011657320894300938\n",
      "epoch: 12 step: 254, loss is 0.07729113847017288\n",
      "epoch: 12 step: 255, loss is 0.031727228313684464\n",
      "epoch: 12 step: 256, loss is 0.006523162126541138\n",
      "epoch: 12 step: 257, loss is 0.07851532846689224\n",
      "epoch: 12 step: 258, loss is 0.04766131564974785\n",
      "epoch: 12 step: 259, loss is 0.044320471584796906\n",
      "epoch: 12 step: 260, loss is 0.017057448625564575\n",
      "epoch: 12 step: 261, loss is 0.0649774819612503\n",
      "epoch: 12 step: 262, loss is 0.08629415929317474\n",
      "epoch: 12 step: 263, loss is 0.06315424293279648\n",
      "epoch: 12 step: 264, loss is 0.08054818958044052\n",
      "epoch: 12 step: 265, loss is 0.03584869205951691\n",
      "epoch: 12 step: 266, loss is 0.10281608998775482\n",
      "epoch: 12 step: 267, loss is 0.01938706636428833\n",
      "epoch: 12 step: 268, loss is 0.01126550231128931\n",
      "epoch: 12 step: 269, loss is 0.1440214365720749\n",
      "epoch: 12 step: 270, loss is 0.05172250419855118\n",
      "epoch: 12 step: 271, loss is 0.03429052606225014\n",
      "epoch: 12 step: 272, loss is 0.0017492417246103287\n",
      "epoch: 12 step: 273, loss is 0.021349065005779266\n",
      "epoch: 12 step: 274, loss is 0.004461297299712896\n",
      "epoch: 12 step: 275, loss is 0.08227016031742096\n",
      "epoch: 12 step: 276, loss is 0.024944940581917763\n",
      "epoch: 12 step: 277, loss is 0.038286127150058746\n",
      "epoch: 12 step: 278, loss is 0.01429284643381834\n",
      "epoch: 12 step: 279, loss is 0.055817458778619766\n",
      "epoch: 12 step: 280, loss is 0.0036627205554395914\n",
      "epoch: 12 step: 281, loss is 0.017701774835586548\n",
      "epoch: 12 step: 282, loss is 0.009104004129767418\n",
      "epoch: 12 step: 283, loss is 0.013795623555779457\n",
      "epoch: 12 step: 284, loss is 0.011024820618331432\n",
      "epoch: 12 step: 285, loss is 0.08484162390232086\n",
      "epoch: 12 step: 286, loss is 0.031040223315358162\n",
      "epoch: 12 step: 287, loss is 0.06398450583219528\n",
      "epoch: 12 step: 288, loss is 0.009790834039449692\n",
      "epoch: 12 step: 289, loss is 0.023066528141498566\n",
      "epoch: 12 step: 290, loss is 0.03880910947918892\n",
      "epoch: 12 step: 291, loss is 0.10169766843318939\n",
      "epoch: 12 step: 292, loss is 0.017127010971307755\n",
      "epoch: 12 step: 293, loss is 0.022686216980218887\n",
      "epoch: 12 step: 294, loss is 0.022489789873361588\n",
      "epoch: 12 step: 295, loss is 0.05951789766550064\n",
      "epoch: 12 step: 296, loss is 0.02162492275238037\n",
      "epoch: 12 step: 297, loss is 0.014123920351266861\n",
      "epoch: 12 step: 298, loss is 0.03878391161561012\n",
      "epoch: 12 step: 299, loss is 0.0034689677413553\n",
      "epoch: 12 step: 300, loss is 0.039646729826927185\n",
      "epoch: 12 step: 301, loss is 0.04844072833657265\n",
      "epoch: 12 step: 302, loss is 0.031016364693641663\n",
      "epoch: 12 step: 303, loss is 0.10905555635690689\n",
      "epoch: 12 step: 304, loss is 0.04027395322918892\n",
      "epoch: 12 step: 305, loss is 0.006273675709962845\n",
      "epoch: 12 step: 306, loss is 0.03411088138818741\n",
      "epoch: 12 step: 307, loss is 0.08926835656166077\n",
      "epoch: 12 step: 308, loss is 0.040014393627643585\n",
      "epoch: 12 step: 309, loss is 0.02474488876760006\n",
      "epoch: 12 step: 310, loss is 0.049683209508657455\n",
      "epoch: 12 step: 311, loss is 0.017841467633843422\n",
      "epoch: 12 step: 312, loss is 0.06234712526202202\n",
      "epoch: 12 step: 313, loss is 0.13184107840061188\n",
      "epoch: 12 step: 314, loss is 0.017079278826713562\n",
      "epoch: 12 step: 315, loss is 0.0386492945253849\n",
      "epoch: 12 step: 316, loss is 0.01569386012852192\n",
      "epoch: 12 step: 317, loss is 0.09857192635536194\n",
      "epoch: 12 step: 318, loss is 0.05756828561425209\n",
      "epoch: 12 step: 319, loss is 0.05774690955877304\n",
      "epoch: 12 step: 320, loss is 0.06686203181743622\n",
      "epoch: 12 step: 321, loss is 0.1207817867398262\n",
      "epoch: 12 step: 322, loss is 0.020527176558971405\n",
      "epoch: 12 step: 323, loss is 0.025270938873291016\n",
      "epoch: 12 step: 324, loss is 0.03951893374323845\n",
      "epoch: 12 step: 325, loss is 0.020979711785912514\n",
      "epoch: 12 step: 326, loss is 0.022292688488960266\n",
      "epoch: 12 step: 327, loss is 0.03841754049062729\n",
      "epoch: 12 step: 328, loss is 0.0072211530059576035\n",
      "epoch: 12 step: 329, loss is 0.06508006155490875\n",
      "epoch: 12 step: 330, loss is 0.008252915926277637\n",
      "epoch: 12 step: 331, loss is 0.06847012042999268\n",
      "epoch: 12 step: 332, loss is 0.021510804072022438\n",
      "epoch: 12 step: 333, loss is 0.08746902644634247\n",
      "epoch: 12 step: 334, loss is 0.015273485332727432\n",
      "epoch: 12 step: 335, loss is 0.12252958118915558\n",
      "epoch: 12 step: 336, loss is 0.08526243269443512\n",
      "epoch: 12 step: 337, loss is 0.09015844762325287\n",
      "epoch: 12 step: 338, loss is 0.029812783002853394\n",
      "epoch: 12 step: 339, loss is 0.048415396362543106\n",
      "epoch: 12 step: 340, loss is 0.01292253378778696\n",
      "epoch: 12 step: 341, loss is 0.01798645406961441\n",
      "epoch: 12 step: 342, loss is 0.07533958554267883\n",
      "epoch: 12 step: 343, loss is 0.022946201264858246\n",
      "epoch: 12 step: 344, loss is 0.06249934434890747\n",
      "epoch: 12 step: 345, loss is 0.019148671999573708\n",
      "epoch: 12 step: 346, loss is 0.024492628872394562\n",
      "epoch: 12 step: 347, loss is 0.022938981652259827\n",
      "epoch: 12 step: 348, loss is 0.0034425228368490934\n",
      "epoch: 12 step: 349, loss is 0.011728091165423393\n",
      "epoch: 12 step: 350, loss is 0.04435058683156967\n",
      "epoch: 12 step: 351, loss is 0.04821367934346199\n",
      "epoch: 12 step: 352, loss is 0.004680324345827103\n",
      "epoch: 12 step: 353, loss is 0.018351806327700615\n",
      "epoch: 12 step: 354, loss is 0.04022867977619171\n",
      "epoch: 12 step: 355, loss is 0.03202822059392929\n",
      "epoch: 12 step: 356, loss is 0.0030587054789066315\n",
      "epoch: 12 step: 357, loss is 0.04119327291846275\n",
      "epoch: 12 step: 358, loss is 0.05601844564080238\n",
      "epoch: 12 step: 359, loss is 0.01796805113554001\n",
      "epoch: 12 step: 360, loss is 0.011223290115594864\n",
      "epoch: 12 step: 361, loss is 0.05585979297757149\n",
      "epoch: 12 step: 362, loss is 0.08271776139736176\n",
      "epoch: 12 step: 363, loss is 0.019722916185855865\n",
      "epoch: 12 step: 364, loss is 0.04190949350595474\n",
      "epoch: 12 step: 365, loss is 0.011482348665595055\n",
      "epoch: 12 step: 366, loss is 0.006941866595298052\n",
      "epoch: 12 step: 367, loss is 0.10165780782699585\n",
      "epoch: 12 step: 368, loss is 0.027888191863894463\n",
      "epoch: 12 step: 369, loss is 0.0018343362025916576\n",
      "epoch: 12 step: 370, loss is 0.05389421433210373\n",
      "epoch: 12 step: 371, loss is 0.00995247159153223\n",
      "epoch: 12 step: 372, loss is 0.04250939190387726\n",
      "epoch: 12 step: 373, loss is 0.05420129373669624\n",
      "epoch: 12 step: 374, loss is 0.013763584196567535\n",
      "epoch: 12 step: 375, loss is 0.09405934810638428\n",
      "epoch: 12 step: 376, loss is 0.015678448602557182\n",
      "epoch: 12 step: 377, loss is 0.008383804000914097\n",
      "epoch: 12 step: 378, loss is 0.06299613416194916\n",
      "epoch: 12 step: 379, loss is 0.001756270881742239\n",
      "epoch: 12 step: 380, loss is 0.026076214388012886\n",
      "epoch: 12 step: 381, loss is 0.014214436523616314\n",
      "epoch: 12 step: 382, loss is 0.03416397422552109\n",
      "epoch: 12 step: 383, loss is 0.06241987645626068\n",
      "epoch: 12 step: 384, loss is 0.018249481916427612\n",
      "epoch: 12 step: 385, loss is 0.004896753001958132\n",
      "epoch: 12 step: 386, loss is 0.07480037212371826\n",
      "epoch: 12 step: 387, loss is 0.025965407490730286\n",
      "epoch: 12 step: 388, loss is 0.0009878792334347963\n",
      "epoch: 12 step: 389, loss is 0.00738562224432826\n",
      "epoch: 12 step: 390, loss is 0.03554851934313774\n",
      "epoch: 12 step: 391, loss is 0.005393544677644968\n",
      "epoch: 12 step: 392, loss is 0.04585693031549454\n",
      "epoch: 12 step: 393, loss is 0.07553163915872574\n",
      "epoch: 12 step: 394, loss is 0.0035023184027522802\n",
      "epoch: 12 step: 395, loss is 0.020305491983890533\n",
      "epoch: 12 step: 396, loss is 0.03761801868677139\n",
      "epoch: 12 step: 397, loss is 0.008254733867943287\n",
      "epoch: 12 step: 398, loss is 0.08048881590366364\n",
      "epoch: 12 step: 399, loss is 0.02545318566262722\n",
      "epoch: 12 step: 400, loss is 0.030838754028081894\n",
      "epoch: 12 step: 401, loss is 0.21034856140613556\n",
      "epoch: 12 step: 402, loss is 0.04263419657945633\n",
      "epoch: 12 step: 403, loss is 0.021381346508860588\n",
      "epoch: 12 step: 404, loss is 0.006288850214332342\n",
      "epoch: 12 step: 405, loss is 0.04348177835345268\n",
      "epoch: 12 step: 406, loss is 0.019966837018728256\n",
      "epoch: 12 step: 407, loss is 0.003631338244304061\n",
      "epoch: 12 step: 408, loss is 0.0708373486995697\n",
      "epoch: 12 step: 409, loss is 0.01567910425364971\n",
      "epoch: 12 step: 410, loss is 0.0011202230816707015\n",
      "epoch: 12 step: 411, loss is 0.0316108837723732\n",
      "epoch: 12 step: 412, loss is 0.012880662456154823\n",
      "epoch: 12 step: 413, loss is 0.05240096524357796\n",
      "epoch: 12 step: 414, loss is 0.0935521125793457\n",
      "epoch: 12 step: 415, loss is 0.029638156294822693\n",
      "epoch: 12 step: 416, loss is 0.025942763313651085\n",
      "epoch: 12 step: 417, loss is 0.09323401004076004\n",
      "epoch: 12 step: 418, loss is 0.11654038727283478\n",
      "epoch: 12 step: 419, loss is 0.062071070075035095\n",
      "epoch: 12 step: 420, loss is 0.038026392459869385\n",
      "epoch: 12 step: 421, loss is 0.0322718508541584\n",
      "epoch: 12 step: 422, loss is 0.04480823501944542\n",
      "epoch: 12 step: 423, loss is 0.05301886796951294\n",
      "epoch: 12 step: 424, loss is 0.11878620833158493\n",
      "epoch: 12 step: 425, loss is 0.05179401487112045\n",
      "epoch: 12 step: 426, loss is 0.006275651045143604\n",
      "epoch: 12 step: 427, loss is 0.09910421073436737\n",
      "epoch: 12 step: 428, loss is 0.002747458405792713\n",
      "epoch: 12 step: 429, loss is 0.19288918375968933\n",
      "epoch: 12 step: 430, loss is 0.0586073063313961\n",
      "epoch: 12 step: 431, loss is 0.03979402780532837\n",
      "epoch: 12 step: 432, loss is 0.07380799949169159\n",
      "epoch: 12 step: 433, loss is 0.023450389504432678\n",
      "epoch: 12 step: 434, loss is 0.06977624446153641\n",
      "epoch: 12 step: 435, loss is 0.012476930394768715\n",
      "epoch: 12 step: 436, loss is 0.05271582677960396\n",
      "epoch: 12 step: 437, loss is 0.04334677383303642\n",
      "epoch: 12 step: 438, loss is 0.052025917917490005\n",
      "epoch: 12 step: 439, loss is 0.00187246548011899\n",
      "epoch: 12 step: 440, loss is 0.09093869477510452\n",
      "epoch: 12 step: 441, loss is 0.02593832090497017\n",
      "epoch: 12 step: 442, loss is 0.039441876113414764\n",
      "epoch: 12 step: 443, loss is 0.016106637194752693\n",
      "epoch: 12 step: 444, loss is 0.10541744530200958\n",
      "epoch: 12 step: 445, loss is 0.0786844789981842\n",
      "epoch: 12 step: 446, loss is 0.1602533757686615\n",
      "epoch: 12 step: 447, loss is 0.08583964407444\n",
      "epoch: 12 step: 448, loss is 0.07226768136024475\n",
      "epoch: 12 step: 449, loss is 0.06412619352340698\n",
      "epoch: 12 step: 450, loss is 0.04291201010346413\n",
      "epoch: 12 step: 451, loss is 0.08517660945653915\n",
      "epoch: 12 step: 452, loss is 0.08635831624269485\n",
      "epoch: 12 step: 453, loss is 0.05970604345202446\n",
      "epoch: 12 step: 454, loss is 0.04419350251555443\n",
      "epoch: 12 step: 455, loss is 0.027860285714268684\n",
      "epoch: 12 step: 456, loss is 0.04249807819724083\n",
      "epoch: 12 step: 457, loss is 0.029842890799045563\n",
      "epoch: 12 step: 458, loss is 0.02264910563826561\n",
      "epoch: 12 step: 459, loss is 0.04800264909863472\n",
      "epoch: 12 step: 460, loss is 0.03690945729613304\n",
      "epoch: 12 step: 461, loss is 0.04681754484772682\n",
      "epoch: 12 step: 462, loss is 0.021693933755159378\n",
      "epoch: 12 step: 463, loss is 0.019755428656935692\n",
      "epoch: 12 step: 464, loss is 0.020362479612231255\n",
      "epoch: 12 step: 465, loss is 0.03291453793644905\n",
      "epoch: 12 step: 466, loss is 0.012363987974822521\n",
      "epoch: 12 step: 467, loss is 0.019927555695176125\n",
      "epoch: 12 step: 468, loss is 0.06253589689731598\n",
      "epoch: 12 step: 469, loss is 0.026405060663819313\n",
      "epoch: 12 step: 470, loss is 0.09441698342561722\n",
      "epoch: 12 step: 471, loss is 0.04465369135141373\n",
      "epoch: 12 step: 472, loss is 0.005954093299806118\n",
      "epoch: 12 step: 473, loss is 0.052176665514707565\n",
      "epoch: 12 step: 474, loss is 0.08008800446987152\n",
      "epoch: 12 step: 475, loss is 0.10299596935510635\n",
      "epoch: 12 step: 476, loss is 0.03528030216693878\n",
      "epoch: 12 step: 477, loss is 0.03352277725934982\n",
      "epoch: 12 step: 478, loss is 0.10867480933666229\n",
      "epoch: 12 step: 479, loss is 0.040074992924928665\n",
      "epoch: 12 step: 480, loss is 0.07250389456748962\n",
      "epoch: 12 step: 481, loss is 0.004645137116312981\n",
      "epoch: 12 step: 482, loss is 0.040490396320819855\n",
      "epoch: 12 step: 483, loss is 0.0738232359290123\n",
      "epoch: 12 step: 484, loss is 0.041219230741262436\n",
      "epoch: 12 step: 485, loss is 0.06427290290594101\n",
      "epoch: 12 step: 486, loss is 0.03439681977033615\n",
      "epoch: 12 step: 487, loss is 0.0606355294585228\n",
      "epoch: 12 step: 488, loss is 0.11896232515573502\n",
      "epoch: 12 step: 489, loss is 0.011408386752009392\n",
      "epoch: 12 step: 490, loss is 0.056664541363716125\n",
      "epoch: 12 step: 491, loss is 0.10275749862194061\n",
      "epoch: 12 step: 492, loss is 0.044621795415878296\n",
      "epoch: 12 step: 493, loss is 0.056153081357479095\n",
      "epoch: 12 step: 494, loss is 0.013231905177235603\n",
      "epoch: 12 step: 495, loss is 0.05496513098478317\n",
      "epoch: 12 step: 496, loss is 0.01956879533827305\n",
      "epoch: 12 step: 497, loss is 0.03170941025018692\n",
      "epoch: 12 step: 498, loss is 0.06335416436195374\n",
      "epoch: 12 step: 499, loss is 0.004603023175150156\n",
      "epoch: 12 step: 500, loss is 0.0333574116230011\n",
      "epoch: 12 step: 501, loss is 0.057362012565135956\n",
      "epoch: 12 step: 502, loss is 0.16560477018356323\n",
      "epoch: 12 step: 503, loss is 0.03660406544804573\n",
      "epoch: 12 step: 504, loss is 0.060680147260427475\n",
      "epoch: 12 step: 505, loss is 0.12463689595460892\n",
      "epoch: 12 step: 506, loss is 0.05707214027643204\n",
      "epoch: 12 step: 507, loss is 0.06109921634197235\n",
      "epoch: 12 step: 508, loss is 0.026772893965244293\n",
      "epoch: 12 step: 509, loss is 0.02171647921204567\n",
      "epoch: 12 step: 510, loss is 0.023815041407942772\n",
      "epoch: 12 step: 511, loss is 0.02036631479859352\n",
      "epoch: 12 step: 512, loss is 0.10773965716362\n",
      "epoch: 12 step: 513, loss is 0.12173271179199219\n",
      "epoch: 12 step: 514, loss is 0.01551402360200882\n",
      "epoch: 12 step: 515, loss is 0.03314763307571411\n",
      "epoch: 12 step: 516, loss is 0.05571959540247917\n",
      "epoch: 12 step: 517, loss is 0.02372043952345848\n",
      "epoch: 12 step: 518, loss is 0.0030289883725345135\n",
      "epoch: 12 step: 519, loss is 0.0320647731423378\n",
      "epoch: 12 step: 520, loss is 0.01616336777806282\n",
      "epoch: 12 step: 521, loss is 0.05675828456878662\n",
      "epoch: 12 step: 522, loss is 0.02062438614666462\n",
      "epoch: 12 step: 523, loss is 0.054198578000068665\n",
      "epoch: 12 step: 524, loss is 0.08730105310678482\n",
      "epoch: 12 step: 525, loss is 0.02786741591989994\n",
      "epoch: 12 step: 526, loss is 0.07544229179620743\n",
      "epoch: 12 step: 527, loss is 0.08867618441581726\n",
      "epoch: 12 step: 528, loss is 0.00837289821356535\n",
      "epoch: 12 step: 529, loss is 0.10527081042528152\n",
      "epoch: 12 step: 530, loss is 0.031118236482143402\n",
      "epoch: 12 step: 531, loss is 0.012772253714501858\n",
      "epoch: 12 step: 532, loss is 0.13535144925117493\n",
      "epoch: 12 step: 533, loss is 0.038405124098062515\n",
      "epoch: 12 step: 534, loss is 0.0022834574338048697\n",
      "epoch: 12 step: 535, loss is 0.007876129820942879\n",
      "epoch: 12 step: 536, loss is 0.0981014147400856\n",
      "epoch: 12 step: 537, loss is 0.05426667630672455\n",
      "epoch: 12 step: 538, loss is 0.009791440330445766\n",
      "epoch: 12 step: 539, loss is 0.06293100863695145\n",
      "epoch: 12 step: 540, loss is 0.05600566416978836\n",
      "epoch: 12 step: 541, loss is 0.05553693324327469\n",
      "epoch: 12 step: 542, loss is 0.010978387668728828\n",
      "epoch: 12 step: 543, loss is 0.0646653026342392\n",
      "epoch: 12 step: 544, loss is 0.08361802995204926\n",
      "epoch: 12 step: 545, loss is 0.13943566381931305\n",
      "epoch: 12 step: 546, loss is 0.03551861643791199\n",
      "epoch: 12 step: 547, loss is 0.04255776107311249\n",
      "epoch: 12 step: 548, loss is 0.07866155356168747\n",
      "epoch: 12 step: 549, loss is 0.005522573366761208\n",
      "epoch: 12 step: 550, loss is 0.018857302144169807\n",
      "epoch: 12 step: 551, loss is 0.01761142536997795\n",
      "epoch: 12 step: 552, loss is 0.04055679589509964\n",
      "epoch: 12 step: 553, loss is 0.057819515466690063\n",
      "epoch: 12 step: 554, loss is 0.04051613062620163\n",
      "epoch: 12 step: 555, loss is 0.03761739656329155\n",
      "epoch: 12 step: 556, loss is 0.045709189027547836\n",
      "epoch: 12 step: 557, loss is 0.050536222755908966\n",
      "epoch: 12 step: 558, loss is 0.14197689294815063\n",
      "epoch: 12 step: 559, loss is 0.0950101837515831\n",
      "epoch: 12 step: 560, loss is 0.04439302161335945\n",
      "epoch: 12 step: 561, loss is 0.033842410892248154\n",
      "epoch: 12 step: 562, loss is 0.017154419794678688\n",
      "epoch: 12 step: 563, loss is 0.21434558928012848\n",
      "epoch: 12 step: 564, loss is 0.04348525404930115\n",
      "epoch: 12 step: 565, loss is 0.06068579852581024\n",
      "epoch: 12 step: 566, loss is 0.10923822224140167\n",
      "epoch: 12 step: 567, loss is 0.06087876111268997\n",
      "epoch: 12 step: 568, loss is 0.013126705773174763\n",
      "epoch: 12 step: 569, loss is 0.002711112378165126\n",
      "epoch: 12 step: 570, loss is 0.010998649522662163\n",
      "epoch: 12 step: 571, loss is 0.060156285762786865\n",
      "epoch: 12 step: 572, loss is 0.06184876337647438\n",
      "epoch: 12 step: 573, loss is 0.010750955902040005\n",
      "epoch: 12 step: 574, loss is 0.23361696302890778\n",
      "epoch: 12 step: 575, loss is 0.13558466732501984\n",
      "epoch: 12 step: 576, loss is 0.06109213829040527\n",
      "epoch: 12 step: 577, loss is 0.07447218894958496\n",
      "epoch: 12 step: 578, loss is 0.04951731488108635\n",
      "epoch: 12 step: 579, loss is 0.1586167812347412\n",
      "epoch: 12 step: 580, loss is 0.0148507971316576\n",
      "epoch: 12 step: 581, loss is 0.10348475724458694\n",
      "epoch: 12 step: 582, loss is 0.04617869481444359\n",
      "epoch: 12 step: 583, loss is 0.08264297991991043\n",
      "epoch: 12 step: 584, loss is 0.07010995596647263\n",
      "epoch: 12 step: 585, loss is 0.024268079549074173\n",
      "epoch: 12 step: 586, loss is 0.04839980602264404\n",
      "epoch: 12 step: 587, loss is 0.04362546280026436\n",
      "epoch: 12 step: 588, loss is 0.1617276966571808\n",
      "epoch: 12 step: 589, loss is 0.06286154687404633\n",
      "epoch: 12 step: 590, loss is 0.08573412895202637\n",
      "epoch: 12 step: 591, loss is 0.06767164915800095\n",
      "epoch: 12 step: 592, loss is 0.06383892148733139\n",
      "epoch: 12 step: 593, loss is 0.10544374585151672\n",
      "epoch: 12 step: 594, loss is 0.02806706540286541\n",
      "epoch: 12 step: 595, loss is 0.07493690401315689\n",
      "epoch: 12 step: 596, loss is 0.019978994503617287\n",
      "epoch: 12 step: 597, loss is 0.02693628892302513\n",
      "epoch: 12 step: 598, loss is 0.0262067262083292\n",
      "epoch: 12 step: 599, loss is 0.013631630688905716\n",
      "epoch: 12 step: 600, loss is 0.061966992914676666\n",
      "epoch: 12 step: 601, loss is 0.043835289776325226\n",
      "epoch: 12 step: 602, loss is 0.0453961081802845\n",
      "epoch: 12 step: 603, loss is 0.0950155258178711\n",
      "epoch: 12 step: 604, loss is 0.027762213721871376\n",
      "epoch: 12 step: 605, loss is 0.06938446313142776\n",
      "epoch: 12 step: 606, loss is 0.02202029526233673\n",
      "epoch: 12 step: 607, loss is 0.0764121562242508\n",
      "epoch: 12 step: 608, loss is 0.012475572526454926\n",
      "epoch: 12 step: 609, loss is 0.034102946519851685\n",
      "epoch: 12 step: 610, loss is 0.03981310501694679\n",
      "epoch: 12 step: 611, loss is 0.0067457291297614574\n",
      "epoch: 12 step: 612, loss is 0.03597555682063103\n",
      "epoch: 12 step: 613, loss is 0.08030261844396591\n",
      "epoch: 12 step: 614, loss is 0.002893531695008278\n",
      "epoch: 12 step: 615, loss is 0.0385253019630909\n",
      "epoch: 12 step: 616, loss is 0.07528408616781235\n",
      "epoch: 12 step: 617, loss is 0.10033343732357025\n",
      "epoch: 12 step: 618, loss is 0.08277899026870728\n",
      "epoch: 12 step: 619, loss is 0.008321689441800117\n",
      "epoch: 12 step: 620, loss is 0.038278497755527496\n",
      "epoch: 12 step: 621, loss is 0.03259294480085373\n",
      "epoch: 12 step: 622, loss is 0.06124347075819969\n",
      "epoch: 12 step: 623, loss is 0.05946717783808708\n",
      "epoch: 12 step: 624, loss is 0.03834421932697296\n",
      "epoch: 12 step: 625, loss is 0.05864794924855232\n",
      "epoch: 12 step: 626, loss is 0.021502504125237465\n",
      "epoch: 12 step: 627, loss is 0.017260588705539703\n",
      "epoch: 12 step: 628, loss is 0.010318241082131863\n",
      "epoch: 12 step: 629, loss is 0.052011072635650635\n",
      "epoch: 12 step: 630, loss is 0.025099143385887146\n",
      "epoch: 12 step: 631, loss is 0.01473536528646946\n",
      "epoch: 12 step: 632, loss is 0.005640655290335417\n",
      "epoch: 12 step: 633, loss is 0.010556413792073727\n",
      "epoch: 12 step: 634, loss is 0.016649559140205383\n",
      "epoch: 12 step: 635, loss is 0.020110461860895157\n",
      "epoch: 12 step: 636, loss is 0.05532919615507126\n",
      "epoch: 12 step: 637, loss is 0.01191766932606697\n",
      "epoch: 12 step: 638, loss is 0.031352512538433075\n",
      "epoch: 12 step: 639, loss is 0.019849229604005814\n",
      "epoch: 12 step: 640, loss is 0.02097225748002529\n",
      "epoch: 12 step: 641, loss is 0.08041639626026154\n",
      "epoch: 12 step: 642, loss is 0.049857884645462036\n",
      "epoch: 12 step: 643, loss is 0.011267665773630142\n",
      "epoch: 12 step: 644, loss is 0.04544578865170479\n",
      "epoch: 12 step: 645, loss is 0.011612718924880028\n",
      "epoch: 12 step: 646, loss is 0.015203209593892097\n",
      "epoch: 12 step: 647, loss is 0.02590186893939972\n",
      "epoch: 12 step: 648, loss is 0.11424548923969269\n",
      "epoch: 12 step: 649, loss is 0.08483104407787323\n",
      "epoch: 12 step: 650, loss is 0.01894504949450493\n",
      "epoch: 12 step: 651, loss is 0.014047267846763134\n",
      "epoch: 12 step: 652, loss is 0.004460486117750406\n",
      "epoch: 12 step: 653, loss is 0.08339972794055939\n",
      "epoch: 12 step: 654, loss is 0.07632218301296234\n",
      "epoch: 12 step: 655, loss is 0.007039144169539213\n",
      "epoch: 12 step: 656, loss is 0.03224189206957817\n",
      "epoch: 12 step: 657, loss is 0.02082194574177265\n",
      "epoch: 12 step: 658, loss is 0.03051309660077095\n",
      "epoch: 12 step: 659, loss is 0.029400842264294624\n",
      "epoch: 12 step: 660, loss is 0.014501698315143585\n",
      "epoch: 12 step: 661, loss is 0.1007012352347374\n",
      "epoch: 12 step: 662, loss is 0.005496510304510593\n",
      "epoch: 12 step: 663, loss is 0.009540568105876446\n",
      "epoch: 12 step: 664, loss is 0.07774902135133743\n",
      "epoch: 12 step: 665, loss is 0.08875373750925064\n",
      "epoch: 12 step: 666, loss is 0.022988056764006615\n",
      "epoch: 12 step: 667, loss is 0.01073169894516468\n",
      "epoch: 12 step: 668, loss is 0.0760367140173912\n",
      "epoch: 12 step: 669, loss is 0.057233214378356934\n",
      "epoch: 12 step: 670, loss is 0.03693896159529686\n",
      "epoch: 12 step: 671, loss is 0.00883182231336832\n",
      "epoch: 12 step: 672, loss is 0.010079516097903252\n",
      "epoch: 12 step: 673, loss is 0.05799489468336105\n",
      "epoch: 12 step: 674, loss is 0.039454031735658646\n",
      "epoch: 12 step: 675, loss is 0.07635388523340225\n",
      "epoch: 12 step: 676, loss is 0.03388109803199768\n",
      "epoch: 12 step: 677, loss is 0.025848526507616043\n",
      "epoch: 12 step: 678, loss is 0.018954085186123848\n",
      "epoch: 12 step: 679, loss is 0.06246335804462433\n",
      "epoch: 12 step: 680, loss is 0.1695900559425354\n",
      "epoch: 12 step: 681, loss is 0.1765880137681961\n",
      "epoch: 12 step: 682, loss is 0.05469667911529541\n",
      "epoch: 12 step: 683, loss is 0.015355391427874565\n",
      "epoch: 12 step: 684, loss is 0.029177211225032806\n",
      "epoch: 12 step: 685, loss is 0.022838633507490158\n",
      "epoch: 12 step: 686, loss is 0.010023347102105618\n",
      "epoch: 12 step: 687, loss is 0.04867827519774437\n",
      "epoch: 12 step: 688, loss is 0.05056467652320862\n",
      "epoch: 12 step: 689, loss is 0.01885812170803547\n",
      "epoch: 12 step: 690, loss is 0.0639796257019043\n",
      "epoch: 12 step: 691, loss is 0.0584399551153183\n",
      "epoch: 12 step: 692, loss is 0.009370362386107445\n",
      "epoch: 12 step: 693, loss is 0.09272847324609756\n",
      "epoch: 12 step: 694, loss is 0.008609838783740997\n",
      "epoch: 12 step: 695, loss is 0.06704507768154144\n",
      "epoch: 12 step: 696, loss is 0.07566003501415253\n",
      "epoch: 12 step: 697, loss is 0.04078839346766472\n",
      "epoch: 12 step: 698, loss is 0.09046763926744461\n",
      "epoch: 12 step: 699, loss is 0.0766826719045639\n",
      "epoch: 12 step: 700, loss is 0.08306466042995453\n",
      "epoch: 12 step: 701, loss is 0.0538637712597847\n",
      "epoch: 12 step: 702, loss is 0.03104611299932003\n",
      "epoch: 12 step: 703, loss is 0.007775060832500458\n",
      "epoch: 12 step: 704, loss is 0.035092681646347046\n",
      "epoch: 12 step: 705, loss is 0.0391831137239933\n",
      "epoch: 12 step: 706, loss is 0.048483218997716904\n",
      "epoch: 12 step: 707, loss is 0.015387268736958504\n",
      "epoch: 12 step: 708, loss is 0.07605502754449844\n",
      "epoch: 12 step: 709, loss is 0.06361699849367142\n",
      "epoch: 12 step: 710, loss is 0.0284967552870512\n",
      "epoch: 12 step: 711, loss is 0.01873626559972763\n",
      "epoch: 12 step: 712, loss is 0.06010512635111809\n",
      "epoch: 12 step: 713, loss is 0.00923161767423153\n",
      "epoch: 12 step: 714, loss is 0.06733930855989456\n",
      "epoch: 12 step: 715, loss is 0.07553336769342422\n",
      "epoch: 12 step: 716, loss is 0.022383060306310654\n",
      "epoch: 12 step: 717, loss is 0.033627044409513474\n",
      "epoch: 12 step: 718, loss is 0.09665773808956146\n",
      "epoch: 12 step: 719, loss is 0.022868039086461067\n",
      "epoch: 12 step: 720, loss is 0.11014796793460846\n",
      "epoch: 12 step: 721, loss is 0.022208688780665398\n",
      "epoch: 12 step: 722, loss is 0.05199148878455162\n",
      "epoch: 12 step: 723, loss is 0.011767537333071232\n",
      "epoch: 12 step: 724, loss is 0.07565943896770477\n",
      "epoch: 12 step: 725, loss is 0.04342186450958252\n",
      "epoch: 12 step: 726, loss is 0.09523214399814606\n",
      "epoch: 12 step: 727, loss is 0.07947301119565964\n",
      "epoch: 12 step: 728, loss is 0.02019624412059784\n",
      "epoch: 12 step: 729, loss is 0.036502208560705185\n",
      "epoch: 12 step: 730, loss is 0.0367593988776207\n",
      "epoch: 12 step: 731, loss is 0.05452447012066841\n",
      "epoch: 12 step: 732, loss is 0.0522417314350605\n",
      "epoch: 12 step: 733, loss is 0.0967816561460495\n",
      "epoch: 12 step: 734, loss is 0.08982641249895096\n",
      "epoch: 12 step: 735, loss is 0.11210579425096512\n",
      "epoch: 12 step: 736, loss is 0.09605596214532852\n",
      "epoch: 12 step: 737, loss is 0.030948422849178314\n",
      "epoch: 12 step: 738, loss is 0.07727472484111786\n",
      "epoch: 12 step: 739, loss is 0.06710752099752426\n",
      "epoch: 12 step: 740, loss is 0.10518637299537659\n",
      "epoch: 12 step: 741, loss is 0.11036592721939087\n",
      "epoch: 12 step: 742, loss is 0.016850080341100693\n",
      "epoch: 12 step: 743, loss is 0.009939928539097309\n",
      "epoch: 12 step: 744, loss is 0.007983618415892124\n",
      "epoch: 12 step: 745, loss is 0.08475660532712936\n",
      "epoch: 12 step: 746, loss is 0.11182936280965805\n",
      "epoch: 12 step: 747, loss is 0.08188962191343307\n",
      "epoch: 12 step: 748, loss is 0.030502429232001305\n",
      "epoch: 12 step: 749, loss is 0.06293772906064987\n",
      "epoch: 12 step: 750, loss is 0.13238133490085602\n",
      "epoch: 12 step: 751, loss is 0.05548384413123131\n",
      "epoch: 12 step: 752, loss is 0.0902688205242157\n",
      "epoch: 12 step: 753, loss is 0.09116589277982712\n",
      "epoch: 12 step: 754, loss is 0.07917527854442596\n",
      "epoch: 12 step: 755, loss is 0.029242346063256264\n",
      "epoch: 12 step: 756, loss is 0.01639268919825554\n",
      "epoch: 12 step: 757, loss is 0.03404061496257782\n",
      "epoch: 12 step: 758, loss is 0.023648547008633614\n",
      "epoch: 12 step: 759, loss is 0.03589407354593277\n",
      "epoch: 12 step: 760, loss is 0.0981946587562561\n",
      "epoch: 12 step: 761, loss is 0.020116165280342102\n",
      "epoch: 12 step: 762, loss is 0.09158496558666229\n",
      "epoch: 12 step: 763, loss is 0.008848041296005249\n",
      "epoch: 12 step: 764, loss is 0.08404778689146042\n",
      "epoch: 12 step: 765, loss is 0.015555300749838352\n",
      "epoch: 12 step: 766, loss is 0.0774720162153244\n",
      "epoch: 12 step: 767, loss is 0.0548902191221714\n",
      "epoch: 12 step: 768, loss is 0.02952965535223484\n",
      "epoch: 12 step: 769, loss is 0.009094302542507648\n",
      "epoch: 12 step: 770, loss is 0.03556051105260849\n",
      "epoch: 12 step: 771, loss is 0.008802802301943302\n",
      "epoch: 12 step: 772, loss is 0.03731629252433777\n",
      "epoch: 12 step: 773, loss is 0.06916387379169464\n",
      "epoch: 12 step: 774, loss is 0.012080110609531403\n",
      "epoch: 12 step: 775, loss is 0.0036337811034172773\n",
      "epoch: 12 step: 776, loss is 0.005756734404712915\n",
      "epoch: 12 step: 777, loss is 0.01671462692320347\n",
      "epoch: 12 step: 778, loss is 0.1161721795797348\n",
      "epoch: 12 step: 779, loss is 0.009134749881923199\n",
      "epoch: 12 step: 780, loss is 0.02185407653450966\n",
      "epoch: 12 step: 781, loss is 0.11403799802064896\n",
      "epoch: 12 step: 782, loss is 0.02955324575304985\n",
      "epoch: 12 step: 783, loss is 0.1494525671005249\n",
      "epoch: 12 step: 784, loss is 0.009237812831997871\n",
      "epoch: 12 step: 785, loss is 0.03682922571897507\n",
      "epoch: 12 step: 786, loss is 0.03830357640981674\n",
      "epoch: 12 step: 787, loss is 0.03675636276602745\n",
      "epoch: 12 step: 788, loss is 0.017811058089137077\n",
      "epoch: 12 step: 789, loss is 0.015418041497468948\n",
      "epoch: 12 step: 790, loss is 0.04214806482195854\n",
      "epoch: 12 step: 791, loss is 0.06341012567281723\n",
      "epoch: 12 step: 792, loss is 0.05266650393605232\n",
      "epoch: 12 step: 793, loss is 0.022643011063337326\n",
      "epoch: 12 step: 794, loss is 0.013595295138657093\n",
      "epoch: 12 step: 795, loss is 0.08141449838876724\n",
      "epoch: 12 step: 796, loss is 0.10081016272306442\n",
      "epoch: 12 step: 797, loss is 0.01660333201289177\n",
      "epoch: 12 step: 798, loss is 0.014303576201200485\n",
      "epoch: 12 step: 799, loss is 0.008858175948262215\n",
      "epoch: 12 step: 800, loss is 0.012056502513587475\n",
      "epoch: 12 step: 801, loss is 0.13573095202445984\n",
      "epoch: 12 step: 802, loss is 0.01659192331135273\n",
      "epoch: 12 step: 803, loss is 0.009611781686544418\n",
      "epoch: 12 step: 804, loss is 0.043783560395240784\n",
      "epoch: 12 step: 805, loss is 0.05838007852435112\n",
      "epoch: 12 step: 806, loss is 0.03760942816734314\n",
      "epoch: 12 step: 807, loss is 0.07236869633197784\n",
      "epoch: 12 step: 808, loss is 0.016815537586808205\n",
      "epoch: 12 step: 809, loss is 0.08017650991678238\n",
      "epoch: 12 step: 810, loss is 0.0035394697915762663\n",
      "epoch: 12 step: 811, loss is 0.07296069711446762\n",
      "epoch: 12 step: 812, loss is 0.01815730705857277\n",
      "epoch: 12 step: 813, loss is 0.010166862048208714\n",
      "epoch: 12 step: 814, loss is 0.00888705812394619\n",
      "epoch: 12 step: 815, loss is 0.06076759099960327\n",
      "epoch: 12 step: 816, loss is 0.024291623383760452\n",
      "epoch: 12 step: 817, loss is 0.01918388158082962\n",
      "epoch: 12 step: 818, loss is 0.026611778885126114\n",
      "epoch: 12 step: 819, loss is 0.003821744117885828\n",
      "epoch: 12 step: 820, loss is 0.02157421037554741\n",
      "epoch: 12 step: 821, loss is 0.03456469997763634\n",
      "epoch: 12 step: 822, loss is 0.03316469117999077\n",
      "epoch: 12 step: 823, loss is 0.0699792429804802\n",
      "epoch: 12 step: 824, loss is 0.03218024596571922\n",
      "epoch: 12 step: 825, loss is 0.12372621893882751\n",
      "epoch: 12 step: 826, loss is 0.07857818156480789\n",
      "epoch: 12 step: 827, loss is 0.049970172345638275\n",
      "epoch: 12 step: 828, loss is 0.009293079376220703\n",
      "epoch: 12 step: 829, loss is 0.017792649567127228\n",
      "epoch: 12 step: 830, loss is 0.013174399733543396\n",
      "epoch: 12 step: 831, loss is 0.05878201127052307\n",
      "epoch: 12 step: 832, loss is 0.0035313719417899847\n",
      "epoch: 12 step: 833, loss is 0.02238408848643303\n",
      "epoch: 12 step: 834, loss is 0.08984863013029099\n",
      "epoch: 12 step: 835, loss is 0.023752762004733086\n",
      "epoch: 12 step: 836, loss is 0.04492099955677986\n",
      "epoch: 12 step: 837, loss is 0.07661677896976471\n",
      "epoch: 12 step: 838, loss is 0.043962154537439346\n",
      "epoch: 12 step: 839, loss is 0.05823555216193199\n",
      "epoch: 12 step: 840, loss is 0.014674280770123005\n",
      "epoch: 12 step: 841, loss is 0.0849582776427269\n",
      "epoch: 12 step: 842, loss is 0.0075905462726950645\n",
      "epoch: 12 step: 843, loss is 0.04388430714607239\n",
      "epoch: 12 step: 844, loss is 0.016343964263796806\n",
      "epoch: 12 step: 845, loss is 0.026124881580471992\n",
      "epoch: 12 step: 846, loss is 0.09946420788764954\n",
      "epoch: 12 step: 847, loss is 0.01922733522951603\n",
      "epoch: 12 step: 848, loss is 0.0775819718837738\n",
      "epoch: 12 step: 849, loss is 0.04909656196832657\n",
      "epoch: 12 step: 850, loss is 0.049593012779951096\n",
      "epoch: 12 step: 851, loss is 0.04046483710408211\n",
      "epoch: 12 step: 852, loss is 0.023995067924261093\n",
      "epoch: 12 step: 853, loss is 0.04617733880877495\n",
      "epoch: 12 step: 854, loss is 0.1122652068734169\n",
      "epoch: 12 step: 855, loss is 0.04633093252778053\n",
      "epoch: 12 step: 856, loss is 0.10852807760238647\n",
      "epoch: 12 step: 857, loss is 0.054447852075099945\n",
      "epoch: 12 step: 858, loss is 0.024772152304649353\n",
      "epoch: 12 step: 859, loss is 0.008511699736118317\n",
      "epoch: 12 step: 860, loss is 0.05700317770242691\n",
      "epoch: 12 step: 861, loss is 0.1206875815987587\n",
      "epoch: 12 step: 862, loss is 0.04722099006175995\n",
      "epoch: 12 step: 863, loss is 0.017295336350798607\n",
      "epoch: 12 step: 864, loss is 0.08056452870368958\n",
      "epoch: 12 step: 865, loss is 0.041695162653923035\n",
      "epoch: 12 step: 866, loss is 0.1027345135807991\n",
      "epoch: 12 step: 867, loss is 0.010565589182078838\n",
      "epoch: 12 step: 868, loss is 0.010007769800722599\n",
      "epoch: 12 step: 869, loss is 0.03706049919128418\n",
      "epoch: 12 step: 870, loss is 0.04626462236046791\n",
      "epoch: 12 step: 871, loss is 0.08393554389476776\n",
      "epoch: 12 step: 872, loss is 0.0764990895986557\n",
      "epoch: 12 step: 873, loss is 0.03859129920601845\n",
      "epoch: 12 step: 874, loss is 0.017476923763751984\n",
      "epoch: 12 step: 875, loss is 0.05142195522785187\n",
      "epoch: 12 step: 876, loss is 0.035456717014312744\n",
      "epoch: 12 step: 877, loss is 0.04933023452758789\n",
      "epoch: 12 step: 878, loss is 0.03175836428999901\n",
      "epoch: 12 step: 879, loss is 0.04441189393401146\n",
      "epoch: 12 step: 880, loss is 0.15050113201141357\n",
      "epoch: 12 step: 881, loss is 0.03538553789258003\n",
      "epoch: 12 step: 882, loss is 0.02583847939968109\n",
      "epoch: 12 step: 883, loss is 0.06252507865428925\n",
      "epoch: 12 step: 884, loss is 0.038555808365345\n",
      "epoch: 12 step: 885, loss is 0.0028942767530679703\n",
      "epoch: 12 step: 886, loss is 0.01714293472468853\n",
      "epoch: 12 step: 887, loss is 0.031545281410217285\n",
      "epoch: 12 step: 888, loss is 0.04912859573960304\n",
      "epoch: 12 step: 889, loss is 0.01775970682501793\n",
      "epoch: 12 step: 890, loss is 0.04774157702922821\n",
      "epoch: 12 step: 891, loss is 0.027931086719036102\n",
      "epoch: 12 step: 892, loss is 0.04173905774950981\n",
      "epoch: 12 step: 893, loss is 0.08813498914241791\n",
      "epoch: 12 step: 894, loss is 0.05706806108355522\n",
      "epoch: 12 step: 895, loss is 0.12103663384914398\n",
      "epoch: 12 step: 896, loss is 0.03236396238207817\n",
      "epoch: 12 step: 897, loss is 0.005119418725371361\n",
      "epoch: 12 step: 898, loss is 0.09481716901063919\n",
      "epoch: 12 step: 899, loss is 0.03169790282845497\n",
      "epoch: 12 step: 900, loss is 0.06941480934619904\n",
      "epoch: 12 step: 901, loss is 0.018791241571307182\n",
      "epoch: 12 step: 902, loss is 0.05528053268790245\n",
      "epoch: 12 step: 903, loss is 0.01440174225717783\n",
      "epoch: 12 step: 904, loss is 0.025809846818447113\n",
      "epoch: 12 step: 905, loss is 0.04078703001141548\n",
      "epoch: 12 step: 906, loss is 0.06765294820070267\n",
      "epoch: 12 step: 907, loss is 0.06392321735620499\n",
      "epoch: 12 step: 908, loss is 0.06560948491096497\n",
      "epoch: 12 step: 909, loss is 0.02635173499584198\n",
      "epoch: 12 step: 910, loss is 0.02007189765572548\n",
      "epoch: 12 step: 911, loss is 0.0700087621808052\n",
      "epoch: 12 step: 912, loss is 0.051286667585372925\n",
      "epoch: 12 step: 913, loss is 0.010251513682305813\n",
      "epoch: 12 step: 914, loss is 0.007445837836712599\n",
      "epoch: 12 step: 915, loss is 0.027866683900356293\n",
      "epoch: 12 step: 916, loss is 0.015939729288220406\n",
      "epoch: 12 step: 917, loss is 0.010454652830958366\n",
      "epoch: 12 step: 918, loss is 0.04540373757481575\n",
      "epoch: 12 step: 919, loss is 0.04669366404414177\n",
      "epoch: 12 step: 920, loss is 0.010442102327942848\n",
      "epoch: 12 step: 921, loss is 0.011346987448632717\n",
      "epoch: 12 step: 922, loss is 0.016171475872397423\n",
      "epoch: 12 step: 923, loss is 0.020010042935609818\n",
      "epoch: 12 step: 924, loss is 0.05628436058759689\n",
      "epoch: 12 step: 925, loss is 0.014102023094892502\n",
      "epoch: 12 step: 926, loss is 0.060992006212472916\n",
      "epoch: 12 step: 927, loss is 0.04836392030119896\n",
      "epoch: 12 step: 928, loss is 0.013598375022411346\n",
      "epoch: 12 step: 929, loss is 0.10025504976511002\n",
      "epoch: 12 step: 930, loss is 0.021128026768565178\n",
      "epoch: 12 step: 931, loss is 0.055077504366636276\n",
      "epoch: 12 step: 932, loss is 0.019434185698628426\n",
      "epoch: 12 step: 933, loss is 0.04432330280542374\n",
      "epoch: 12 step: 934, loss is 0.021623484790325165\n",
      "epoch: 12 step: 935, loss is 0.013496735133230686\n",
      "epoch: 12 step: 936, loss is 0.02089044824242592\n",
      "epoch: 12 step: 937, loss is 0.006933063734322786\n",
      "epoch: 13 step: 1, loss is 0.016481326892971992\n",
      "epoch: 13 step: 2, loss is 0.007933772169053555\n",
      "epoch: 13 step: 3, loss is 0.008125639520585537\n",
      "epoch: 13 step: 4, loss is 0.06344204396009445\n",
      "epoch: 13 step: 5, loss is 0.05215830355882645\n",
      "epoch: 13 step: 6, loss is 0.022203728556632996\n",
      "epoch: 13 step: 7, loss is 0.0040762764401733875\n",
      "epoch: 13 step: 8, loss is 0.03420667722821236\n",
      "epoch: 13 step: 9, loss is 0.020707130432128906\n",
      "epoch: 13 step: 10, loss is 0.011918552219867706\n",
      "epoch: 13 step: 11, loss is 0.01536609884351492\n",
      "epoch: 13 step: 12, loss is 0.005176428705453873\n",
      "epoch: 13 step: 13, loss is 0.01665409281849861\n",
      "epoch: 13 step: 14, loss is 0.0288237351924181\n",
      "epoch: 13 step: 15, loss is 0.004428884945809841\n",
      "epoch: 13 step: 16, loss is 0.02911393530666828\n",
      "epoch: 13 step: 17, loss is 0.0635591670870781\n",
      "epoch: 13 step: 18, loss is 0.03009852021932602\n",
      "epoch: 13 step: 19, loss is 0.011002697050571442\n",
      "epoch: 13 step: 20, loss is 0.018520358949899673\n",
      "epoch: 13 step: 21, loss is 0.07547134906053543\n",
      "epoch: 13 step: 22, loss is 0.012177737429738045\n",
      "epoch: 13 step: 23, loss is 0.006055687088519335\n",
      "epoch: 13 step: 24, loss is 0.05078989267349243\n",
      "epoch: 13 step: 25, loss is 0.07141193002462387\n",
      "epoch: 13 step: 26, loss is 0.029045497998595238\n",
      "epoch: 13 step: 27, loss is 0.005524924024939537\n",
      "epoch: 13 step: 28, loss is 0.013750395737588406\n",
      "epoch: 13 step: 29, loss is 0.0068650213070213795\n",
      "epoch: 13 step: 30, loss is 0.07842829823493958\n",
      "epoch: 13 step: 31, loss is 0.01560286432504654\n",
      "epoch: 13 step: 32, loss is 0.01793559268116951\n",
      "epoch: 13 step: 33, loss is 0.0022455945145338774\n",
      "epoch: 13 step: 34, loss is 0.023181578144431114\n",
      "epoch: 13 step: 35, loss is 0.006021631415933371\n",
      "epoch: 13 step: 36, loss is 0.011939452961087227\n",
      "epoch: 13 step: 37, loss is 0.00524315657094121\n",
      "epoch: 13 step: 38, loss is 0.02103894203901291\n",
      "epoch: 13 step: 39, loss is 0.0762300118803978\n",
      "epoch: 13 step: 40, loss is 0.01811407133936882\n",
      "epoch: 13 step: 41, loss is 0.006210903637111187\n",
      "epoch: 13 step: 42, loss is 0.011473573744297028\n",
      "epoch: 13 step: 43, loss is 0.025984829291701317\n",
      "epoch: 13 step: 44, loss is 0.004446739796549082\n",
      "epoch: 13 step: 45, loss is 0.12824781239032745\n",
      "epoch: 13 step: 46, loss is 0.03509436920285225\n",
      "epoch: 13 step: 47, loss is 0.028002919629216194\n",
      "epoch: 13 step: 48, loss is 0.0410732738673687\n",
      "epoch: 13 step: 49, loss is 0.048281170427799225\n",
      "epoch: 13 step: 50, loss is 0.04766472056508064\n",
      "epoch: 13 step: 51, loss is 0.03413010016083717\n",
      "epoch: 13 step: 52, loss is 0.024664059281349182\n",
      "epoch: 13 step: 53, loss is 0.018736092373728752\n",
      "epoch: 13 step: 54, loss is 0.07952960580587387\n",
      "epoch: 13 step: 55, loss is 0.008344917558133602\n",
      "epoch: 13 step: 56, loss is 0.005083318799734116\n",
      "epoch: 13 step: 57, loss is 0.04855852574110031\n",
      "epoch: 13 step: 58, loss is 0.009500978514552116\n",
      "epoch: 13 step: 59, loss is 0.0038264449685811996\n",
      "epoch: 13 step: 60, loss is 0.024743877351284027\n",
      "epoch: 13 step: 61, loss is 0.007453290279954672\n",
      "epoch: 13 step: 62, loss is 0.05043485388159752\n",
      "epoch: 13 step: 63, loss is 0.009565918706357479\n",
      "epoch: 13 step: 64, loss is 0.0365678034722805\n",
      "epoch: 13 step: 65, loss is 0.0017586472677066922\n",
      "epoch: 13 step: 66, loss is 0.015277031809091568\n",
      "epoch: 13 step: 67, loss is 0.02651299349963665\n",
      "epoch: 13 step: 68, loss is 0.023080777376890182\n",
      "epoch: 13 step: 69, loss is 0.003915478941053152\n",
      "epoch: 13 step: 70, loss is 0.048147790133953094\n",
      "epoch: 13 step: 71, loss is 0.0043899305164813995\n",
      "epoch: 13 step: 72, loss is 0.0571075938642025\n",
      "epoch: 13 step: 73, loss is 0.012483552098274231\n",
      "epoch: 13 step: 74, loss is 0.023183317855000496\n",
      "epoch: 13 step: 75, loss is 0.0400773286819458\n",
      "epoch: 13 step: 76, loss is 0.012034263461828232\n",
      "epoch: 13 step: 77, loss is 0.00660732202231884\n",
      "epoch: 13 step: 78, loss is 0.013485061004757881\n",
      "epoch: 13 step: 79, loss is 0.011657175607979298\n",
      "epoch: 13 step: 80, loss is 0.03641849383711815\n",
      "epoch: 13 step: 81, loss is 0.013355685397982597\n",
      "epoch: 13 step: 82, loss is 0.00805584155023098\n",
      "epoch: 13 step: 83, loss is 0.007168381009250879\n",
      "epoch: 13 step: 84, loss is 0.0033940281718969345\n",
      "epoch: 13 step: 85, loss is 0.010401204228401184\n",
      "epoch: 13 step: 86, loss is 0.02417990192770958\n",
      "epoch: 13 step: 87, loss is 0.010716674849390984\n",
      "epoch: 13 step: 88, loss is 0.004455041140317917\n",
      "epoch: 13 step: 89, loss is 0.09503529220819473\n",
      "epoch: 13 step: 90, loss is 0.08405882865190506\n",
      "epoch: 13 step: 91, loss is 0.010318029671907425\n",
      "epoch: 13 step: 92, loss is 0.010890242643654346\n",
      "epoch: 13 step: 93, loss is 0.07087427377700806\n",
      "epoch: 13 step: 94, loss is 0.014580844901502132\n",
      "epoch: 13 step: 95, loss is 0.011205470189452171\n",
      "epoch: 13 step: 96, loss is 0.10703203827142715\n",
      "epoch: 13 step: 97, loss is 0.020633380860090256\n",
      "epoch: 13 step: 98, loss is 0.06062260642647743\n",
      "epoch: 13 step: 99, loss is 0.009372031316161156\n",
      "epoch: 13 step: 100, loss is 0.09168054163455963\n",
      "epoch: 13 step: 101, loss is 0.0042144181206822395\n",
      "epoch: 13 step: 102, loss is 0.003370892722159624\n",
      "epoch: 13 step: 103, loss is 0.006003507878631353\n",
      "epoch: 13 step: 104, loss is 0.02467937022447586\n",
      "epoch: 13 step: 105, loss is 0.0019332515075802803\n",
      "epoch: 13 step: 106, loss is 0.08353804796934128\n",
      "epoch: 13 step: 107, loss is 0.03356826677918434\n",
      "epoch: 13 step: 108, loss is 0.027800071984529495\n",
      "epoch: 13 step: 109, loss is 0.006806220859289169\n",
      "epoch: 13 step: 110, loss is 0.0016128974966704845\n",
      "epoch: 13 step: 111, loss is 0.021948469802737236\n",
      "epoch: 13 step: 112, loss is 0.08600462973117828\n",
      "epoch: 13 step: 113, loss is 0.0461551733314991\n",
      "epoch: 13 step: 114, loss is 0.004698142874985933\n",
      "epoch: 13 step: 115, loss is 0.07006499916315079\n",
      "epoch: 13 step: 116, loss is 0.00700260978192091\n",
      "epoch: 13 step: 117, loss is 0.013605847023427486\n",
      "epoch: 13 step: 118, loss is 0.005271367263048887\n",
      "epoch: 13 step: 119, loss is 0.029290780425071716\n",
      "epoch: 13 step: 120, loss is 0.009662286378443241\n",
      "epoch: 13 step: 121, loss is 0.019384723156690598\n",
      "epoch: 13 step: 122, loss is 0.025812147185206413\n",
      "epoch: 13 step: 123, loss is 0.012673528864979744\n",
      "epoch: 13 step: 124, loss is 0.008474314585328102\n",
      "epoch: 13 step: 125, loss is 0.05534924194216728\n",
      "epoch: 13 step: 126, loss is 0.004023219458758831\n",
      "epoch: 13 step: 127, loss is 0.040862031280994415\n",
      "epoch: 13 step: 128, loss is 0.00896679051220417\n",
      "epoch: 13 step: 129, loss is 0.007301198318600655\n",
      "epoch: 13 step: 130, loss is 0.017800873145461082\n",
      "epoch: 13 step: 131, loss is 0.019588200375437737\n",
      "epoch: 13 step: 132, loss is 0.021936574950814247\n",
      "epoch: 13 step: 133, loss is 0.013238104060292244\n",
      "epoch: 13 step: 134, loss is 0.005884884856641293\n",
      "epoch: 13 step: 135, loss is 0.05545007064938545\n",
      "epoch: 13 step: 136, loss is 0.003263048594817519\n",
      "epoch: 13 step: 137, loss is 0.002386554144322872\n",
      "epoch: 13 step: 138, loss is 0.03651950880885124\n",
      "epoch: 13 step: 139, loss is 0.04287337511777878\n",
      "epoch: 13 step: 140, loss is 0.020253093913197517\n",
      "epoch: 13 step: 141, loss is 0.009031461551785469\n",
      "epoch: 13 step: 142, loss is 0.01186364982277155\n",
      "epoch: 13 step: 143, loss is 0.04630530998110771\n",
      "epoch: 13 step: 144, loss is 0.02302432805299759\n",
      "epoch: 13 step: 145, loss is 0.04016080126166344\n",
      "epoch: 13 step: 146, loss is 0.03655688092112541\n",
      "epoch: 13 step: 147, loss is 0.012672996148467064\n",
      "epoch: 13 step: 148, loss is 0.002865197602659464\n",
      "epoch: 13 step: 149, loss is 0.009425456635653973\n",
      "epoch: 13 step: 150, loss is 0.006803812924772501\n",
      "epoch: 13 step: 151, loss is 0.005392633844166994\n",
      "epoch: 13 step: 152, loss is 0.02854703925549984\n",
      "epoch: 13 step: 153, loss is 0.0029994023498147726\n",
      "epoch: 13 step: 154, loss is 0.018613941967487335\n",
      "epoch: 13 step: 155, loss is 0.021689487621188164\n",
      "epoch: 13 step: 156, loss is 0.011319437995553017\n",
      "epoch: 13 step: 157, loss is 0.0016327594639733434\n",
      "epoch: 13 step: 158, loss is 0.049717094749212265\n",
      "epoch: 13 step: 159, loss is 0.0015726325800642371\n",
      "epoch: 13 step: 160, loss is 0.031665872782468796\n",
      "epoch: 13 step: 161, loss is 0.03316836804151535\n",
      "epoch: 13 step: 162, loss is 0.005172214936465025\n",
      "epoch: 13 step: 163, loss is 0.07375474274158478\n",
      "epoch: 13 step: 164, loss is 0.03602338582277298\n",
      "epoch: 13 step: 165, loss is 0.004933255724608898\n",
      "epoch: 13 step: 166, loss is 0.012922518886625767\n",
      "epoch: 13 step: 167, loss is 0.015699241310358047\n",
      "epoch: 13 step: 168, loss is 0.0006164071965031326\n",
      "epoch: 13 step: 169, loss is 0.004744748119264841\n",
      "epoch: 13 step: 170, loss is 0.009739082306623459\n",
      "epoch: 13 step: 171, loss is 0.018361564725637436\n",
      "epoch: 13 step: 172, loss is 0.044853635132312775\n",
      "epoch: 13 step: 173, loss is 0.07705007493495941\n",
      "epoch: 13 step: 174, loss is 0.0011927214218303561\n",
      "epoch: 13 step: 175, loss is 0.02462484873831272\n",
      "epoch: 13 step: 176, loss is 0.045256052166223526\n",
      "epoch: 13 step: 177, loss is 0.03146940842270851\n",
      "epoch: 13 step: 178, loss is 0.08001218736171722\n",
      "epoch: 13 step: 179, loss is 0.03638531640172005\n",
      "epoch: 13 step: 180, loss is 0.029042674228549004\n",
      "epoch: 13 step: 181, loss is 0.05172322690486908\n",
      "epoch: 13 step: 182, loss is 0.011324996128678322\n",
      "epoch: 13 step: 183, loss is 0.01989288069307804\n",
      "epoch: 13 step: 184, loss is 0.027701035141944885\n",
      "epoch: 13 step: 185, loss is 0.04518720880150795\n",
      "epoch: 13 step: 186, loss is 0.007208149414509535\n",
      "epoch: 13 step: 187, loss is 0.0033708009868860245\n",
      "epoch: 13 step: 188, loss is 0.01764034666121006\n",
      "epoch: 13 step: 189, loss is 0.011733132414519787\n",
      "epoch: 13 step: 190, loss is 0.003146438393741846\n",
      "epoch: 13 step: 191, loss is 0.036738041788339615\n",
      "epoch: 13 step: 192, loss is 0.006893698591738939\n",
      "epoch: 13 step: 193, loss is 0.029714122414588928\n",
      "epoch: 13 step: 194, loss is 0.056096985936164856\n",
      "epoch: 13 step: 195, loss is 0.006074029952287674\n",
      "epoch: 13 step: 196, loss is 0.060337334871292114\n",
      "epoch: 13 step: 197, loss is 0.11953724175691605\n",
      "epoch: 13 step: 198, loss is 0.01279914565384388\n",
      "epoch: 13 step: 199, loss is 0.028285562992095947\n",
      "epoch: 13 step: 200, loss is 0.02075381390750408\n",
      "epoch: 13 step: 201, loss is 0.013252747245132923\n",
      "epoch: 13 step: 202, loss is 0.015152368694543839\n",
      "epoch: 13 step: 203, loss is 0.026263630017638206\n",
      "epoch: 13 step: 204, loss is 0.0707169622182846\n",
      "epoch: 13 step: 205, loss is 0.029798081144690514\n",
      "epoch: 13 step: 206, loss is 0.0792468786239624\n",
      "epoch: 13 step: 207, loss is 0.008900715038180351\n",
      "epoch: 13 step: 208, loss is 0.01335278432816267\n",
      "epoch: 13 step: 209, loss is 0.012238367460668087\n",
      "epoch: 13 step: 210, loss is 0.012229380197823048\n",
      "epoch: 13 step: 211, loss is 0.07940714806318283\n",
      "epoch: 13 step: 212, loss is 0.035264115780591965\n",
      "epoch: 13 step: 213, loss is 0.05645391345024109\n",
      "epoch: 13 step: 214, loss is 0.0012024333700537682\n",
      "epoch: 13 step: 215, loss is 0.00044242548756301403\n",
      "epoch: 13 step: 216, loss is 0.03654132038354874\n",
      "epoch: 13 step: 217, loss is 0.003868705593049526\n",
      "epoch: 13 step: 218, loss is 0.0075708311051130295\n",
      "epoch: 13 step: 219, loss is 0.03984803333878517\n",
      "epoch: 13 step: 220, loss is 0.1171972006559372\n",
      "epoch: 13 step: 221, loss is 0.036196961998939514\n",
      "epoch: 13 step: 222, loss is 0.12419992685317993\n",
      "epoch: 13 step: 223, loss is 0.03716426342725754\n",
      "epoch: 13 step: 224, loss is 0.004300365690141916\n",
      "epoch: 13 step: 225, loss is 0.011029449291527271\n",
      "epoch: 13 step: 226, loss is 0.04923225939273834\n",
      "epoch: 13 step: 227, loss is 0.029588181525468826\n",
      "epoch: 13 step: 228, loss is 0.04255593940615654\n",
      "epoch: 13 step: 229, loss is 0.015448505990207195\n",
      "epoch: 13 step: 230, loss is 0.030598655343055725\n",
      "epoch: 13 step: 231, loss is 0.06703543663024902\n",
      "epoch: 13 step: 232, loss is 0.030533142387866974\n",
      "epoch: 13 step: 233, loss is 0.032627567648887634\n",
      "epoch: 13 step: 234, loss is 0.06698399782180786\n",
      "epoch: 13 step: 235, loss is 0.02536996640264988\n",
      "epoch: 13 step: 236, loss is 0.08683332055807114\n",
      "epoch: 13 step: 237, loss is 0.018988395109772682\n",
      "epoch: 13 step: 238, loss is 0.02549830824136734\n",
      "epoch: 13 step: 239, loss is 0.013750914484262466\n",
      "epoch: 13 step: 240, loss is 0.03264637663960457\n",
      "epoch: 13 step: 241, loss is 0.00715161906555295\n",
      "epoch: 13 step: 242, loss is 0.01833348721265793\n",
      "epoch: 13 step: 243, loss is 0.014895274303853512\n",
      "epoch: 13 step: 244, loss is 0.016650138422846794\n",
      "epoch: 13 step: 245, loss is 0.026317602023482323\n",
      "epoch: 13 step: 246, loss is 0.07717302441596985\n",
      "epoch: 13 step: 247, loss is 0.014464538544416428\n",
      "epoch: 13 step: 248, loss is 0.043072015047073364\n",
      "epoch: 13 step: 249, loss is 0.0311628095805645\n",
      "epoch: 13 step: 250, loss is 0.08539728820323944\n",
      "epoch: 13 step: 251, loss is 0.044713765382766724\n",
      "epoch: 13 step: 252, loss is 0.044767677783966064\n",
      "epoch: 13 step: 253, loss is 0.1284903883934021\n",
      "epoch: 13 step: 254, loss is 0.023052675649523735\n",
      "epoch: 13 step: 255, loss is 0.020045416429638863\n",
      "epoch: 13 step: 256, loss is 0.04499686509370804\n",
      "epoch: 13 step: 257, loss is 0.07860715687274933\n",
      "epoch: 13 step: 258, loss is 0.0074119241908192635\n",
      "epoch: 13 step: 259, loss is 0.0033839482348412275\n",
      "epoch: 13 step: 260, loss is 0.005500790197402239\n",
      "epoch: 13 step: 261, loss is 0.01499985996633768\n",
      "epoch: 13 step: 262, loss is 0.01591813564300537\n",
      "epoch: 13 step: 263, loss is 0.00204085954464972\n",
      "epoch: 13 step: 264, loss is 0.040395256131887436\n",
      "epoch: 13 step: 265, loss is 0.00415753573179245\n",
      "epoch: 13 step: 266, loss is 0.03400585800409317\n",
      "epoch: 13 step: 267, loss is 0.06984370946884155\n",
      "epoch: 13 step: 268, loss is 0.014273140579462051\n",
      "epoch: 13 step: 269, loss is 0.0034169170539826155\n",
      "epoch: 13 step: 270, loss is 0.0033276614267379045\n",
      "epoch: 13 step: 271, loss is 0.053663820028305054\n",
      "epoch: 13 step: 272, loss is 0.007755199447274208\n",
      "epoch: 13 step: 273, loss is 0.021137887611985207\n",
      "epoch: 13 step: 274, loss is 0.010536271147429943\n",
      "epoch: 13 step: 275, loss is 0.02717348001897335\n",
      "epoch: 13 step: 276, loss is 0.0420382097363472\n",
      "epoch: 13 step: 277, loss is 0.028147514909505844\n",
      "epoch: 13 step: 278, loss is 0.013391879387199879\n",
      "epoch: 13 step: 279, loss is 0.005085581913590431\n",
      "epoch: 13 step: 280, loss is 0.004072779789566994\n",
      "epoch: 13 step: 281, loss is 0.050858739763498306\n",
      "epoch: 13 step: 282, loss is 0.01835690811276436\n",
      "epoch: 13 step: 283, loss is 0.009047326631844044\n",
      "epoch: 13 step: 284, loss is 0.012271929532289505\n",
      "epoch: 13 step: 285, loss is 0.01278679445385933\n",
      "epoch: 13 step: 286, loss is 0.010255105793476105\n",
      "epoch: 13 step: 287, loss is 0.004158886149525642\n",
      "epoch: 13 step: 288, loss is 0.0032242159359157085\n",
      "epoch: 13 step: 289, loss is 0.10823365300893784\n",
      "epoch: 13 step: 290, loss is 0.031591251492500305\n",
      "epoch: 13 step: 291, loss is 0.003429568372666836\n",
      "epoch: 13 step: 292, loss is 0.02040969580411911\n",
      "epoch: 13 step: 293, loss is 0.0167172159999609\n",
      "epoch: 13 step: 294, loss is 0.006642097141593695\n",
      "epoch: 13 step: 295, loss is 0.10413812845945358\n",
      "epoch: 13 step: 296, loss is 0.06463243812322617\n",
      "epoch: 13 step: 297, loss is 0.040468424558639526\n",
      "epoch: 13 step: 298, loss is 0.03182777762413025\n",
      "epoch: 13 step: 299, loss is 0.00014940573601052165\n",
      "epoch: 13 step: 300, loss is 0.001340012066066265\n",
      "epoch: 13 step: 301, loss is 0.05078587308526039\n",
      "epoch: 13 step: 302, loss is 0.014850566163659096\n",
      "epoch: 13 step: 303, loss is 0.006712871603667736\n",
      "epoch: 13 step: 304, loss is 0.02865804359316826\n",
      "epoch: 13 step: 305, loss is 0.017930960282683372\n",
      "epoch: 13 step: 306, loss is 0.006014990154653788\n",
      "epoch: 13 step: 307, loss is 0.005369242746382952\n",
      "epoch: 13 step: 308, loss is 0.026890093460679054\n",
      "epoch: 13 step: 309, loss is 0.04435427114367485\n",
      "epoch: 13 step: 310, loss is 0.05314922705292702\n",
      "epoch: 13 step: 311, loss is 0.011060566641390324\n",
      "epoch: 13 step: 312, loss is 0.06903325766324997\n",
      "epoch: 13 step: 313, loss is 0.03764738887548447\n",
      "epoch: 13 step: 314, loss is 0.026225466281175613\n",
      "epoch: 13 step: 315, loss is 0.006078910548239946\n",
      "epoch: 13 step: 316, loss is 0.0036706964019685984\n",
      "epoch: 13 step: 317, loss is 0.0019693898502737284\n",
      "epoch: 13 step: 318, loss is 0.011211485601961613\n",
      "epoch: 13 step: 319, loss is 0.029491882771253586\n",
      "epoch: 13 step: 320, loss is 0.025375818833708763\n",
      "epoch: 13 step: 321, loss is 0.022390685975551605\n",
      "epoch: 13 step: 322, loss is 0.008632795885205269\n",
      "epoch: 13 step: 323, loss is 0.030567746609449387\n",
      "epoch: 13 step: 324, loss is 0.04551675543189049\n",
      "epoch: 13 step: 325, loss is 0.010895675048232079\n",
      "epoch: 13 step: 326, loss is 0.05389229953289032\n",
      "epoch: 13 step: 327, loss is 0.00653172517195344\n",
      "epoch: 13 step: 328, loss is 0.04111732542514801\n",
      "epoch: 13 step: 329, loss is 0.014520215801894665\n",
      "epoch: 13 step: 330, loss is 0.06667570769786835\n",
      "epoch: 13 step: 331, loss is 0.041189875453710556\n",
      "epoch: 13 step: 332, loss is 0.002905616071075201\n",
      "epoch: 13 step: 333, loss is 0.00506184296682477\n",
      "epoch: 13 step: 334, loss is 0.01865847036242485\n",
      "epoch: 13 step: 335, loss is 0.013899949379265308\n",
      "epoch: 13 step: 336, loss is 0.027218207716941833\n",
      "epoch: 13 step: 337, loss is 0.008054220117628574\n",
      "epoch: 13 step: 338, loss is 0.08012336492538452\n",
      "epoch: 13 step: 339, loss is 0.07113264501094818\n",
      "epoch: 13 step: 340, loss is 0.03819747641682625\n",
      "epoch: 13 step: 341, loss is 0.09363400936126709\n",
      "epoch: 13 step: 342, loss is 0.04135897010564804\n",
      "epoch: 13 step: 343, loss is 0.041554149240255356\n",
      "epoch: 13 step: 344, loss is 0.029304638504981995\n",
      "epoch: 13 step: 345, loss is 0.18920490145683289\n",
      "epoch: 13 step: 346, loss is 0.01855117455124855\n",
      "epoch: 13 step: 347, loss is 0.0008826237171888351\n",
      "epoch: 13 step: 348, loss is 0.12285815179347992\n",
      "epoch: 13 step: 349, loss is 0.012103335000574589\n",
      "epoch: 13 step: 350, loss is 0.07427622377872467\n",
      "epoch: 13 step: 351, loss is 0.059357013553380966\n",
      "epoch: 13 step: 352, loss is 0.029457468539476395\n",
      "epoch: 13 step: 353, loss is 0.010859759524464607\n",
      "epoch: 13 step: 354, loss is 0.01264622900635004\n",
      "epoch: 13 step: 355, loss is 0.015013121999800205\n",
      "epoch: 13 step: 356, loss is 0.08497431874275208\n",
      "epoch: 13 step: 357, loss is 0.04291572794318199\n",
      "epoch: 13 step: 358, loss is 0.041424788534641266\n",
      "epoch: 13 step: 359, loss is 0.036413129419088364\n",
      "epoch: 13 step: 360, loss is 0.04593301936984062\n",
      "epoch: 13 step: 361, loss is 0.06025353819131851\n",
      "epoch: 13 step: 362, loss is 0.07068662345409393\n",
      "epoch: 13 step: 363, loss is 0.023315338417887688\n",
      "epoch: 13 step: 364, loss is 0.016474062576889992\n",
      "epoch: 13 step: 365, loss is 0.05223624408245087\n",
      "epoch: 13 step: 366, loss is 0.026278307661414146\n",
      "epoch: 13 step: 367, loss is 0.06351844221353531\n",
      "epoch: 13 step: 368, loss is 0.06679654866456985\n",
      "epoch: 13 step: 369, loss is 0.01740020141005516\n",
      "epoch: 13 step: 370, loss is 0.044330254197120667\n",
      "epoch: 13 step: 371, loss is 0.009850461967289448\n",
      "epoch: 13 step: 372, loss is 0.02404521219432354\n",
      "epoch: 13 step: 373, loss is 0.0382709726691246\n",
      "epoch: 13 step: 374, loss is 0.03004612773656845\n",
      "epoch: 13 step: 375, loss is 0.011569736525416374\n",
      "epoch: 13 step: 376, loss is 0.1327916383743286\n",
      "epoch: 13 step: 377, loss is 0.07239195704460144\n",
      "epoch: 13 step: 378, loss is 0.02940570004284382\n",
      "epoch: 13 step: 379, loss is 0.03202293440699577\n",
      "epoch: 13 step: 380, loss is 0.0027742849197238684\n",
      "epoch: 13 step: 381, loss is 0.03732989355921745\n",
      "epoch: 13 step: 382, loss is 0.013166418299078941\n",
      "epoch: 13 step: 383, loss is 0.006397888530045748\n",
      "epoch: 13 step: 384, loss is 0.03793630376458168\n",
      "epoch: 13 step: 385, loss is 0.010660149157047272\n",
      "epoch: 13 step: 386, loss is 0.12251129746437073\n",
      "epoch: 13 step: 387, loss is 0.014666164293885231\n",
      "epoch: 13 step: 388, loss is 0.04312284663319588\n",
      "epoch: 13 step: 389, loss is 0.01609267294406891\n",
      "epoch: 13 step: 390, loss is 0.03071785345673561\n",
      "epoch: 13 step: 391, loss is 0.07608615607023239\n",
      "epoch: 13 step: 392, loss is 0.05953706428408623\n",
      "epoch: 13 step: 393, loss is 0.0007528114365413785\n",
      "epoch: 13 step: 394, loss is 0.004041809588670731\n",
      "epoch: 13 step: 395, loss is 0.007196427322924137\n",
      "epoch: 13 step: 396, loss is 0.045431140810251236\n",
      "epoch: 13 step: 397, loss is 0.05439792945981026\n",
      "epoch: 13 step: 398, loss is 0.004141793120652437\n",
      "epoch: 13 step: 399, loss is 0.03262702375650406\n",
      "epoch: 13 step: 400, loss is 0.059095822274684906\n",
      "epoch: 13 step: 401, loss is 0.0515231117606163\n",
      "epoch: 13 step: 402, loss is 0.018346110358834267\n",
      "epoch: 13 step: 403, loss is 0.051364798098802567\n",
      "epoch: 13 step: 404, loss is 0.04308813437819481\n",
      "epoch: 13 step: 405, loss is 0.07934176921844482\n",
      "epoch: 13 step: 406, loss is 0.021100210025906563\n",
      "epoch: 13 step: 407, loss is 0.11955682933330536\n",
      "epoch: 13 step: 408, loss is 0.07237155735492706\n",
      "epoch: 13 step: 409, loss is 0.009227681905031204\n",
      "epoch: 13 step: 410, loss is 0.002160699339583516\n",
      "epoch: 13 step: 411, loss is 0.02157779410481453\n",
      "epoch: 13 step: 412, loss is 0.02193434163928032\n",
      "epoch: 13 step: 413, loss is 0.0231858491897583\n",
      "epoch: 13 step: 414, loss is 0.047891661524772644\n",
      "epoch: 13 step: 415, loss is 0.011623082682490349\n",
      "epoch: 13 step: 416, loss is 0.026257004588842392\n",
      "epoch: 13 step: 417, loss is 0.006005235482007265\n",
      "epoch: 13 step: 418, loss is 0.03258076310157776\n",
      "epoch: 13 step: 419, loss is 0.0715985894203186\n",
      "epoch: 13 step: 420, loss is 0.0076323566026985645\n",
      "epoch: 13 step: 421, loss is 0.007416443899273872\n",
      "epoch: 13 step: 422, loss is 0.072979636490345\n",
      "epoch: 13 step: 423, loss is 0.02061655931174755\n",
      "epoch: 13 step: 424, loss is 0.021328724920749664\n",
      "epoch: 13 step: 425, loss is 0.058426715433597565\n",
      "epoch: 13 step: 426, loss is 0.027457762509584427\n",
      "epoch: 13 step: 427, loss is 0.16261178255081177\n",
      "epoch: 13 step: 428, loss is 0.00858750008046627\n",
      "epoch: 13 step: 429, loss is 0.0013555190525949001\n",
      "epoch: 13 step: 430, loss is 0.07179052382707596\n",
      "epoch: 13 step: 431, loss is 0.0024630678817629814\n",
      "epoch: 13 step: 432, loss is 0.002102362923324108\n",
      "epoch: 13 step: 433, loss is 0.007382893934845924\n",
      "epoch: 13 step: 434, loss is 0.04177064821124077\n",
      "epoch: 13 step: 435, loss is 0.008751901797950268\n",
      "epoch: 13 step: 436, loss is 0.019538432359695435\n",
      "epoch: 13 step: 437, loss is 0.029858581721782684\n",
      "epoch: 13 step: 438, loss is 0.03755410388112068\n",
      "epoch: 13 step: 439, loss is 0.04065762087702751\n",
      "epoch: 13 step: 440, loss is 0.036955591291189194\n",
      "epoch: 13 step: 441, loss is 0.009554767981171608\n",
      "epoch: 13 step: 442, loss is 0.0635802149772644\n",
      "epoch: 13 step: 443, loss is 0.015340340323746204\n",
      "epoch: 13 step: 444, loss is 0.07155389338731766\n",
      "epoch: 13 step: 445, loss is 0.024048244580626488\n",
      "epoch: 13 step: 446, loss is 0.002777957124635577\n",
      "epoch: 13 step: 447, loss is 0.00964014045894146\n",
      "epoch: 13 step: 448, loss is 0.03665006533265114\n",
      "epoch: 13 step: 449, loss is 0.013688428327441216\n",
      "epoch: 13 step: 450, loss is 0.019656972959637642\n",
      "epoch: 13 step: 451, loss is 0.06102009117603302\n",
      "epoch: 13 step: 452, loss is 0.011255478486418724\n",
      "epoch: 13 step: 453, loss is 0.13785910606384277\n",
      "epoch: 13 step: 454, loss is 0.09789983928203583\n",
      "epoch: 13 step: 455, loss is 0.012934918515384197\n",
      "epoch: 13 step: 456, loss is 0.01829511858522892\n",
      "epoch: 13 step: 457, loss is 0.07300933450460434\n",
      "epoch: 13 step: 458, loss is 0.005233668722212315\n",
      "epoch: 13 step: 459, loss is 0.016993766650557518\n",
      "epoch: 13 step: 460, loss is 0.024871818721294403\n",
      "epoch: 13 step: 461, loss is 0.006907035131007433\n",
      "epoch: 13 step: 462, loss is 0.00754173006862402\n",
      "epoch: 13 step: 463, loss is 0.01870940811932087\n",
      "epoch: 13 step: 464, loss is 0.06172682344913483\n",
      "epoch: 13 step: 465, loss is 0.0603652186691761\n",
      "epoch: 13 step: 466, loss is 0.050187867134809494\n",
      "epoch: 13 step: 467, loss is 0.036983516067266464\n",
      "epoch: 13 step: 468, loss is 0.004449964966624975\n",
      "epoch: 13 step: 469, loss is 0.004752003587782383\n",
      "epoch: 13 step: 470, loss is 0.010721748694777489\n",
      "epoch: 13 step: 471, loss is 0.18805018067359924\n",
      "epoch: 13 step: 472, loss is 0.167846217751503\n",
      "epoch: 13 step: 473, loss is 0.03495005518198013\n",
      "epoch: 13 step: 474, loss is 0.09508485347032547\n",
      "epoch: 13 step: 475, loss is 0.010131461545825005\n",
      "epoch: 13 step: 476, loss is 0.08440642803907394\n",
      "epoch: 13 step: 477, loss is 0.0032215439714491367\n",
      "epoch: 13 step: 478, loss is 0.04440506175160408\n",
      "epoch: 13 step: 479, loss is 0.005221478175371885\n",
      "epoch: 13 step: 480, loss is 0.039302531629800797\n",
      "epoch: 13 step: 481, loss is 0.05313446745276451\n",
      "epoch: 13 step: 482, loss is 0.02134295552968979\n",
      "epoch: 13 step: 483, loss is 0.039816781878471375\n",
      "epoch: 13 step: 484, loss is 0.09692005813121796\n",
      "epoch: 13 step: 485, loss is 0.014458058401942253\n",
      "epoch: 13 step: 486, loss is 0.004879966843873262\n",
      "epoch: 13 step: 487, loss is 0.013866267167031765\n",
      "epoch: 13 step: 488, loss is 0.05766460299491882\n",
      "epoch: 13 step: 489, loss is 0.03539900481700897\n",
      "epoch: 13 step: 490, loss is 0.017459705471992493\n",
      "epoch: 13 step: 491, loss is 0.06227763369679451\n",
      "epoch: 13 step: 492, loss is 0.020016372203826904\n",
      "epoch: 13 step: 493, loss is 0.04155685380101204\n",
      "epoch: 13 step: 494, loss is 0.06031071022152901\n",
      "epoch: 13 step: 495, loss is 0.011892794631421566\n",
      "epoch: 13 step: 496, loss is 0.04234148934483528\n",
      "epoch: 13 step: 497, loss is 0.013922329992055893\n",
      "epoch: 13 step: 498, loss is 0.04261378198862076\n",
      "epoch: 13 step: 499, loss is 0.12531107664108276\n",
      "epoch: 13 step: 500, loss is 0.0012580851325765252\n",
      "epoch: 13 step: 501, loss is 0.009157955646514893\n",
      "epoch: 13 step: 502, loss is 0.015745341777801514\n",
      "epoch: 13 step: 503, loss is 0.004874194040894508\n",
      "epoch: 13 step: 504, loss is 0.053445879369974136\n",
      "epoch: 13 step: 505, loss is 0.01167601440101862\n",
      "epoch: 13 step: 506, loss is 0.007700310088694096\n",
      "epoch: 13 step: 507, loss is 0.05766012892127037\n",
      "epoch: 13 step: 508, loss is 0.0159012321382761\n",
      "epoch: 13 step: 509, loss is 0.05524371936917305\n",
      "epoch: 13 step: 510, loss is 0.03714338317513466\n",
      "epoch: 13 step: 511, loss is 0.03821069374680519\n",
      "epoch: 13 step: 512, loss is 0.04849217087030411\n",
      "epoch: 13 step: 513, loss is 0.03453119844198227\n",
      "epoch: 13 step: 514, loss is 0.018611041828989983\n",
      "epoch: 13 step: 515, loss is 0.02811995893716812\n",
      "epoch: 13 step: 516, loss is 0.03282410651445389\n",
      "epoch: 13 step: 517, loss is 0.005756359547376633\n",
      "epoch: 13 step: 518, loss is 0.014959444291889668\n",
      "epoch: 13 step: 519, loss is 0.05802127346396446\n",
      "epoch: 13 step: 520, loss is 0.013425666838884354\n",
      "epoch: 13 step: 521, loss is 0.10816965997219086\n",
      "epoch: 13 step: 522, loss is 0.01843489706516266\n",
      "epoch: 13 step: 523, loss is 0.04517321661114693\n",
      "epoch: 13 step: 524, loss is 0.015376775525510311\n",
      "epoch: 13 step: 525, loss is 0.009839527308940887\n",
      "epoch: 13 step: 526, loss is 0.02507629618048668\n",
      "epoch: 13 step: 527, loss is 0.030811147764325142\n",
      "epoch: 13 step: 528, loss is 0.0331265814602375\n",
      "epoch: 13 step: 529, loss is 0.04427781328558922\n",
      "epoch: 13 step: 530, loss is 0.04119440168142319\n",
      "epoch: 13 step: 531, loss is 0.04150937497615814\n",
      "epoch: 13 step: 532, loss is 0.01680096425116062\n",
      "epoch: 13 step: 533, loss is 0.04084305465221405\n",
      "epoch: 13 step: 534, loss is 0.030429759994149208\n",
      "epoch: 13 step: 535, loss is 0.006470894441008568\n",
      "epoch: 13 step: 536, loss is 0.02375468797981739\n",
      "epoch: 13 step: 537, loss is 0.08463908731937408\n",
      "epoch: 13 step: 538, loss is 0.0026916195638477802\n",
      "epoch: 13 step: 539, loss is 0.049536217004060745\n",
      "epoch: 13 step: 540, loss is 0.07390134036540985\n",
      "epoch: 13 step: 541, loss is 0.046465519815683365\n",
      "epoch: 13 step: 542, loss is 0.006609021686017513\n",
      "epoch: 13 step: 543, loss is 0.13532428443431854\n",
      "epoch: 13 step: 544, loss is 0.061008427292108536\n",
      "epoch: 13 step: 545, loss is 0.01755077950656414\n",
      "epoch: 13 step: 546, loss is 0.04465067759156227\n",
      "epoch: 13 step: 547, loss is 0.027726497501134872\n",
      "epoch: 13 step: 548, loss is 0.05342480167746544\n",
      "epoch: 13 step: 549, loss is 0.04354739189147949\n",
      "epoch: 13 step: 550, loss is 0.06493286043405533\n",
      "epoch: 13 step: 551, loss is 0.07899390161037445\n",
      "epoch: 13 step: 552, loss is 0.028879642486572266\n",
      "epoch: 13 step: 553, loss is 0.01823253557085991\n",
      "epoch: 13 step: 554, loss is 0.017754031345248222\n",
      "epoch: 13 step: 555, loss is 0.05144452676177025\n",
      "epoch: 13 step: 556, loss is 0.047844331711530685\n",
      "epoch: 13 step: 557, loss is 0.00924257468432188\n",
      "epoch: 13 step: 558, loss is 0.03954530879855156\n",
      "epoch: 13 step: 559, loss is 0.07107946276664734\n",
      "epoch: 13 step: 560, loss is 0.011428755708038807\n",
      "epoch: 13 step: 561, loss is 0.10396304726600647\n",
      "epoch: 13 step: 562, loss is 0.08528704941272736\n",
      "epoch: 13 step: 563, loss is 0.02944938838481903\n",
      "epoch: 13 step: 564, loss is 0.024093877524137497\n",
      "epoch: 13 step: 565, loss is 0.004023103509098291\n",
      "epoch: 13 step: 566, loss is 0.008015524595975876\n",
      "epoch: 13 step: 567, loss is 0.029933195561170578\n",
      "epoch: 13 step: 568, loss is 0.014937503263354301\n",
      "epoch: 13 step: 569, loss is 0.01993054710328579\n",
      "epoch: 13 step: 570, loss is 0.006625809241086245\n",
      "epoch: 13 step: 571, loss is 0.027395011857151985\n",
      "epoch: 13 step: 572, loss is 0.025993527844548225\n",
      "epoch: 13 step: 573, loss is 0.006606932729482651\n",
      "epoch: 13 step: 574, loss is 0.017837973311543465\n",
      "epoch: 13 step: 575, loss is 0.05163084343075752\n",
      "epoch: 13 step: 576, loss is 0.21093478798866272\n",
      "epoch: 13 step: 577, loss is 0.009590215981006622\n",
      "epoch: 13 step: 578, loss is 0.024492796510457993\n",
      "epoch: 13 step: 579, loss is 0.15631121397018433\n",
      "epoch: 13 step: 580, loss is 0.020440656691789627\n",
      "epoch: 13 step: 581, loss is 0.025695670396089554\n",
      "epoch: 13 step: 582, loss is 0.04099636897444725\n",
      "epoch: 13 step: 583, loss is 0.0018580692121759057\n",
      "epoch: 13 step: 584, loss is 0.053144827485084534\n",
      "epoch: 13 step: 585, loss is 0.014104862697422504\n",
      "epoch: 13 step: 586, loss is 0.00540910754352808\n",
      "epoch: 13 step: 587, loss is 0.058671046048402786\n",
      "epoch: 13 step: 588, loss is 0.03907063603401184\n",
      "epoch: 13 step: 589, loss is 0.035212960094213486\n",
      "epoch: 13 step: 590, loss is 0.1319708377122879\n",
      "epoch: 13 step: 591, loss is 0.06108694523572922\n",
      "epoch: 13 step: 592, loss is 0.030741896480321884\n",
      "epoch: 13 step: 593, loss is 0.0058474307879805565\n",
      "epoch: 13 step: 594, loss is 0.03404964134097099\n",
      "epoch: 13 step: 595, loss is 0.05094689503312111\n",
      "epoch: 13 step: 596, loss is 0.01876468025147915\n",
      "epoch: 13 step: 597, loss is 0.010579791851341724\n",
      "epoch: 13 step: 598, loss is 0.06989312916994095\n",
      "epoch: 13 step: 599, loss is 0.17239364981651306\n",
      "epoch: 13 step: 600, loss is 0.03334435448050499\n",
      "epoch: 13 step: 601, loss is 0.03788457438349724\n",
      "epoch: 13 step: 602, loss is 0.052992891520261765\n",
      "epoch: 13 step: 603, loss is 0.008921477012336254\n",
      "epoch: 13 step: 604, loss is 0.02985747903585434\n",
      "epoch: 13 step: 605, loss is 0.020367667078971863\n",
      "epoch: 13 step: 606, loss is 0.12797696888446808\n",
      "epoch: 13 step: 607, loss is 0.004565836861729622\n",
      "epoch: 13 step: 608, loss is 0.035642314702272415\n",
      "epoch: 13 step: 609, loss is 0.027077222242951393\n",
      "epoch: 13 step: 610, loss is 0.03802758455276489\n",
      "epoch: 13 step: 611, loss is 0.014931300655007362\n",
      "epoch: 13 step: 612, loss is 0.00603755796328187\n",
      "epoch: 13 step: 613, loss is 0.020660636946558952\n",
      "epoch: 13 step: 614, loss is 0.08666716516017914\n",
      "epoch: 13 step: 615, loss is 0.023099137470126152\n",
      "epoch: 13 step: 616, loss is 0.018715208396315575\n",
      "epoch: 13 step: 617, loss is 0.0655168741941452\n",
      "epoch: 13 step: 618, loss is 0.029270675033330917\n",
      "epoch: 13 step: 619, loss is 0.03049452416598797\n",
      "epoch: 13 step: 620, loss is 0.01174934022128582\n",
      "epoch: 13 step: 621, loss is 0.020870188251137733\n",
      "epoch: 13 step: 622, loss is 0.06618711352348328\n",
      "epoch: 13 step: 623, loss is 0.006833961233496666\n",
      "epoch: 13 step: 624, loss is 0.06269824504852295\n",
      "epoch: 13 step: 625, loss is 0.07973447442054749\n",
      "epoch: 13 step: 626, loss is 0.00400427496060729\n",
      "epoch: 13 step: 627, loss is 0.015815362334251404\n",
      "epoch: 13 step: 628, loss is 0.06563851982355118\n",
      "epoch: 13 step: 629, loss is 0.09167248755693436\n",
      "epoch: 13 step: 630, loss is 0.016322579234838486\n",
      "epoch: 13 step: 631, loss is 0.05724136531352997\n",
      "epoch: 13 step: 632, loss is 0.04193181172013283\n",
      "epoch: 13 step: 633, loss is 0.024583157151937485\n",
      "epoch: 13 step: 634, loss is 0.006470465566962957\n",
      "epoch: 13 step: 635, loss is 0.028618905693292618\n",
      "epoch: 13 step: 636, loss is 0.04733278974890709\n",
      "epoch: 13 step: 637, loss is 0.1130611002445221\n",
      "epoch: 13 step: 638, loss is 0.008073408156633377\n",
      "epoch: 13 step: 639, loss is 0.020730886608362198\n",
      "epoch: 13 step: 640, loss is 0.028200527653098106\n",
      "epoch: 13 step: 641, loss is 0.04388528689742088\n",
      "epoch: 13 step: 642, loss is 0.007428663317114115\n",
      "epoch: 13 step: 643, loss is 0.043408386409282684\n",
      "epoch: 13 step: 644, loss is 0.056035783141851425\n",
      "epoch: 13 step: 645, loss is 0.015350093133747578\n",
      "epoch: 13 step: 646, loss is 0.002278659027069807\n",
      "epoch: 13 step: 647, loss is 0.0037694221828132868\n",
      "epoch: 13 step: 648, loss is 0.021299265325069427\n",
      "epoch: 13 step: 649, loss is 0.07250317931175232\n",
      "epoch: 13 step: 650, loss is 0.046264324337244034\n",
      "epoch: 13 step: 651, loss is 0.12243759632110596\n",
      "epoch: 13 step: 652, loss is 0.006178602576255798\n",
      "epoch: 13 step: 653, loss is 0.025285864248871803\n",
      "epoch: 13 step: 654, loss is 0.027438919991254807\n",
      "epoch: 13 step: 655, loss is 0.014951743185520172\n",
      "epoch: 13 step: 656, loss is 0.01924927346408367\n",
      "epoch: 13 step: 657, loss is 0.0841575339436531\n",
      "epoch: 13 step: 658, loss is 0.05828413367271423\n",
      "epoch: 13 step: 659, loss is 0.07275997847318649\n",
      "epoch: 13 step: 660, loss is 0.012174531817436218\n",
      "epoch: 13 step: 661, loss is 0.004430728033185005\n",
      "epoch: 13 step: 662, loss is 0.04970156401395798\n",
      "epoch: 13 step: 663, loss is 0.04579097777605057\n",
      "epoch: 13 step: 664, loss is 0.07294461131095886\n",
      "epoch: 13 step: 665, loss is 0.09851568937301636\n",
      "epoch: 13 step: 666, loss is 0.007574264891445637\n",
      "epoch: 13 step: 667, loss is 0.014416529797017574\n",
      "epoch: 13 step: 668, loss is 0.012910808436572552\n",
      "epoch: 13 step: 669, loss is 0.02475336566567421\n",
      "epoch: 13 step: 670, loss is 0.05755704268813133\n",
      "epoch: 13 step: 671, loss is 0.02212890237569809\n",
      "epoch: 13 step: 672, loss is 0.027407310903072357\n",
      "epoch: 13 step: 673, loss is 0.02226576954126358\n",
      "epoch: 13 step: 674, loss is 0.08831261098384857\n",
      "epoch: 13 step: 675, loss is 0.021516364067792892\n",
      "epoch: 13 step: 676, loss is 0.012872596271336079\n",
      "epoch: 13 step: 677, loss is 0.014639254659414291\n",
      "epoch: 13 step: 678, loss is 0.01603682152926922\n",
      "epoch: 13 step: 679, loss is 0.03065134398639202\n",
      "epoch: 13 step: 680, loss is 0.014281487092375755\n",
      "epoch: 13 step: 681, loss is 0.0547952726483345\n",
      "epoch: 13 step: 682, loss is 0.004399097058922052\n",
      "epoch: 13 step: 683, loss is 0.0026443391107022762\n",
      "epoch: 13 step: 684, loss is 0.07324269413948059\n",
      "epoch: 13 step: 685, loss is 0.0591108500957489\n",
      "epoch: 13 step: 686, loss is 0.0004358287842478603\n",
      "epoch: 13 step: 687, loss is 0.031189514324069023\n",
      "epoch: 13 step: 688, loss is 0.007558920886367559\n",
      "epoch: 13 step: 689, loss is 0.09036138653755188\n",
      "epoch: 13 step: 690, loss is 0.061625026166439056\n",
      "epoch: 13 step: 691, loss is 0.06845329701900482\n",
      "epoch: 13 step: 692, loss is 0.034879885613918304\n",
      "epoch: 13 step: 693, loss is 0.01762048527598381\n",
      "epoch: 13 step: 694, loss is 0.04225883260369301\n",
      "epoch: 13 step: 695, loss is 0.010844597592949867\n",
      "epoch: 13 step: 696, loss is 0.05230695754289627\n",
      "epoch: 13 step: 697, loss is 0.009151384234428406\n",
      "epoch: 13 step: 698, loss is 0.05128749832510948\n",
      "epoch: 13 step: 699, loss is 0.09335675090551376\n",
      "epoch: 13 step: 700, loss is 0.050669003278017044\n",
      "epoch: 13 step: 701, loss is 0.035884201526641846\n",
      "epoch: 13 step: 702, loss is 0.01173013262450695\n",
      "epoch: 13 step: 703, loss is 0.006833365652710199\n",
      "epoch: 13 step: 704, loss is 0.02198687568306923\n",
      "epoch: 13 step: 705, loss is 0.007127608638256788\n",
      "epoch: 13 step: 706, loss is 0.06257983297109604\n",
      "epoch: 13 step: 707, loss is 0.04155243933200836\n",
      "epoch: 13 step: 708, loss is 0.09921394288539886\n",
      "epoch: 13 step: 709, loss is 0.017933940514922142\n",
      "epoch: 13 step: 710, loss is 0.05063367262482643\n",
      "epoch: 13 step: 711, loss is 0.06364475935697556\n",
      "epoch: 13 step: 712, loss is 0.010344233363866806\n",
      "epoch: 13 step: 713, loss is 0.04191340506076813\n",
      "epoch: 13 step: 714, loss is 0.00904294103384018\n",
      "epoch: 13 step: 715, loss is 0.013705560937523842\n",
      "epoch: 13 step: 716, loss is 0.03838570788502693\n",
      "epoch: 13 step: 717, loss is 0.08030449599027634\n",
      "epoch: 13 step: 718, loss is 0.019235553219914436\n",
      "epoch: 13 step: 719, loss is 0.05476842448115349\n",
      "epoch: 13 step: 720, loss is 0.05681653320789337\n",
      "epoch: 13 step: 721, loss is 0.007543415762484074\n",
      "epoch: 13 step: 722, loss is 0.02846078760921955\n",
      "epoch: 13 step: 723, loss is 0.04835128411650658\n",
      "epoch: 13 step: 724, loss is 0.005022721830755472\n",
      "epoch: 13 step: 725, loss is 0.016303468495607376\n",
      "epoch: 13 step: 726, loss is 0.0874682292342186\n",
      "epoch: 13 step: 727, loss is 0.054708342999219894\n",
      "epoch: 13 step: 728, loss is 0.023152047768235207\n",
      "epoch: 13 step: 729, loss is 0.033619314432144165\n",
      "epoch: 13 step: 730, loss is 0.14061780273914337\n",
      "epoch: 13 step: 731, loss is 0.041085146367549896\n",
      "epoch: 13 step: 732, loss is 0.10138097405433655\n",
      "epoch: 13 step: 733, loss is 0.016483191400766373\n",
      "epoch: 13 step: 734, loss is 0.01410523522645235\n",
      "epoch: 13 step: 735, loss is 0.1228710189461708\n",
      "epoch: 13 step: 736, loss is 0.11601805686950684\n",
      "epoch: 13 step: 737, loss is 0.014779320918023586\n",
      "epoch: 13 step: 738, loss is 0.02427510730922222\n",
      "epoch: 13 step: 739, loss is 0.03420844301581383\n",
      "epoch: 13 step: 740, loss is 0.020611878484487534\n",
      "epoch: 13 step: 741, loss is 0.020312407985329628\n",
      "epoch: 13 step: 742, loss is 0.053267356008291245\n",
      "epoch: 13 step: 743, loss is 0.03790416941046715\n",
      "epoch: 13 step: 744, loss is 0.08304312080144882\n",
      "epoch: 13 step: 745, loss is 0.03926771506667137\n",
      "epoch: 13 step: 746, loss is 0.06923326849937439\n",
      "epoch: 13 step: 747, loss is 0.06799263507127762\n",
      "epoch: 13 step: 748, loss is 0.042305197566747665\n",
      "epoch: 13 step: 749, loss is 0.03441714495420456\n",
      "epoch: 13 step: 750, loss is 0.033023957163095474\n",
      "epoch: 13 step: 751, loss is 0.017285652458667755\n",
      "epoch: 13 step: 752, loss is 0.08818523585796356\n",
      "epoch: 13 step: 753, loss is 0.0035668385680764914\n",
      "epoch: 13 step: 754, loss is 0.03870819881558418\n",
      "epoch: 13 step: 755, loss is 0.01895499788224697\n",
      "epoch: 13 step: 756, loss is 0.01544024795293808\n",
      "epoch: 13 step: 757, loss is 0.06789486110210419\n",
      "epoch: 13 step: 758, loss is 0.05366351827979088\n",
      "epoch: 13 step: 759, loss is 0.09218841791152954\n",
      "epoch: 13 step: 760, loss is 0.14515845477581024\n",
      "epoch: 13 step: 761, loss is 0.04357454925775528\n",
      "epoch: 13 step: 762, loss is 0.0063559385016560555\n",
      "epoch: 13 step: 763, loss is 0.014409183524549007\n",
      "epoch: 13 step: 764, loss is 0.029484741389751434\n",
      "epoch: 13 step: 765, loss is 0.01566440612077713\n",
      "epoch: 13 step: 766, loss is 0.13251012563705444\n",
      "epoch: 13 step: 767, loss is 0.02495793253183365\n",
      "epoch: 13 step: 768, loss is 0.06032466143369675\n",
      "epoch: 13 step: 769, loss is 0.07774479687213898\n",
      "epoch: 13 step: 770, loss is 0.04707418382167816\n",
      "epoch: 13 step: 771, loss is 0.05349338427186012\n",
      "epoch: 13 step: 772, loss is 0.010743366554379463\n",
      "epoch: 13 step: 773, loss is 0.05859856307506561\n",
      "epoch: 13 step: 774, loss is 0.02747742086648941\n",
      "epoch: 13 step: 775, loss is 0.08564803004264832\n",
      "epoch: 13 step: 776, loss is 0.0270960982888937\n",
      "epoch: 13 step: 777, loss is 0.011586499400436878\n",
      "epoch: 13 step: 778, loss is 0.13191144168376923\n",
      "epoch: 13 step: 779, loss is 0.027190525084733963\n",
      "epoch: 13 step: 780, loss is 0.03889350965619087\n",
      "epoch: 13 step: 781, loss is 0.016896383836865425\n",
      "epoch: 13 step: 782, loss is 0.05132501199841499\n",
      "epoch: 13 step: 783, loss is 0.04738181084394455\n",
      "epoch: 13 step: 784, loss is 0.015584095381200314\n",
      "epoch: 13 step: 785, loss is 0.0088790999725461\n",
      "epoch: 13 step: 786, loss is 0.045956626534461975\n",
      "epoch: 13 step: 787, loss is 0.03124164044857025\n",
      "epoch: 13 step: 788, loss is 0.09757006913423538\n",
      "epoch: 13 step: 789, loss is 0.0342111736536026\n",
      "epoch: 13 step: 790, loss is 0.016543731093406677\n",
      "epoch: 13 step: 791, loss is 0.017027979716658592\n",
      "epoch: 13 step: 792, loss is 0.019261356443166733\n",
      "epoch: 13 step: 793, loss is 0.09311215579509735\n",
      "epoch: 13 step: 794, loss is 0.02021266147494316\n",
      "epoch: 13 step: 795, loss is 0.013149410486221313\n",
      "epoch: 13 step: 796, loss is 0.0623725950717926\n",
      "epoch: 13 step: 797, loss is 0.05341937392950058\n",
      "epoch: 13 step: 798, loss is 0.09556837379932404\n",
      "epoch: 13 step: 799, loss is 0.05096855014562607\n",
      "epoch: 13 step: 800, loss is 0.07206032425165176\n",
      "epoch: 13 step: 801, loss is 0.031774237751960754\n",
      "epoch: 13 step: 802, loss is 0.009532134979963303\n",
      "epoch: 13 step: 803, loss is 0.07577797770500183\n",
      "epoch: 13 step: 804, loss is 0.04910477623343468\n",
      "epoch: 13 step: 805, loss is 0.02924204058945179\n",
      "epoch: 13 step: 806, loss is 0.0651310458779335\n",
      "epoch: 13 step: 807, loss is 0.007118550129234791\n",
      "epoch: 13 step: 808, loss is 0.0379280261695385\n",
      "epoch: 13 step: 809, loss is 0.051599692553281784\n",
      "epoch: 13 step: 810, loss is 0.1598581075668335\n",
      "epoch: 13 step: 811, loss is 0.013911589980125427\n",
      "epoch: 13 step: 812, loss is 0.03172249719500542\n",
      "epoch: 13 step: 813, loss is 0.03909680247306824\n",
      "epoch: 13 step: 814, loss is 0.04174784943461418\n",
      "epoch: 13 step: 815, loss is 0.02947792410850525\n",
      "epoch: 13 step: 816, loss is 0.058564189821481705\n",
      "epoch: 13 step: 817, loss is 0.014757255092263222\n",
      "epoch: 13 step: 818, loss is 0.011640512384474277\n",
      "epoch: 13 step: 819, loss is 0.019259143620729446\n",
      "epoch: 13 step: 820, loss is 0.052477575838565826\n",
      "epoch: 13 step: 821, loss is 0.01706203632056713\n",
      "epoch: 13 step: 822, loss is 0.07445728033781052\n",
      "epoch: 13 step: 823, loss is 0.024684658274054527\n",
      "epoch: 13 step: 824, loss is 0.02316548116505146\n",
      "epoch: 13 step: 825, loss is 0.0287383571267128\n",
      "epoch: 13 step: 826, loss is 0.10470717400312424\n",
      "epoch: 13 step: 827, loss is 0.07062774151563644\n",
      "epoch: 13 step: 828, loss is 0.02721370942890644\n",
      "epoch: 13 step: 829, loss is 0.03779955208301544\n",
      "epoch: 13 step: 830, loss is 0.03627028316259384\n",
      "epoch: 13 step: 831, loss is 0.02339072711765766\n",
      "epoch: 13 step: 832, loss is 0.042544301599264145\n",
      "epoch: 13 step: 833, loss is 0.011110659688711166\n",
      "epoch: 13 step: 834, loss is 0.023295694962143898\n",
      "epoch: 13 step: 835, loss is 0.03922010958194733\n",
      "epoch: 13 step: 836, loss is 0.04989945515990257\n",
      "epoch: 13 step: 837, loss is 0.032997798174619675\n",
      "epoch: 13 step: 838, loss is 0.018277768045663834\n",
      "epoch: 13 step: 839, loss is 0.06961356848478317\n",
      "epoch: 13 step: 840, loss is 0.019784636795520782\n",
      "epoch: 13 step: 841, loss is 0.022311415523290634\n",
      "epoch: 13 step: 842, loss is 0.05188683792948723\n",
      "epoch: 13 step: 843, loss is 0.013934517279267311\n",
      "epoch: 13 step: 844, loss is 0.021366233006119728\n",
      "epoch: 13 step: 845, loss is 0.05073956400156021\n",
      "epoch: 13 step: 846, loss is 0.005897480994462967\n",
      "epoch: 13 step: 847, loss is 0.006725043058395386\n",
      "epoch: 13 step: 848, loss is 0.029486538842320442\n",
      "epoch: 13 step: 849, loss is 0.06529756635427475\n",
      "epoch: 13 step: 850, loss is 0.02749454975128174\n",
      "epoch: 13 step: 851, loss is 0.019275706261396408\n",
      "epoch: 13 step: 852, loss is 0.00183952902443707\n",
      "epoch: 13 step: 853, loss is 0.013059955090284348\n",
      "epoch: 13 step: 854, loss is 0.12740959227085114\n",
      "epoch: 13 step: 855, loss is 0.018960049375891685\n",
      "epoch: 13 step: 856, loss is 0.07957040518522263\n",
      "epoch: 13 step: 857, loss is 0.11397340893745422\n",
      "epoch: 13 step: 858, loss is 0.02508094348013401\n",
      "epoch: 13 step: 859, loss is 0.020228486508131027\n",
      "epoch: 13 step: 860, loss is 0.02967776171863079\n",
      "epoch: 13 step: 861, loss is 0.046911951154470444\n",
      "epoch: 13 step: 862, loss is 0.022210076451301575\n",
      "epoch: 13 step: 863, loss is 0.05165873095393181\n",
      "epoch: 13 step: 864, loss is 0.02368147298693657\n",
      "epoch: 13 step: 865, loss is 0.012591217644512653\n",
      "epoch: 13 step: 866, loss is 0.01854497566819191\n",
      "epoch: 13 step: 867, loss is 0.07217369973659515\n",
      "epoch: 13 step: 868, loss is 0.050569191575050354\n",
      "epoch: 13 step: 869, loss is 0.033171672374010086\n",
      "epoch: 13 step: 870, loss is 0.02665274776518345\n",
      "epoch: 13 step: 871, loss is 0.0736382007598877\n",
      "epoch: 13 step: 872, loss is 0.050748128443956375\n",
      "epoch: 13 step: 873, loss is 0.08444888144731522\n",
      "epoch: 13 step: 874, loss is 0.03235561400651932\n",
      "epoch: 13 step: 875, loss is 0.060447320342063904\n",
      "epoch: 13 step: 876, loss is 0.14059223234653473\n",
      "epoch: 13 step: 877, loss is 0.007509138435125351\n",
      "epoch: 13 step: 878, loss is 0.03568458929657936\n",
      "epoch: 13 step: 879, loss is 0.0017340509220957756\n",
      "epoch: 13 step: 880, loss is 0.07888088375329971\n",
      "epoch: 13 step: 881, loss is 0.14816159009933472\n",
      "epoch: 13 step: 882, loss is 0.12986741960048676\n",
      "epoch: 13 step: 883, loss is 0.00938415341079235\n",
      "epoch: 13 step: 884, loss is 0.01436666864901781\n",
      "epoch: 13 step: 885, loss is 0.045088786631822586\n",
      "epoch: 13 step: 886, loss is 0.13088487088680267\n",
      "epoch: 13 step: 887, loss is 0.05213285982608795\n",
      "epoch: 13 step: 888, loss is 0.08812731504440308\n",
      "epoch: 13 step: 889, loss is 0.15127313137054443\n",
      "epoch: 13 step: 890, loss is 0.007077805697917938\n",
      "epoch: 13 step: 891, loss is 0.010828587226569653\n",
      "epoch: 13 step: 892, loss is 0.10113503783941269\n",
      "epoch: 13 step: 893, loss is 0.007038458716124296\n",
      "epoch: 13 step: 894, loss is 0.062388259917497635\n",
      "epoch: 13 step: 895, loss is 0.004004674963653088\n",
      "epoch: 13 step: 896, loss is 0.11904164403676987\n",
      "epoch: 13 step: 897, loss is 0.06901469081640244\n",
      "epoch: 13 step: 898, loss is 0.039279092103242874\n",
      "epoch: 13 step: 899, loss is 0.014908275566995144\n",
      "epoch: 13 step: 900, loss is 0.05802861973643303\n",
      "epoch: 13 step: 901, loss is 0.08592195808887482\n",
      "epoch: 13 step: 902, loss is 0.01660057343542576\n",
      "epoch: 13 step: 903, loss is 0.06347520649433136\n",
      "epoch: 13 step: 904, loss is 0.03995249792933464\n",
      "epoch: 13 step: 905, loss is 0.045321591198444366\n",
      "epoch: 13 step: 906, loss is 0.012592356652021408\n",
      "epoch: 13 step: 907, loss is 0.0508234016597271\n",
      "epoch: 13 step: 908, loss is 0.006078390870243311\n",
      "epoch: 13 step: 909, loss is 0.018617665395140648\n",
      "epoch: 13 step: 910, loss is 0.01928381249308586\n",
      "epoch: 13 step: 911, loss is 0.006212248466908932\n",
      "epoch: 13 step: 912, loss is 0.02961241640150547\n",
      "epoch: 13 step: 913, loss is 0.06835482269525528\n",
      "epoch: 13 step: 914, loss is 0.01989797130227089\n",
      "epoch: 13 step: 915, loss is 0.020723214372992516\n",
      "epoch: 13 step: 916, loss is 0.057655252516269684\n",
      "epoch: 13 step: 917, loss is 0.013805722817778587\n",
      "epoch: 13 step: 918, loss is 0.04880763590335846\n",
      "epoch: 13 step: 919, loss is 0.007601718418300152\n",
      "epoch: 13 step: 920, loss is 0.03242896497249603\n",
      "epoch: 13 step: 921, loss is 0.16832862794399261\n",
      "epoch: 13 step: 922, loss is 0.07368873059749603\n",
      "epoch: 13 step: 923, loss is 0.03597133234143257\n",
      "epoch: 13 step: 924, loss is 0.012721657752990723\n",
      "epoch: 13 step: 925, loss is 0.01564525067806244\n",
      "epoch: 13 step: 926, loss is 0.07577583193778992\n",
      "epoch: 13 step: 927, loss is 0.003951675724238157\n",
      "epoch: 13 step: 928, loss is 0.06362190842628479\n",
      "epoch: 13 step: 929, loss is 0.07161868363618851\n",
      "epoch: 13 step: 930, loss is 0.01813901960849762\n",
      "epoch: 13 step: 931, loss is 0.0359770730137825\n",
      "epoch: 13 step: 932, loss is 0.03600890561938286\n",
      "epoch: 13 step: 933, loss is 0.02670365199446678\n",
      "epoch: 13 step: 934, loss is 0.005911346524953842\n",
      "epoch: 13 step: 935, loss is 0.08730507642030716\n",
      "epoch: 13 step: 936, loss is 0.041134510189294815\n",
      "epoch: 13 step: 937, loss is 0.017515607178211212\n",
      "epoch: 14 step: 1, loss is 0.025499923154711723\n",
      "epoch: 14 step: 2, loss is 0.005776609294116497\n",
      "epoch: 14 step: 3, loss is 0.045201949775218964\n",
      "epoch: 14 step: 4, loss is 0.03066508285701275\n",
      "epoch: 14 step: 5, loss is 0.0061438437551259995\n",
      "epoch: 14 step: 6, loss is 0.024838577955961227\n",
      "epoch: 14 step: 7, loss is 0.009884490631520748\n",
      "epoch: 14 step: 8, loss is 0.015583896078169346\n",
      "epoch: 14 step: 9, loss is 0.0044351378455758095\n",
      "epoch: 14 step: 10, loss is 0.0020981570705771446\n",
      "epoch: 14 step: 11, loss is 0.03849967196583748\n",
      "epoch: 14 step: 12, loss is 0.011175918392837048\n",
      "epoch: 14 step: 13, loss is 0.004475068766623735\n",
      "epoch: 14 step: 14, loss is 0.02654738537967205\n",
      "epoch: 14 step: 15, loss is 0.019501008093357086\n",
      "epoch: 14 step: 16, loss is 0.13715238869190216\n",
      "epoch: 14 step: 17, loss is 0.029096340760588646\n",
      "epoch: 14 step: 18, loss is 0.005456588696688414\n",
      "epoch: 14 step: 19, loss is 0.0029371713753789663\n",
      "epoch: 14 step: 20, loss is 0.07773367315530777\n",
      "epoch: 14 step: 21, loss is 0.005232121795415878\n",
      "epoch: 14 step: 22, loss is 0.004205517936497927\n",
      "epoch: 14 step: 23, loss is 0.011333699338138103\n",
      "epoch: 14 step: 24, loss is 0.009636400267481804\n",
      "epoch: 14 step: 25, loss is 0.006283600348979235\n",
      "epoch: 14 step: 26, loss is 0.01784052513539791\n",
      "epoch: 14 step: 27, loss is 0.006440844852477312\n",
      "epoch: 14 step: 28, loss is 0.019648656249046326\n",
      "epoch: 14 step: 29, loss is 0.024322133511304855\n",
      "epoch: 14 step: 30, loss is 0.043066684156656265\n",
      "epoch: 14 step: 31, loss is 0.0035575907677412033\n",
      "epoch: 14 step: 32, loss is 0.005390639416873455\n",
      "epoch: 14 step: 33, loss is 0.007007800508290529\n",
      "epoch: 14 step: 34, loss is 0.019258728250861168\n",
      "epoch: 14 step: 35, loss is 0.01474248431622982\n",
      "epoch: 14 step: 36, loss is 0.004446322564035654\n",
      "epoch: 14 step: 37, loss is 0.04853313788771629\n",
      "epoch: 14 step: 38, loss is 0.026157112792134285\n",
      "epoch: 14 step: 39, loss is 0.025334330275654793\n",
      "epoch: 14 step: 40, loss is 0.0037113293074071407\n",
      "epoch: 14 step: 41, loss is 0.0159810408949852\n",
      "epoch: 14 step: 42, loss is 0.0344080813229084\n",
      "epoch: 14 step: 43, loss is 0.022644422948360443\n",
      "epoch: 14 step: 44, loss is 0.019971121102571487\n",
      "epoch: 14 step: 45, loss is 0.018287869170308113\n",
      "epoch: 14 step: 46, loss is 0.004385739099234343\n",
      "epoch: 14 step: 47, loss is 0.025566233322024345\n",
      "epoch: 14 step: 48, loss is 0.021261028945446014\n",
      "epoch: 14 step: 49, loss is 0.015415290370583534\n",
      "epoch: 14 step: 50, loss is 0.017004428431391716\n",
      "epoch: 14 step: 51, loss is 0.04309053346514702\n",
      "epoch: 14 step: 52, loss is 0.02250121720135212\n",
      "epoch: 14 step: 53, loss is 0.0074020130559802055\n",
      "epoch: 14 step: 54, loss is 0.013777592219412327\n",
      "epoch: 14 step: 55, loss is 0.07592268288135529\n",
      "epoch: 14 step: 56, loss is 0.00624094856902957\n",
      "epoch: 14 step: 57, loss is 0.03774617612361908\n",
      "epoch: 14 step: 58, loss is 0.0023975090589374304\n",
      "epoch: 14 step: 59, loss is 0.06399041414260864\n",
      "epoch: 14 step: 60, loss is 0.009741702117025852\n",
      "epoch: 14 step: 61, loss is 0.0042914943769574165\n",
      "epoch: 14 step: 62, loss is 0.04120594635605812\n",
      "epoch: 14 step: 63, loss is 0.0015952012035995722\n",
      "epoch: 14 step: 64, loss is 0.007889102213084698\n",
      "epoch: 14 step: 65, loss is 0.013545508496463299\n",
      "epoch: 14 step: 66, loss is 0.000524459988810122\n",
      "epoch: 14 step: 67, loss is 0.009683203883469105\n",
      "epoch: 14 step: 68, loss is 0.011554829776287079\n",
      "epoch: 14 step: 69, loss is 0.043877311050891876\n",
      "epoch: 14 step: 70, loss is 0.10186172276735306\n",
      "epoch: 14 step: 71, loss is 0.004117392469197512\n",
      "epoch: 14 step: 72, loss is 0.006131114438176155\n",
      "epoch: 14 step: 73, loss is 0.0086599076166749\n",
      "epoch: 14 step: 74, loss is 0.02235100045800209\n",
      "epoch: 14 step: 75, loss is 0.005600874777883291\n",
      "epoch: 14 step: 76, loss is 0.011528322473168373\n",
      "epoch: 14 step: 77, loss is 0.042365822941064835\n",
      "epoch: 14 step: 78, loss is 0.008251291699707508\n",
      "epoch: 14 step: 79, loss is 0.04916966333985329\n",
      "epoch: 14 step: 80, loss is 0.005626901984214783\n",
      "epoch: 14 step: 81, loss is 0.06099028140306473\n",
      "epoch: 14 step: 82, loss is 0.00402434915304184\n",
      "epoch: 14 step: 83, loss is 0.008596467785537243\n",
      "epoch: 14 step: 84, loss is 0.07047240436077118\n",
      "epoch: 14 step: 85, loss is 0.07049129158258438\n",
      "epoch: 14 step: 86, loss is 0.015671804547309875\n",
      "epoch: 14 step: 87, loss is 0.06496142596006393\n",
      "epoch: 14 step: 88, loss is 0.01390970777720213\n",
      "epoch: 14 step: 89, loss is 0.012733682990074158\n",
      "epoch: 14 step: 90, loss is 0.011366840451955795\n",
      "epoch: 14 step: 91, loss is 0.00204620324075222\n",
      "epoch: 14 step: 92, loss is 0.004555482417345047\n",
      "epoch: 14 step: 93, loss is 0.010921988636255264\n",
      "epoch: 14 step: 94, loss is 0.013313631527125835\n",
      "epoch: 14 step: 95, loss is 0.009982314892113209\n",
      "epoch: 14 step: 96, loss is 0.00233743479475379\n",
      "epoch: 14 step: 97, loss is 0.004412297625094652\n",
      "epoch: 14 step: 98, loss is 0.017917683348059654\n",
      "epoch: 14 step: 99, loss is 0.01220550388097763\n",
      "epoch: 14 step: 100, loss is 0.0010561519302427769\n",
      "epoch: 14 step: 101, loss is 0.0040674395859241486\n",
      "epoch: 14 step: 102, loss is 0.012236029841005802\n",
      "epoch: 14 step: 103, loss is 0.038471952080726624\n",
      "epoch: 14 step: 104, loss is 0.08230021595954895\n",
      "epoch: 14 step: 105, loss is 0.037236686795949936\n",
      "epoch: 14 step: 106, loss is 0.004083339124917984\n",
      "epoch: 14 step: 107, loss is 0.018332350999116898\n",
      "epoch: 14 step: 108, loss is 0.0045846267603337765\n",
      "epoch: 14 step: 109, loss is 0.006106984801590443\n",
      "epoch: 14 step: 110, loss is 0.012070845812559128\n",
      "epoch: 14 step: 111, loss is 0.009311774745583534\n",
      "epoch: 14 step: 112, loss is 0.018092986196279526\n",
      "epoch: 14 step: 113, loss is 0.04645969718694687\n",
      "epoch: 14 step: 114, loss is 0.012776797637343407\n",
      "epoch: 14 step: 115, loss is 0.01085750013589859\n",
      "epoch: 14 step: 116, loss is 0.018790479749441147\n",
      "epoch: 14 step: 117, loss is 0.02036859467625618\n",
      "epoch: 14 step: 118, loss is 0.007764854002743959\n",
      "epoch: 14 step: 119, loss is 0.011542673222720623\n",
      "epoch: 14 step: 120, loss is 0.02282203733921051\n",
      "epoch: 14 step: 121, loss is 0.005658479407429695\n",
      "epoch: 14 step: 122, loss is 0.022799596190452576\n",
      "epoch: 14 step: 123, loss is 0.0011583765735849738\n",
      "epoch: 14 step: 124, loss is 0.014064446091651917\n",
      "epoch: 14 step: 125, loss is 0.007421341259032488\n",
      "epoch: 14 step: 126, loss is 0.02881952002644539\n",
      "epoch: 14 step: 127, loss is 0.019414575770497322\n",
      "epoch: 14 step: 128, loss is 0.036189522594213486\n",
      "epoch: 14 step: 129, loss is 0.040612008422613144\n",
      "epoch: 14 step: 130, loss is 0.0029101341497153044\n",
      "epoch: 14 step: 131, loss is 0.007028868887573481\n",
      "epoch: 14 step: 132, loss is 0.029491832479834557\n",
      "epoch: 14 step: 133, loss is 0.006692278664559126\n",
      "epoch: 14 step: 134, loss is 0.007095503155142069\n",
      "epoch: 14 step: 135, loss is 0.002921696286648512\n",
      "epoch: 14 step: 136, loss is 0.024079076945781708\n",
      "epoch: 14 step: 137, loss is 0.012273318134248257\n",
      "epoch: 14 step: 138, loss is 0.019702957943081856\n",
      "epoch: 14 step: 139, loss is 0.0017587457550689578\n",
      "epoch: 14 step: 140, loss is 0.02619808539748192\n",
      "epoch: 14 step: 141, loss is 0.012569325044751167\n",
      "epoch: 14 step: 142, loss is 0.007492602337151766\n",
      "epoch: 14 step: 143, loss is 0.000826154719106853\n",
      "epoch: 14 step: 144, loss is 0.05698235332965851\n",
      "epoch: 14 step: 145, loss is 0.006974688731133938\n",
      "epoch: 14 step: 146, loss is 0.006316616199910641\n",
      "epoch: 14 step: 147, loss is 0.007266603875905275\n",
      "epoch: 14 step: 148, loss is 0.0441729836165905\n",
      "epoch: 14 step: 149, loss is 0.0034583015367388725\n",
      "epoch: 14 step: 150, loss is 0.10383112728595734\n",
      "epoch: 14 step: 151, loss is 0.00828986894339323\n",
      "epoch: 14 step: 152, loss is 0.017211657017469406\n",
      "epoch: 14 step: 153, loss is 0.03315990790724754\n",
      "epoch: 14 step: 154, loss is 0.008287947624921799\n",
      "epoch: 14 step: 155, loss is 0.0025758761912584305\n",
      "epoch: 14 step: 156, loss is 0.0810125395655632\n",
      "epoch: 14 step: 157, loss is 0.003150197910144925\n",
      "epoch: 14 step: 158, loss is 0.004686364904046059\n",
      "epoch: 14 step: 159, loss is 0.013227207586169243\n",
      "epoch: 14 step: 160, loss is 0.006955706514418125\n",
      "epoch: 14 step: 161, loss is 0.009017894975841045\n",
      "epoch: 14 step: 162, loss is 0.039940785616636276\n",
      "epoch: 14 step: 163, loss is 0.006628375966101885\n",
      "epoch: 14 step: 164, loss is 0.046398211270570755\n",
      "epoch: 14 step: 165, loss is 0.007276725955307484\n",
      "epoch: 14 step: 166, loss is 0.011328822001814842\n",
      "epoch: 14 step: 167, loss is 0.029781511053442955\n",
      "epoch: 14 step: 168, loss is 0.01862979121506214\n",
      "epoch: 14 step: 169, loss is 0.046618569642305374\n",
      "epoch: 14 step: 170, loss is 0.05515319108963013\n",
      "epoch: 14 step: 171, loss is 0.016849229112267494\n",
      "epoch: 14 step: 172, loss is 0.01772480644285679\n",
      "epoch: 14 step: 173, loss is 0.012432039715349674\n",
      "epoch: 14 step: 174, loss is 0.02756877988576889\n",
      "epoch: 14 step: 175, loss is 0.012172536924481392\n",
      "epoch: 14 step: 176, loss is 0.013731739483773708\n",
      "epoch: 14 step: 177, loss is 0.009000219404697418\n",
      "epoch: 14 step: 178, loss is 0.011373914778232574\n",
      "epoch: 14 step: 179, loss is 0.02604549191892147\n",
      "epoch: 14 step: 180, loss is 0.0022425143979489803\n",
      "epoch: 14 step: 181, loss is 0.015754351392388344\n",
      "epoch: 14 step: 182, loss is 0.03996849060058594\n",
      "epoch: 14 step: 183, loss is 0.010997211560606956\n",
      "epoch: 14 step: 184, loss is 0.008547169156372547\n",
      "epoch: 14 step: 185, loss is 0.011954315938055515\n",
      "epoch: 14 step: 186, loss is 0.0023633961100131273\n",
      "epoch: 14 step: 187, loss is 0.020830951631069183\n",
      "epoch: 14 step: 188, loss is 0.013656608760356903\n",
      "epoch: 14 step: 189, loss is 0.04259292408823967\n",
      "epoch: 14 step: 190, loss is 0.06162279099225998\n",
      "epoch: 14 step: 191, loss is 0.04559772461652756\n",
      "epoch: 14 step: 192, loss is 0.021269124001264572\n",
      "epoch: 14 step: 193, loss is 0.10943614691495895\n",
      "epoch: 14 step: 194, loss is 0.04686415567994118\n",
      "epoch: 14 step: 195, loss is 0.0009342123521491885\n",
      "epoch: 14 step: 196, loss is 0.004249287769198418\n",
      "epoch: 14 step: 197, loss is 0.006794257089495659\n",
      "epoch: 14 step: 198, loss is 0.005467116367071867\n",
      "epoch: 14 step: 199, loss is 0.012136710807681084\n",
      "epoch: 14 step: 200, loss is 0.02041775919497013\n",
      "epoch: 14 step: 201, loss is 0.018849745392799377\n",
      "epoch: 14 step: 202, loss is 0.060609370470047\n",
      "epoch: 14 step: 203, loss is 0.02483251877129078\n",
      "epoch: 14 step: 204, loss is 0.004664617124944925\n",
      "epoch: 14 step: 205, loss is 0.03308866173028946\n",
      "epoch: 14 step: 206, loss is 0.004522560629993677\n",
      "epoch: 14 step: 207, loss is 0.010703797452151775\n",
      "epoch: 14 step: 208, loss is 0.03733600303530693\n",
      "epoch: 14 step: 209, loss is 0.015410695225000381\n",
      "epoch: 14 step: 210, loss is 0.09294895827770233\n",
      "epoch: 14 step: 211, loss is 0.012508884072303772\n",
      "epoch: 14 step: 212, loss is 0.10104622691869736\n",
      "epoch: 14 step: 213, loss is 0.0893409326672554\n",
      "epoch: 14 step: 214, loss is 0.054504942148923874\n",
      "epoch: 14 step: 215, loss is 0.027812499552965164\n",
      "epoch: 14 step: 216, loss is 0.01864381693303585\n",
      "epoch: 14 step: 217, loss is 0.005565395578742027\n",
      "epoch: 14 step: 218, loss is 0.03563692420721054\n",
      "epoch: 14 step: 219, loss is 0.023142056539654732\n",
      "epoch: 14 step: 220, loss is 0.024095097556710243\n",
      "epoch: 14 step: 221, loss is 0.007042748387902975\n",
      "epoch: 14 step: 222, loss is 0.06998995691537857\n",
      "epoch: 14 step: 223, loss is 0.06060366705060005\n",
      "epoch: 14 step: 224, loss is 0.04376187175512314\n",
      "epoch: 14 step: 225, loss is 0.0055768368765711784\n",
      "epoch: 14 step: 226, loss is 0.003267761319875717\n",
      "epoch: 14 step: 227, loss is 0.025830663740634918\n",
      "epoch: 14 step: 228, loss is 0.036619123071432114\n",
      "epoch: 14 step: 229, loss is 0.0055151162669062614\n",
      "epoch: 14 step: 230, loss is 0.001232972601428628\n",
      "epoch: 14 step: 231, loss is 0.005062874406576157\n",
      "epoch: 14 step: 232, loss is 0.01724490523338318\n",
      "epoch: 14 step: 233, loss is 0.007782596629112959\n",
      "epoch: 14 step: 234, loss is 0.021663380786776543\n",
      "epoch: 14 step: 235, loss is 0.05548703670501709\n",
      "epoch: 14 step: 236, loss is 0.017239239066839218\n",
      "epoch: 14 step: 237, loss is 0.028017189353704453\n",
      "epoch: 14 step: 238, loss is 0.0241345576941967\n",
      "epoch: 14 step: 239, loss is 0.033277347683906555\n",
      "epoch: 14 step: 240, loss is 0.04188936948776245\n",
      "epoch: 14 step: 241, loss is 0.037494003772735596\n",
      "epoch: 14 step: 242, loss is 0.0034913949202746153\n",
      "epoch: 14 step: 243, loss is 0.07679221779108047\n",
      "epoch: 14 step: 244, loss is 0.01859348453581333\n",
      "epoch: 14 step: 245, loss is 0.0284134428948164\n",
      "epoch: 14 step: 246, loss is 0.012244896031916142\n",
      "epoch: 14 step: 247, loss is 0.0071130660362541676\n",
      "epoch: 14 step: 248, loss is 0.012995274737477303\n",
      "epoch: 14 step: 249, loss is 0.026288015767931938\n",
      "epoch: 14 step: 250, loss is 0.006672697141766548\n",
      "epoch: 14 step: 251, loss is 0.04099424555897713\n",
      "epoch: 14 step: 252, loss is 0.020036527886986732\n",
      "epoch: 14 step: 253, loss is 0.024197131395339966\n",
      "epoch: 14 step: 254, loss is 0.008162167854607105\n",
      "epoch: 14 step: 255, loss is 0.01456354558467865\n",
      "epoch: 14 step: 256, loss is 0.010760506615042686\n",
      "epoch: 14 step: 257, loss is 0.06175168231129646\n",
      "epoch: 14 step: 258, loss is 0.0423380546271801\n",
      "epoch: 14 step: 259, loss is 0.0251510888338089\n",
      "epoch: 14 step: 260, loss is 0.029490679502487183\n",
      "epoch: 14 step: 261, loss is 0.005465677008032799\n",
      "epoch: 14 step: 262, loss is 0.025800341740250587\n",
      "epoch: 14 step: 263, loss is 0.012890401296317577\n",
      "epoch: 14 step: 264, loss is 0.0029650151263922453\n",
      "epoch: 14 step: 265, loss is 0.03778824582695961\n",
      "epoch: 14 step: 266, loss is 0.009624071419239044\n",
      "epoch: 14 step: 267, loss is 0.00109085813164711\n",
      "epoch: 14 step: 268, loss is 0.029664255678653717\n",
      "epoch: 14 step: 269, loss is 0.0014394978061318398\n",
      "epoch: 14 step: 270, loss is 0.017412662506103516\n",
      "epoch: 14 step: 271, loss is 0.0681757852435112\n",
      "epoch: 14 step: 272, loss is 0.05999435856938362\n",
      "epoch: 14 step: 273, loss is 0.11910558491945267\n",
      "epoch: 14 step: 274, loss is 0.02163773588836193\n",
      "epoch: 14 step: 275, loss is 0.0278183426707983\n",
      "epoch: 14 step: 276, loss is 0.0013150263112038374\n",
      "epoch: 14 step: 277, loss is 0.009260228835046291\n",
      "epoch: 14 step: 278, loss is 0.010293330997228622\n",
      "epoch: 14 step: 279, loss is 0.01688820868730545\n",
      "epoch: 14 step: 280, loss is 0.0029392708092927933\n",
      "epoch: 14 step: 281, loss is 0.005797901656478643\n",
      "epoch: 14 step: 282, loss is 0.035622645169496536\n",
      "epoch: 14 step: 283, loss is 0.012385866604745388\n",
      "epoch: 14 step: 284, loss is 0.051467783749103546\n",
      "epoch: 14 step: 285, loss is 0.013410452753305435\n",
      "epoch: 14 step: 286, loss is 0.022737134248018265\n",
      "epoch: 14 step: 287, loss is 0.048077359795570374\n",
      "epoch: 14 step: 288, loss is 0.05705256760120392\n",
      "epoch: 14 step: 289, loss is 0.027954168617725372\n",
      "epoch: 14 step: 290, loss is 0.04931621626019478\n",
      "epoch: 14 step: 291, loss is 0.08797451108694077\n",
      "epoch: 14 step: 292, loss is 0.03749161958694458\n",
      "epoch: 14 step: 293, loss is 0.016216808930039406\n",
      "epoch: 14 step: 294, loss is 0.11929120123386383\n",
      "epoch: 14 step: 295, loss is 0.007939019240438938\n",
      "epoch: 14 step: 296, loss is 0.004091764334589243\n",
      "epoch: 14 step: 297, loss is 0.0364261120557785\n",
      "epoch: 14 step: 298, loss is 0.06870020180940628\n",
      "epoch: 14 step: 299, loss is 0.004362915642559528\n",
      "epoch: 14 step: 300, loss is 0.005752427037805319\n",
      "epoch: 14 step: 301, loss is 0.05330318585038185\n",
      "epoch: 14 step: 302, loss is 0.05795186385512352\n",
      "epoch: 14 step: 303, loss is 0.014177601784467697\n",
      "epoch: 14 step: 304, loss is 0.004828259348869324\n",
      "epoch: 14 step: 305, loss is 0.006874317303299904\n",
      "epoch: 14 step: 306, loss is 0.1721876710653305\n",
      "epoch: 14 step: 307, loss is 0.014872807078063488\n",
      "epoch: 14 step: 308, loss is 0.014022225514054298\n",
      "epoch: 14 step: 309, loss is 0.03941782936453819\n",
      "epoch: 14 step: 310, loss is 0.048169784247875214\n",
      "epoch: 14 step: 311, loss is 0.04564328491687775\n",
      "epoch: 14 step: 312, loss is 0.02519225887954235\n",
      "epoch: 14 step: 313, loss is 0.0588035061955452\n",
      "epoch: 14 step: 314, loss is 0.0036552671808749437\n",
      "epoch: 14 step: 315, loss is 0.007717067375779152\n",
      "epoch: 14 step: 316, loss is 0.009854822419583797\n",
      "epoch: 14 step: 317, loss is 0.007129881996661425\n",
      "epoch: 14 step: 318, loss is 0.11485127359628677\n",
      "epoch: 14 step: 319, loss is 0.00994236208498478\n",
      "epoch: 14 step: 320, loss is 0.005595160182565451\n",
      "epoch: 14 step: 321, loss is 0.0532001294195652\n",
      "epoch: 14 step: 322, loss is 0.03450848534703255\n",
      "epoch: 14 step: 323, loss is 0.033191751688718796\n",
      "epoch: 14 step: 324, loss is 0.004976698663085699\n",
      "epoch: 14 step: 325, loss is 0.024977272376418114\n",
      "epoch: 14 step: 326, loss is 0.0739121213555336\n",
      "epoch: 14 step: 327, loss is 0.0008365409448742867\n",
      "epoch: 14 step: 328, loss is 0.21054159104824066\n",
      "epoch: 14 step: 329, loss is 0.016442343592643738\n",
      "epoch: 14 step: 330, loss is 0.03201260045170784\n",
      "epoch: 14 step: 331, loss is 0.010738088749349117\n",
      "epoch: 14 step: 332, loss is 0.016487402841448784\n",
      "epoch: 14 step: 333, loss is 0.02753959409892559\n",
      "epoch: 14 step: 334, loss is 0.06152591109275818\n",
      "epoch: 14 step: 335, loss is 0.00795451458543539\n",
      "epoch: 14 step: 336, loss is 0.03486251085996628\n",
      "epoch: 14 step: 337, loss is 0.021385936066508293\n",
      "epoch: 14 step: 338, loss is 0.09340870380401611\n",
      "epoch: 14 step: 339, loss is 0.006439831107854843\n",
      "epoch: 14 step: 340, loss is 0.05529690533876419\n",
      "epoch: 14 step: 341, loss is 0.0022023944184184074\n",
      "epoch: 14 step: 342, loss is 0.02403710037469864\n",
      "epoch: 14 step: 343, loss is 0.00911130104213953\n",
      "epoch: 14 step: 344, loss is 0.06244196370244026\n",
      "epoch: 14 step: 345, loss is 0.09382379800081253\n",
      "epoch: 14 step: 346, loss is 0.05699523910880089\n",
      "epoch: 14 step: 347, loss is 0.008698699995875359\n",
      "epoch: 14 step: 348, loss is 0.21368660032749176\n",
      "epoch: 14 step: 349, loss is 0.013538240455091\n",
      "epoch: 14 step: 350, loss is 0.15385504066944122\n",
      "epoch: 14 step: 351, loss is 0.005982648115605116\n",
      "epoch: 14 step: 352, loss is 0.0016010961262509227\n",
      "epoch: 14 step: 353, loss is 0.02339407242834568\n",
      "epoch: 14 step: 354, loss is 0.02904103323817253\n",
      "epoch: 14 step: 355, loss is 0.06856083124876022\n",
      "epoch: 14 step: 356, loss is 0.011166262440383434\n",
      "epoch: 14 step: 357, loss is 0.012569436803460121\n",
      "epoch: 14 step: 358, loss is 0.1608290672302246\n",
      "epoch: 14 step: 359, loss is 0.02433811128139496\n",
      "epoch: 14 step: 360, loss is 0.06536415219306946\n",
      "epoch: 14 step: 361, loss is 0.025104587897658348\n",
      "epoch: 14 step: 362, loss is 0.007098484318703413\n",
      "epoch: 14 step: 363, loss is 0.03874064236879349\n",
      "epoch: 14 step: 364, loss is 0.022076845169067383\n",
      "epoch: 14 step: 365, loss is 0.018307384103536606\n",
      "epoch: 14 step: 366, loss is 0.0020729133393615484\n",
      "epoch: 14 step: 367, loss is 0.019565841183066368\n",
      "epoch: 14 step: 368, loss is 0.07018941640853882\n",
      "epoch: 14 step: 369, loss is 0.2700003385543823\n",
      "epoch: 14 step: 370, loss is 0.018823636695742607\n",
      "epoch: 14 step: 371, loss is 0.013853134587407112\n",
      "epoch: 14 step: 372, loss is 0.012226655147969723\n",
      "epoch: 14 step: 373, loss is 0.011536379344761372\n",
      "epoch: 14 step: 374, loss is 0.04110071435570717\n",
      "epoch: 14 step: 375, loss is 0.026879537850618362\n",
      "epoch: 14 step: 376, loss is 0.028737276792526245\n",
      "epoch: 14 step: 377, loss is 0.01029349584132433\n",
      "epoch: 14 step: 378, loss is 0.011276526376605034\n",
      "epoch: 14 step: 379, loss is 0.0014449208974838257\n",
      "epoch: 14 step: 380, loss is 0.006633673328906298\n",
      "epoch: 14 step: 381, loss is 0.011582225561141968\n",
      "epoch: 14 step: 382, loss is 0.010758357122540474\n",
      "epoch: 14 step: 383, loss is 0.01669284701347351\n",
      "epoch: 14 step: 384, loss is 0.003004902508109808\n",
      "epoch: 14 step: 385, loss is 0.03612753748893738\n",
      "epoch: 14 step: 386, loss is 0.006159352138638496\n",
      "epoch: 14 step: 387, loss is 0.00505889393389225\n",
      "epoch: 14 step: 388, loss is 0.024323131889104843\n",
      "epoch: 14 step: 389, loss is 0.03228980302810669\n",
      "epoch: 14 step: 390, loss is 0.09613833576440811\n",
      "epoch: 14 step: 391, loss is 0.20694179832935333\n",
      "epoch: 14 step: 392, loss is 0.013419421389698982\n",
      "epoch: 14 step: 393, loss is 0.0038435938768088818\n",
      "epoch: 14 step: 394, loss is 0.013799573294818401\n",
      "epoch: 14 step: 395, loss is 0.019250843673944473\n",
      "epoch: 14 step: 396, loss is 0.1004231795668602\n",
      "epoch: 14 step: 397, loss is 0.008854241110384464\n",
      "epoch: 14 step: 398, loss is 0.03015134669840336\n",
      "epoch: 14 step: 399, loss is 0.05665557458996773\n",
      "epoch: 14 step: 400, loss is 0.013371525332331657\n",
      "epoch: 14 step: 401, loss is 0.009451805613934994\n",
      "epoch: 14 step: 402, loss is 0.0044856504537165165\n",
      "epoch: 14 step: 403, loss is 0.0020100704859942198\n",
      "epoch: 14 step: 404, loss is 0.014031571336090565\n",
      "epoch: 14 step: 405, loss is 0.013736962340772152\n",
      "epoch: 14 step: 406, loss is 0.01740175299346447\n",
      "epoch: 14 step: 407, loss is 0.005686535965651274\n",
      "epoch: 14 step: 408, loss is 0.029665786772966385\n",
      "epoch: 14 step: 409, loss is 0.009408981539309025\n",
      "epoch: 14 step: 410, loss is 0.024341879412531853\n",
      "epoch: 14 step: 411, loss is 0.004031962715089321\n",
      "epoch: 14 step: 412, loss is 0.006761725526303053\n",
      "epoch: 14 step: 413, loss is 0.04122359678149223\n",
      "epoch: 14 step: 414, loss is 0.06847690790891647\n",
      "epoch: 14 step: 415, loss is 0.04764499142765999\n",
      "epoch: 14 step: 416, loss is 0.0009996609296649694\n",
      "epoch: 14 step: 417, loss is 0.006203400902450085\n",
      "epoch: 14 step: 418, loss is 0.01958533376455307\n",
      "epoch: 14 step: 419, loss is 0.04963359236717224\n",
      "epoch: 14 step: 420, loss is 0.007024932652711868\n",
      "epoch: 14 step: 421, loss is 0.03480292856693268\n",
      "epoch: 14 step: 422, loss is 0.0644732415676117\n",
      "epoch: 14 step: 423, loss is 0.123815156519413\n",
      "epoch: 14 step: 424, loss is 0.008815365843474865\n",
      "epoch: 14 step: 425, loss is 0.03949619457125664\n",
      "epoch: 14 step: 426, loss is 0.09336986392736435\n",
      "epoch: 14 step: 427, loss is 0.005435747094452381\n",
      "epoch: 14 step: 428, loss is 0.13707514107227325\n",
      "epoch: 14 step: 429, loss is 0.009333469904959202\n",
      "epoch: 14 step: 430, loss is 0.03652102127671242\n",
      "epoch: 14 step: 431, loss is 0.031484343111515045\n",
      "epoch: 14 step: 432, loss is 0.014278656803071499\n",
      "epoch: 14 step: 433, loss is 0.024722931906580925\n",
      "epoch: 14 step: 434, loss is 0.012778252363204956\n",
      "epoch: 14 step: 435, loss is 0.026166418567299843\n",
      "epoch: 14 step: 436, loss is 0.10440143942832947\n",
      "epoch: 14 step: 437, loss is 0.024063019081950188\n",
      "epoch: 14 step: 438, loss is 0.055429842323064804\n",
      "epoch: 14 step: 439, loss is 0.004189874045550823\n",
      "epoch: 14 step: 440, loss is 0.07637932151556015\n",
      "epoch: 14 step: 441, loss is 0.030777353793382645\n",
      "epoch: 14 step: 442, loss is 0.020686887204647064\n",
      "epoch: 14 step: 443, loss is 0.008743996731936932\n",
      "epoch: 14 step: 444, loss is 0.011303353123366833\n",
      "epoch: 14 step: 445, loss is 0.010918418876826763\n",
      "epoch: 14 step: 446, loss is 0.015193183906376362\n",
      "epoch: 14 step: 447, loss is 0.09597692638635635\n",
      "epoch: 14 step: 448, loss is 0.007034949958324432\n",
      "epoch: 14 step: 449, loss is 0.003620740957558155\n",
      "epoch: 14 step: 450, loss is 0.16313175857067108\n",
      "epoch: 14 step: 451, loss is 0.0023522786796092987\n",
      "epoch: 14 step: 452, loss is 0.026326416060328484\n",
      "epoch: 14 step: 453, loss is 0.07904665172100067\n",
      "epoch: 14 step: 454, loss is 0.00879569724202156\n",
      "epoch: 14 step: 455, loss is 0.07372588664293289\n",
      "epoch: 14 step: 456, loss is 0.0008891200996004045\n",
      "epoch: 14 step: 457, loss is 0.07895869016647339\n",
      "epoch: 14 step: 458, loss is 0.10005554556846619\n",
      "epoch: 14 step: 459, loss is 0.01659434102475643\n",
      "epoch: 14 step: 460, loss is 0.0049029747024178505\n",
      "epoch: 14 step: 461, loss is 0.062213946133852005\n",
      "epoch: 14 step: 462, loss is 0.04696974158287048\n",
      "epoch: 14 step: 463, loss is 0.005639628041535616\n",
      "epoch: 14 step: 464, loss is 0.011202004738152027\n",
      "epoch: 14 step: 465, loss is 0.0036269903648644686\n",
      "epoch: 14 step: 466, loss is 0.024258838966488838\n",
      "epoch: 14 step: 467, loss is 0.013459675014019012\n",
      "epoch: 14 step: 468, loss is 0.009812557138502598\n",
      "epoch: 14 step: 469, loss is 0.019478658214211464\n",
      "epoch: 14 step: 470, loss is 0.014410985633730888\n",
      "epoch: 14 step: 471, loss is 0.003748239018023014\n",
      "epoch: 14 step: 472, loss is 0.006271061021834612\n",
      "epoch: 14 step: 473, loss is 0.026489509269595146\n",
      "epoch: 14 step: 474, loss is 0.014654049649834633\n",
      "epoch: 14 step: 475, loss is 0.006183365359902382\n",
      "epoch: 14 step: 476, loss is 0.06161396577954292\n",
      "epoch: 14 step: 477, loss is 0.02646210789680481\n",
      "epoch: 14 step: 478, loss is 0.027729719877243042\n",
      "epoch: 14 step: 479, loss is 0.0056644282303750515\n",
      "epoch: 14 step: 480, loss is 0.015316399745643139\n",
      "epoch: 14 step: 481, loss is 0.009933707304298878\n",
      "epoch: 14 step: 482, loss is 0.02491225115954876\n",
      "epoch: 14 step: 483, loss is 0.09897788614034653\n",
      "epoch: 14 step: 484, loss is 0.002083288738504052\n",
      "epoch: 14 step: 485, loss is 0.05761602520942688\n",
      "epoch: 14 step: 486, loss is 0.008616934530436993\n",
      "epoch: 14 step: 487, loss is 0.005935321096330881\n",
      "epoch: 14 step: 488, loss is 0.03322514891624451\n",
      "epoch: 14 step: 489, loss is 0.06953452527523041\n",
      "epoch: 14 step: 490, loss is 0.024009082466363907\n",
      "epoch: 14 step: 491, loss is 0.016384359449148178\n",
      "epoch: 14 step: 492, loss is 0.006354969460517168\n",
      "epoch: 14 step: 493, loss is 0.09343353658914566\n",
      "epoch: 14 step: 494, loss is 0.0075467610731720924\n",
      "epoch: 14 step: 495, loss is 0.0029018865898251534\n",
      "epoch: 14 step: 496, loss is 0.0022169349249452353\n",
      "epoch: 14 step: 497, loss is 0.09068843722343445\n",
      "epoch: 14 step: 498, loss is 0.0066230082884430885\n",
      "epoch: 14 step: 499, loss is 0.0034191610757261515\n",
      "epoch: 14 step: 500, loss is 0.02758858911693096\n",
      "epoch: 14 step: 501, loss is 0.016705971211194992\n",
      "epoch: 14 step: 502, loss is 0.03856107220053673\n",
      "epoch: 14 step: 503, loss is 0.06620509922504425\n",
      "epoch: 14 step: 504, loss is 0.01012133713811636\n",
      "epoch: 14 step: 505, loss is 0.04115626960992813\n",
      "epoch: 14 step: 506, loss is 0.006295377854257822\n",
      "epoch: 14 step: 507, loss is 0.004909394308924675\n",
      "epoch: 14 step: 508, loss is 0.002133701229467988\n",
      "epoch: 14 step: 509, loss is 0.02543707750737667\n",
      "epoch: 14 step: 510, loss is 0.037549007683992386\n",
      "epoch: 14 step: 511, loss is 0.0318964347243309\n",
      "epoch: 14 step: 512, loss is 0.005489651579409838\n",
      "epoch: 14 step: 513, loss is 0.0018700379878282547\n",
      "epoch: 14 step: 514, loss is 0.01054728589951992\n",
      "epoch: 14 step: 515, loss is 0.03398355841636658\n",
      "epoch: 14 step: 516, loss is 0.004329605493694544\n",
      "epoch: 14 step: 517, loss is 0.06483061611652374\n",
      "epoch: 14 step: 518, loss is 0.07698667049407959\n",
      "epoch: 14 step: 519, loss is 0.002499209251254797\n",
      "epoch: 14 step: 520, loss is 0.021169377490878105\n",
      "epoch: 14 step: 521, loss is 0.00785338506102562\n",
      "epoch: 14 step: 522, loss is 0.002123219193890691\n",
      "epoch: 14 step: 523, loss is 0.04595573991537094\n",
      "epoch: 14 step: 524, loss is 0.004086882341653109\n",
      "epoch: 14 step: 525, loss is 0.04292219132184982\n",
      "epoch: 14 step: 526, loss is 0.018830783665180206\n",
      "epoch: 14 step: 527, loss is 0.047993674874305725\n",
      "epoch: 14 step: 528, loss is 0.016378019005060196\n",
      "epoch: 14 step: 529, loss is 0.005278772674500942\n",
      "epoch: 14 step: 530, loss is 0.003432841971516609\n",
      "epoch: 14 step: 531, loss is 0.009566710330545902\n",
      "epoch: 14 step: 532, loss is 0.002696627052500844\n",
      "epoch: 14 step: 533, loss is 0.012509411200881004\n",
      "epoch: 14 step: 534, loss is 0.07196276634931564\n",
      "epoch: 14 step: 535, loss is 0.01891924813389778\n",
      "epoch: 14 step: 536, loss is 0.012825557962059975\n",
      "epoch: 14 step: 537, loss is 0.005402757786214352\n",
      "epoch: 14 step: 538, loss is 0.010733181610703468\n",
      "epoch: 14 step: 539, loss is 0.056276626884937286\n",
      "epoch: 14 step: 540, loss is 0.07813039422035217\n",
      "epoch: 14 step: 541, loss is 0.04626347869634628\n",
      "epoch: 14 step: 542, loss is 0.004854847677052021\n",
      "epoch: 14 step: 543, loss is 0.0006241807132028043\n",
      "epoch: 14 step: 544, loss is 0.027163133025169373\n",
      "epoch: 14 step: 545, loss is 0.07202591747045517\n",
      "epoch: 14 step: 546, loss is 0.03493950143456459\n",
      "epoch: 14 step: 547, loss is 0.0023208651691675186\n",
      "epoch: 14 step: 548, loss is 0.017909260466694832\n",
      "epoch: 14 step: 549, loss is 0.031101003289222717\n",
      "epoch: 14 step: 550, loss is 0.006696542724967003\n",
      "epoch: 14 step: 551, loss is 0.0028421529568731785\n",
      "epoch: 14 step: 552, loss is 0.0017749479738995433\n",
      "epoch: 14 step: 553, loss is 0.01742030680179596\n",
      "epoch: 14 step: 554, loss is 0.0015005276072770357\n",
      "epoch: 14 step: 555, loss is 0.027199851348996162\n",
      "epoch: 14 step: 556, loss is 0.030416985973715782\n",
      "epoch: 14 step: 557, loss is 0.01971724070608616\n",
      "epoch: 14 step: 558, loss is 0.016826141625642776\n",
      "epoch: 14 step: 559, loss is 0.005802132189273834\n",
      "epoch: 14 step: 560, loss is 0.016358355060219765\n",
      "epoch: 14 step: 561, loss is 0.05871003121137619\n",
      "epoch: 14 step: 562, loss is 0.053081922233104706\n",
      "epoch: 14 step: 563, loss is 0.04374055936932564\n",
      "epoch: 14 step: 564, loss is 0.013840769417583942\n",
      "epoch: 14 step: 565, loss is 0.016566768288612366\n",
      "epoch: 14 step: 566, loss is 0.053551796823740005\n",
      "epoch: 14 step: 567, loss is 0.051775336265563965\n",
      "epoch: 14 step: 568, loss is 0.03902347385883331\n",
      "epoch: 14 step: 569, loss is 0.0492057204246521\n",
      "epoch: 14 step: 570, loss is 0.021617958322167397\n",
      "epoch: 14 step: 571, loss is 0.05288708209991455\n",
      "epoch: 14 step: 572, loss is 0.15440571308135986\n",
      "epoch: 14 step: 573, loss is 0.03695056587457657\n",
      "epoch: 14 step: 574, loss is 0.023984942585229874\n",
      "epoch: 14 step: 575, loss is 0.0056685577146708965\n",
      "epoch: 14 step: 576, loss is 0.021976124495267868\n",
      "epoch: 14 step: 577, loss is 0.059634435921907425\n",
      "epoch: 14 step: 578, loss is 0.02664232812821865\n",
      "epoch: 14 step: 579, loss is 0.001121725421398878\n",
      "epoch: 14 step: 580, loss is 0.018509982153773308\n",
      "epoch: 14 step: 581, loss is 0.08136873692274094\n",
      "epoch: 14 step: 582, loss is 0.011547215282917023\n",
      "epoch: 14 step: 583, loss is 0.004058706108480692\n",
      "epoch: 14 step: 584, loss is 0.05405253916978836\n",
      "epoch: 14 step: 585, loss is 0.01794818975031376\n",
      "epoch: 14 step: 586, loss is 0.010087497532367706\n",
      "epoch: 14 step: 587, loss is 0.1556086242198944\n",
      "epoch: 14 step: 588, loss is 0.040973495692014694\n",
      "epoch: 14 step: 589, loss is 0.003555405419319868\n",
      "epoch: 14 step: 590, loss is 0.09060423821210861\n",
      "epoch: 14 step: 591, loss is 0.0960916131734848\n",
      "epoch: 14 step: 592, loss is 0.030582208186388016\n",
      "epoch: 14 step: 593, loss is 0.015077434480190277\n",
      "epoch: 14 step: 594, loss is 0.01967102289199829\n",
      "epoch: 14 step: 595, loss is 0.049889013171195984\n",
      "epoch: 14 step: 596, loss is 0.0070780678652226925\n",
      "epoch: 14 step: 597, loss is 0.013206105679273605\n",
      "epoch: 14 step: 598, loss is 0.0024260911159217358\n",
      "epoch: 14 step: 599, loss is 0.001218556659296155\n",
      "epoch: 14 step: 600, loss is 0.187013179063797\n",
      "epoch: 14 step: 601, loss is 0.024183204397559166\n",
      "epoch: 14 step: 602, loss is 0.05709715932607651\n",
      "epoch: 14 step: 603, loss is 0.007635485380887985\n",
      "epoch: 14 step: 604, loss is 0.044065311551094055\n",
      "epoch: 14 step: 605, loss is 0.004335155710577965\n",
      "epoch: 14 step: 606, loss is 0.11892994493246078\n",
      "epoch: 14 step: 607, loss is 0.011197549290955067\n",
      "epoch: 14 step: 608, loss is 0.018889786675572395\n",
      "epoch: 14 step: 609, loss is 0.0075483256950974464\n",
      "epoch: 14 step: 610, loss is 0.08294528722763062\n",
      "epoch: 14 step: 611, loss is 0.029075564816594124\n",
      "epoch: 14 step: 612, loss is 0.01669524796307087\n",
      "epoch: 14 step: 613, loss is 0.0834847018122673\n",
      "epoch: 14 step: 614, loss is 0.12828056514263153\n",
      "epoch: 14 step: 615, loss is 0.024064423516392708\n",
      "epoch: 14 step: 616, loss is 0.024326849728822708\n",
      "epoch: 14 step: 617, loss is 0.07487177103757858\n",
      "epoch: 14 step: 618, loss is 0.025763463228940964\n",
      "epoch: 14 step: 619, loss is 0.0021441886201500893\n",
      "epoch: 14 step: 620, loss is 0.054299354553222656\n",
      "epoch: 14 step: 621, loss is 0.015168772079050541\n",
      "epoch: 14 step: 622, loss is 0.05769229680299759\n",
      "epoch: 14 step: 623, loss is 0.08831179887056351\n",
      "epoch: 14 step: 624, loss is 0.046828821301460266\n",
      "epoch: 14 step: 625, loss is 0.02261478267610073\n",
      "epoch: 14 step: 626, loss is 0.05567317456007004\n",
      "epoch: 14 step: 627, loss is 0.006820628419518471\n",
      "epoch: 14 step: 628, loss is 0.022619128227233887\n",
      "epoch: 14 step: 629, loss is 0.11837682127952576\n",
      "epoch: 14 step: 630, loss is 0.030470317229628563\n",
      "epoch: 14 step: 631, loss is 0.02895743027329445\n",
      "epoch: 14 step: 632, loss is 0.1376832127571106\n",
      "epoch: 14 step: 633, loss is 0.027151908725500107\n",
      "epoch: 14 step: 634, loss is 0.013300711289048195\n",
      "epoch: 14 step: 635, loss is 0.003777862526476383\n",
      "epoch: 14 step: 636, loss is 0.09409584105014801\n",
      "epoch: 14 step: 637, loss is 0.0030182679183781147\n",
      "epoch: 14 step: 638, loss is 0.057476550340652466\n",
      "epoch: 14 step: 639, loss is 0.04968205466866493\n",
      "epoch: 14 step: 640, loss is 0.06053060665726662\n",
      "epoch: 14 step: 641, loss is 0.008822320960462093\n",
      "epoch: 14 step: 642, loss is 0.061141304671764374\n",
      "epoch: 14 step: 643, loss is 0.009107861667871475\n",
      "epoch: 14 step: 644, loss is 0.004757466726005077\n",
      "epoch: 14 step: 645, loss is 0.02000483125448227\n",
      "epoch: 14 step: 646, loss is 0.034727148711681366\n",
      "epoch: 14 step: 647, loss is 0.011887931264936924\n",
      "epoch: 14 step: 648, loss is 0.043822940438985825\n",
      "epoch: 14 step: 649, loss is 0.005953316576778889\n",
      "epoch: 14 step: 650, loss is 0.0686100646853447\n",
      "epoch: 14 step: 651, loss is 0.008684172295033932\n",
      "epoch: 14 step: 652, loss is 0.012443950399756432\n",
      "epoch: 14 step: 653, loss is 0.09502845257520676\n",
      "epoch: 14 step: 654, loss is 0.0013599414378404617\n",
      "epoch: 14 step: 655, loss is 0.08708389848470688\n",
      "epoch: 14 step: 656, loss is 0.06727034598588943\n",
      "epoch: 14 step: 657, loss is 0.05162191018462181\n",
      "epoch: 14 step: 658, loss is 0.006105164997279644\n",
      "epoch: 14 step: 659, loss is 0.03357529267668724\n",
      "epoch: 14 step: 660, loss is 0.14948491752147675\n",
      "epoch: 14 step: 661, loss is 0.24186751246452332\n",
      "epoch: 14 step: 662, loss is 0.014786717481911182\n",
      "epoch: 14 step: 663, loss is 0.005461432505398989\n",
      "epoch: 14 step: 664, loss is 0.04838739335536957\n",
      "epoch: 14 step: 665, loss is 0.11105015128850937\n",
      "epoch: 14 step: 666, loss is 0.01736888848245144\n",
      "epoch: 14 step: 667, loss is 0.017759384587407112\n",
      "epoch: 14 step: 668, loss is 0.05887587368488312\n",
      "epoch: 14 step: 669, loss is 0.047871749848127365\n",
      "epoch: 14 step: 670, loss is 0.015616181306540966\n",
      "epoch: 14 step: 671, loss is 0.07736491411924362\n",
      "epoch: 14 step: 672, loss is 0.03076580911874771\n",
      "epoch: 14 step: 673, loss is 0.03368794545531273\n",
      "epoch: 14 step: 674, loss is 0.06399118155241013\n",
      "epoch: 14 step: 675, loss is 0.0889095589518547\n",
      "epoch: 14 step: 676, loss is 0.010970572009682655\n",
      "epoch: 14 step: 677, loss is 0.0022760166320949793\n",
      "epoch: 14 step: 678, loss is 0.034142617136240005\n",
      "epoch: 14 step: 679, loss is 0.08577363938093185\n",
      "epoch: 14 step: 680, loss is 0.026896366849541664\n",
      "epoch: 14 step: 681, loss is 0.04045896977186203\n",
      "epoch: 14 step: 682, loss is 0.011388979852199554\n",
      "epoch: 14 step: 683, loss is 0.008935512974858284\n",
      "epoch: 14 step: 684, loss is 0.026680098846554756\n",
      "epoch: 14 step: 685, loss is 0.02486100047826767\n",
      "epoch: 14 step: 686, loss is 0.011995073407888412\n",
      "epoch: 14 step: 687, loss is 0.08161622285842896\n",
      "epoch: 14 step: 688, loss is 0.01199962105602026\n",
      "epoch: 14 step: 689, loss is 0.008831635117530823\n",
      "epoch: 14 step: 690, loss is 0.0365569144487381\n",
      "epoch: 14 step: 691, loss is 0.006628606002777815\n",
      "epoch: 14 step: 692, loss is 0.003868349129334092\n",
      "epoch: 14 step: 693, loss is 0.06457875669002533\n",
      "epoch: 14 step: 694, loss is 0.00775800459086895\n",
      "epoch: 14 step: 695, loss is 0.01961960643529892\n",
      "epoch: 14 step: 696, loss is 0.05134434252977371\n",
      "epoch: 14 step: 697, loss is 0.014330790378153324\n",
      "epoch: 14 step: 698, loss is 0.01074183452874422\n",
      "epoch: 14 step: 699, loss is 0.015623684972524643\n",
      "epoch: 14 step: 700, loss is 0.04954621195793152\n",
      "epoch: 14 step: 701, loss is 0.027855228632688522\n",
      "epoch: 14 step: 702, loss is 0.005286851432174444\n",
      "epoch: 14 step: 703, loss is 0.13030387461185455\n",
      "epoch: 14 step: 704, loss is 0.006221238058060408\n",
      "epoch: 14 step: 705, loss is 0.0002759643248282373\n",
      "epoch: 14 step: 706, loss is 0.037354666739702225\n",
      "epoch: 14 step: 707, loss is 0.04671640321612358\n",
      "epoch: 14 step: 708, loss is 0.05587289109826088\n",
      "epoch: 14 step: 709, loss is 0.05759921669960022\n",
      "epoch: 14 step: 710, loss is 0.1102079376578331\n",
      "epoch: 14 step: 711, loss is 0.08042869716882706\n",
      "epoch: 14 step: 712, loss is 0.03783264756202698\n",
      "epoch: 14 step: 713, loss is 0.07481162250041962\n",
      "epoch: 14 step: 714, loss is 0.0213560089468956\n",
      "epoch: 14 step: 715, loss is 0.018550563603639603\n",
      "epoch: 14 step: 716, loss is 0.011358998715877533\n",
      "epoch: 14 step: 717, loss is 0.04311155155301094\n",
      "epoch: 14 step: 718, loss is 0.1597314178943634\n",
      "epoch: 14 step: 719, loss is 0.032598722726106644\n",
      "epoch: 14 step: 720, loss is 0.21779268980026245\n",
      "epoch: 14 step: 721, loss is 0.00549220759421587\n",
      "epoch: 14 step: 722, loss is 0.1595061868429184\n",
      "epoch: 14 step: 723, loss is 0.058791857212781906\n",
      "epoch: 14 step: 724, loss is 0.04852478206157684\n",
      "epoch: 14 step: 725, loss is 0.038439515978097916\n",
      "epoch: 14 step: 726, loss is 0.15666329860687256\n",
      "epoch: 14 step: 727, loss is 0.028341149911284447\n",
      "epoch: 14 step: 728, loss is 0.03415648266673088\n",
      "epoch: 14 step: 729, loss is 0.05646660923957825\n",
      "epoch: 14 step: 730, loss is 0.005268604960292578\n",
      "epoch: 14 step: 731, loss is 0.09028979390859604\n",
      "epoch: 14 step: 732, loss is 0.11185199022293091\n",
      "epoch: 14 step: 733, loss is 0.032508209347724915\n",
      "epoch: 14 step: 734, loss is 0.02091049775481224\n",
      "epoch: 14 step: 735, loss is 0.11102702468633652\n",
      "epoch: 14 step: 736, loss is 0.06868058443069458\n",
      "epoch: 14 step: 737, loss is 0.08599688857793808\n",
      "epoch: 14 step: 738, loss is 0.08013008534908295\n",
      "epoch: 14 step: 739, loss is 0.0321817584335804\n",
      "epoch: 14 step: 740, loss is 0.039734337478876114\n",
      "epoch: 14 step: 741, loss is 0.019144726917147636\n",
      "epoch: 14 step: 742, loss is 0.10303662717342377\n",
      "epoch: 14 step: 743, loss is 0.016652267426252365\n",
      "epoch: 14 step: 744, loss is 0.008322776295244694\n",
      "epoch: 14 step: 745, loss is 0.02048393338918686\n",
      "epoch: 14 step: 746, loss is 0.04242529347538948\n",
      "epoch: 14 step: 747, loss is 0.06198998540639877\n",
      "epoch: 14 step: 748, loss is 0.10244477540254593\n",
      "epoch: 14 step: 749, loss is 0.031902700662612915\n",
      "epoch: 14 step: 750, loss is 0.0403553768992424\n",
      "epoch: 14 step: 751, loss is 0.014109628275036812\n",
      "epoch: 14 step: 752, loss is 0.04322763904929161\n",
      "epoch: 14 step: 753, loss is 0.04053891822695732\n",
      "epoch: 14 step: 754, loss is 0.008079678751528263\n",
      "epoch: 14 step: 755, loss is 0.15687748789787292\n",
      "epoch: 14 step: 756, loss is 0.15882179141044617\n",
      "epoch: 14 step: 757, loss is 0.04403292015194893\n",
      "epoch: 14 step: 758, loss is 0.07176996022462845\n",
      "epoch: 14 step: 759, loss is 0.027681520208716393\n",
      "epoch: 14 step: 760, loss is 0.012370551936328411\n",
      "epoch: 14 step: 761, loss is 0.08866111934185028\n",
      "epoch: 14 step: 762, loss is 0.048118628561496735\n",
      "epoch: 14 step: 763, loss is 0.028168167918920517\n",
      "epoch: 14 step: 764, loss is 0.04458033666014671\n",
      "epoch: 14 step: 765, loss is 0.009787033312022686\n",
      "epoch: 14 step: 766, loss is 0.019779371097683907\n",
      "epoch: 14 step: 767, loss is 0.06848760694265366\n",
      "epoch: 14 step: 768, loss is 0.021411430090665817\n",
      "epoch: 14 step: 769, loss is 0.006030629854649305\n",
      "epoch: 14 step: 770, loss is 0.008936773985624313\n",
      "epoch: 14 step: 771, loss is 0.07331489771604538\n",
      "epoch: 14 step: 772, loss is 0.018169857561588287\n",
      "epoch: 14 step: 773, loss is 0.044955216348171234\n",
      "epoch: 14 step: 774, loss is 0.02223380096256733\n",
      "epoch: 14 step: 775, loss is 0.031263090670108795\n",
      "epoch: 14 step: 776, loss is 0.0193027313798666\n",
      "epoch: 14 step: 777, loss is 0.020619720220565796\n",
      "epoch: 14 step: 778, loss is 0.061202097684144974\n",
      "epoch: 14 step: 779, loss is 0.023121478036046028\n",
      "epoch: 14 step: 780, loss is 0.012719033285975456\n",
      "epoch: 14 step: 781, loss is 0.013431326486170292\n",
      "epoch: 14 step: 782, loss is 0.007125877775251865\n",
      "epoch: 14 step: 783, loss is 0.014557491056621075\n",
      "epoch: 14 step: 784, loss is 0.1193481832742691\n",
      "epoch: 14 step: 785, loss is 0.020367063581943512\n",
      "epoch: 14 step: 786, loss is 0.05956920608878136\n",
      "epoch: 14 step: 787, loss is 0.04144229367375374\n",
      "epoch: 14 step: 788, loss is 0.045398954302072525\n",
      "epoch: 14 step: 789, loss is 0.017148176208138466\n",
      "epoch: 14 step: 790, loss is 0.023298203945159912\n",
      "epoch: 14 step: 791, loss is 0.044424958527088165\n",
      "epoch: 14 step: 792, loss is 0.024930136278271675\n",
      "epoch: 14 step: 793, loss is 0.01860951818525791\n",
      "epoch: 14 step: 794, loss is 0.023378439247608185\n",
      "epoch: 14 step: 795, loss is 0.01699044741690159\n",
      "epoch: 14 step: 796, loss is 0.005856601521372795\n",
      "epoch: 14 step: 797, loss is 0.04887077212333679\n",
      "epoch: 14 step: 798, loss is 0.014432632364332676\n",
      "epoch: 14 step: 799, loss is 0.031960368156433105\n",
      "epoch: 14 step: 800, loss is 0.009464193135499954\n",
      "epoch: 14 step: 801, loss is 0.15969888865947723\n",
      "epoch: 14 step: 802, loss is 0.012687286362051964\n",
      "epoch: 14 step: 803, loss is 0.024057995527982712\n",
      "epoch: 14 step: 804, loss is 0.0031412935350090265\n",
      "epoch: 14 step: 805, loss is 0.04182452708482742\n",
      "epoch: 14 step: 806, loss is 0.011763528920710087\n",
      "epoch: 14 step: 807, loss is 0.026138823479413986\n",
      "epoch: 14 step: 808, loss is 0.0017943541752174497\n",
      "epoch: 14 step: 809, loss is 0.0563066191971302\n",
      "epoch: 14 step: 810, loss is 0.0316743366420269\n",
      "epoch: 14 step: 811, loss is 0.017653921619057655\n",
      "epoch: 14 step: 812, loss is 0.034469060599803925\n",
      "epoch: 14 step: 813, loss is 0.012179316021502018\n",
      "epoch: 14 step: 814, loss is 0.08991947025060654\n",
      "epoch: 14 step: 815, loss is 0.16905814409255981\n",
      "epoch: 14 step: 816, loss is 0.024339627474546432\n",
      "epoch: 14 step: 817, loss is 0.022336043417453766\n",
      "epoch: 14 step: 818, loss is 0.05755438655614853\n",
      "epoch: 14 step: 819, loss is 0.041133660823106766\n",
      "epoch: 14 step: 820, loss is 0.01916508376598358\n",
      "epoch: 14 step: 821, loss is 0.012136289849877357\n",
      "epoch: 14 step: 822, loss is 0.020686767995357513\n",
      "epoch: 14 step: 823, loss is 0.005831622984260321\n",
      "epoch: 14 step: 824, loss is 0.04710278660058975\n",
      "epoch: 14 step: 825, loss is 0.025049855932593346\n",
      "epoch: 14 step: 826, loss is 0.03579218313097954\n",
      "epoch: 14 step: 827, loss is 0.006299222819507122\n",
      "epoch: 14 step: 828, loss is 0.012417047284543514\n",
      "epoch: 14 step: 829, loss is 0.004573552869260311\n",
      "epoch: 14 step: 830, loss is 0.04957745224237442\n",
      "epoch: 14 step: 831, loss is 0.014724552631378174\n",
      "epoch: 14 step: 832, loss is 0.08043263107538223\n",
      "epoch: 14 step: 833, loss is 0.023445621132850647\n",
      "epoch: 14 step: 834, loss is 0.009519976563751698\n",
      "epoch: 14 step: 835, loss is 0.0065818834118545055\n",
      "epoch: 14 step: 836, loss is 0.02424955554306507\n",
      "epoch: 14 step: 837, loss is 0.08122459053993225\n",
      "epoch: 14 step: 838, loss is 0.006195458117872477\n",
      "epoch: 14 step: 839, loss is 0.010611414909362793\n",
      "epoch: 14 step: 840, loss is 0.008855000138282776\n",
      "epoch: 14 step: 841, loss is 0.06424173712730408\n",
      "epoch: 14 step: 842, loss is 0.020123057067394257\n",
      "epoch: 14 step: 843, loss is 0.06846041977405548\n",
      "epoch: 14 step: 844, loss is 0.022462517023086548\n",
      "epoch: 14 step: 845, loss is 0.045610059052705765\n",
      "epoch: 14 step: 846, loss is 0.030118197202682495\n",
      "epoch: 14 step: 847, loss is 0.008817128837108612\n",
      "epoch: 14 step: 848, loss is 0.04984742030501366\n",
      "epoch: 14 step: 849, loss is 0.012379471212625504\n",
      "epoch: 14 step: 850, loss is 0.04087258502840996\n",
      "epoch: 14 step: 851, loss is 0.030832916498184204\n",
      "epoch: 14 step: 852, loss is 0.0170514527708292\n",
      "epoch: 14 step: 853, loss is 0.022777535021305084\n",
      "epoch: 14 step: 854, loss is 0.07499414682388306\n",
      "epoch: 14 step: 855, loss is 0.01565522514283657\n",
      "epoch: 14 step: 856, loss is 0.008052985183894634\n",
      "epoch: 14 step: 857, loss is 0.08705152571201324\n",
      "epoch: 14 step: 858, loss is 0.012694993987679482\n",
      "epoch: 14 step: 859, loss is 0.028645308688282967\n",
      "epoch: 14 step: 860, loss is 0.014209355227649212\n",
      "epoch: 14 step: 861, loss is 0.026613662019371986\n",
      "epoch: 14 step: 862, loss is 0.023264631628990173\n",
      "epoch: 14 step: 863, loss is 0.0037905138451606035\n",
      "epoch: 14 step: 864, loss is 0.008245470002293587\n",
      "epoch: 14 step: 865, loss is 0.0029465712141245604\n",
      "epoch: 14 step: 866, loss is 0.046356625854969025\n",
      "epoch: 14 step: 867, loss is 0.036336351186037064\n",
      "epoch: 14 step: 868, loss is 0.023837413638830185\n",
      "epoch: 14 step: 869, loss is 0.017401501536369324\n",
      "epoch: 14 step: 870, loss is 0.07556724548339844\n",
      "epoch: 14 step: 871, loss is 0.03306248039007187\n",
      "epoch: 14 step: 872, loss is 0.021606406196951866\n",
      "epoch: 14 step: 873, loss is 0.02708536572754383\n",
      "epoch: 14 step: 874, loss is 0.0225584264844656\n",
      "epoch: 14 step: 875, loss is 0.0010396402794867754\n",
      "epoch: 14 step: 876, loss is 0.010491677559912205\n",
      "epoch: 14 step: 877, loss is 0.023766694590449333\n",
      "epoch: 14 step: 878, loss is 0.07194847613573074\n",
      "epoch: 14 step: 879, loss is 0.048985809087753296\n",
      "epoch: 14 step: 880, loss is 0.02123415656387806\n",
      "epoch: 14 step: 881, loss is 0.04497203975915909\n",
      "epoch: 14 step: 882, loss is 0.0012098036240786314\n",
      "epoch: 14 step: 883, loss is 0.019458957016468048\n",
      "epoch: 14 step: 884, loss is 0.01383563969284296\n",
      "epoch: 14 step: 885, loss is 0.017581313848495483\n",
      "epoch: 14 step: 886, loss is 0.057372476905584335\n",
      "epoch: 14 step: 887, loss is 0.09885406494140625\n",
      "epoch: 14 step: 888, loss is 0.04431147873401642\n",
      "epoch: 14 step: 889, loss is 0.016926782205700874\n",
      "epoch: 14 step: 890, loss is 0.004871651064604521\n",
      "epoch: 14 step: 891, loss is 0.01969013549387455\n",
      "epoch: 14 step: 892, loss is 0.001567311235703528\n",
      "epoch: 14 step: 893, loss is 0.20714004337787628\n",
      "epoch: 14 step: 894, loss is 0.1051984503865242\n",
      "epoch: 14 step: 895, loss is 0.01887948252260685\n",
      "epoch: 14 step: 896, loss is 0.1518218219280243\n",
      "epoch: 14 step: 897, loss is 0.07174104452133179\n",
      "epoch: 14 step: 898, loss is 0.02996223419904709\n",
      "epoch: 14 step: 899, loss is 0.022499674931168556\n",
      "epoch: 14 step: 900, loss is 0.01573306694626808\n",
      "epoch: 14 step: 901, loss is 0.11494249850511551\n",
      "epoch: 14 step: 902, loss is 0.007267313543707132\n",
      "epoch: 14 step: 903, loss is 0.0765497162938118\n",
      "epoch: 14 step: 904, loss is 0.017888404428958893\n",
      "epoch: 14 step: 905, loss is 0.0007174929487518966\n",
      "epoch: 14 step: 906, loss is 0.016609597951173782\n",
      "epoch: 14 step: 907, loss is 0.0027244945522397757\n",
      "epoch: 14 step: 908, loss is 0.05009857192635536\n",
      "epoch: 14 step: 909, loss is 0.008540566079318523\n",
      "epoch: 14 step: 910, loss is 0.12889182567596436\n",
      "epoch: 14 step: 911, loss is 0.10136936604976654\n",
      "epoch: 14 step: 912, loss is 0.09291289746761322\n",
      "epoch: 14 step: 913, loss is 0.05422172695398331\n",
      "epoch: 14 step: 914, loss is 0.03254729509353638\n",
      "epoch: 14 step: 915, loss is 0.0061927917413413525\n",
      "epoch: 14 step: 916, loss is 0.004757962189614773\n",
      "epoch: 14 step: 917, loss is 0.13160806894302368\n",
      "epoch: 14 step: 918, loss is 0.02380458451807499\n",
      "epoch: 14 step: 919, loss is 0.007106207311153412\n",
      "epoch: 14 step: 920, loss is 0.0018577054142951965\n",
      "epoch: 14 step: 921, loss is 0.01876620575785637\n",
      "epoch: 14 step: 922, loss is 0.07379443943500519\n",
      "epoch: 14 step: 923, loss is 0.005707098636776209\n",
      "epoch: 14 step: 924, loss is 0.023041412234306335\n",
      "epoch: 14 step: 925, loss is 0.012809092178940773\n",
      "epoch: 14 step: 926, loss is 0.014795788563787937\n",
      "epoch: 14 step: 927, loss is 0.03531733527779579\n",
      "epoch: 14 step: 928, loss is 0.04042857140302658\n",
      "epoch: 14 step: 929, loss is 0.029881145805120468\n",
      "epoch: 14 step: 930, loss is 0.07738786935806274\n",
      "epoch: 14 step: 931, loss is 0.0045801252126693726\n",
      "epoch: 14 step: 932, loss is 0.023070914670825005\n",
      "epoch: 14 step: 933, loss is 0.06793631613254547\n",
      "epoch: 14 step: 934, loss is 0.03847222402691841\n",
      "epoch: 14 step: 935, loss is 0.11788804084062576\n",
      "epoch: 14 step: 936, loss is 0.019675826653838158\n",
      "epoch: 14 step: 937, loss is 0.04208584129810333\n",
      "epoch: 15 step: 1, loss is 0.010071203112602234\n",
      "epoch: 15 step: 2, loss is 0.020543590188026428\n",
      "epoch: 15 step: 3, loss is 0.003998935222625732\n",
      "epoch: 15 step: 4, loss is 0.021097803488373756\n",
      "epoch: 15 step: 5, loss is 0.02129177376627922\n",
      "epoch: 15 step: 6, loss is 0.0431038960814476\n",
      "epoch: 15 step: 7, loss is 0.028264502063393593\n",
      "epoch: 15 step: 8, loss is 0.03402618318796158\n",
      "epoch: 15 step: 9, loss is 0.016754673793911934\n",
      "epoch: 15 step: 10, loss is 0.005969017744064331\n",
      "epoch: 15 step: 11, loss is 0.004933243617415428\n",
      "epoch: 15 step: 12, loss is 0.08446487784385681\n",
      "epoch: 15 step: 13, loss is 0.0017235216218978167\n",
      "epoch: 15 step: 14, loss is 0.0013549476861953735\n",
      "epoch: 15 step: 15, loss is 0.04379761591553688\n",
      "epoch: 15 step: 16, loss is 0.002891633892431855\n",
      "epoch: 15 step: 17, loss is 0.025432515889406204\n",
      "epoch: 15 step: 18, loss is 0.021828068420290947\n",
      "epoch: 15 step: 19, loss is 0.04618147015571594\n",
      "epoch: 15 step: 20, loss is 0.015494107268750668\n",
      "epoch: 15 step: 21, loss is 0.028433142229914665\n",
      "epoch: 15 step: 22, loss is 0.011868834495544434\n",
      "epoch: 15 step: 23, loss is 0.022223321720957756\n",
      "epoch: 15 step: 24, loss is 0.032953616231679916\n",
      "epoch: 15 step: 25, loss is 0.002806080039590597\n",
      "epoch: 15 step: 26, loss is 0.020021948963403702\n",
      "epoch: 15 step: 27, loss is 0.0007958831847645342\n",
      "epoch: 15 step: 28, loss is 0.07421354204416275\n",
      "epoch: 15 step: 29, loss is 0.00933824572712183\n",
      "epoch: 15 step: 30, loss is 0.023916618898510933\n",
      "epoch: 15 step: 31, loss is 0.006538438610732555\n",
      "epoch: 15 step: 32, loss is 0.045165177434682846\n",
      "epoch: 15 step: 33, loss is 0.012581489980220795\n",
      "epoch: 15 step: 34, loss is 0.11672469228506088\n",
      "epoch: 15 step: 35, loss is 0.017232811078429222\n",
      "epoch: 15 step: 36, loss is 0.0003535269061103463\n",
      "epoch: 15 step: 37, loss is 0.012460676021873951\n",
      "epoch: 15 step: 38, loss is 0.0036531430669128895\n",
      "epoch: 15 step: 39, loss is 0.047880373895168304\n",
      "epoch: 15 step: 40, loss is 0.012101298198103905\n",
      "epoch: 15 step: 41, loss is 0.011719543486833572\n",
      "epoch: 15 step: 42, loss is 0.010358184576034546\n",
      "epoch: 15 step: 43, loss is 0.01360113825649023\n",
      "epoch: 15 step: 44, loss is 0.05804026871919632\n",
      "epoch: 15 step: 45, loss is 0.006053284741938114\n",
      "epoch: 15 step: 46, loss is 0.017643768340349197\n",
      "epoch: 15 step: 47, loss is 0.011720453388988972\n",
      "epoch: 15 step: 48, loss is 0.012675915844738483\n",
      "epoch: 15 step: 49, loss is 0.004233207553625107\n",
      "epoch: 15 step: 50, loss is 0.07571928948163986\n",
      "epoch: 15 step: 51, loss is 0.02183268964290619\n",
      "epoch: 15 step: 52, loss is 0.009316422045230865\n",
      "epoch: 15 step: 53, loss is 0.007871636189520359\n",
      "epoch: 15 step: 54, loss is 0.020044714212417603\n",
      "epoch: 15 step: 55, loss is 0.06776101142168045\n",
      "epoch: 15 step: 56, loss is 0.011314751580357552\n",
      "epoch: 15 step: 57, loss is 0.01594158075749874\n",
      "epoch: 15 step: 58, loss is 0.022294024005532265\n",
      "epoch: 15 step: 59, loss is 0.015755580738186836\n",
      "epoch: 15 step: 60, loss is 0.00299527938477695\n",
      "epoch: 15 step: 61, loss is 0.02222881093621254\n",
      "epoch: 15 step: 62, loss is 0.00249820901080966\n",
      "epoch: 15 step: 63, loss is 0.0014512345660477877\n",
      "epoch: 15 step: 64, loss is 0.024601073935627937\n",
      "epoch: 15 step: 65, loss is 0.003291066735982895\n",
      "epoch: 15 step: 66, loss is 0.025422949343919754\n",
      "epoch: 15 step: 67, loss is 0.01803836040198803\n",
      "epoch: 15 step: 68, loss is 0.010537590831518173\n",
      "epoch: 15 step: 69, loss is 0.0008373262244276702\n",
      "epoch: 15 step: 70, loss is 0.001966207753866911\n",
      "epoch: 15 step: 71, loss is 0.0323970690369606\n",
      "epoch: 15 step: 72, loss is 0.01034573558717966\n",
      "epoch: 15 step: 73, loss is 0.06446155905723572\n",
      "epoch: 15 step: 74, loss is 0.017252257093787193\n",
      "epoch: 15 step: 75, loss is 0.055030543357133865\n",
      "epoch: 15 step: 76, loss is 0.07403036206960678\n",
      "epoch: 15 step: 77, loss is 0.010646834969520569\n",
      "epoch: 15 step: 78, loss is 0.08549432456493378\n",
      "epoch: 15 step: 79, loss is 0.0011309172259643674\n",
      "epoch: 15 step: 80, loss is 0.000574447971303016\n",
      "epoch: 15 step: 81, loss is 0.0024292657617479563\n",
      "epoch: 15 step: 82, loss is 0.0018044636817649007\n",
      "epoch: 15 step: 83, loss is 0.057112619280815125\n",
      "epoch: 15 step: 84, loss is 0.011228623799979687\n",
      "epoch: 15 step: 85, loss is 0.027030525729060173\n",
      "epoch: 15 step: 86, loss is 0.028591006994247437\n",
      "epoch: 15 step: 87, loss is 0.007518491707742214\n",
      "epoch: 15 step: 88, loss is 0.023969339206814766\n",
      "epoch: 15 step: 89, loss is 0.006558805238455534\n",
      "epoch: 15 step: 90, loss is 0.02439647726714611\n",
      "epoch: 15 step: 91, loss is 0.007159292232245207\n",
      "epoch: 15 step: 92, loss is 0.014845306985080242\n",
      "epoch: 15 step: 93, loss is 0.014503366313874722\n",
      "epoch: 15 step: 94, loss is 0.04432413727045059\n",
      "epoch: 15 step: 95, loss is 0.018668094649910927\n",
      "epoch: 15 step: 96, loss is 0.0831427350640297\n",
      "epoch: 15 step: 97, loss is 0.006001438945531845\n",
      "epoch: 15 step: 98, loss is 0.050967972725629807\n",
      "epoch: 15 step: 99, loss is 0.04089723154902458\n",
      "epoch: 15 step: 100, loss is 0.007645672187209129\n",
      "epoch: 15 step: 101, loss is 0.06756030768156052\n",
      "epoch: 15 step: 102, loss is 0.0021251090802252293\n",
      "epoch: 15 step: 103, loss is 0.01568073220551014\n",
      "epoch: 15 step: 104, loss is 0.027035262435674667\n",
      "epoch: 15 step: 105, loss is 0.017365913838148117\n",
      "epoch: 15 step: 106, loss is 0.011324887163937092\n",
      "epoch: 15 step: 107, loss is 0.005546762607991695\n",
      "epoch: 15 step: 108, loss is 0.0015334904892370105\n",
      "epoch: 15 step: 109, loss is 0.01923369988799095\n",
      "epoch: 15 step: 110, loss is 0.004500857088714838\n",
      "epoch: 15 step: 111, loss is 0.00443879421800375\n",
      "epoch: 15 step: 112, loss is 0.007157077081501484\n",
      "epoch: 15 step: 113, loss is 0.005539389327168465\n",
      "epoch: 15 step: 114, loss is 0.013914290815591812\n",
      "epoch: 15 step: 115, loss is 0.0025067641399800777\n",
      "epoch: 15 step: 116, loss is 0.0023773526772856712\n",
      "epoch: 15 step: 117, loss is 0.04691363126039505\n",
      "epoch: 15 step: 118, loss is 0.03745480626821518\n",
      "epoch: 15 step: 119, loss is 0.040060196071863174\n",
      "epoch: 15 step: 120, loss is 0.026058701798319817\n",
      "epoch: 15 step: 121, loss is 0.022132979705929756\n",
      "epoch: 15 step: 122, loss is 0.02211742103099823\n",
      "epoch: 15 step: 123, loss is 0.012998363934457302\n",
      "epoch: 15 step: 124, loss is 0.03669840097427368\n",
      "epoch: 15 step: 125, loss is 0.03643457591533661\n",
      "epoch: 15 step: 126, loss is 0.05688737332820892\n",
      "epoch: 15 step: 127, loss is 0.06348863989114761\n",
      "epoch: 15 step: 128, loss is 0.0037556393072009087\n",
      "epoch: 15 step: 129, loss is 0.01422271877527237\n",
      "epoch: 15 step: 130, loss is 0.053821105509996414\n",
      "epoch: 15 step: 131, loss is 0.00328721571713686\n",
      "epoch: 15 step: 132, loss is 0.008108669891953468\n",
      "epoch: 15 step: 133, loss is 0.018982503563165665\n",
      "epoch: 15 step: 134, loss is 0.0005501957493834198\n",
      "epoch: 15 step: 135, loss is 0.02909318171441555\n",
      "epoch: 15 step: 136, loss is 0.03924483433365822\n",
      "epoch: 15 step: 137, loss is 0.011944967322051525\n",
      "epoch: 15 step: 138, loss is 0.0038787717930972576\n",
      "epoch: 15 step: 139, loss is 0.032020203769207\n",
      "epoch: 15 step: 140, loss is 0.0019285480957478285\n",
      "epoch: 15 step: 141, loss is 0.04866692051291466\n",
      "epoch: 15 step: 142, loss is 0.020834052935242653\n",
      "epoch: 15 step: 143, loss is 0.06401890516281128\n",
      "epoch: 15 step: 144, loss is 0.00412784842774272\n",
      "epoch: 15 step: 145, loss is 0.0011697381269186735\n",
      "epoch: 15 step: 146, loss is 0.008039134554564953\n",
      "epoch: 15 step: 147, loss is 0.05176330730319023\n",
      "epoch: 15 step: 148, loss is 0.0028087624814361334\n",
      "epoch: 15 step: 149, loss is 0.019699575379490852\n",
      "epoch: 15 step: 150, loss is 0.0135531947016716\n",
      "epoch: 15 step: 151, loss is 0.036377906799316406\n",
      "epoch: 15 step: 152, loss is 0.008346730843186378\n",
      "epoch: 15 step: 153, loss is 0.007898127660155296\n",
      "epoch: 15 step: 154, loss is 0.03899432718753815\n",
      "epoch: 15 step: 155, loss is 0.003914299886673689\n",
      "epoch: 15 step: 156, loss is 0.057343821972608566\n",
      "epoch: 15 step: 157, loss is 0.011163045652210712\n",
      "epoch: 15 step: 158, loss is 0.011517900973558426\n",
      "epoch: 15 step: 159, loss is 0.013982020318508148\n",
      "epoch: 15 step: 160, loss is 0.0499628446996212\n",
      "epoch: 15 step: 161, loss is 0.009220479056239128\n",
      "epoch: 15 step: 162, loss is 0.0024971473030745983\n",
      "epoch: 15 step: 163, loss is 0.0010464314837008715\n",
      "epoch: 15 step: 164, loss is 0.011998621746897697\n",
      "epoch: 15 step: 165, loss is 0.02307051233947277\n",
      "epoch: 15 step: 166, loss is 0.010565447621047497\n",
      "epoch: 15 step: 167, loss is 0.022940419614315033\n",
      "epoch: 15 step: 168, loss is 0.005681253038346767\n",
      "epoch: 15 step: 169, loss is 0.0016936898464336991\n",
      "epoch: 15 step: 170, loss is 0.08028292655944824\n",
      "epoch: 15 step: 171, loss is 0.011630523018538952\n",
      "epoch: 15 step: 172, loss is 0.0035673913080245256\n",
      "epoch: 15 step: 173, loss is 0.030627954751253128\n",
      "epoch: 15 step: 174, loss is 0.03333108872175217\n",
      "epoch: 15 step: 175, loss is 0.00645817955955863\n",
      "epoch: 15 step: 176, loss is 0.0053784665651619434\n",
      "epoch: 15 step: 177, loss is 0.00309654395096004\n",
      "epoch: 15 step: 178, loss is 0.006761352531611919\n",
      "epoch: 15 step: 179, loss is 0.018740525469183922\n",
      "epoch: 15 step: 180, loss is 0.005155739840120077\n",
      "epoch: 15 step: 181, loss is 0.0725383311510086\n",
      "epoch: 15 step: 182, loss is 0.046094026416540146\n",
      "epoch: 15 step: 183, loss is 0.007589688524603844\n",
      "epoch: 15 step: 184, loss is 0.03328415006399155\n",
      "epoch: 15 step: 185, loss is 0.0025611151941120625\n",
      "epoch: 15 step: 186, loss is 0.06939363479614258\n",
      "epoch: 15 step: 187, loss is 0.003833489026874304\n",
      "epoch: 15 step: 188, loss is 0.00214130780659616\n",
      "epoch: 15 step: 189, loss is 0.005280857440084219\n",
      "epoch: 15 step: 190, loss is 0.03425963968038559\n",
      "epoch: 15 step: 191, loss is 0.003017981071025133\n",
      "epoch: 15 step: 192, loss is 0.009506280533969402\n",
      "epoch: 15 step: 193, loss is 0.0009169059921987355\n",
      "epoch: 15 step: 194, loss is 0.019155459478497505\n",
      "epoch: 15 step: 195, loss is 0.08891402930021286\n",
      "epoch: 15 step: 196, loss is 0.02996785193681717\n",
      "epoch: 15 step: 197, loss is 0.007034007925540209\n",
      "epoch: 15 step: 198, loss is 0.002381837461143732\n",
      "epoch: 15 step: 199, loss is 0.016502702608704567\n",
      "epoch: 15 step: 200, loss is 0.021112661808729172\n",
      "epoch: 15 step: 201, loss is 0.012093392200767994\n",
      "epoch: 15 step: 202, loss is 0.006954004522413015\n",
      "epoch: 15 step: 203, loss is 0.0073829228058457375\n",
      "epoch: 15 step: 204, loss is 0.006859691347926855\n",
      "epoch: 15 step: 205, loss is 0.0018305163830518723\n",
      "epoch: 15 step: 206, loss is 0.005426402669399977\n",
      "epoch: 15 step: 207, loss is 0.0337291918694973\n",
      "epoch: 15 step: 208, loss is 0.009104224853217602\n",
      "epoch: 15 step: 209, loss is 0.1464833915233612\n",
      "epoch: 15 step: 210, loss is 0.0020745836663991213\n",
      "epoch: 15 step: 211, loss is 0.017212355509400368\n",
      "epoch: 15 step: 212, loss is 0.04583452641963959\n",
      "epoch: 15 step: 213, loss is 0.0003995431761723012\n",
      "epoch: 15 step: 214, loss is 0.024840839207172394\n",
      "epoch: 15 step: 215, loss is 0.018471796065568924\n",
      "epoch: 15 step: 216, loss is 0.10927538573741913\n",
      "epoch: 15 step: 217, loss is 0.0059557631611824036\n",
      "epoch: 15 step: 218, loss is 0.0143751734867692\n",
      "epoch: 15 step: 219, loss is 0.021931851282715797\n",
      "epoch: 15 step: 220, loss is 0.02640257216989994\n",
      "epoch: 15 step: 221, loss is 0.021734146401286125\n",
      "epoch: 15 step: 222, loss is 0.03753130882978439\n",
      "epoch: 15 step: 223, loss is 0.030176322907209396\n",
      "epoch: 15 step: 224, loss is 0.034531962126493454\n",
      "epoch: 15 step: 225, loss is 0.004596589133143425\n",
      "epoch: 15 step: 226, loss is 0.012486403807997704\n",
      "epoch: 15 step: 227, loss is 0.0016087887343019247\n",
      "epoch: 15 step: 228, loss is 0.03002506121993065\n",
      "epoch: 15 step: 229, loss is 0.12532712519168854\n",
      "epoch: 15 step: 230, loss is 0.02763230912387371\n",
      "epoch: 15 step: 231, loss is 0.06314670294523239\n",
      "epoch: 15 step: 232, loss is 0.015864282846450806\n",
      "epoch: 15 step: 233, loss is 0.0026679097209125757\n",
      "epoch: 15 step: 234, loss is 0.00914474856108427\n",
      "epoch: 15 step: 235, loss is 0.011395350098609924\n",
      "epoch: 15 step: 236, loss is 0.010542044416069984\n",
      "epoch: 15 step: 237, loss is 0.003356508444994688\n",
      "epoch: 15 step: 238, loss is 0.0021071431692689657\n",
      "epoch: 15 step: 239, loss is 0.04554202780127525\n",
      "epoch: 15 step: 240, loss is 0.04494667053222656\n",
      "epoch: 15 step: 241, loss is 0.0005421463865786791\n",
      "epoch: 15 step: 242, loss is 0.013158881105482578\n",
      "epoch: 15 step: 243, loss is 0.006411734968423843\n",
      "epoch: 15 step: 244, loss is 0.01100179273635149\n",
      "epoch: 15 step: 245, loss is 0.015234658494591713\n",
      "epoch: 15 step: 246, loss is 0.011921560391783714\n",
      "epoch: 15 step: 247, loss is 0.029200848191976547\n",
      "epoch: 15 step: 248, loss is 0.03172815591096878\n",
      "epoch: 15 step: 249, loss is 0.020511070266366005\n",
      "epoch: 15 step: 250, loss is 0.019280781969428062\n",
      "epoch: 15 step: 251, loss is 0.011834423057734966\n",
      "epoch: 15 step: 252, loss is 0.029580214992165565\n",
      "epoch: 15 step: 253, loss is 0.003145714057609439\n",
      "epoch: 15 step: 254, loss is 0.1285911500453949\n",
      "epoch: 15 step: 255, loss is 0.012754647992551327\n",
      "epoch: 15 step: 256, loss is 0.01003306359052658\n",
      "epoch: 15 step: 257, loss is 0.004542906302958727\n",
      "epoch: 15 step: 258, loss is 0.010578214190900326\n",
      "epoch: 15 step: 259, loss is 0.0405372716486454\n",
      "epoch: 15 step: 260, loss is 0.023120667785406113\n",
      "epoch: 15 step: 261, loss is 0.009434016421437263\n",
      "epoch: 15 step: 262, loss is 0.013150952756404877\n",
      "epoch: 15 step: 263, loss is 0.02920941263437271\n",
      "epoch: 15 step: 264, loss is 0.003838628763332963\n",
      "epoch: 15 step: 265, loss is 0.032427605241537094\n",
      "epoch: 15 step: 266, loss is 0.0052899885922670364\n",
      "epoch: 15 step: 267, loss is 0.061379220336675644\n",
      "epoch: 15 step: 268, loss is 0.0055373962968587875\n",
      "epoch: 15 step: 269, loss is 0.011225472204387188\n",
      "epoch: 15 step: 270, loss is 0.028178203850984573\n",
      "epoch: 15 step: 271, loss is 0.049556534737348557\n",
      "epoch: 15 step: 272, loss is 0.01976308971643448\n",
      "epoch: 15 step: 273, loss is 0.033255789428949356\n",
      "epoch: 15 step: 274, loss is 0.02054518088698387\n",
      "epoch: 15 step: 275, loss is 0.0020360592752695084\n",
      "epoch: 15 step: 276, loss is 0.0005513554788194597\n",
      "epoch: 15 step: 277, loss is 0.036139409989118576\n",
      "epoch: 15 step: 278, loss is 0.05102025717496872\n",
      "epoch: 15 step: 279, loss is 0.0014995320234447718\n",
      "epoch: 15 step: 280, loss is 0.01638505794107914\n",
      "epoch: 15 step: 281, loss is 0.0978371798992157\n",
      "epoch: 15 step: 282, loss is 0.01809639111161232\n",
      "epoch: 15 step: 283, loss is 0.006934169679880142\n",
      "epoch: 15 step: 284, loss is 0.0005812722956761718\n",
      "epoch: 15 step: 285, loss is 0.05523844063282013\n",
      "epoch: 15 step: 286, loss is 0.0015870045172050595\n",
      "epoch: 15 step: 287, loss is 0.004035566002130508\n",
      "epoch: 15 step: 288, loss is 0.04528319835662842\n",
      "epoch: 15 step: 289, loss is 0.03004583902657032\n",
      "epoch: 15 step: 290, loss is 0.00026260275626555085\n",
      "epoch: 15 step: 291, loss is 0.05700640007853508\n",
      "epoch: 15 step: 292, loss is 0.005367349833250046\n",
      "epoch: 15 step: 293, loss is 0.003007203107699752\n",
      "epoch: 15 step: 294, loss is 0.07684554904699326\n",
      "epoch: 15 step: 295, loss is 0.07207131385803223\n",
      "epoch: 15 step: 296, loss is 0.006706793326884508\n",
      "epoch: 15 step: 297, loss is 0.051205385476350784\n",
      "epoch: 15 step: 298, loss is 0.036561720073223114\n",
      "epoch: 15 step: 299, loss is 0.0046327561140060425\n",
      "epoch: 15 step: 300, loss is 0.049786150455474854\n",
      "epoch: 15 step: 301, loss is 0.001810383633710444\n",
      "epoch: 15 step: 302, loss is 0.020412981510162354\n",
      "epoch: 15 step: 303, loss is 0.009756268933415413\n",
      "epoch: 15 step: 304, loss is 0.05628800019621849\n",
      "epoch: 15 step: 305, loss is 0.0007467935211025178\n",
      "epoch: 15 step: 306, loss is 0.010047988034784794\n",
      "epoch: 15 step: 307, loss is 0.006837489083409309\n",
      "epoch: 15 step: 308, loss is 0.015169759280979633\n",
      "epoch: 15 step: 309, loss is 0.01467057317495346\n",
      "epoch: 15 step: 310, loss is 0.025079838931560516\n",
      "epoch: 15 step: 311, loss is 0.03392130509018898\n",
      "epoch: 15 step: 312, loss is 0.0317087359726429\n",
      "epoch: 15 step: 313, loss is 0.03170580789446831\n",
      "epoch: 15 step: 314, loss is 0.0072227343916893005\n",
      "epoch: 15 step: 315, loss is 0.02588796243071556\n",
      "epoch: 15 step: 316, loss is 0.007426696829497814\n",
      "epoch: 15 step: 317, loss is 0.0370938777923584\n",
      "epoch: 15 step: 318, loss is 0.003991033416241407\n",
      "epoch: 15 step: 319, loss is 0.0026841817889362574\n",
      "epoch: 15 step: 320, loss is 0.03107181377708912\n",
      "epoch: 15 step: 321, loss is 0.025416841730475426\n",
      "epoch: 15 step: 322, loss is 0.1273212730884552\n",
      "epoch: 15 step: 323, loss is 0.04073258116841316\n",
      "epoch: 15 step: 324, loss is 0.06890168786048889\n",
      "epoch: 15 step: 325, loss is 0.012180184945464134\n",
      "epoch: 15 step: 326, loss is 0.012749915942549706\n",
      "epoch: 15 step: 327, loss is 0.004108542576432228\n",
      "epoch: 15 step: 328, loss is 0.002793725347146392\n",
      "epoch: 15 step: 329, loss is 0.03520377725362778\n",
      "epoch: 15 step: 330, loss is 0.02329433709383011\n",
      "epoch: 15 step: 331, loss is 0.008550849743187428\n",
      "epoch: 15 step: 332, loss is 0.033372409641742706\n",
      "epoch: 15 step: 333, loss is 0.015367169864475727\n",
      "epoch: 15 step: 334, loss is 0.019937627017498016\n",
      "epoch: 15 step: 335, loss is 0.07398197054862976\n",
      "epoch: 15 step: 336, loss is 0.015146108344197273\n",
      "epoch: 15 step: 337, loss is 0.03148118034005165\n",
      "epoch: 15 step: 338, loss is 0.02608499303460121\n",
      "epoch: 15 step: 339, loss is 0.007827084511518478\n",
      "epoch: 15 step: 340, loss is 0.028298277407884598\n",
      "epoch: 15 step: 341, loss is 0.004350657109171152\n",
      "epoch: 15 step: 342, loss is 0.007129927631467581\n",
      "epoch: 15 step: 343, loss is 0.022607285529375076\n",
      "epoch: 15 step: 344, loss is 0.019130919128656387\n",
      "epoch: 15 step: 345, loss is 0.016782546415925026\n",
      "epoch: 15 step: 346, loss is 0.008401764556765556\n",
      "epoch: 15 step: 347, loss is 0.02202967181801796\n",
      "epoch: 15 step: 348, loss is 0.017475541681051254\n",
      "epoch: 15 step: 349, loss is 0.014857426285743713\n",
      "epoch: 15 step: 350, loss is 0.08463935554027557\n",
      "epoch: 15 step: 351, loss is 0.021827930584549904\n",
      "epoch: 15 step: 352, loss is 0.07052818685770035\n",
      "epoch: 15 step: 353, loss is 0.011361148208379745\n",
      "epoch: 15 step: 354, loss is 0.05267737805843353\n",
      "epoch: 15 step: 355, loss is 0.006426818203181028\n",
      "epoch: 15 step: 356, loss is 0.019105853512883186\n",
      "epoch: 15 step: 357, loss is 0.01532576885074377\n",
      "epoch: 15 step: 358, loss is 0.01263133529573679\n",
      "epoch: 15 step: 359, loss is 0.024356285110116005\n",
      "epoch: 15 step: 360, loss is 0.004217131994664669\n",
      "epoch: 15 step: 361, loss is 0.003623886499553919\n",
      "epoch: 15 step: 362, loss is 0.00625865813344717\n",
      "epoch: 15 step: 363, loss is 0.0019291365751996636\n",
      "epoch: 15 step: 364, loss is 0.00814317911863327\n",
      "epoch: 15 step: 365, loss is 0.01623101346194744\n",
      "epoch: 15 step: 366, loss is 0.0207882858812809\n",
      "epoch: 15 step: 367, loss is 0.0396064929664135\n",
      "epoch: 15 step: 368, loss is 0.0017569016199558973\n",
      "epoch: 15 step: 369, loss is 0.06548503786325455\n",
      "epoch: 15 step: 370, loss is 0.08237366378307343\n",
      "epoch: 15 step: 371, loss is 0.021084727719426155\n",
      "epoch: 15 step: 372, loss is 0.017777998000383377\n",
      "epoch: 15 step: 373, loss is 0.04995214566588402\n",
      "epoch: 15 step: 374, loss is 0.0006075141718611121\n",
      "epoch: 15 step: 375, loss is 0.0245391633361578\n",
      "epoch: 15 step: 376, loss is 0.004497857298702002\n",
      "epoch: 15 step: 377, loss is 0.012015772983431816\n",
      "epoch: 15 step: 378, loss is 0.0026171356439590454\n",
      "epoch: 15 step: 379, loss is 0.011658990755677223\n",
      "epoch: 15 step: 380, loss is 0.02162724733352661\n",
      "epoch: 15 step: 381, loss is 0.028786150738596916\n",
      "epoch: 15 step: 382, loss is 0.02067401632666588\n",
      "epoch: 15 step: 383, loss is 0.004598493687808514\n",
      "epoch: 15 step: 384, loss is 0.02317199483513832\n",
      "epoch: 15 step: 385, loss is 0.0083168288692832\n",
      "epoch: 15 step: 386, loss is 0.012009178288280964\n",
      "epoch: 15 step: 387, loss is 0.10819974541664124\n",
      "epoch: 15 step: 388, loss is 0.001461193198338151\n",
      "epoch: 15 step: 389, loss is 0.0008518950780853629\n",
      "epoch: 15 step: 390, loss is 0.016215218231081963\n",
      "epoch: 15 step: 391, loss is 0.009069927036762238\n",
      "epoch: 15 step: 392, loss is 0.0166489128023386\n",
      "epoch: 15 step: 393, loss is 0.07349558174610138\n",
      "epoch: 15 step: 394, loss is 0.019562438130378723\n",
      "epoch: 15 step: 395, loss is 0.16778185963630676\n",
      "epoch: 15 step: 396, loss is 0.03773411363363266\n",
      "epoch: 15 step: 397, loss is 0.017652496695518494\n",
      "epoch: 15 step: 398, loss is 0.10046537965536118\n",
      "epoch: 15 step: 399, loss is 0.0015841827262192965\n",
      "epoch: 15 step: 400, loss is 0.004433869384229183\n",
      "epoch: 15 step: 401, loss is 0.0011315880110487342\n",
      "epoch: 15 step: 402, loss is 0.0037079958710819483\n",
      "epoch: 15 step: 403, loss is 0.0019100654171779752\n",
      "epoch: 15 step: 404, loss is 0.03956359997391701\n",
      "epoch: 15 step: 405, loss is 0.02804597280919552\n",
      "epoch: 15 step: 406, loss is 0.023124411702156067\n",
      "epoch: 15 step: 407, loss is 0.035821981728076935\n",
      "epoch: 15 step: 408, loss is 0.009973986074328423\n",
      "epoch: 15 step: 409, loss is 0.05198446288704872\n",
      "epoch: 15 step: 410, loss is 0.0009034923277795315\n",
      "epoch: 15 step: 411, loss is 0.04660113528370857\n",
      "epoch: 15 step: 412, loss is 0.0026061763055622578\n",
      "epoch: 15 step: 413, loss is 0.004341461695730686\n",
      "epoch: 15 step: 414, loss is 0.035344187170267105\n",
      "epoch: 15 step: 415, loss is 0.007886762730777264\n",
      "epoch: 15 step: 416, loss is 0.04999914392828941\n",
      "epoch: 15 step: 417, loss is 0.022144898772239685\n",
      "epoch: 15 step: 418, loss is 0.0095926932990551\n",
      "epoch: 15 step: 419, loss is 0.038486283272504807\n",
      "epoch: 15 step: 420, loss is 0.06222844123840332\n",
      "epoch: 15 step: 421, loss is 0.04559563100337982\n",
      "epoch: 15 step: 422, loss is 0.019225675612688065\n",
      "epoch: 15 step: 423, loss is 0.019803602248430252\n",
      "epoch: 15 step: 424, loss is 0.06358099728822708\n",
      "epoch: 15 step: 425, loss is 0.024019993841648102\n",
      "epoch: 15 step: 426, loss is 0.0033703534863889217\n",
      "epoch: 15 step: 427, loss is 0.04869089648127556\n",
      "epoch: 15 step: 428, loss is 0.013517839834094048\n",
      "epoch: 15 step: 429, loss is 0.0023195799440145493\n",
      "epoch: 15 step: 430, loss is 0.02957681193947792\n",
      "epoch: 15 step: 431, loss is 0.023005878552794456\n",
      "epoch: 15 step: 432, loss is 0.022921795025467873\n",
      "epoch: 15 step: 433, loss is 0.005105295684188604\n",
      "epoch: 15 step: 434, loss is 0.03955959901213646\n",
      "epoch: 15 step: 435, loss is 0.014261085540056229\n",
      "epoch: 15 step: 436, loss is 0.054503947496414185\n",
      "epoch: 15 step: 437, loss is 0.05996035784482956\n",
      "epoch: 15 step: 438, loss is 0.022874005138874054\n",
      "epoch: 15 step: 439, loss is 0.00840074010193348\n",
      "epoch: 15 step: 440, loss is 0.017041051760315895\n",
      "epoch: 15 step: 441, loss is 0.002281068591400981\n",
      "epoch: 15 step: 442, loss is 0.06499551236629486\n",
      "epoch: 15 step: 443, loss is 0.07384654134511948\n",
      "epoch: 15 step: 444, loss is 0.01355447806417942\n",
      "epoch: 15 step: 445, loss is 0.011531344614923\n",
      "epoch: 15 step: 446, loss is 0.0061461240984499454\n",
      "epoch: 15 step: 447, loss is 0.008241867646574974\n",
      "epoch: 15 step: 448, loss is 0.023958031088113785\n",
      "epoch: 15 step: 449, loss is 0.001204150845296681\n",
      "epoch: 15 step: 450, loss is 0.01147290039807558\n",
      "epoch: 15 step: 451, loss is 0.04730796813964844\n",
      "epoch: 15 step: 452, loss is 0.010661846026778221\n",
      "epoch: 15 step: 453, loss is 0.0964738056063652\n",
      "epoch: 15 step: 454, loss is 0.02317718230187893\n",
      "epoch: 15 step: 455, loss is 0.01237879041582346\n",
      "epoch: 15 step: 456, loss is 0.01611207239329815\n",
      "epoch: 15 step: 457, loss is 0.0016155866906046867\n",
      "epoch: 15 step: 458, loss is 0.022356148809194565\n",
      "epoch: 15 step: 459, loss is 0.019627058878540993\n",
      "epoch: 15 step: 460, loss is 0.04311665892601013\n",
      "epoch: 15 step: 461, loss is 0.007280402351170778\n",
      "epoch: 15 step: 462, loss is 0.025060607120394707\n",
      "epoch: 15 step: 463, loss is 0.004197947680950165\n",
      "epoch: 15 step: 464, loss is 0.0294545479118824\n",
      "epoch: 15 step: 465, loss is 0.0040665119886398315\n",
      "epoch: 15 step: 466, loss is 0.01225768867880106\n",
      "epoch: 15 step: 467, loss is 0.04511931538581848\n",
      "epoch: 15 step: 468, loss is 0.05638829246163368\n",
      "epoch: 15 step: 469, loss is 0.0005293835420161486\n",
      "epoch: 15 step: 470, loss is 0.029474308714270592\n",
      "epoch: 15 step: 471, loss is 0.03984418138861656\n",
      "epoch: 15 step: 472, loss is 0.07555388659238815\n",
      "epoch: 15 step: 473, loss is 0.0020182952284812927\n",
      "epoch: 15 step: 474, loss is 0.021997792646288872\n",
      "epoch: 15 step: 475, loss is 0.009036882780492306\n",
      "epoch: 15 step: 476, loss is 0.007078642025589943\n",
      "epoch: 15 step: 477, loss is 0.03148775175213814\n",
      "epoch: 15 step: 478, loss is 0.030948705971240997\n",
      "epoch: 15 step: 479, loss is 0.03238031640648842\n",
      "epoch: 15 step: 480, loss is 0.0557529479265213\n",
      "epoch: 15 step: 481, loss is 0.012009814381599426\n",
      "epoch: 15 step: 482, loss is 0.04205256327986717\n",
      "epoch: 15 step: 483, loss is 0.1130765751004219\n",
      "epoch: 15 step: 484, loss is 0.025893142446875572\n",
      "epoch: 15 step: 485, loss is 0.04016801342368126\n",
      "epoch: 15 step: 486, loss is 0.04119028523564339\n",
      "epoch: 15 step: 487, loss is 0.06129295378923416\n",
      "epoch: 15 step: 488, loss is 0.01112661324441433\n",
      "epoch: 15 step: 489, loss is 0.016781674697995186\n",
      "epoch: 15 step: 490, loss is 0.0054708486422896385\n",
      "epoch: 15 step: 491, loss is 0.02151651121675968\n",
      "epoch: 15 step: 492, loss is 0.046535443514585495\n",
      "epoch: 15 step: 493, loss is 0.007286901120096445\n",
      "epoch: 15 step: 494, loss is 0.02142680622637272\n",
      "epoch: 15 step: 495, loss is 0.0050293575040996075\n",
      "epoch: 15 step: 496, loss is 0.002406658371910453\n",
      "epoch: 15 step: 497, loss is 0.03736695647239685\n",
      "epoch: 15 step: 498, loss is 0.007925756275653839\n",
      "epoch: 15 step: 499, loss is 0.010340576991438866\n",
      "epoch: 15 step: 500, loss is 0.003115267027169466\n",
      "epoch: 15 step: 501, loss is 0.09792377054691315\n",
      "epoch: 15 step: 502, loss is 0.013836859725415707\n",
      "epoch: 15 step: 503, loss is 0.24312818050384521\n",
      "epoch: 15 step: 504, loss is 0.02438855543732643\n",
      "epoch: 15 step: 505, loss is 0.04324674606323242\n",
      "epoch: 15 step: 506, loss is 0.0350620374083519\n",
      "epoch: 15 step: 507, loss is 0.09263426065444946\n",
      "epoch: 15 step: 508, loss is 0.03764170780777931\n",
      "epoch: 15 step: 509, loss is 0.0030686724931001663\n",
      "epoch: 15 step: 510, loss is 0.0008994027157314122\n",
      "epoch: 15 step: 511, loss is 0.0020109829492866993\n",
      "epoch: 15 step: 512, loss is 0.0014790378045290709\n",
      "epoch: 15 step: 513, loss is 0.07812966406345367\n",
      "epoch: 15 step: 514, loss is 0.025928955525159836\n",
      "epoch: 15 step: 515, loss is 0.014977807179093361\n",
      "epoch: 15 step: 516, loss is 0.026648538187146187\n",
      "epoch: 15 step: 517, loss is 0.037407346069812775\n",
      "epoch: 15 step: 518, loss is 0.0634114146232605\n",
      "epoch: 15 step: 519, loss is 0.06958646327257156\n",
      "epoch: 15 step: 520, loss is 0.008604236878454685\n",
      "epoch: 15 step: 521, loss is 0.03500544652342796\n",
      "epoch: 15 step: 522, loss is 0.014245802536606789\n",
      "epoch: 15 step: 523, loss is 0.014326078817248344\n",
      "epoch: 15 step: 524, loss is 0.021238092333078384\n",
      "epoch: 15 step: 525, loss is 0.02702045813202858\n",
      "epoch: 15 step: 526, loss is 0.2937254309654236\n",
      "epoch: 15 step: 527, loss is 0.003989015240222216\n",
      "epoch: 15 step: 528, loss is 0.008406040258705616\n",
      "epoch: 15 step: 529, loss is 0.07350494712591171\n",
      "epoch: 15 step: 530, loss is 0.027710407972335815\n",
      "epoch: 15 step: 531, loss is 0.06558717787265778\n",
      "epoch: 15 step: 532, loss is 0.02573593147099018\n",
      "epoch: 15 step: 533, loss is 0.040447548031806946\n",
      "epoch: 15 step: 534, loss is 0.007625820580869913\n",
      "epoch: 15 step: 535, loss is 0.02880464307963848\n",
      "epoch: 15 step: 536, loss is 0.015764322131872177\n",
      "epoch: 15 step: 537, loss is 0.012019084766507149\n",
      "epoch: 15 step: 538, loss is 0.010272719897329807\n",
      "epoch: 15 step: 539, loss is 0.0037272018380463123\n",
      "epoch: 15 step: 540, loss is 0.023876138031482697\n",
      "epoch: 15 step: 541, loss is 0.00584994675591588\n",
      "epoch: 15 step: 542, loss is 0.008048385381698608\n",
      "epoch: 15 step: 543, loss is 0.01697647199034691\n",
      "epoch: 15 step: 544, loss is 0.01701357401907444\n",
      "epoch: 15 step: 545, loss is 0.0255756676197052\n",
      "epoch: 15 step: 546, loss is 0.0051902709528803825\n",
      "epoch: 15 step: 547, loss is 0.007647543679922819\n",
      "epoch: 15 step: 548, loss is 0.02186521515250206\n",
      "epoch: 15 step: 549, loss is 0.0061425878666341305\n",
      "epoch: 15 step: 550, loss is 0.04651206359267235\n",
      "epoch: 15 step: 551, loss is 0.006864665541797876\n",
      "epoch: 15 step: 552, loss is 0.04399292171001434\n",
      "epoch: 15 step: 553, loss is 0.002085166983306408\n",
      "epoch: 15 step: 554, loss is 0.08651259541511536\n",
      "epoch: 15 step: 555, loss is 0.04830092936754227\n",
      "epoch: 15 step: 556, loss is 0.0466219037771225\n",
      "epoch: 15 step: 557, loss is 0.0037835785187780857\n",
      "epoch: 15 step: 558, loss is 0.006989148911088705\n",
      "epoch: 15 step: 559, loss is 0.03605515882372856\n",
      "epoch: 15 step: 560, loss is 0.03653388470411301\n",
      "epoch: 15 step: 561, loss is 0.018267987295985222\n",
      "epoch: 15 step: 562, loss is 0.009592818096280098\n",
      "epoch: 15 step: 563, loss is 0.0363895408809185\n",
      "epoch: 15 step: 564, loss is 0.004390346817672253\n",
      "epoch: 15 step: 565, loss is 0.0011913259513676167\n",
      "epoch: 15 step: 566, loss is 0.03992629423737526\n",
      "epoch: 15 step: 567, loss is 0.005564744118601084\n",
      "epoch: 15 step: 568, loss is 0.03871111199259758\n",
      "epoch: 15 step: 569, loss is 0.07173822075128555\n",
      "epoch: 15 step: 570, loss is 0.009499918669462204\n",
      "epoch: 15 step: 571, loss is 0.012193923816084862\n",
      "epoch: 15 step: 572, loss is 0.012638142332434654\n",
      "epoch: 15 step: 573, loss is 0.006406229455024004\n",
      "epoch: 15 step: 574, loss is 0.01552450843155384\n",
      "epoch: 15 step: 575, loss is 0.005172687117010355\n",
      "epoch: 15 step: 576, loss is 0.0034821857698261738\n",
      "epoch: 15 step: 577, loss is 0.018162453547120094\n",
      "epoch: 15 step: 578, loss is 0.010862097144126892\n",
      "epoch: 15 step: 579, loss is 0.009278110228478909\n",
      "epoch: 15 step: 580, loss is 0.029598476365208626\n",
      "epoch: 15 step: 581, loss is 0.00881118606775999\n",
      "epoch: 15 step: 582, loss is 0.006852606311440468\n",
      "epoch: 15 step: 583, loss is 0.003545474959537387\n",
      "epoch: 15 step: 584, loss is 0.04032690450549126\n",
      "epoch: 15 step: 585, loss is 0.016357015818357468\n",
      "epoch: 15 step: 586, loss is 0.0034069556277245283\n",
      "epoch: 15 step: 587, loss is 0.011911987327039242\n",
      "epoch: 15 step: 588, loss is 0.04696429893374443\n",
      "epoch: 15 step: 589, loss is 0.00046985779772512615\n",
      "epoch: 15 step: 590, loss is 0.06855568289756775\n",
      "epoch: 15 step: 591, loss is 0.053260382264852524\n",
      "epoch: 15 step: 592, loss is 0.01764618046581745\n",
      "epoch: 15 step: 593, loss is 0.01565612107515335\n",
      "epoch: 15 step: 594, loss is 0.0009128388483077288\n",
      "epoch: 15 step: 595, loss is 0.041027653962373734\n",
      "epoch: 15 step: 596, loss is 0.08703780919313431\n",
      "epoch: 15 step: 597, loss is 0.008704133331775665\n",
      "epoch: 15 step: 598, loss is 0.1132100448012352\n",
      "epoch: 15 step: 599, loss is 0.06450387835502625\n",
      "epoch: 15 step: 600, loss is 0.06414378434419632\n",
      "epoch: 15 step: 601, loss is 0.011593492701649666\n",
      "epoch: 15 step: 602, loss is 0.11844533681869507\n",
      "epoch: 15 step: 603, loss is 0.0148314218968153\n",
      "epoch: 15 step: 604, loss is 0.06346618384122849\n",
      "epoch: 15 step: 605, loss is 0.03596651554107666\n",
      "epoch: 15 step: 606, loss is 0.03915752097964287\n",
      "epoch: 15 step: 607, loss is 0.007674955762922764\n",
      "epoch: 15 step: 608, loss is 0.0035397729370743036\n",
      "epoch: 15 step: 609, loss is 0.0020051291212439537\n",
      "epoch: 15 step: 610, loss is 0.0069387080147862434\n",
      "epoch: 15 step: 611, loss is 0.034319017082452774\n",
      "epoch: 15 step: 612, loss is 0.016634929925203323\n",
      "epoch: 15 step: 613, loss is 0.00234536686912179\n",
      "epoch: 15 step: 614, loss is 0.002877764170989394\n",
      "epoch: 15 step: 615, loss is 0.08098968863487244\n",
      "epoch: 15 step: 616, loss is 0.01901109516620636\n",
      "epoch: 15 step: 617, loss is 0.012419491074979305\n",
      "epoch: 15 step: 618, loss is 0.029527252539992332\n",
      "epoch: 15 step: 619, loss is 0.011767742224037647\n",
      "epoch: 15 step: 620, loss is 0.05011828616261482\n",
      "epoch: 15 step: 621, loss is 0.1054440587759018\n",
      "epoch: 15 step: 622, loss is 0.013524492271244526\n",
      "epoch: 15 step: 623, loss is 0.004651553463190794\n",
      "epoch: 15 step: 624, loss is 0.04086713492870331\n",
      "epoch: 15 step: 625, loss is 0.0026033399626612663\n",
      "epoch: 15 step: 626, loss is 0.011824321933090687\n",
      "epoch: 15 step: 627, loss is 0.0055721658281981945\n",
      "epoch: 15 step: 628, loss is 0.007038451265543699\n",
      "epoch: 15 step: 629, loss is 0.06990007311105728\n",
      "epoch: 15 step: 630, loss is 0.05811123549938202\n",
      "epoch: 15 step: 631, loss is 0.05502976104617119\n",
      "epoch: 15 step: 632, loss is 0.05840815603733063\n",
      "epoch: 15 step: 633, loss is 0.012460689060389996\n",
      "epoch: 15 step: 634, loss is 0.0016115569742396474\n",
      "epoch: 15 step: 635, loss is 0.06429412215948105\n",
      "epoch: 15 step: 636, loss is 0.06796423345804214\n",
      "epoch: 15 step: 637, loss is 0.005182687193155289\n",
      "epoch: 15 step: 638, loss is 0.03979223594069481\n",
      "epoch: 15 step: 639, loss is 0.01900257170200348\n",
      "epoch: 15 step: 640, loss is 0.028589528053998947\n",
      "epoch: 15 step: 641, loss is 0.039053548127412796\n",
      "epoch: 15 step: 642, loss is 0.004948015324771404\n",
      "epoch: 15 step: 643, loss is 0.003803843632340431\n",
      "epoch: 15 step: 644, loss is 0.23139773309230804\n",
      "epoch: 15 step: 645, loss is 0.05729437991976738\n",
      "epoch: 15 step: 646, loss is 0.03409036248922348\n",
      "epoch: 15 step: 647, loss is 0.03549782186746597\n",
      "epoch: 15 step: 648, loss is 0.02467922866344452\n",
      "epoch: 15 step: 649, loss is 0.005852176807820797\n",
      "epoch: 15 step: 650, loss is 0.06710129976272583\n",
      "epoch: 15 step: 651, loss is 0.17449691891670227\n",
      "epoch: 15 step: 652, loss is 0.016762051731348038\n",
      "epoch: 15 step: 653, loss is 0.08732590824365616\n",
      "epoch: 15 step: 654, loss is 0.019793150946497917\n",
      "epoch: 15 step: 655, loss is 0.007061956450343132\n",
      "epoch: 15 step: 656, loss is 0.01464819721877575\n",
      "epoch: 15 step: 657, loss is 0.06567230075597763\n",
      "epoch: 15 step: 658, loss is 0.016436951234936714\n",
      "epoch: 15 step: 659, loss is 0.061186160892248154\n",
      "epoch: 15 step: 660, loss is 0.056543298065662384\n",
      "epoch: 15 step: 661, loss is 0.004698149859905243\n",
      "epoch: 15 step: 662, loss is 0.12388142943382263\n",
      "epoch: 15 step: 663, loss is 0.014918960630893707\n",
      "epoch: 15 step: 664, loss is 0.016692988574504852\n",
      "epoch: 15 step: 665, loss is 0.053483378142118454\n",
      "epoch: 15 step: 666, loss is 0.023510653525590897\n",
      "epoch: 15 step: 667, loss is 0.0067345695570111275\n",
      "epoch: 15 step: 668, loss is 0.15702176094055176\n",
      "epoch: 15 step: 669, loss is 0.01572311669588089\n",
      "epoch: 15 step: 670, loss is 0.010705131106078625\n",
      "epoch: 15 step: 671, loss is 0.07809732854366302\n",
      "epoch: 15 step: 672, loss is 0.022788044065237045\n",
      "epoch: 15 step: 673, loss is 0.062417589128017426\n",
      "epoch: 15 step: 674, loss is 0.004793851636350155\n",
      "epoch: 15 step: 675, loss is 0.060653891414403915\n",
      "epoch: 15 step: 676, loss is 0.012645130045711994\n",
      "epoch: 15 step: 677, loss is 0.005589829292148352\n",
      "epoch: 15 step: 678, loss is 0.029056265950202942\n",
      "epoch: 15 step: 679, loss is 0.004070938564836979\n",
      "epoch: 15 step: 680, loss is 0.006566041149199009\n",
      "epoch: 15 step: 681, loss is 0.06506894528865814\n",
      "epoch: 15 step: 682, loss is 0.06627509742975235\n",
      "epoch: 15 step: 683, loss is 0.011243089102208614\n",
      "epoch: 15 step: 684, loss is 0.013089155778288841\n",
      "epoch: 15 step: 685, loss is 0.050994645804166794\n",
      "epoch: 15 step: 686, loss is 0.01233728788793087\n",
      "epoch: 15 step: 687, loss is 0.06041112542152405\n",
      "epoch: 15 step: 688, loss is 0.006904341280460358\n",
      "epoch: 15 step: 689, loss is 0.031516291201114655\n",
      "epoch: 15 step: 690, loss is 0.023163577541708946\n",
      "epoch: 15 step: 691, loss is 0.028591034933924675\n",
      "epoch: 15 step: 692, loss is 0.001875955844298005\n",
      "epoch: 15 step: 693, loss is 0.011564363725483418\n",
      "epoch: 15 step: 694, loss is 0.0233484897762537\n",
      "epoch: 15 step: 695, loss is 0.016814591363072395\n",
      "epoch: 15 step: 696, loss is 0.022982127964496613\n",
      "epoch: 15 step: 697, loss is 0.02326006069779396\n",
      "epoch: 15 step: 698, loss is 0.010852588340640068\n",
      "epoch: 15 step: 699, loss is 0.0014089175965636969\n",
      "epoch: 15 step: 700, loss is 0.002070061396807432\n",
      "epoch: 15 step: 701, loss is 0.08625664561986923\n",
      "epoch: 15 step: 702, loss is 0.015544267371296883\n",
      "epoch: 15 step: 703, loss is 0.13925664126873016\n",
      "epoch: 15 step: 704, loss is 0.0037977281026542187\n",
      "epoch: 15 step: 705, loss is 0.0013985632685944438\n",
      "epoch: 15 step: 706, loss is 0.0034340384881943464\n",
      "epoch: 15 step: 707, loss is 0.01021814625710249\n",
      "epoch: 15 step: 708, loss is 0.026032108813524246\n",
      "epoch: 15 step: 709, loss is 0.012324911542236805\n",
      "epoch: 15 step: 710, loss is 0.012870214879512787\n",
      "epoch: 15 step: 711, loss is 0.03724187612533569\n",
      "epoch: 15 step: 712, loss is 0.009126456454396248\n",
      "epoch: 15 step: 713, loss is 0.0018883396405726671\n",
      "epoch: 15 step: 714, loss is 0.02055184915661812\n",
      "epoch: 15 step: 715, loss is 0.012002445757389069\n",
      "epoch: 15 step: 716, loss is 0.006681383121758699\n",
      "epoch: 15 step: 717, loss is 0.014503322541713715\n",
      "epoch: 15 step: 718, loss is 0.03487402945756912\n",
      "epoch: 15 step: 719, loss is 0.039229508489370346\n",
      "epoch: 15 step: 720, loss is 0.004871434532105923\n",
      "epoch: 15 step: 721, loss is 0.06313708424568176\n",
      "epoch: 15 step: 722, loss is 0.0015783872222527862\n",
      "epoch: 15 step: 723, loss is 0.04508811980485916\n",
      "epoch: 15 step: 724, loss is 0.0026623739395290613\n",
      "epoch: 15 step: 725, loss is 0.0006967600784264505\n",
      "epoch: 15 step: 726, loss is 0.03289351612329483\n",
      "epoch: 15 step: 727, loss is 0.054428186267614365\n",
      "epoch: 15 step: 728, loss is 0.09810908138751984\n",
      "epoch: 15 step: 729, loss is 0.030252519994974136\n",
      "epoch: 15 step: 730, loss is 0.015168189071118832\n",
      "epoch: 15 step: 731, loss is 0.005182378925383091\n",
      "epoch: 15 step: 732, loss is 0.00531056709587574\n",
      "epoch: 15 step: 733, loss is 0.011281692422926426\n",
      "epoch: 15 step: 734, loss is 0.003912584390491247\n",
      "epoch: 15 step: 735, loss is 0.017992107197642326\n",
      "epoch: 15 step: 736, loss is 0.004119966644793749\n",
      "epoch: 15 step: 737, loss is 0.08012081682682037\n",
      "epoch: 15 step: 738, loss is 0.019456686452031136\n",
      "epoch: 15 step: 739, loss is 0.04730497673153877\n",
      "epoch: 15 step: 740, loss is 0.008805780671536922\n",
      "epoch: 15 step: 741, loss is 0.007285941857844591\n",
      "epoch: 15 step: 742, loss is 0.00036963634192943573\n",
      "epoch: 15 step: 743, loss is 0.003294309601187706\n",
      "epoch: 15 step: 744, loss is 0.0017705955542623997\n",
      "epoch: 15 step: 745, loss is 0.0025225167628377676\n",
      "epoch: 15 step: 746, loss is 0.003227503504604101\n",
      "epoch: 15 step: 747, loss is 0.011383588425815105\n",
      "epoch: 15 step: 748, loss is 0.053820300847291946\n",
      "epoch: 15 step: 749, loss is 0.004741866607218981\n",
      "epoch: 15 step: 750, loss is 0.002505368087440729\n",
      "epoch: 15 step: 751, loss is 0.008140254765748978\n",
      "epoch: 15 step: 752, loss is 0.02218841202557087\n",
      "epoch: 15 step: 753, loss is 0.009964654222130775\n",
      "epoch: 15 step: 754, loss is 0.015379691496491432\n",
      "epoch: 15 step: 755, loss is 0.04503697529435158\n",
      "epoch: 15 step: 756, loss is 0.08293034136295319\n",
      "epoch: 15 step: 757, loss is 0.016049379482865334\n",
      "epoch: 15 step: 758, loss is 0.005293181166052818\n",
      "epoch: 15 step: 759, loss is 0.052715983241796494\n",
      "epoch: 15 step: 760, loss is 0.0008385299588553607\n",
      "epoch: 15 step: 761, loss is 0.01769614964723587\n",
      "epoch: 15 step: 762, loss is 0.015496453270316124\n",
      "epoch: 15 step: 763, loss is 0.007118091452866793\n",
      "epoch: 15 step: 764, loss is 0.03787543624639511\n",
      "epoch: 15 step: 765, loss is 0.004256472457200289\n",
      "epoch: 15 step: 766, loss is 0.060909006744623184\n",
      "epoch: 15 step: 767, loss is 0.007958552800118923\n",
      "epoch: 15 step: 768, loss is 0.037092797458171844\n",
      "epoch: 15 step: 769, loss is 0.004450359381735325\n",
      "epoch: 15 step: 770, loss is 0.030824434012174606\n",
      "epoch: 15 step: 771, loss is 0.005238199606537819\n",
      "epoch: 15 step: 772, loss is 0.05764065682888031\n",
      "epoch: 15 step: 773, loss is 0.007025853265076876\n",
      "epoch: 15 step: 774, loss is 0.04597202315926552\n",
      "epoch: 15 step: 775, loss is 0.06302377581596375\n",
      "epoch: 15 step: 776, loss is 0.0017260693712159991\n",
      "epoch: 15 step: 777, loss is 0.005742559675127268\n",
      "epoch: 15 step: 778, loss is 0.022671956568956375\n",
      "epoch: 15 step: 779, loss is 0.03484710305929184\n",
      "epoch: 15 step: 780, loss is 0.06929968297481537\n",
      "epoch: 15 step: 781, loss is 0.03316865116357803\n",
      "epoch: 15 step: 782, loss is 0.004944505635648966\n",
      "epoch: 15 step: 783, loss is 0.010910634882748127\n",
      "epoch: 15 step: 784, loss is 0.004676372278481722\n",
      "epoch: 15 step: 785, loss is 0.03181299567222595\n",
      "epoch: 15 step: 786, loss is 0.027833998203277588\n",
      "epoch: 15 step: 787, loss is 0.006550535559654236\n",
      "epoch: 15 step: 788, loss is 0.05101367086172104\n",
      "epoch: 15 step: 789, loss is 0.03890451043844223\n",
      "epoch: 15 step: 790, loss is 0.0029186310712248087\n",
      "epoch: 15 step: 791, loss is 0.0821036696434021\n",
      "epoch: 15 step: 792, loss is 0.03423523157835007\n",
      "epoch: 15 step: 793, loss is 0.02149786427617073\n",
      "epoch: 15 step: 794, loss is 0.0037334116641432047\n",
      "epoch: 15 step: 795, loss is 0.013479512184858322\n",
      "epoch: 15 step: 796, loss is 0.00426114909350872\n",
      "epoch: 15 step: 797, loss is 0.017001226544380188\n",
      "epoch: 15 step: 798, loss is 0.06475534290075302\n",
      "epoch: 15 step: 799, loss is 0.0495179183781147\n",
      "epoch: 15 step: 800, loss is 0.002151496009901166\n",
      "epoch: 15 step: 801, loss is 0.021010247990489006\n",
      "epoch: 15 step: 802, loss is 0.028113147243857384\n",
      "epoch: 15 step: 803, loss is 0.02954433113336563\n",
      "epoch: 15 step: 804, loss is 0.04404463991522789\n",
      "epoch: 15 step: 805, loss is 0.0013223867863416672\n",
      "epoch: 15 step: 806, loss is 0.04192478954792023\n",
      "epoch: 15 step: 807, loss is 0.013905041851103306\n",
      "epoch: 15 step: 808, loss is 0.0017869876464828849\n",
      "epoch: 15 step: 809, loss is 0.01842532306909561\n",
      "epoch: 15 step: 810, loss is 0.004374205134809017\n",
      "epoch: 15 step: 811, loss is 0.012778685428202152\n",
      "epoch: 15 step: 812, loss is 0.02938733994960785\n",
      "epoch: 15 step: 813, loss is 0.03037996031343937\n",
      "epoch: 15 step: 814, loss is 0.026641860604286194\n",
      "epoch: 15 step: 815, loss is 0.008004892617464066\n",
      "epoch: 15 step: 816, loss is 0.0005330371204763651\n",
      "epoch: 15 step: 817, loss is 0.006257227156311274\n",
      "epoch: 15 step: 818, loss is 0.0327032096683979\n",
      "epoch: 15 step: 819, loss is 0.011308260262012482\n",
      "epoch: 15 step: 820, loss is 0.030915912240743637\n",
      "epoch: 15 step: 821, loss is 0.012847909703850746\n",
      "epoch: 15 step: 822, loss is 0.01090291514992714\n",
      "epoch: 15 step: 823, loss is 0.0515797883272171\n",
      "epoch: 15 step: 824, loss is 0.008712013252079487\n",
      "epoch: 15 step: 825, loss is 0.04854313284158707\n",
      "epoch: 15 step: 826, loss is 0.000905025692190975\n",
      "epoch: 15 step: 827, loss is 0.049780670553445816\n",
      "epoch: 15 step: 828, loss is 0.020380984991788864\n",
      "epoch: 15 step: 829, loss is 0.02013779804110527\n",
      "epoch: 15 step: 830, loss is 0.00542450463399291\n",
      "epoch: 15 step: 831, loss is 0.0018944706534966826\n",
      "epoch: 15 step: 832, loss is 0.0034362245351076126\n",
      "epoch: 15 step: 833, loss is 0.05676858127117157\n",
      "epoch: 15 step: 834, loss is 0.010614113882184029\n",
      "epoch: 15 step: 835, loss is 0.04665030166506767\n",
      "epoch: 15 step: 836, loss is 0.010501976124942303\n",
      "epoch: 15 step: 837, loss is 0.10329697281122208\n",
      "epoch: 15 step: 838, loss is 0.023196475580334663\n",
      "epoch: 15 step: 839, loss is 0.13401545584201813\n",
      "epoch: 15 step: 840, loss is 0.057984113693237305\n",
      "epoch: 15 step: 841, loss is 0.008696332573890686\n",
      "epoch: 15 step: 842, loss is 0.02007347159087658\n",
      "epoch: 15 step: 843, loss is 0.055592868477106094\n",
      "epoch: 15 step: 844, loss is 0.0013783760368824005\n",
      "epoch: 15 step: 845, loss is 0.020453333854675293\n",
      "epoch: 15 step: 846, loss is 0.0030154173728078604\n",
      "epoch: 15 step: 847, loss is 0.08770088851451874\n",
      "epoch: 15 step: 848, loss is 0.020736221224069595\n",
      "epoch: 15 step: 849, loss is 0.04884473234415054\n",
      "epoch: 15 step: 850, loss is 0.014953396283090115\n",
      "epoch: 15 step: 851, loss is 0.001746935653500259\n",
      "epoch: 15 step: 852, loss is 0.018838999792933464\n",
      "epoch: 15 step: 853, loss is 0.07058914750814438\n",
      "epoch: 15 step: 854, loss is 0.018961260095238686\n",
      "epoch: 15 step: 855, loss is 0.04831396043300629\n",
      "epoch: 15 step: 856, loss is 0.012831587344408035\n",
      "epoch: 15 step: 857, loss is 0.027982890605926514\n",
      "epoch: 15 step: 858, loss is 0.010746976360678673\n",
      "epoch: 15 step: 859, loss is 0.04466177150607109\n",
      "epoch: 15 step: 860, loss is 0.0033364086411893368\n",
      "epoch: 15 step: 861, loss is 0.0031102087814360857\n",
      "epoch: 15 step: 862, loss is 0.013373480178415775\n",
      "epoch: 15 step: 863, loss is 0.02368978038430214\n",
      "epoch: 15 step: 864, loss is 0.02259059250354767\n",
      "epoch: 15 step: 865, loss is 0.013262712396681309\n",
      "epoch: 15 step: 866, loss is 0.007459147367626429\n",
      "epoch: 15 step: 867, loss is 0.007124339230358601\n",
      "epoch: 15 step: 868, loss is 0.008930924348533154\n",
      "epoch: 15 step: 869, loss is 0.0306426752358675\n",
      "epoch: 15 step: 870, loss is 0.0817769467830658\n",
      "epoch: 15 step: 871, loss is 0.007218266371637583\n",
      "epoch: 15 step: 872, loss is 0.06259787827730179\n",
      "epoch: 15 step: 873, loss is 0.0512719564139843\n",
      "epoch: 15 step: 874, loss is 0.010122423060238361\n",
      "epoch: 15 step: 875, loss is 0.0006877075647935271\n",
      "epoch: 15 step: 876, loss is 0.0017021720996126533\n",
      "epoch: 15 step: 877, loss is 0.007381873205304146\n",
      "epoch: 15 step: 878, loss is 0.001186870038509369\n",
      "epoch: 15 step: 879, loss is 0.01127226185053587\n",
      "epoch: 15 step: 880, loss is 0.003975982312113047\n",
      "epoch: 15 step: 881, loss is 0.009218973107635975\n",
      "epoch: 15 step: 882, loss is 0.005957859102636576\n",
      "epoch: 15 step: 883, loss is 0.016808796674013138\n",
      "epoch: 15 step: 884, loss is 0.0791221484541893\n",
      "epoch: 15 step: 885, loss is 0.00014558681868948042\n",
      "epoch: 15 step: 886, loss is 0.0076480102725327015\n",
      "epoch: 15 step: 887, loss is 0.008966773748397827\n",
      "epoch: 15 step: 888, loss is 0.06324220448732376\n",
      "epoch: 15 step: 889, loss is 0.014224891550838947\n",
      "epoch: 15 step: 890, loss is 0.0032147925812751055\n",
      "epoch: 15 step: 891, loss is 0.001894757035188377\n",
      "epoch: 15 step: 892, loss is 0.009844878688454628\n",
      "epoch: 15 step: 893, loss is 0.0184317696839571\n",
      "epoch: 15 step: 894, loss is 0.0032520561944693327\n",
      "epoch: 15 step: 895, loss is 0.012885094620287418\n",
      "epoch: 15 step: 896, loss is 0.021862979978322983\n",
      "epoch: 15 step: 897, loss is 0.0734211876988411\n",
      "epoch: 15 step: 898, loss is 0.035002451390028\n",
      "epoch: 15 step: 899, loss is 0.013925209641456604\n",
      "epoch: 15 step: 900, loss is 0.009727932512760162\n",
      "epoch: 15 step: 901, loss is 0.05895506218075752\n",
      "epoch: 15 step: 902, loss is 0.03215964883565903\n",
      "epoch: 15 step: 903, loss is 0.022285060957074165\n",
      "epoch: 15 step: 904, loss is 0.01728745922446251\n",
      "epoch: 15 step: 905, loss is 0.008890621364116669\n",
      "epoch: 15 step: 906, loss is 0.007154369726777077\n",
      "epoch: 15 step: 907, loss is 0.047534022480249405\n",
      "epoch: 15 step: 908, loss is 0.003462090389803052\n",
      "epoch: 15 step: 909, loss is 0.0067262472584843636\n",
      "epoch: 15 step: 910, loss is 0.021140072494745255\n",
      "epoch: 15 step: 911, loss is 0.0081415306776762\n",
      "epoch: 15 step: 912, loss is 0.08491046726703644\n",
      "epoch: 15 step: 913, loss is 0.02635822631418705\n",
      "epoch: 15 step: 914, loss is 0.001206829329021275\n",
      "epoch: 15 step: 915, loss is 0.03124847635626793\n",
      "epoch: 15 step: 916, loss is 0.016464298591017723\n",
      "epoch: 15 step: 917, loss is 0.008591081015765667\n",
      "epoch: 15 step: 918, loss is 0.0003532420378178358\n",
      "epoch: 15 step: 919, loss is 0.0025968984700739384\n",
      "epoch: 15 step: 920, loss is 0.0076005966402590275\n",
      "epoch: 15 step: 921, loss is 0.08072815090417862\n",
      "epoch: 15 step: 922, loss is 0.014151740819215775\n",
      "epoch: 15 step: 923, loss is 0.01550347451120615\n",
      "epoch: 15 step: 924, loss is 0.027255259454250336\n",
      "epoch: 15 step: 925, loss is 0.03833380341529846\n",
      "epoch: 15 step: 926, loss is 0.013292654417455196\n",
      "epoch: 15 step: 927, loss is 0.012980906292796135\n",
      "epoch: 15 step: 928, loss is 0.006711427588015795\n",
      "epoch: 15 step: 929, loss is 0.048897914588451385\n",
      "epoch: 15 step: 930, loss is 0.01262710615992546\n",
      "epoch: 15 step: 931, loss is 0.004497898742556572\n",
      "epoch: 15 step: 932, loss is 0.056808266788721085\n",
      "epoch: 15 step: 933, loss is 0.09971337020397186\n",
      "epoch: 15 step: 934, loss is 0.007721045520156622\n",
      "epoch: 15 step: 935, loss is 0.027063004672527313\n",
      "epoch: 15 step: 936, loss is 0.005282340571284294\n",
      "epoch: 15 step: 937, loss is 0.006355222314596176\n",
      "epoch: 16 step: 1, loss is 0.008961396291851997\n",
      "epoch: 16 step: 2, loss is 0.031884368509054184\n",
      "epoch: 16 step: 3, loss is 0.03323294222354889\n",
      "epoch: 16 step: 4, loss is 0.012172536924481392\n",
      "epoch: 16 step: 5, loss is 0.001366879791021347\n",
      "epoch: 16 step: 6, loss is 0.025345444679260254\n",
      "epoch: 16 step: 7, loss is 0.015786776319146156\n",
      "epoch: 16 step: 8, loss is 0.030018042773008347\n",
      "epoch: 16 step: 9, loss is 0.006751196458935738\n",
      "epoch: 16 step: 10, loss is 0.02196318469941616\n",
      "epoch: 16 step: 11, loss is 0.09553960710763931\n",
      "epoch: 16 step: 12, loss is 0.02774702198803425\n",
      "epoch: 16 step: 13, loss is 0.013039384968578815\n",
      "epoch: 16 step: 14, loss is 0.06813313066959381\n",
      "epoch: 16 step: 15, loss is 0.004860379733145237\n",
      "epoch: 16 step: 16, loss is 0.01127979438751936\n",
      "epoch: 16 step: 17, loss is 0.001458435319364071\n",
      "epoch: 16 step: 18, loss is 0.012863839976489544\n",
      "epoch: 16 step: 19, loss is 0.008949166163802147\n",
      "epoch: 16 step: 20, loss is 0.005377457942813635\n",
      "epoch: 16 step: 21, loss is 0.00706964498385787\n",
      "epoch: 16 step: 22, loss is 0.018322262912988663\n",
      "epoch: 16 step: 23, loss is 0.026078730821609497\n",
      "epoch: 16 step: 24, loss is 0.0005604136968031526\n",
      "epoch: 16 step: 25, loss is 0.0009791080374270678\n",
      "epoch: 16 step: 26, loss is 0.010900120250880718\n",
      "epoch: 16 step: 27, loss is 0.022121751680970192\n",
      "epoch: 16 step: 28, loss is 0.0029443830717355013\n",
      "epoch: 16 step: 29, loss is 0.011313765309751034\n",
      "epoch: 16 step: 30, loss is 0.05057216435670853\n",
      "epoch: 16 step: 31, loss is 0.006326043047010899\n",
      "epoch: 16 step: 32, loss is 0.02009012922644615\n",
      "epoch: 16 step: 33, loss is 0.001568466774187982\n",
      "epoch: 16 step: 34, loss is 0.0016850780230015516\n",
      "epoch: 16 step: 35, loss is 0.01117381639778614\n",
      "epoch: 16 step: 36, loss is 0.02061568945646286\n",
      "epoch: 16 step: 37, loss is 0.005100848153233528\n",
      "epoch: 16 step: 38, loss is 0.006261639762669802\n",
      "epoch: 16 step: 39, loss is 0.003329170634970069\n",
      "epoch: 16 step: 40, loss is 0.003485866356641054\n",
      "epoch: 16 step: 41, loss is 0.027590833604335785\n",
      "epoch: 16 step: 42, loss is 0.004835822153836489\n",
      "epoch: 16 step: 43, loss is 0.02004929445683956\n",
      "epoch: 16 step: 44, loss is 0.011319713667035103\n",
      "epoch: 16 step: 45, loss is 0.0008015293278731406\n",
      "epoch: 16 step: 46, loss is 0.007871722802519798\n",
      "epoch: 16 step: 47, loss is 0.0011819802457466722\n",
      "epoch: 16 step: 48, loss is 0.021881848573684692\n",
      "epoch: 16 step: 49, loss is 0.025635937228798866\n",
      "epoch: 16 step: 50, loss is 0.006637129932641983\n",
      "epoch: 16 step: 51, loss is 0.009607930667698383\n",
      "epoch: 16 step: 52, loss is 0.009334562346339226\n",
      "epoch: 16 step: 53, loss is 0.03565475717186928\n",
      "epoch: 16 step: 54, loss is 0.006166225299239159\n",
      "epoch: 16 step: 55, loss is 0.03305210918188095\n",
      "epoch: 16 step: 56, loss is 0.01752306893467903\n",
      "epoch: 16 step: 57, loss is 0.0025541530922055244\n",
      "epoch: 16 step: 58, loss is 0.0021403604187071323\n",
      "epoch: 16 step: 59, loss is 0.0026019227225333452\n",
      "epoch: 16 step: 60, loss is 0.0017227550270035863\n",
      "epoch: 16 step: 61, loss is 0.002075787167996168\n",
      "epoch: 16 step: 62, loss is 0.025545388460159302\n",
      "epoch: 16 step: 63, loss is 0.029168764129281044\n",
      "epoch: 16 step: 64, loss is 0.01838005520403385\n",
      "epoch: 16 step: 65, loss is 0.005857993382960558\n",
      "epoch: 16 step: 66, loss is 0.004976721480488777\n",
      "epoch: 16 step: 67, loss is 0.0057884519919753075\n",
      "epoch: 16 step: 68, loss is 0.0002339175553061068\n",
      "epoch: 16 step: 69, loss is 0.004347709473222494\n",
      "epoch: 16 step: 70, loss is 0.009717516601085663\n",
      "epoch: 16 step: 71, loss is 0.0001741486048558727\n",
      "epoch: 16 step: 72, loss is 0.07038625329732895\n",
      "epoch: 16 step: 73, loss is 0.005488745868206024\n",
      "epoch: 16 step: 74, loss is 0.025148240849375725\n",
      "epoch: 16 step: 75, loss is 0.009576142765581608\n",
      "epoch: 16 step: 76, loss is 0.02617473714053631\n",
      "epoch: 16 step: 77, loss is 0.009081256575882435\n",
      "epoch: 16 step: 78, loss is 0.014783155173063278\n",
      "epoch: 16 step: 79, loss is 0.003903522389009595\n",
      "epoch: 16 step: 80, loss is 0.010763571597635746\n",
      "epoch: 16 step: 81, loss is 0.0025641415268182755\n",
      "epoch: 16 step: 82, loss is 5.9797723224619403e-05\n",
      "epoch: 16 step: 83, loss is 0.01847999170422554\n",
      "epoch: 16 step: 84, loss is 0.006048416253179312\n",
      "epoch: 16 step: 85, loss is 0.004165950231254101\n",
      "epoch: 16 step: 86, loss is 0.0020940129179507494\n",
      "epoch: 16 step: 87, loss is 0.01952156238257885\n",
      "epoch: 16 step: 88, loss is 0.025517268106341362\n",
      "epoch: 16 step: 89, loss is 0.03553640842437744\n",
      "epoch: 16 step: 90, loss is 0.0006800090195611119\n",
      "epoch: 16 step: 91, loss is 0.007085128221660852\n",
      "epoch: 16 step: 92, loss is 0.00037055645952932537\n",
      "epoch: 16 step: 93, loss is 0.003524220082908869\n",
      "epoch: 16 step: 94, loss is 0.03677860274910927\n",
      "epoch: 16 step: 95, loss is 0.0013610199093818665\n",
      "epoch: 16 step: 96, loss is 0.012194500304758549\n",
      "epoch: 16 step: 97, loss is 0.010162580758333206\n",
      "epoch: 16 step: 98, loss is 0.036440759897232056\n",
      "epoch: 16 step: 99, loss is 0.00797250960022211\n",
      "epoch: 16 step: 100, loss is 0.018772823736071587\n",
      "epoch: 16 step: 101, loss is 0.0026982484851032495\n",
      "epoch: 16 step: 102, loss is 0.048582423478364944\n",
      "epoch: 16 step: 103, loss is 0.020158369094133377\n",
      "epoch: 16 step: 104, loss is 0.012957129627466202\n",
      "epoch: 16 step: 105, loss is 0.041783545166254044\n",
      "epoch: 16 step: 106, loss is 0.07104583829641342\n",
      "epoch: 16 step: 107, loss is 0.004801908042281866\n",
      "epoch: 16 step: 108, loss is 0.0728604644536972\n",
      "epoch: 16 step: 109, loss is 0.033287413418293\n",
      "epoch: 16 step: 110, loss is 0.0015987532678991556\n",
      "epoch: 16 step: 111, loss is 0.008104003965854645\n",
      "epoch: 16 step: 112, loss is 0.05554446578025818\n",
      "epoch: 16 step: 113, loss is 0.004354327917098999\n",
      "epoch: 16 step: 114, loss is 0.0005446249851956964\n",
      "epoch: 16 step: 115, loss is 0.008672131225466728\n",
      "epoch: 16 step: 116, loss is 0.04071567952632904\n",
      "epoch: 16 step: 117, loss is 0.0012141988845542073\n",
      "epoch: 16 step: 118, loss is 0.21058006584644318\n",
      "epoch: 16 step: 119, loss is 0.014689045958220959\n",
      "epoch: 16 step: 120, loss is 0.04762851074337959\n",
      "epoch: 16 step: 121, loss is 0.0023943304549902678\n",
      "epoch: 16 step: 122, loss is 0.04901760444045067\n",
      "epoch: 16 step: 123, loss is 0.04482412338256836\n",
      "epoch: 16 step: 124, loss is 0.00015341007383540273\n",
      "epoch: 16 step: 125, loss is 0.03780997544527054\n",
      "epoch: 16 step: 126, loss is 0.022029906511306763\n",
      "epoch: 16 step: 127, loss is 0.010340492241084576\n",
      "epoch: 16 step: 128, loss is 0.006809866055846214\n",
      "epoch: 16 step: 129, loss is 0.07290598005056381\n",
      "epoch: 16 step: 130, loss is 0.029620565474033356\n",
      "epoch: 16 step: 131, loss is 0.000473638967378065\n",
      "epoch: 16 step: 132, loss is 0.05477222800254822\n",
      "epoch: 16 step: 133, loss is 0.0025526045355945826\n",
      "epoch: 16 step: 134, loss is 0.004634377080947161\n",
      "epoch: 16 step: 135, loss is 0.0010167122818529606\n",
      "epoch: 16 step: 136, loss is 0.006958849728107452\n",
      "epoch: 16 step: 137, loss is 0.048203226178884506\n",
      "epoch: 16 step: 138, loss is 0.00362969096750021\n",
      "epoch: 16 step: 139, loss is 0.01407016534358263\n",
      "epoch: 16 step: 140, loss is 0.032875075936317444\n",
      "epoch: 16 step: 141, loss is 0.006628051865845919\n",
      "epoch: 16 step: 142, loss is 0.005917592439800501\n",
      "epoch: 16 step: 143, loss is 0.0596192330121994\n",
      "epoch: 16 step: 144, loss is 0.007359696552157402\n",
      "epoch: 16 step: 145, loss is 0.0017790264682844281\n",
      "epoch: 16 step: 146, loss is 0.00630972720682621\n",
      "epoch: 16 step: 147, loss is 0.008938561193645\n",
      "epoch: 16 step: 148, loss is 0.006935920566320419\n",
      "epoch: 16 step: 149, loss is 0.03532104939222336\n",
      "epoch: 16 step: 150, loss is 0.006253779400140047\n",
      "epoch: 16 step: 151, loss is 0.01688719168305397\n",
      "epoch: 16 step: 152, loss is 0.010561038739979267\n",
      "epoch: 16 step: 153, loss is 0.018869979307055473\n",
      "epoch: 16 step: 154, loss is 0.003455332014709711\n",
      "epoch: 16 step: 155, loss is 0.003848346183076501\n",
      "epoch: 16 step: 156, loss is 0.004122606478631496\n",
      "epoch: 16 step: 157, loss is 0.005369857419282198\n",
      "epoch: 16 step: 158, loss is 0.0019498550100252032\n",
      "epoch: 16 step: 159, loss is 0.007380692288279533\n",
      "epoch: 16 step: 160, loss is 0.006588382180780172\n",
      "epoch: 16 step: 161, loss is 0.10027297586202621\n",
      "epoch: 16 step: 162, loss is 0.022081958130002022\n",
      "epoch: 16 step: 163, loss is 0.03241456672549248\n",
      "epoch: 16 step: 164, loss is 0.0063077048398554325\n",
      "epoch: 16 step: 165, loss is 0.0014309018151834607\n",
      "epoch: 16 step: 166, loss is 0.011778758838772774\n",
      "epoch: 16 step: 167, loss is 0.01760229282081127\n",
      "epoch: 16 step: 168, loss is 0.01377545204013586\n",
      "epoch: 16 step: 169, loss is 0.006628172937780619\n",
      "epoch: 16 step: 170, loss is 0.03376104310154915\n",
      "epoch: 16 step: 171, loss is 0.005150007549673319\n",
      "epoch: 16 step: 172, loss is 0.02418721653521061\n",
      "epoch: 16 step: 173, loss is 0.005913067609071732\n",
      "epoch: 16 step: 174, loss is 0.007905138656497002\n",
      "epoch: 16 step: 175, loss is 0.0029169872868806124\n",
      "epoch: 16 step: 176, loss is 0.038864243775606155\n",
      "epoch: 16 step: 177, loss is 0.038568589836359024\n",
      "epoch: 16 step: 178, loss is 0.003968971315771341\n",
      "epoch: 16 step: 179, loss is 0.010799032635986805\n",
      "epoch: 16 step: 180, loss is 0.023318439722061157\n",
      "epoch: 16 step: 181, loss is 0.10381835699081421\n",
      "epoch: 16 step: 182, loss is 0.027734287083148956\n",
      "epoch: 16 step: 183, loss is 0.025249145925045013\n",
      "epoch: 16 step: 184, loss is 0.015574942342936993\n",
      "epoch: 16 step: 185, loss is 0.025605861097574234\n",
      "epoch: 16 step: 186, loss is 0.02311587706208229\n",
      "epoch: 16 step: 187, loss is 0.00032436277251690626\n",
      "epoch: 16 step: 188, loss is 0.15398260951042175\n",
      "epoch: 16 step: 189, loss is 0.05661476403474808\n",
      "epoch: 16 step: 190, loss is 0.02441662922501564\n",
      "epoch: 16 step: 191, loss is 0.004390794318169355\n",
      "epoch: 16 step: 192, loss is 0.0019865245558321476\n",
      "epoch: 16 step: 193, loss is 0.004787782672792673\n",
      "epoch: 16 step: 194, loss is 0.04424821585416794\n",
      "epoch: 16 step: 195, loss is 0.012589829973876476\n",
      "epoch: 16 step: 196, loss is 0.029506999999284744\n",
      "epoch: 16 step: 197, loss is 0.006585672032088041\n",
      "epoch: 16 step: 198, loss is 0.004338699392974377\n",
      "epoch: 16 step: 199, loss is 0.008905871771275997\n",
      "epoch: 16 step: 200, loss is 0.009054062888026237\n",
      "epoch: 16 step: 201, loss is 0.0019825680647045374\n",
      "epoch: 16 step: 202, loss is 0.006211476866155863\n",
      "epoch: 16 step: 203, loss is 0.013560778461396694\n",
      "epoch: 16 step: 204, loss is 0.013663308694958687\n",
      "epoch: 16 step: 205, loss is 0.0006219188799150288\n",
      "epoch: 16 step: 206, loss is 0.0028959019109606743\n",
      "epoch: 16 step: 207, loss is 0.0013117025373503566\n",
      "epoch: 16 step: 208, loss is 0.0023162593133747578\n",
      "epoch: 16 step: 209, loss is 0.0012667246628552675\n",
      "epoch: 16 step: 210, loss is 0.0007837453158572316\n",
      "epoch: 16 step: 211, loss is 0.056723497807979584\n",
      "epoch: 16 step: 212, loss is 0.00578185822814703\n",
      "epoch: 16 step: 213, loss is 0.006381814368069172\n",
      "epoch: 16 step: 214, loss is 0.01963118277490139\n",
      "epoch: 16 step: 215, loss is 0.04839010909199715\n",
      "epoch: 16 step: 216, loss is 0.02383871003985405\n",
      "epoch: 16 step: 217, loss is 0.009267447516322136\n",
      "epoch: 16 step: 218, loss is 0.005643372889608145\n",
      "epoch: 16 step: 219, loss is 0.01656884327530861\n",
      "epoch: 16 step: 220, loss is 0.004586070775985718\n",
      "epoch: 16 step: 221, loss is 0.025613319128751755\n",
      "epoch: 16 step: 222, loss is 0.0015329103916883469\n",
      "epoch: 16 step: 223, loss is 0.01666119694709778\n",
      "epoch: 16 step: 224, loss is 0.0010923126246780157\n",
      "epoch: 16 step: 225, loss is 0.0006324063870124519\n",
      "epoch: 16 step: 226, loss is 0.006601317320019007\n",
      "epoch: 16 step: 227, loss is 0.0060167438350617886\n",
      "epoch: 16 step: 228, loss is 0.061347104609012604\n",
      "epoch: 16 step: 229, loss is 0.0011037186486646533\n",
      "epoch: 16 step: 230, loss is 0.015373353846371174\n",
      "epoch: 16 step: 231, loss is 0.0007902804063633084\n",
      "epoch: 16 step: 232, loss is 0.0009171207202598453\n",
      "epoch: 16 step: 233, loss is 0.0005649607628583908\n",
      "epoch: 16 step: 234, loss is 0.0427415706217289\n",
      "epoch: 16 step: 235, loss is 0.011691627092659473\n",
      "epoch: 16 step: 236, loss is 0.004204383585602045\n",
      "epoch: 16 step: 237, loss is 0.005338198971003294\n",
      "epoch: 16 step: 238, loss is 0.0039406102150678635\n",
      "epoch: 16 step: 239, loss is 0.05922088027000427\n",
      "epoch: 16 step: 240, loss is 0.0026073663029819727\n",
      "epoch: 16 step: 241, loss is 0.0026841280050575733\n",
      "epoch: 16 step: 242, loss is 0.04367668926715851\n",
      "epoch: 16 step: 243, loss is 0.002436141250655055\n",
      "epoch: 16 step: 244, loss is 0.008017502725124359\n",
      "epoch: 16 step: 245, loss is 0.005102895200252533\n",
      "epoch: 16 step: 246, loss is 0.0005579040152952075\n",
      "epoch: 16 step: 247, loss is 0.006833998952060938\n",
      "epoch: 16 step: 248, loss is 0.03515489399433136\n",
      "epoch: 16 step: 249, loss is 0.01165310200303793\n",
      "epoch: 16 step: 250, loss is 0.014733672142028809\n",
      "epoch: 16 step: 251, loss is 0.002486662706360221\n",
      "epoch: 16 step: 252, loss is 0.059777066111564636\n",
      "epoch: 16 step: 253, loss is 0.002679374534636736\n",
      "epoch: 16 step: 254, loss is 0.012810952961444855\n",
      "epoch: 16 step: 255, loss is 0.00500288512557745\n",
      "epoch: 16 step: 256, loss is 0.004104395862668753\n",
      "epoch: 16 step: 257, loss is 0.006304299924522638\n",
      "epoch: 16 step: 258, loss is 0.03369147703051567\n",
      "epoch: 16 step: 259, loss is 0.04237505421042442\n",
      "epoch: 16 step: 260, loss is 0.0024569968227297068\n",
      "epoch: 16 step: 261, loss is 0.007171174045652151\n",
      "epoch: 16 step: 262, loss is 0.04682524874806404\n",
      "epoch: 16 step: 263, loss is 0.0036135073751211166\n",
      "epoch: 16 step: 264, loss is 0.00523952953517437\n",
      "epoch: 16 step: 265, loss is 0.0009479101281613111\n",
      "epoch: 16 step: 266, loss is 0.002139043528586626\n",
      "epoch: 16 step: 267, loss is 0.006171060726046562\n",
      "epoch: 16 step: 268, loss is 0.05577783286571503\n",
      "epoch: 16 step: 269, loss is 0.04477304592728615\n",
      "epoch: 16 step: 270, loss is 0.007415447849780321\n",
      "epoch: 16 step: 271, loss is 0.001484989421442151\n",
      "epoch: 16 step: 272, loss is 0.12376714497804642\n",
      "epoch: 16 step: 273, loss is 0.0026422422379255295\n",
      "epoch: 16 step: 274, loss is 0.0037195119075477123\n",
      "epoch: 16 step: 275, loss is 0.004536597058176994\n",
      "epoch: 16 step: 276, loss is 0.0027137326542288065\n",
      "epoch: 16 step: 277, loss is 0.009933866560459137\n",
      "epoch: 16 step: 278, loss is 0.0007630733889527619\n",
      "epoch: 16 step: 279, loss is 0.001489755348302424\n",
      "epoch: 16 step: 280, loss is 0.007061390206217766\n",
      "epoch: 16 step: 281, loss is 0.004815859254449606\n",
      "epoch: 16 step: 282, loss is 0.0009807889582589269\n",
      "epoch: 16 step: 283, loss is 0.02265895903110504\n",
      "epoch: 16 step: 284, loss is 0.0019777733832597733\n",
      "epoch: 16 step: 285, loss is 0.0022461023181676865\n",
      "epoch: 16 step: 286, loss is 0.001187352230772376\n",
      "epoch: 16 step: 287, loss is 0.015755675733089447\n",
      "epoch: 16 step: 288, loss is 0.02522076666355133\n",
      "epoch: 16 step: 289, loss is 0.049401067197322845\n",
      "epoch: 16 step: 290, loss is 0.001241031102836132\n",
      "epoch: 16 step: 291, loss is 0.0017526705050840974\n",
      "epoch: 16 step: 292, loss is 0.010724775493144989\n",
      "epoch: 16 step: 293, loss is 0.0008955178782343864\n",
      "epoch: 16 step: 294, loss is 0.041425760835409164\n",
      "epoch: 16 step: 295, loss is 0.02256595343351364\n",
      "epoch: 16 step: 296, loss is 0.012831544503569603\n",
      "epoch: 16 step: 297, loss is 0.0049151829443871975\n",
      "epoch: 16 step: 298, loss is 0.012167910113930702\n",
      "epoch: 16 step: 299, loss is 0.004812725353986025\n",
      "epoch: 16 step: 300, loss is 0.024228043854236603\n",
      "epoch: 16 step: 301, loss is 0.005596432834863663\n",
      "epoch: 16 step: 302, loss is 0.015698354691267014\n",
      "epoch: 16 step: 303, loss is 0.00588822178542614\n",
      "epoch: 16 step: 304, loss is 0.02617020532488823\n",
      "epoch: 16 step: 305, loss is 0.0029516934882849455\n",
      "epoch: 16 step: 306, loss is 0.0031352511141449213\n",
      "epoch: 16 step: 307, loss is 0.0035386960953474045\n",
      "epoch: 16 step: 308, loss is 0.011336689814925194\n",
      "epoch: 16 step: 309, loss is 0.027426714077591896\n",
      "epoch: 16 step: 310, loss is 0.0017986599123105407\n",
      "epoch: 16 step: 311, loss is 0.0002442921104375273\n",
      "epoch: 16 step: 312, loss is 0.0035557025112211704\n",
      "epoch: 16 step: 313, loss is 0.03529557213187218\n",
      "epoch: 16 step: 314, loss is 0.004524382296949625\n",
      "epoch: 16 step: 315, loss is 0.0041975295171141624\n",
      "epoch: 16 step: 316, loss is 0.013934636488556862\n",
      "epoch: 16 step: 317, loss is 0.0006398677360266447\n",
      "epoch: 16 step: 318, loss is 0.08487393707036972\n",
      "epoch: 16 step: 319, loss is 0.04065681993961334\n",
      "epoch: 16 step: 320, loss is 0.006254036910831928\n",
      "epoch: 16 step: 321, loss is 0.016712820157408714\n",
      "epoch: 16 step: 322, loss is 0.006439077667891979\n",
      "epoch: 16 step: 323, loss is 0.006806429475545883\n",
      "epoch: 16 step: 324, loss is 0.02092045173048973\n",
      "epoch: 16 step: 325, loss is 0.00769297918304801\n",
      "epoch: 16 step: 326, loss is 0.01952209323644638\n",
      "epoch: 16 step: 327, loss is 0.012182722799479961\n",
      "epoch: 16 step: 328, loss is 0.021963855251669884\n",
      "epoch: 16 step: 329, loss is 0.00042681131162680686\n",
      "epoch: 16 step: 330, loss is 0.02312510460615158\n",
      "epoch: 16 step: 331, loss is 0.02546551451086998\n",
      "epoch: 16 step: 332, loss is 0.0014901523245498538\n",
      "epoch: 16 step: 333, loss is 0.008803199976682663\n",
      "epoch: 16 step: 334, loss is 0.10782559216022491\n",
      "epoch: 16 step: 335, loss is 0.03289839252829552\n",
      "epoch: 16 step: 336, loss is 0.002457280410453677\n",
      "epoch: 16 step: 337, loss is 0.057483598589897156\n",
      "epoch: 16 step: 338, loss is 0.033336855471134186\n",
      "epoch: 16 step: 339, loss is 0.008108234032988548\n",
      "epoch: 16 step: 340, loss is 0.0027976923156529665\n",
      "epoch: 16 step: 341, loss is 0.0072577823884785175\n",
      "epoch: 16 step: 342, loss is 0.06470299512147903\n",
      "epoch: 16 step: 343, loss is 0.002168710343539715\n",
      "epoch: 16 step: 344, loss is 0.06297299265861511\n",
      "epoch: 16 step: 345, loss is 0.027483372017741203\n",
      "epoch: 16 step: 346, loss is 0.003914734348654747\n",
      "epoch: 16 step: 347, loss is 0.03704190254211426\n",
      "epoch: 16 step: 348, loss is 0.05177883803844452\n",
      "epoch: 16 step: 349, loss is 0.05719270557165146\n",
      "epoch: 16 step: 350, loss is 0.07991810888051987\n",
      "epoch: 16 step: 351, loss is 0.029017535969614983\n",
      "epoch: 16 step: 352, loss is 0.028164101764559746\n",
      "epoch: 16 step: 353, loss is 0.013947082683444023\n",
      "epoch: 16 step: 354, loss is 0.007544906809926033\n",
      "epoch: 16 step: 355, loss is 0.027337605133652687\n",
      "epoch: 16 step: 356, loss is 0.008395344018936157\n",
      "epoch: 16 step: 357, loss is 0.04438796266913414\n",
      "epoch: 16 step: 358, loss is 0.04458150640130043\n",
      "epoch: 16 step: 359, loss is 0.07897680997848511\n",
      "epoch: 16 step: 360, loss is 0.0033615687862038612\n",
      "epoch: 16 step: 361, loss is 0.004893122240900993\n",
      "epoch: 16 step: 362, loss is 0.0025873275008052588\n",
      "epoch: 16 step: 363, loss is 0.012263501062989235\n",
      "epoch: 16 step: 364, loss is 0.05717896297574043\n",
      "epoch: 16 step: 365, loss is 0.07169931381940842\n",
      "epoch: 16 step: 366, loss is 0.012290745973587036\n",
      "epoch: 16 step: 367, loss is 0.002832678845152259\n",
      "epoch: 16 step: 368, loss is 0.04499289393424988\n",
      "epoch: 16 step: 369, loss is 0.004435633309185505\n",
      "epoch: 16 step: 370, loss is 0.01717798411846161\n",
      "epoch: 16 step: 371, loss is 0.07479476928710938\n",
      "epoch: 16 step: 372, loss is 0.00876130536198616\n",
      "epoch: 16 step: 373, loss is 0.04239309951663017\n",
      "epoch: 16 step: 374, loss is 0.08050303161144257\n",
      "epoch: 16 step: 375, loss is 0.08792845904827118\n",
      "epoch: 16 step: 376, loss is 0.008083673194050789\n",
      "epoch: 16 step: 377, loss is 0.13015717267990112\n",
      "epoch: 16 step: 378, loss is 0.03490753844380379\n",
      "epoch: 16 step: 379, loss is 0.006498884409666061\n",
      "epoch: 16 step: 380, loss is 0.0018211603164672852\n",
      "epoch: 16 step: 381, loss is 0.021708687767386436\n",
      "epoch: 16 step: 382, loss is 0.019968176260590553\n",
      "epoch: 16 step: 383, loss is 0.03762383013963699\n",
      "epoch: 16 step: 384, loss is 0.02279900200664997\n",
      "epoch: 16 step: 385, loss is 0.0055050537921488285\n",
      "epoch: 16 step: 386, loss is 0.12546998262405396\n",
      "epoch: 16 step: 387, loss is 0.005097007378935814\n",
      "epoch: 16 step: 388, loss is 0.05551937222480774\n",
      "epoch: 16 step: 389, loss is 0.005648121237754822\n",
      "epoch: 16 step: 390, loss is 0.04352676868438721\n",
      "epoch: 16 step: 391, loss is 0.07242821902036667\n",
      "epoch: 16 step: 392, loss is 0.006168540101498365\n",
      "epoch: 16 step: 393, loss is 0.003674816805869341\n",
      "epoch: 16 step: 394, loss is 0.03005775436758995\n",
      "epoch: 16 step: 395, loss is 0.058217573910951614\n",
      "epoch: 16 step: 396, loss is 0.000805376679636538\n",
      "epoch: 16 step: 397, loss is 0.09590453654527664\n",
      "epoch: 16 step: 398, loss is 0.007594055030494928\n",
      "epoch: 16 step: 399, loss is 0.11282037198543549\n",
      "epoch: 16 step: 400, loss is 0.007866837084293365\n",
      "epoch: 16 step: 401, loss is 0.0240362249314785\n",
      "epoch: 16 step: 402, loss is 0.014964388683438301\n",
      "epoch: 16 step: 403, loss is 0.05109093710780144\n",
      "epoch: 16 step: 404, loss is 0.03076034225523472\n",
      "epoch: 16 step: 405, loss is 0.03449070081114769\n",
      "epoch: 16 step: 406, loss is 0.0872885137796402\n",
      "epoch: 16 step: 407, loss is 0.010134284384548664\n",
      "epoch: 16 step: 408, loss is 0.01885448396205902\n",
      "epoch: 16 step: 409, loss is 0.03486450016498566\n",
      "epoch: 16 step: 410, loss is 0.05833498015999794\n",
      "epoch: 16 step: 411, loss is 0.05166490003466606\n",
      "epoch: 16 step: 412, loss is 0.007469600066542625\n",
      "epoch: 16 step: 413, loss is 0.006196311209350824\n",
      "epoch: 16 step: 414, loss is 0.00308334082365036\n",
      "epoch: 16 step: 415, loss is 0.04830887168645859\n",
      "epoch: 16 step: 416, loss is 0.02393997274339199\n",
      "epoch: 16 step: 417, loss is 0.06527775526046753\n",
      "epoch: 16 step: 418, loss is 0.015830662101507187\n",
      "epoch: 16 step: 419, loss is 0.08576904237270355\n",
      "epoch: 16 step: 420, loss is 0.06993913650512695\n",
      "epoch: 16 step: 421, loss is 0.009466403163969517\n",
      "epoch: 16 step: 422, loss is 0.0174085795879364\n",
      "epoch: 16 step: 423, loss is 0.0005217166035436094\n",
      "epoch: 16 step: 424, loss is 0.001637179870158434\n",
      "epoch: 16 step: 425, loss is 0.011063638143241405\n",
      "epoch: 16 step: 426, loss is 0.005349553190171719\n",
      "epoch: 16 step: 427, loss is 0.001562677789479494\n",
      "epoch: 16 step: 428, loss is 0.001111527904868126\n",
      "epoch: 16 step: 429, loss is 0.013201684691011906\n",
      "epoch: 16 step: 430, loss is 0.0037667383439838886\n",
      "epoch: 16 step: 431, loss is 0.014093097299337387\n",
      "epoch: 16 step: 432, loss is 0.002644470427185297\n",
      "epoch: 16 step: 433, loss is 0.07359202951192856\n",
      "epoch: 16 step: 434, loss is 0.06679372489452362\n",
      "epoch: 16 step: 435, loss is 0.025800317525863647\n",
      "epoch: 16 step: 436, loss is 0.19628886878490448\n",
      "epoch: 16 step: 437, loss is 0.01318262703716755\n",
      "epoch: 16 step: 438, loss is 0.019099557772278786\n",
      "epoch: 16 step: 439, loss is 0.03573239594697952\n",
      "epoch: 16 step: 440, loss is 0.007564102299511433\n",
      "epoch: 16 step: 441, loss is 0.005535098724067211\n",
      "epoch: 16 step: 442, loss is 0.04641168192028999\n",
      "epoch: 16 step: 443, loss is 0.014578916132450104\n",
      "epoch: 16 step: 444, loss is 0.032877061516046524\n",
      "epoch: 16 step: 445, loss is 0.017658540979027748\n",
      "epoch: 16 step: 446, loss is 0.12008339911699295\n",
      "epoch: 16 step: 447, loss is 0.022806663066148758\n",
      "epoch: 16 step: 448, loss is 0.08486918359994888\n",
      "epoch: 16 step: 449, loss is 0.05787031352519989\n",
      "epoch: 16 step: 450, loss is 0.006308683194220066\n",
      "epoch: 16 step: 451, loss is 0.022523464635014534\n",
      "epoch: 16 step: 452, loss is 0.03629113361239433\n",
      "epoch: 16 step: 453, loss is 0.019308408722281456\n",
      "epoch: 16 step: 454, loss is 0.05829766020178795\n",
      "epoch: 16 step: 455, loss is 0.057275936007499695\n",
      "epoch: 16 step: 456, loss is 0.019850309938192368\n",
      "epoch: 16 step: 457, loss is 0.01559448428452015\n",
      "epoch: 16 step: 458, loss is 0.00824008509516716\n",
      "epoch: 16 step: 459, loss is 0.01831113174557686\n",
      "epoch: 16 step: 460, loss is 0.04376031830906868\n",
      "epoch: 16 step: 461, loss is 0.09456092119216919\n",
      "epoch: 16 step: 462, loss is 0.06307735294103622\n",
      "epoch: 16 step: 463, loss is 0.10120423138141632\n",
      "epoch: 16 step: 464, loss is 0.06639009714126587\n",
      "epoch: 16 step: 465, loss is 0.0515226311981678\n",
      "epoch: 16 step: 466, loss is 0.028451301157474518\n",
      "epoch: 16 step: 467, loss is 0.04669751599431038\n",
      "epoch: 16 step: 468, loss is 0.05448504164814949\n",
      "epoch: 16 step: 469, loss is 0.0006906477501615882\n",
      "epoch: 16 step: 470, loss is 0.02808620035648346\n",
      "epoch: 16 step: 471, loss is 0.03607126325368881\n",
      "epoch: 16 step: 472, loss is 0.02170618623495102\n",
      "epoch: 16 step: 473, loss is 0.0038354885764420033\n",
      "epoch: 16 step: 474, loss is 0.1712651252746582\n",
      "epoch: 16 step: 475, loss is 0.07709275186061859\n",
      "epoch: 16 step: 476, loss is 0.030780810862779617\n",
      "epoch: 16 step: 477, loss is 0.007758994121104479\n",
      "epoch: 16 step: 478, loss is 0.0431445948779583\n",
      "epoch: 16 step: 479, loss is 0.0011613464448601007\n",
      "epoch: 16 step: 480, loss is 0.02504931017756462\n",
      "epoch: 16 step: 481, loss is 0.014890048652887344\n",
      "epoch: 16 step: 482, loss is 0.0006224762182682753\n",
      "epoch: 16 step: 483, loss is 0.010733844712376595\n",
      "epoch: 16 step: 484, loss is 0.02108640782535076\n",
      "epoch: 16 step: 485, loss is 0.021530000492930412\n",
      "epoch: 16 step: 486, loss is 0.017014816403388977\n",
      "epoch: 16 step: 487, loss is 0.007738805841654539\n",
      "epoch: 16 step: 488, loss is 0.046790558844804764\n",
      "epoch: 16 step: 489, loss is 0.056120991706848145\n",
      "epoch: 16 step: 490, loss is 0.0208607017993927\n",
      "epoch: 16 step: 491, loss is 0.08844687789678574\n",
      "epoch: 16 step: 492, loss is 0.02193111926317215\n",
      "epoch: 16 step: 493, loss is 0.008635497651994228\n",
      "epoch: 16 step: 494, loss is 0.019926931709051132\n",
      "epoch: 16 step: 495, loss is 0.06697472929954529\n",
      "epoch: 16 step: 496, loss is 0.006164125632494688\n",
      "epoch: 16 step: 497, loss is 0.07439003884792328\n",
      "epoch: 16 step: 498, loss is 0.0649048313498497\n",
      "epoch: 16 step: 499, loss is 0.10001536458730698\n",
      "epoch: 16 step: 500, loss is 0.027497462928295135\n",
      "epoch: 16 step: 501, loss is 0.03914269059896469\n",
      "epoch: 16 step: 502, loss is 0.010334462858736515\n",
      "epoch: 16 step: 503, loss is 0.03285572677850723\n",
      "epoch: 16 step: 504, loss is 0.06392256170511246\n",
      "epoch: 16 step: 505, loss is 0.008383944630622864\n",
      "epoch: 16 step: 506, loss is 0.05778089910745621\n",
      "epoch: 16 step: 507, loss is 0.028447676450014114\n",
      "epoch: 16 step: 508, loss is 0.03349243104457855\n",
      "epoch: 16 step: 509, loss is 0.01628025993704796\n",
      "epoch: 16 step: 510, loss is 0.021639125421643257\n",
      "epoch: 16 step: 511, loss is 0.00949872750788927\n",
      "epoch: 16 step: 512, loss is 0.009444012306630611\n",
      "epoch: 16 step: 513, loss is 0.011759771034121513\n",
      "epoch: 16 step: 514, loss is 0.025477390736341476\n",
      "epoch: 16 step: 515, loss is 0.0424138680100441\n",
      "epoch: 16 step: 516, loss is 0.05506962537765503\n",
      "epoch: 16 step: 517, loss is 0.008881226181983948\n",
      "epoch: 16 step: 518, loss is 0.00040037758299149573\n",
      "epoch: 16 step: 519, loss is 0.03382665663957596\n",
      "epoch: 16 step: 520, loss is 0.026697395369410515\n",
      "epoch: 16 step: 521, loss is 0.01369260810315609\n",
      "epoch: 16 step: 522, loss is 0.02018827572464943\n",
      "epoch: 16 step: 523, loss is 0.0867677852511406\n",
      "epoch: 16 step: 524, loss is 0.021181786432862282\n",
      "epoch: 16 step: 525, loss is 0.0017117715906351805\n",
      "epoch: 16 step: 526, loss is 0.0008513968205079436\n",
      "epoch: 16 step: 527, loss is 0.04215897619724274\n",
      "epoch: 16 step: 528, loss is 0.025821655988693237\n",
      "epoch: 16 step: 529, loss is 0.05664440989494324\n",
      "epoch: 16 step: 530, loss is 0.0008288185927085578\n",
      "epoch: 16 step: 531, loss is 0.026992302387952805\n",
      "epoch: 16 step: 532, loss is 0.007780227344483137\n",
      "epoch: 16 step: 533, loss is 0.0038599190302193165\n",
      "epoch: 16 step: 534, loss is 0.06874604523181915\n",
      "epoch: 16 step: 535, loss is 0.04427500069141388\n",
      "epoch: 16 step: 536, loss is 0.042448677122592926\n",
      "epoch: 16 step: 537, loss is 0.026079827919602394\n",
      "epoch: 16 step: 538, loss is 0.003385349875316024\n",
      "epoch: 16 step: 539, loss is 0.0011942221317440271\n",
      "epoch: 16 step: 540, loss is 0.0021765585988759995\n",
      "epoch: 16 step: 541, loss is 0.13006465137004852\n",
      "epoch: 16 step: 542, loss is 0.004812519997358322\n",
      "epoch: 16 step: 543, loss is 0.06447191536426544\n",
      "epoch: 16 step: 544, loss is 0.005274128168821335\n",
      "epoch: 16 step: 545, loss is 0.008912979625165462\n",
      "epoch: 16 step: 546, loss is 0.007138724904507399\n",
      "epoch: 16 step: 547, loss is 0.01156909205019474\n",
      "epoch: 16 step: 548, loss is 0.016383595764636993\n",
      "epoch: 16 step: 549, loss is 0.000760051712859422\n",
      "epoch: 16 step: 550, loss is 0.08186568319797516\n",
      "epoch: 16 step: 551, loss is 0.026358358561992645\n",
      "epoch: 16 step: 552, loss is 0.020994825288653374\n",
      "epoch: 16 step: 553, loss is 0.010390725918114185\n",
      "epoch: 16 step: 554, loss is 0.03737281635403633\n",
      "epoch: 16 step: 555, loss is 0.050174497067928314\n",
      "epoch: 16 step: 556, loss is 0.004686735104769468\n",
      "epoch: 16 step: 557, loss is 0.044688835740089417\n",
      "epoch: 16 step: 558, loss is 0.06979195028543472\n",
      "epoch: 16 step: 559, loss is 0.005885642021894455\n",
      "epoch: 16 step: 560, loss is 0.14240381121635437\n",
      "epoch: 16 step: 561, loss is 0.006438118871301413\n",
      "epoch: 16 step: 562, loss is 0.019598785787820816\n",
      "epoch: 16 step: 563, loss is 0.017163582146167755\n",
      "epoch: 16 step: 564, loss is 0.00364436162635684\n",
      "epoch: 16 step: 565, loss is 0.0027189527172595263\n",
      "epoch: 16 step: 566, loss is 0.032955601811409\n",
      "epoch: 16 step: 567, loss is 0.095042385160923\n",
      "epoch: 16 step: 568, loss is 0.0012700635707005858\n",
      "epoch: 16 step: 569, loss is 0.00879740808159113\n",
      "epoch: 16 step: 570, loss is 0.0732433944940567\n",
      "epoch: 16 step: 571, loss is 0.03845015913248062\n",
      "epoch: 16 step: 572, loss is 0.0016777446726337075\n",
      "epoch: 16 step: 573, loss is 0.002553255995735526\n",
      "epoch: 16 step: 574, loss is 0.015308153815567493\n",
      "epoch: 16 step: 575, loss is 0.024947989732027054\n",
      "epoch: 16 step: 576, loss is 0.0197613388299942\n",
      "epoch: 16 step: 577, loss is 0.0011238286970183253\n",
      "epoch: 16 step: 578, loss is 0.010042980313301086\n",
      "epoch: 16 step: 579, loss is 0.011419064365327358\n",
      "epoch: 16 step: 580, loss is 0.003947766963392496\n",
      "epoch: 16 step: 581, loss is 0.004439000505954027\n",
      "epoch: 16 step: 582, loss is 0.006524847354739904\n",
      "epoch: 16 step: 583, loss is 0.02477109432220459\n",
      "epoch: 16 step: 584, loss is 0.017325421795248985\n",
      "epoch: 16 step: 585, loss is 0.04215563088655472\n",
      "epoch: 16 step: 586, loss is 0.12486305087804794\n",
      "epoch: 16 step: 587, loss is 0.01299669686704874\n",
      "epoch: 16 step: 588, loss is 0.024300845339894295\n",
      "epoch: 16 step: 589, loss is 0.009179346263408661\n",
      "epoch: 16 step: 590, loss is 0.0009254442993551493\n",
      "epoch: 16 step: 591, loss is 0.024240221828222275\n",
      "epoch: 16 step: 592, loss is 0.0025656158104538918\n",
      "epoch: 16 step: 593, loss is 0.02704026736319065\n",
      "epoch: 16 step: 594, loss is 0.004354285076260567\n",
      "epoch: 16 step: 595, loss is 0.03822660818696022\n",
      "epoch: 16 step: 596, loss is 0.010128478519618511\n",
      "epoch: 16 step: 597, loss is 0.012080530636012554\n",
      "epoch: 16 step: 598, loss is 0.0051247370429337025\n",
      "epoch: 16 step: 599, loss is 0.006093640346080065\n",
      "epoch: 16 step: 600, loss is 0.007469383999705315\n",
      "epoch: 16 step: 601, loss is 0.07657778263092041\n",
      "epoch: 16 step: 602, loss is 0.05562366545200348\n",
      "epoch: 16 step: 603, loss is 0.011536488309502602\n",
      "epoch: 16 step: 604, loss is 0.03079024702310562\n",
      "epoch: 16 step: 605, loss is 0.01324048824608326\n",
      "epoch: 16 step: 606, loss is 0.08183334022760391\n",
      "epoch: 16 step: 607, loss is 0.019606973975896835\n",
      "epoch: 16 step: 608, loss is 0.013379123993217945\n",
      "epoch: 16 step: 609, loss is 0.0019636417273432016\n",
      "epoch: 16 step: 610, loss is 0.03394748270511627\n",
      "epoch: 16 step: 611, loss is 0.023990044370293617\n",
      "epoch: 16 step: 612, loss is 0.07427909970283508\n",
      "epoch: 16 step: 613, loss is 0.009512647986412048\n",
      "epoch: 16 step: 614, loss is 0.013819258660078049\n",
      "epoch: 16 step: 615, loss is 0.006331229116767645\n",
      "epoch: 16 step: 616, loss is 0.004371174145489931\n",
      "epoch: 16 step: 617, loss is 0.01858219876885414\n",
      "epoch: 16 step: 618, loss is 0.009012244641780853\n",
      "epoch: 16 step: 619, loss is 0.009715774096548557\n",
      "epoch: 16 step: 620, loss is 0.009034727700054646\n",
      "epoch: 16 step: 621, loss is 0.00034406004124321043\n",
      "epoch: 16 step: 622, loss is 0.030798358842730522\n",
      "epoch: 16 step: 623, loss is 0.010904226452112198\n",
      "epoch: 16 step: 624, loss is 0.01408581156283617\n",
      "epoch: 16 step: 625, loss is 0.0435587503015995\n",
      "epoch: 16 step: 626, loss is 0.012879499234259129\n",
      "epoch: 16 step: 627, loss is 0.03434664011001587\n",
      "epoch: 16 step: 628, loss is 0.0008347352268174291\n",
      "epoch: 16 step: 629, loss is 0.06190478056669235\n",
      "epoch: 16 step: 630, loss is 0.025527194142341614\n",
      "epoch: 16 step: 631, loss is 0.006214717403054237\n",
      "epoch: 16 step: 632, loss is 0.01931534893810749\n",
      "epoch: 16 step: 633, loss is 0.06123596057295799\n",
      "epoch: 16 step: 634, loss is 0.008167282678186893\n",
      "epoch: 16 step: 635, loss is 0.010600583627820015\n",
      "epoch: 16 step: 636, loss is 0.0020681475289165974\n",
      "epoch: 16 step: 637, loss is 0.015183675102889538\n",
      "epoch: 16 step: 638, loss is 0.0069291978143155575\n",
      "epoch: 16 step: 639, loss is 0.00042569966171868145\n",
      "epoch: 16 step: 640, loss is 0.003453226527199149\n",
      "epoch: 16 step: 641, loss is 0.013898576609790325\n",
      "epoch: 16 step: 642, loss is 0.02137855626642704\n",
      "epoch: 16 step: 643, loss is 0.019741326570510864\n",
      "epoch: 16 step: 644, loss is 0.047016553580760956\n",
      "epoch: 16 step: 645, loss is 0.02873513661324978\n",
      "epoch: 16 step: 646, loss is 0.0319429486989975\n",
      "epoch: 16 step: 647, loss is 0.002508648671209812\n",
      "epoch: 16 step: 648, loss is 0.013612198643386364\n",
      "epoch: 16 step: 649, loss is 0.0572122223675251\n",
      "epoch: 16 step: 650, loss is 0.03924215957522392\n",
      "epoch: 16 step: 651, loss is 0.001726097776554525\n",
      "epoch: 16 step: 652, loss is 0.014528960920870304\n",
      "epoch: 16 step: 653, loss is 0.0014575839741155505\n",
      "epoch: 16 step: 654, loss is 0.035625558346509933\n",
      "epoch: 16 step: 655, loss is 0.10806931555271149\n",
      "epoch: 16 step: 656, loss is 0.004730454180389643\n",
      "epoch: 16 step: 657, loss is 0.007479446940124035\n",
      "epoch: 16 step: 658, loss is 0.0473015271127224\n",
      "epoch: 16 step: 659, loss is 0.032229747623205185\n",
      "epoch: 16 step: 660, loss is 0.023726316168904305\n",
      "epoch: 16 step: 661, loss is 0.003756257938221097\n",
      "epoch: 16 step: 662, loss is 0.020129187032580376\n",
      "epoch: 16 step: 663, loss is 0.017651615664362907\n",
      "epoch: 16 step: 664, loss is 0.03331631422042847\n",
      "epoch: 16 step: 665, loss is 0.0029704796615988016\n",
      "epoch: 16 step: 666, loss is 0.03212132304906845\n",
      "epoch: 16 step: 667, loss is 0.0220414437353611\n",
      "epoch: 16 step: 668, loss is 0.014631802216172218\n",
      "epoch: 16 step: 669, loss is 0.07143138349056244\n",
      "epoch: 16 step: 670, loss is 0.14609099924564362\n",
      "epoch: 16 step: 671, loss is 0.012321789748966694\n",
      "epoch: 16 step: 672, loss is 0.016270576044917107\n",
      "epoch: 16 step: 673, loss is 0.005375911481678486\n",
      "epoch: 16 step: 674, loss is 0.005745182279497385\n",
      "epoch: 16 step: 675, loss is 0.07107313722372055\n",
      "epoch: 16 step: 676, loss is 0.012372119352221489\n",
      "epoch: 16 step: 677, loss is 0.009640801697969437\n",
      "epoch: 16 step: 678, loss is 0.033142879605293274\n",
      "epoch: 16 step: 679, loss is 0.01707415282726288\n",
      "epoch: 16 step: 680, loss is 0.001172696822322905\n",
      "epoch: 16 step: 681, loss is 0.012847024947404861\n",
      "epoch: 16 step: 682, loss is 0.02177724428474903\n",
      "epoch: 16 step: 683, loss is 0.004007573705166578\n",
      "epoch: 16 step: 684, loss is 0.028844300657510757\n",
      "epoch: 16 step: 685, loss is 0.02576419897377491\n",
      "epoch: 16 step: 686, loss is 0.00989212840795517\n",
      "epoch: 16 step: 687, loss is 0.022168345749378204\n",
      "epoch: 16 step: 688, loss is 0.06078566983342171\n",
      "epoch: 16 step: 689, loss is 0.003721606684848666\n",
      "epoch: 16 step: 690, loss is 0.015900198370218277\n",
      "epoch: 16 step: 691, loss is 0.002445214195176959\n",
      "epoch: 16 step: 692, loss is 0.11145897209644318\n",
      "epoch: 16 step: 693, loss is 0.04629317298531532\n",
      "epoch: 16 step: 694, loss is 0.018450286239385605\n",
      "epoch: 16 step: 695, loss is 0.0036744102835655212\n",
      "epoch: 16 step: 696, loss is 0.029925966635346413\n",
      "epoch: 16 step: 697, loss is 0.019763972610235214\n",
      "epoch: 16 step: 698, loss is 0.002242238260805607\n",
      "epoch: 16 step: 699, loss is 0.004635545890778303\n",
      "epoch: 16 step: 700, loss is 0.04402662441134453\n",
      "epoch: 16 step: 701, loss is 0.06712865084409714\n",
      "epoch: 16 step: 702, loss is 0.015372936613857746\n",
      "epoch: 16 step: 703, loss is 0.019892752170562744\n",
      "epoch: 16 step: 704, loss is 0.0037852926179766655\n",
      "epoch: 16 step: 705, loss is 0.00395148666575551\n",
      "epoch: 16 step: 706, loss is 0.1253594309091568\n",
      "epoch: 16 step: 707, loss is 0.03842541575431824\n",
      "epoch: 16 step: 708, loss is 0.011564294807612896\n",
      "epoch: 16 step: 709, loss is 0.001017877017147839\n",
      "epoch: 16 step: 710, loss is 0.01144162006676197\n",
      "epoch: 16 step: 711, loss is 0.01090311724692583\n",
      "epoch: 16 step: 712, loss is 0.003920102491974831\n",
      "epoch: 16 step: 713, loss is 0.0016875421861186624\n",
      "epoch: 16 step: 714, loss is 0.050774890929460526\n",
      "epoch: 16 step: 715, loss is 0.18363657593727112\n",
      "epoch: 16 step: 716, loss is 0.00160865462385118\n",
      "epoch: 16 step: 717, loss is 0.011162475682795048\n",
      "epoch: 16 step: 718, loss is 0.19512377679347992\n",
      "epoch: 16 step: 719, loss is 0.008138468489050865\n",
      "epoch: 16 step: 720, loss is 0.024274058640003204\n",
      "epoch: 16 step: 721, loss is 0.02044510841369629\n",
      "epoch: 16 step: 722, loss is 0.04559289291501045\n",
      "epoch: 16 step: 723, loss is 0.016148703172802925\n",
      "epoch: 16 step: 724, loss is 0.031025510281324387\n",
      "epoch: 16 step: 725, loss is 0.047230519354343414\n",
      "epoch: 16 step: 726, loss is 0.015503146685659885\n",
      "epoch: 16 step: 727, loss is 0.005624203011393547\n",
      "epoch: 16 step: 728, loss is 0.01749792881309986\n",
      "epoch: 16 step: 729, loss is 0.014039070345461369\n",
      "epoch: 16 step: 730, loss is 0.0017389015993103385\n",
      "epoch: 16 step: 731, loss is 0.011820539832115173\n",
      "epoch: 16 step: 732, loss is 0.01810138300061226\n",
      "epoch: 16 step: 733, loss is 0.1640716940164566\n",
      "epoch: 16 step: 734, loss is 0.06740029901266098\n",
      "epoch: 16 step: 735, loss is 0.06436827033758163\n",
      "epoch: 16 step: 736, loss is 0.018975816667079926\n",
      "epoch: 16 step: 737, loss is 0.06203527748584747\n",
      "epoch: 16 step: 738, loss is 0.1561935991048813\n",
      "epoch: 16 step: 739, loss is 0.0037232039030641317\n",
      "epoch: 16 step: 740, loss is 0.005285376682877541\n",
      "epoch: 16 step: 741, loss is 0.03358754515647888\n",
      "epoch: 16 step: 742, loss is 0.049969382584095\n",
      "epoch: 16 step: 743, loss is 0.003679919056594372\n",
      "epoch: 16 step: 744, loss is 0.040016479790210724\n",
      "epoch: 16 step: 745, loss is 0.02003411017358303\n",
      "epoch: 16 step: 746, loss is 0.005104484502226114\n",
      "epoch: 16 step: 747, loss is 0.013712006621062756\n",
      "epoch: 16 step: 748, loss is 0.03969743847846985\n",
      "epoch: 16 step: 749, loss is 0.15628598630428314\n",
      "epoch: 16 step: 750, loss is 0.012251312844455242\n",
      "epoch: 16 step: 751, loss is 0.002220962895080447\n",
      "epoch: 16 step: 752, loss is 0.0016098100459203124\n",
      "epoch: 16 step: 753, loss is 0.040959037840366364\n",
      "epoch: 16 step: 754, loss is 0.09349414706230164\n",
      "epoch: 16 step: 755, loss is 0.002354285679757595\n",
      "epoch: 16 step: 756, loss is 0.019555918872356415\n",
      "epoch: 16 step: 757, loss is 0.0208362378180027\n",
      "epoch: 16 step: 758, loss is 0.024456754326820374\n",
      "epoch: 16 step: 759, loss is 0.017352335155010223\n",
      "epoch: 16 step: 760, loss is 0.12810316681861877\n",
      "epoch: 16 step: 761, loss is 0.030670911073684692\n",
      "epoch: 16 step: 762, loss is 0.0022580104414373636\n",
      "epoch: 16 step: 763, loss is 0.004928043577820063\n",
      "epoch: 16 step: 764, loss is 0.0659031942486763\n",
      "epoch: 16 step: 765, loss is 0.006450895685702562\n",
      "epoch: 16 step: 766, loss is 0.011364014819264412\n",
      "epoch: 16 step: 767, loss is 0.006474763620644808\n",
      "epoch: 16 step: 768, loss is 0.01445531751960516\n",
      "epoch: 16 step: 769, loss is 0.006993933115154505\n",
      "epoch: 16 step: 770, loss is 0.09958144277334213\n",
      "epoch: 16 step: 771, loss is 0.00953505840152502\n",
      "epoch: 16 step: 772, loss is 0.017382925376296043\n",
      "epoch: 16 step: 773, loss is 0.020663045346736908\n",
      "epoch: 16 step: 774, loss is 0.04514122009277344\n",
      "epoch: 16 step: 775, loss is 0.018060553818941116\n",
      "epoch: 16 step: 776, loss is 0.0052513680420815945\n",
      "epoch: 16 step: 777, loss is 0.010894976556301117\n",
      "epoch: 16 step: 778, loss is 0.017600974068045616\n",
      "epoch: 16 step: 779, loss is 0.03983072191476822\n",
      "epoch: 16 step: 780, loss is 0.006836698856204748\n",
      "epoch: 16 step: 781, loss is 0.030284620821475983\n",
      "epoch: 16 step: 782, loss is 0.007217736914753914\n",
      "epoch: 16 step: 783, loss is 0.008891475386917591\n",
      "epoch: 16 step: 784, loss is 0.017286865040659904\n",
      "epoch: 16 step: 785, loss is 0.0015120983589440584\n",
      "epoch: 16 step: 786, loss is 0.0019249282777309418\n",
      "epoch: 16 step: 787, loss is 0.07909874618053436\n",
      "epoch: 16 step: 788, loss is 0.08835244923830032\n",
      "epoch: 16 step: 789, loss is 0.07350032776594162\n",
      "epoch: 16 step: 790, loss is 0.0559040866792202\n",
      "epoch: 16 step: 791, loss is 0.062387898564338684\n",
      "epoch: 16 step: 792, loss is 0.007814359851181507\n",
      "epoch: 16 step: 793, loss is 0.07928834855556488\n",
      "epoch: 16 step: 794, loss is 0.012201368808746338\n",
      "epoch: 16 step: 795, loss is 0.006865452975034714\n",
      "epoch: 16 step: 796, loss is 0.026838673278689384\n",
      "epoch: 16 step: 797, loss is 0.022992245852947235\n",
      "epoch: 16 step: 798, loss is 0.007270775269716978\n",
      "epoch: 16 step: 799, loss is 0.013137574307620525\n",
      "epoch: 16 step: 800, loss is 0.054765332490205765\n",
      "epoch: 16 step: 801, loss is 0.005698062479496002\n",
      "epoch: 16 step: 802, loss is 0.00473132124170661\n",
      "epoch: 16 step: 803, loss is 0.024695945903658867\n",
      "epoch: 16 step: 804, loss is 0.01826353929936886\n",
      "epoch: 16 step: 805, loss is 0.13038673996925354\n",
      "epoch: 16 step: 806, loss is 0.00029017956694588065\n",
      "epoch: 16 step: 807, loss is 0.08165118098258972\n",
      "epoch: 16 step: 808, loss is 0.0410374291241169\n",
      "epoch: 16 step: 809, loss is 0.046586304903030396\n",
      "epoch: 16 step: 810, loss is 0.017590831965208054\n",
      "epoch: 16 step: 811, loss is 0.020713502541184425\n",
      "epoch: 16 step: 812, loss is 0.005465899594128132\n",
      "epoch: 16 step: 813, loss is 0.0026865117251873016\n",
      "epoch: 16 step: 814, loss is 0.029486441984772682\n",
      "epoch: 16 step: 815, loss is 0.008420243859291077\n",
      "epoch: 16 step: 816, loss is 0.049195464700460434\n",
      "epoch: 16 step: 817, loss is 0.023210464045405388\n",
      "epoch: 16 step: 818, loss is 0.02579408511519432\n",
      "epoch: 16 step: 819, loss is 0.006802924908697605\n",
      "epoch: 16 step: 820, loss is 0.01885228417813778\n",
      "epoch: 16 step: 821, loss is 0.014310408383607864\n",
      "epoch: 16 step: 822, loss is 0.012643031775951385\n",
      "epoch: 16 step: 823, loss is 0.015440844930708408\n",
      "epoch: 16 step: 824, loss is 0.04644003137946129\n",
      "epoch: 16 step: 825, loss is 0.004865015856921673\n",
      "epoch: 16 step: 826, loss is 0.07207184284925461\n",
      "epoch: 16 step: 827, loss is 0.0862254947423935\n",
      "epoch: 16 step: 828, loss is 0.05341313034296036\n",
      "epoch: 16 step: 829, loss is 0.02680710144340992\n",
      "epoch: 16 step: 830, loss is 0.06677784770727158\n",
      "epoch: 16 step: 831, loss is 0.007669306825846434\n",
      "epoch: 16 step: 832, loss is 0.00186691724229604\n",
      "epoch: 16 step: 833, loss is 0.023928357288241386\n",
      "epoch: 16 step: 834, loss is 0.008991778828203678\n",
      "epoch: 16 step: 835, loss is 0.06530186533927917\n",
      "epoch: 16 step: 836, loss is 0.024935603141784668\n",
      "epoch: 16 step: 837, loss is 0.01044483482837677\n",
      "epoch: 16 step: 838, loss is 0.008620217442512512\n",
      "epoch: 16 step: 839, loss is 0.002548563526943326\n",
      "epoch: 16 step: 840, loss is 0.028222491964697838\n",
      "epoch: 16 step: 841, loss is 0.029620308429002762\n",
      "epoch: 16 step: 842, loss is 0.007492811419069767\n",
      "epoch: 16 step: 843, loss is 0.0017808667616918683\n",
      "epoch: 16 step: 844, loss is 0.01918669044971466\n",
      "epoch: 16 step: 845, loss is 0.005930905230343342\n",
      "epoch: 16 step: 846, loss is 0.004375430755317211\n",
      "epoch: 16 step: 847, loss is 0.007376571651548147\n",
      "epoch: 16 step: 848, loss is 0.00829704012721777\n",
      "epoch: 16 step: 849, loss is 0.07979977130889893\n",
      "epoch: 16 step: 850, loss is 0.00838345754891634\n",
      "epoch: 16 step: 851, loss is 0.0004489957937039435\n",
      "epoch: 16 step: 852, loss is 0.003922551404684782\n",
      "epoch: 16 step: 853, loss is 0.013954546302556992\n",
      "epoch: 16 step: 854, loss is 0.008950472809374332\n",
      "epoch: 16 step: 855, loss is 0.002264592098072171\n",
      "epoch: 16 step: 856, loss is 0.007128259167075157\n",
      "epoch: 16 step: 857, loss is 0.03348345309495926\n",
      "epoch: 16 step: 858, loss is 0.0655006468296051\n",
      "epoch: 16 step: 859, loss is 0.022835003212094307\n",
      "epoch: 16 step: 860, loss is 0.006030609365552664\n",
      "epoch: 16 step: 861, loss is 0.0021531323436647654\n",
      "epoch: 16 step: 862, loss is 0.05606318637728691\n",
      "epoch: 16 step: 863, loss is 0.02375805750489235\n",
      "epoch: 16 step: 864, loss is 0.034879181534051895\n",
      "epoch: 16 step: 865, loss is 0.02781878039240837\n",
      "epoch: 16 step: 866, loss is 0.0005659313756041229\n",
      "epoch: 16 step: 867, loss is 0.0054743424989283085\n",
      "epoch: 16 step: 868, loss is 0.004073031712323427\n",
      "epoch: 16 step: 869, loss is 0.001301714451983571\n",
      "epoch: 16 step: 870, loss is 0.02610492892563343\n",
      "epoch: 16 step: 871, loss is 0.050055988132953644\n",
      "epoch: 16 step: 872, loss is 0.0011694859713315964\n",
      "epoch: 16 step: 873, loss is 0.00445500947535038\n",
      "epoch: 16 step: 874, loss is 0.0248322281986475\n",
      "epoch: 16 step: 875, loss is 0.0003258749784436077\n",
      "epoch: 16 step: 876, loss is 0.0035188994370400906\n",
      "epoch: 16 step: 877, loss is 0.06456520408391953\n",
      "epoch: 16 step: 878, loss is 0.01228042971342802\n",
      "epoch: 16 step: 879, loss is 0.020988550037145615\n",
      "epoch: 16 step: 880, loss is 0.013009573332965374\n",
      "epoch: 16 step: 881, loss is 0.08657814562320709\n",
      "epoch: 16 step: 882, loss is 0.005003768485039473\n",
      "epoch: 16 step: 883, loss is 0.08081376552581787\n",
      "epoch: 16 step: 884, loss is 0.004428287036716938\n",
      "epoch: 16 step: 885, loss is 0.006118752993643284\n",
      "epoch: 16 step: 886, loss is 0.05030115321278572\n",
      "epoch: 16 step: 887, loss is 0.011950841173529625\n",
      "epoch: 16 step: 888, loss is 0.044098708778619766\n",
      "epoch: 16 step: 889, loss is 0.015686964616179466\n",
      "epoch: 16 step: 890, loss is 0.012425709515810013\n",
      "epoch: 16 step: 891, loss is 0.052222512662410736\n",
      "epoch: 16 step: 892, loss is 0.02486572600901127\n",
      "epoch: 16 step: 893, loss is 0.013497335836291313\n",
      "epoch: 16 step: 894, loss is 0.004066087305545807\n",
      "epoch: 16 step: 895, loss is 0.05658111348748207\n",
      "epoch: 16 step: 896, loss is 0.012682676315307617\n",
      "epoch: 16 step: 897, loss is 0.00675481092184782\n",
      "epoch: 16 step: 898, loss is 0.0007988330326043069\n",
      "epoch: 16 step: 899, loss is 0.006125670857727528\n",
      "epoch: 16 step: 900, loss is 0.03694014623761177\n",
      "epoch: 16 step: 901, loss is 0.05175570771098137\n",
      "epoch: 16 step: 902, loss is 0.04447164386510849\n",
      "epoch: 16 step: 903, loss is 0.017927836626768112\n",
      "epoch: 16 step: 904, loss is 0.05995842441916466\n",
      "epoch: 16 step: 905, loss is 0.002295112470164895\n",
      "epoch: 16 step: 906, loss is 0.055672649294137955\n",
      "epoch: 16 step: 907, loss is 0.00535449618473649\n",
      "epoch: 16 step: 908, loss is 0.09956109523773193\n",
      "epoch: 16 step: 909, loss is 0.015279368497431278\n",
      "epoch: 16 step: 910, loss is 0.008931347168982029\n",
      "epoch: 16 step: 911, loss is 0.017414068803191185\n",
      "epoch: 16 step: 912, loss is 0.007294645067304373\n",
      "epoch: 16 step: 913, loss is 0.0039061084389686584\n",
      "epoch: 16 step: 914, loss is 0.04361005872488022\n",
      "epoch: 16 step: 915, loss is 0.0035032974556088448\n",
      "epoch: 16 step: 916, loss is 0.043572310358285904\n",
      "epoch: 16 step: 917, loss is 0.0023673572577536106\n",
      "epoch: 16 step: 918, loss is 0.025370683521032333\n",
      "epoch: 16 step: 919, loss is 0.032948046922683716\n",
      "epoch: 16 step: 920, loss is 0.01258475985378027\n",
      "epoch: 16 step: 921, loss is 0.04076939448714256\n",
      "epoch: 16 step: 922, loss is 0.05902717262506485\n",
      "epoch: 16 step: 923, loss is 0.002229903591796756\n",
      "epoch: 16 step: 924, loss is 0.02816021256148815\n",
      "epoch: 16 step: 925, loss is 0.010888640768826008\n",
      "epoch: 16 step: 926, loss is 0.0073226746171712875\n",
      "epoch: 16 step: 927, loss is 0.07064345479011536\n",
      "epoch: 16 step: 928, loss is 0.05765324831008911\n",
      "epoch: 16 step: 929, loss is 0.10401211678981781\n",
      "epoch: 16 step: 930, loss is 0.027441481128335\n",
      "epoch: 16 step: 931, loss is 0.04225822538137436\n",
      "epoch: 16 step: 932, loss is 0.011259892024099827\n",
      "epoch: 16 step: 933, loss is 0.04017348960042\n",
      "epoch: 16 step: 934, loss is 0.05493936315178871\n",
      "epoch: 16 step: 935, loss is 0.004707886837422848\n",
      "epoch: 16 step: 936, loss is 0.00304006552323699\n",
      "epoch: 16 step: 937, loss is 0.009113921783864498\n",
      "epoch: 17 step: 1, loss is 0.003810201771557331\n",
      "epoch: 17 step: 2, loss is 0.0035468710120767355\n",
      "epoch: 17 step: 3, loss is 0.051356032490730286\n",
      "epoch: 17 step: 4, loss is 0.00037828562199138105\n",
      "epoch: 17 step: 5, loss is 0.004893854726105928\n",
      "epoch: 17 step: 6, loss is 0.0481768362224102\n",
      "epoch: 17 step: 7, loss is 0.06648873537778854\n",
      "epoch: 17 step: 8, loss is 0.056422051042318344\n",
      "epoch: 17 step: 9, loss is 0.0014370839344337583\n",
      "epoch: 17 step: 10, loss is 0.020333267748355865\n",
      "epoch: 17 step: 11, loss is 0.002855343045666814\n",
      "epoch: 17 step: 12, loss is 0.023948494344949722\n",
      "epoch: 17 step: 13, loss is 0.009257452562451363\n",
      "epoch: 17 step: 14, loss is 0.027746427804231644\n",
      "epoch: 17 step: 15, loss is 0.022526713088154793\n",
      "epoch: 17 step: 16, loss is 0.03198739513754845\n",
      "epoch: 17 step: 17, loss is 0.01093263179063797\n",
      "epoch: 17 step: 18, loss is 0.02271701954305172\n",
      "epoch: 17 step: 19, loss is 0.003691562917083502\n",
      "epoch: 17 step: 20, loss is 0.027434799820184708\n",
      "epoch: 17 step: 21, loss is 0.005431315861642361\n",
      "epoch: 17 step: 22, loss is 0.004802466835826635\n",
      "epoch: 17 step: 23, loss is 0.003755436046048999\n",
      "epoch: 17 step: 24, loss is 0.014337209053337574\n",
      "epoch: 17 step: 25, loss is 0.008536871522665024\n",
      "epoch: 17 step: 26, loss is 0.03193745017051697\n",
      "epoch: 17 step: 27, loss is 0.003470746800303459\n",
      "epoch: 17 step: 28, loss is 0.01156292762607336\n",
      "epoch: 17 step: 29, loss is 0.006871941965073347\n",
      "epoch: 17 step: 30, loss is 0.001244380371645093\n",
      "epoch: 17 step: 31, loss is 0.03949056193232536\n",
      "epoch: 17 step: 32, loss is 0.06560558080673218\n",
      "epoch: 17 step: 33, loss is 0.002837961306795478\n",
      "epoch: 17 step: 34, loss is 0.07738912850618362\n",
      "epoch: 17 step: 35, loss is 0.004320516716688871\n",
      "epoch: 17 step: 36, loss is 0.046203553676605225\n",
      "epoch: 17 step: 37, loss is 0.0006456078262999654\n",
      "epoch: 17 step: 38, loss is 0.005217327270656824\n",
      "epoch: 17 step: 39, loss is 0.011106015183031559\n",
      "epoch: 17 step: 40, loss is 0.008066218346357346\n",
      "epoch: 17 step: 41, loss is 0.18437416851520538\n",
      "epoch: 17 step: 42, loss is 0.004618485923856497\n",
      "epoch: 17 step: 43, loss is 0.037757448852062225\n",
      "epoch: 17 step: 44, loss is 0.018173225224018097\n",
      "epoch: 17 step: 45, loss is 0.07910341024398804\n",
      "epoch: 17 step: 46, loss is 0.05339110642671585\n",
      "epoch: 17 step: 47, loss is 0.002878996543586254\n",
      "epoch: 17 step: 48, loss is 0.023240871727466583\n",
      "epoch: 17 step: 49, loss is 0.03367435559630394\n",
      "epoch: 17 step: 50, loss is 0.00825364701449871\n",
      "epoch: 17 step: 51, loss is 0.02477072924375534\n",
      "epoch: 17 step: 52, loss is 0.04201493412256241\n",
      "epoch: 17 step: 53, loss is 0.024265095591545105\n",
      "epoch: 17 step: 54, loss is 0.03646561875939369\n",
      "epoch: 17 step: 55, loss is 0.01354486308991909\n",
      "epoch: 17 step: 56, loss is 0.01484527625143528\n",
      "epoch: 17 step: 57, loss is 0.02889612689614296\n",
      "epoch: 17 step: 58, loss is 0.0020913041662424803\n",
      "epoch: 17 step: 59, loss is 0.007252046838402748\n",
      "epoch: 17 step: 60, loss is 0.007107531651854515\n",
      "epoch: 17 step: 61, loss is 0.0044683064334094524\n",
      "epoch: 17 step: 62, loss is 0.04448925703763962\n",
      "epoch: 17 step: 63, loss is 0.013037762604653835\n",
      "epoch: 17 step: 64, loss is 0.004577843006700277\n",
      "epoch: 17 step: 65, loss is 0.09862881153821945\n",
      "epoch: 17 step: 66, loss is 0.0013173240004107356\n",
      "epoch: 17 step: 67, loss is 0.018425436690449715\n",
      "epoch: 17 step: 68, loss is 0.1643722802400589\n",
      "epoch: 17 step: 69, loss is 0.03756250441074371\n",
      "epoch: 17 step: 70, loss is 0.02062048949301243\n",
      "epoch: 17 step: 71, loss is 0.021072255447506905\n",
      "epoch: 17 step: 72, loss is 0.04180850833654404\n",
      "epoch: 17 step: 73, loss is 0.003949679434299469\n",
      "epoch: 17 step: 74, loss is 0.0026803254149854183\n",
      "epoch: 17 step: 75, loss is 0.04230887442827225\n",
      "epoch: 17 step: 76, loss is 0.03625661879777908\n",
      "epoch: 17 step: 77, loss is 0.06684282422065735\n",
      "epoch: 17 step: 78, loss is 0.012939119711518288\n",
      "epoch: 17 step: 79, loss is 0.0022700256668031216\n",
      "epoch: 17 step: 80, loss is 0.0019158711656928062\n",
      "epoch: 17 step: 81, loss is 0.0031936615705490112\n",
      "epoch: 17 step: 82, loss is 0.027743464335799217\n",
      "epoch: 17 step: 83, loss is 0.012395160272717476\n",
      "epoch: 17 step: 84, loss is 0.0020495052449405193\n",
      "epoch: 17 step: 85, loss is 0.009338202886283398\n",
      "epoch: 17 step: 86, loss is 0.004553504753857851\n",
      "epoch: 17 step: 87, loss is 0.004416245501488447\n",
      "epoch: 17 step: 88, loss is 0.008287683129310608\n",
      "epoch: 17 step: 89, loss is 0.03977194055914879\n",
      "epoch: 17 step: 90, loss is 0.005739428102970123\n",
      "epoch: 17 step: 91, loss is 0.01608005166053772\n",
      "epoch: 17 step: 92, loss is 0.05446455627679825\n",
      "epoch: 17 step: 93, loss is 0.04759838432073593\n",
      "epoch: 17 step: 94, loss is 0.004264947026968002\n",
      "epoch: 17 step: 95, loss is 0.005655164830386639\n",
      "epoch: 17 step: 96, loss is 0.005617306102067232\n",
      "epoch: 17 step: 97, loss is 0.08016907423734665\n",
      "epoch: 17 step: 98, loss is 0.015971219167113304\n",
      "epoch: 17 step: 99, loss is 0.013305265456438065\n",
      "epoch: 17 step: 100, loss is 0.01459032017737627\n",
      "epoch: 17 step: 101, loss is 0.006531530525535345\n",
      "epoch: 17 step: 102, loss is 0.048088204115629196\n",
      "epoch: 17 step: 103, loss is 0.009624563157558441\n",
      "epoch: 17 step: 104, loss is 0.022123930975794792\n",
      "epoch: 17 step: 105, loss is 0.0014514658832922578\n",
      "epoch: 17 step: 106, loss is 0.029853416606783867\n",
      "epoch: 17 step: 107, loss is 0.007378648035228252\n",
      "epoch: 17 step: 108, loss is 0.006372368428856134\n",
      "epoch: 17 step: 109, loss is 0.029563793912529945\n",
      "epoch: 17 step: 110, loss is 0.009729172103106976\n",
      "epoch: 17 step: 111, loss is 0.008694791235029697\n",
      "epoch: 17 step: 112, loss is 0.01373320072889328\n",
      "epoch: 17 step: 113, loss is 0.001728943083435297\n",
      "epoch: 17 step: 114, loss is 0.003889969317242503\n",
      "epoch: 17 step: 115, loss is 0.0035893816966563463\n",
      "epoch: 17 step: 116, loss is 0.008746477775275707\n",
      "epoch: 17 step: 117, loss is 0.01022177841514349\n",
      "epoch: 17 step: 118, loss is 0.018617399036884308\n",
      "epoch: 17 step: 119, loss is 0.03470851108431816\n",
      "epoch: 17 step: 120, loss is 0.000611713738180697\n",
      "epoch: 17 step: 121, loss is 0.032397862523794174\n",
      "epoch: 17 step: 122, loss is 0.005611842032521963\n",
      "epoch: 17 step: 123, loss is 0.03021661378443241\n",
      "epoch: 17 step: 124, loss is 0.002139567630365491\n",
      "epoch: 17 step: 125, loss is 0.09181927889585495\n",
      "epoch: 17 step: 126, loss is 0.014667334035038948\n",
      "epoch: 17 step: 127, loss is 0.0023734578862786293\n",
      "epoch: 17 step: 128, loss is 0.021707070991396904\n",
      "epoch: 17 step: 129, loss is 0.0009667994454503059\n",
      "epoch: 17 step: 130, loss is 0.002300916239619255\n",
      "epoch: 17 step: 131, loss is 0.002195585984736681\n",
      "epoch: 17 step: 132, loss is 0.015798795968294144\n",
      "epoch: 17 step: 133, loss is 0.07759849727153778\n",
      "epoch: 17 step: 134, loss is 0.006750715430825949\n",
      "epoch: 17 step: 135, loss is 0.0004888074472546577\n",
      "epoch: 17 step: 136, loss is 0.0011828094720840454\n",
      "epoch: 17 step: 137, loss is 0.027051914483308792\n",
      "epoch: 17 step: 138, loss is 0.0020652813836932182\n",
      "epoch: 17 step: 139, loss is 0.06561220437288284\n",
      "epoch: 17 step: 140, loss is 0.01581553742289543\n",
      "epoch: 17 step: 141, loss is 0.026877611875534058\n",
      "epoch: 17 step: 142, loss is 0.003099606139585376\n",
      "epoch: 17 step: 143, loss is 0.0008876711945049465\n",
      "epoch: 17 step: 144, loss is 0.0031959961634129286\n",
      "epoch: 17 step: 145, loss is 0.008896457962691784\n",
      "epoch: 17 step: 146, loss is 0.011688492260873318\n",
      "epoch: 17 step: 147, loss is 0.06264986842870712\n",
      "epoch: 17 step: 148, loss is 0.01529739797115326\n",
      "epoch: 17 step: 149, loss is 0.006463626399636269\n",
      "epoch: 17 step: 150, loss is 0.011831195093691349\n",
      "epoch: 17 step: 151, loss is 0.01933908276259899\n",
      "epoch: 17 step: 152, loss is 0.006778922863304615\n",
      "epoch: 17 step: 153, loss is 0.006224214099347591\n",
      "epoch: 17 step: 154, loss is 0.003503012703731656\n",
      "epoch: 17 step: 155, loss is 0.0024454521480947733\n",
      "epoch: 17 step: 156, loss is 0.022462857887148857\n",
      "epoch: 17 step: 157, loss is 0.033632390201091766\n",
      "epoch: 17 step: 158, loss is 0.047111090272665024\n",
      "epoch: 17 step: 159, loss is 0.01967216655611992\n",
      "epoch: 17 step: 160, loss is 0.004348199814558029\n",
      "epoch: 17 step: 161, loss is 0.081453338265419\n",
      "epoch: 17 step: 162, loss is 0.10014031827449799\n",
      "epoch: 17 step: 163, loss is 0.005462618079036474\n",
      "epoch: 17 step: 164, loss is 0.004067340865731239\n",
      "epoch: 17 step: 165, loss is 0.02913803793489933\n",
      "epoch: 17 step: 166, loss is 0.02323903515934944\n",
      "epoch: 17 step: 167, loss is 0.016300471499562263\n",
      "epoch: 17 step: 168, loss is 0.01238554622977972\n",
      "epoch: 17 step: 169, loss is 0.049918510019779205\n",
      "epoch: 17 step: 170, loss is 0.0003820255515165627\n",
      "epoch: 17 step: 171, loss is 0.00142658909317106\n",
      "epoch: 17 step: 172, loss is 0.0036643657367676497\n",
      "epoch: 17 step: 173, loss is 0.0007110986043699086\n",
      "epoch: 17 step: 174, loss is 0.00307607464492321\n",
      "epoch: 17 step: 175, loss is 0.008407355286180973\n",
      "epoch: 17 step: 176, loss is 0.0058464123867452145\n",
      "epoch: 17 step: 177, loss is 0.006301193963736296\n",
      "epoch: 17 step: 178, loss is 0.010748346336185932\n",
      "epoch: 17 step: 179, loss is 0.11822041124105453\n",
      "epoch: 17 step: 180, loss is 0.004416207782924175\n",
      "epoch: 17 step: 181, loss is 0.003111539874225855\n",
      "epoch: 17 step: 182, loss is 0.005691328551620245\n",
      "epoch: 17 step: 183, loss is 0.0023832020815461874\n",
      "epoch: 17 step: 184, loss is 0.00974896177649498\n",
      "epoch: 17 step: 185, loss is 0.0049740020185709\n",
      "epoch: 17 step: 186, loss is 0.01382012665271759\n",
      "epoch: 17 step: 187, loss is 0.0031127838883548975\n",
      "epoch: 17 step: 188, loss is 0.006370665971189737\n",
      "epoch: 17 step: 189, loss is 0.011403094977140427\n",
      "epoch: 17 step: 190, loss is 0.0011407765559852123\n",
      "epoch: 17 step: 191, loss is 0.026732774451375008\n",
      "epoch: 17 step: 192, loss is 0.0010510135907679796\n",
      "epoch: 17 step: 193, loss is 0.010572592727839947\n",
      "epoch: 17 step: 194, loss is 0.0006084782653488219\n",
      "epoch: 17 step: 195, loss is 0.01420880388468504\n",
      "epoch: 17 step: 196, loss is 0.005711302161216736\n",
      "epoch: 17 step: 197, loss is 0.00181459984742105\n",
      "epoch: 17 step: 198, loss is 0.025554800406098366\n",
      "epoch: 17 step: 199, loss is 0.0015650053974241018\n",
      "epoch: 17 step: 200, loss is 0.022714901715517044\n",
      "epoch: 17 step: 201, loss is 0.010009231977164745\n",
      "epoch: 17 step: 202, loss is 0.040059637278318405\n",
      "epoch: 17 step: 203, loss is 0.0013707020552828908\n",
      "epoch: 17 step: 204, loss is 0.011575611308217049\n",
      "epoch: 17 step: 205, loss is 0.002895632991567254\n",
      "epoch: 17 step: 206, loss is 0.018710119649767876\n",
      "epoch: 17 step: 207, loss is 0.0008212212705984712\n",
      "epoch: 17 step: 208, loss is 0.03200596943497658\n",
      "epoch: 17 step: 209, loss is 0.002300861058756709\n",
      "epoch: 17 step: 210, loss is 0.024800894781947136\n",
      "epoch: 17 step: 211, loss is 0.025752224028110504\n",
      "epoch: 17 step: 212, loss is 0.0007116839406080544\n",
      "epoch: 17 step: 213, loss is 0.008499274030327797\n",
      "epoch: 17 step: 214, loss is 0.013388614170253277\n",
      "epoch: 17 step: 215, loss is 0.0030762257520109415\n",
      "epoch: 17 step: 216, loss is 0.004870267119258642\n",
      "epoch: 17 step: 217, loss is 0.006974009331315756\n",
      "epoch: 17 step: 218, loss is 0.006945625878870487\n",
      "epoch: 17 step: 219, loss is 0.0021621540654450655\n",
      "epoch: 17 step: 220, loss is 0.001468098722398281\n",
      "epoch: 17 step: 221, loss is 0.0054552615620195866\n",
      "epoch: 17 step: 222, loss is 0.0016368927899748087\n",
      "epoch: 17 step: 223, loss is 0.03279859200119972\n",
      "epoch: 17 step: 224, loss is 0.020795322954654694\n",
      "epoch: 17 step: 225, loss is 0.009928970597684383\n",
      "epoch: 17 step: 226, loss is 0.006430957932025194\n",
      "epoch: 17 step: 227, loss is 0.012354489415884018\n",
      "epoch: 17 step: 228, loss is 0.0198663342744112\n",
      "epoch: 17 step: 229, loss is 0.0009711611201055348\n",
      "epoch: 17 step: 230, loss is 0.0006005470640957355\n",
      "epoch: 17 step: 231, loss is 0.009355377405881882\n",
      "epoch: 17 step: 232, loss is 0.005511206109076738\n",
      "epoch: 17 step: 233, loss is 0.006462665740400553\n",
      "epoch: 17 step: 234, loss is 0.02774362452328205\n",
      "epoch: 17 step: 235, loss is 6.892066448926926e-05\n",
      "epoch: 17 step: 236, loss is 0.0029003291856497526\n",
      "epoch: 17 step: 237, loss is 0.021414732560515404\n",
      "epoch: 17 step: 238, loss is 0.024336382746696472\n",
      "epoch: 17 step: 239, loss is 0.012332456186413765\n",
      "epoch: 17 step: 240, loss is 0.0030769817531108856\n",
      "epoch: 17 step: 241, loss is 0.000548546842765063\n",
      "epoch: 17 step: 242, loss is 0.0010722159640863538\n",
      "epoch: 17 step: 243, loss is 0.0007618185481987894\n",
      "epoch: 17 step: 244, loss is 0.004421292804181576\n",
      "epoch: 17 step: 245, loss is 0.06308206170797348\n",
      "epoch: 17 step: 246, loss is 0.025990406051278114\n",
      "epoch: 17 step: 247, loss is 0.009293044917285442\n",
      "epoch: 17 step: 248, loss is 0.006157138384878635\n",
      "epoch: 17 step: 249, loss is 0.00811723992228508\n",
      "epoch: 17 step: 250, loss is 0.010576536878943443\n",
      "epoch: 17 step: 251, loss is 0.007414382882416248\n",
      "epoch: 17 step: 252, loss is 0.03440993279218674\n",
      "epoch: 17 step: 253, loss is 0.10888291150331497\n",
      "epoch: 17 step: 254, loss is 0.02602381817996502\n",
      "epoch: 17 step: 255, loss is 0.01928388886153698\n",
      "epoch: 17 step: 256, loss is 0.013678822666406631\n",
      "epoch: 17 step: 257, loss is 9.914298425428569e-05\n",
      "epoch: 17 step: 258, loss is 0.00014788286352995783\n",
      "epoch: 17 step: 259, loss is 0.0006791178602725267\n",
      "epoch: 17 step: 260, loss is 0.027835005894303322\n",
      "epoch: 17 step: 261, loss is 0.03665320947766304\n",
      "epoch: 17 step: 262, loss is 0.004399522207677364\n",
      "epoch: 17 step: 263, loss is 0.025987906381487846\n",
      "epoch: 17 step: 264, loss is 0.0021967017091810703\n",
      "epoch: 17 step: 265, loss is 0.0030948929488658905\n",
      "epoch: 17 step: 266, loss is 0.04086128622293472\n",
      "epoch: 17 step: 267, loss is 0.0023027481511235237\n",
      "epoch: 17 step: 268, loss is 0.002705431543290615\n",
      "epoch: 17 step: 269, loss is 0.003851282875984907\n",
      "epoch: 17 step: 270, loss is 0.13721929490566254\n",
      "epoch: 17 step: 271, loss is 0.039135128259658813\n",
      "epoch: 17 step: 272, loss is 0.0008955111261457205\n",
      "epoch: 17 step: 273, loss is 0.0011328884866088629\n",
      "epoch: 17 step: 274, loss is 0.003137094434350729\n",
      "epoch: 17 step: 275, loss is 0.055195149034261703\n",
      "epoch: 17 step: 276, loss is 0.012298167683184147\n",
      "epoch: 17 step: 277, loss is 0.02406049333512783\n",
      "epoch: 17 step: 278, loss is 0.008086889050900936\n",
      "epoch: 17 step: 279, loss is 0.027558956295251846\n",
      "epoch: 17 step: 280, loss is 0.06220092251896858\n",
      "epoch: 17 step: 281, loss is 0.01976194605231285\n",
      "epoch: 17 step: 282, loss is 0.04027200862765312\n",
      "epoch: 17 step: 283, loss is 0.050368357449769974\n",
      "epoch: 17 step: 284, loss is 0.0006822584546171129\n",
      "epoch: 17 step: 285, loss is 0.06826257705688477\n",
      "epoch: 17 step: 286, loss is 0.002340588951483369\n",
      "epoch: 17 step: 287, loss is 0.08193638175725937\n",
      "epoch: 17 step: 288, loss is 0.001224698731675744\n",
      "epoch: 17 step: 289, loss is 0.0016408805968239903\n",
      "epoch: 17 step: 290, loss is 0.002614966593682766\n",
      "epoch: 17 step: 291, loss is 0.004492252599447966\n",
      "epoch: 17 step: 292, loss is 0.060610219836235046\n",
      "epoch: 17 step: 293, loss is 0.0189154464751482\n",
      "epoch: 17 step: 294, loss is 0.01570277102291584\n",
      "epoch: 17 step: 295, loss is 0.02053193747997284\n",
      "epoch: 17 step: 296, loss is 0.014790692366659641\n",
      "epoch: 17 step: 297, loss is 0.007723492570221424\n",
      "epoch: 17 step: 298, loss is 0.02465461567044258\n",
      "epoch: 17 step: 299, loss is 0.045110974460840225\n",
      "epoch: 17 step: 300, loss is 0.010772005654871464\n",
      "epoch: 17 step: 301, loss is 0.016704415902495384\n",
      "epoch: 17 step: 302, loss is 0.04737475514411926\n",
      "epoch: 17 step: 303, loss is 0.00018007558537647128\n",
      "epoch: 17 step: 304, loss is 0.011671277694404125\n",
      "epoch: 17 step: 305, loss is 0.03389522805809975\n",
      "epoch: 17 step: 306, loss is 0.036055877804756165\n",
      "epoch: 17 step: 307, loss is 0.0006626538233831525\n",
      "epoch: 17 step: 308, loss is 0.0013066699029877782\n",
      "epoch: 17 step: 309, loss is 0.030228540301322937\n",
      "epoch: 17 step: 310, loss is 0.03295459225773811\n",
      "epoch: 17 step: 311, loss is 0.006191170308738947\n",
      "epoch: 17 step: 312, loss is 0.014137472957372665\n",
      "epoch: 17 step: 313, loss is 0.03391226381063461\n",
      "epoch: 17 step: 314, loss is 0.009937254711985588\n",
      "epoch: 17 step: 315, loss is 0.007802117615938187\n",
      "epoch: 17 step: 316, loss is 0.03783690556883812\n",
      "epoch: 17 step: 317, loss is 0.00019417324801906943\n",
      "epoch: 17 step: 318, loss is 0.0014254343695938587\n",
      "epoch: 17 step: 319, loss is 0.01527649536728859\n",
      "epoch: 17 step: 320, loss is 0.02087012678384781\n",
      "epoch: 17 step: 321, loss is 0.02603074163198471\n",
      "epoch: 17 step: 322, loss is 0.00037504054489545524\n",
      "epoch: 17 step: 323, loss is 0.05718148499727249\n",
      "epoch: 17 step: 324, loss is 0.03196930140256882\n",
      "epoch: 17 step: 325, loss is 0.06535815447568893\n",
      "epoch: 17 step: 326, loss is 0.02620241418480873\n",
      "epoch: 17 step: 327, loss is 0.0058919345028698444\n",
      "epoch: 17 step: 328, loss is 0.001929807011038065\n",
      "epoch: 17 step: 329, loss is 0.026021745055913925\n",
      "epoch: 17 step: 330, loss is 0.0004444219230208546\n",
      "epoch: 17 step: 331, loss is 0.014240185730159283\n",
      "epoch: 17 step: 332, loss is 0.0025376619305461645\n",
      "epoch: 17 step: 333, loss is 0.03526594862341881\n",
      "epoch: 17 step: 334, loss is 0.06348993629217148\n",
      "epoch: 17 step: 335, loss is 0.0017202335875481367\n",
      "epoch: 17 step: 336, loss is 0.0016944673843681812\n",
      "epoch: 17 step: 337, loss is 0.07964284718036652\n",
      "epoch: 17 step: 338, loss is 0.005814087577164173\n",
      "epoch: 17 step: 339, loss is 0.0031184833496809006\n",
      "epoch: 17 step: 340, loss is 0.01177383866161108\n",
      "epoch: 17 step: 341, loss is 0.02003815397620201\n",
      "epoch: 17 step: 342, loss is 0.01434264238923788\n",
      "epoch: 17 step: 343, loss is 0.006731254048645496\n",
      "epoch: 17 step: 344, loss is 0.0027132483664900064\n",
      "epoch: 17 step: 345, loss is 0.0013244268484413624\n",
      "epoch: 17 step: 346, loss is 0.013045668601989746\n",
      "epoch: 17 step: 347, loss is 0.008084462955594063\n",
      "epoch: 17 step: 348, loss is 0.0028632914181798697\n",
      "epoch: 17 step: 349, loss is 0.006535287946462631\n",
      "epoch: 17 step: 350, loss is 0.003422993468120694\n",
      "epoch: 17 step: 351, loss is 0.025157596915960312\n",
      "epoch: 17 step: 352, loss is 0.018977761268615723\n",
      "epoch: 17 step: 353, loss is 0.0038472265005111694\n",
      "epoch: 17 step: 354, loss is 0.01059228740632534\n",
      "epoch: 17 step: 355, loss is 0.01559899840503931\n",
      "epoch: 17 step: 356, loss is 0.002245928393676877\n",
      "epoch: 17 step: 357, loss is 0.005082942545413971\n",
      "epoch: 17 step: 358, loss is 0.0208995770663023\n",
      "epoch: 17 step: 359, loss is 0.0010738588171079755\n",
      "epoch: 17 step: 360, loss is 0.024201087653636932\n",
      "epoch: 17 step: 361, loss is 0.007295106537640095\n",
      "epoch: 17 step: 362, loss is 0.0003619133203756064\n",
      "epoch: 17 step: 363, loss is 0.008338033221662045\n",
      "epoch: 17 step: 364, loss is 0.017680445685982704\n",
      "epoch: 17 step: 365, loss is 0.023086965084075928\n",
      "epoch: 17 step: 366, loss is 0.0015341828111559153\n",
      "epoch: 17 step: 367, loss is 0.0714704841375351\n",
      "epoch: 17 step: 368, loss is 0.007017195224761963\n",
      "epoch: 17 step: 369, loss is 0.03314303979277611\n",
      "epoch: 17 step: 370, loss is 0.006199699826538563\n",
      "epoch: 17 step: 371, loss is 0.03687315434217453\n",
      "epoch: 17 step: 372, loss is 0.006799949333071709\n",
      "epoch: 17 step: 373, loss is 0.028395947068929672\n",
      "epoch: 17 step: 374, loss is 0.04188268631696701\n",
      "epoch: 17 step: 375, loss is 0.0007091246079653502\n",
      "epoch: 17 step: 376, loss is 0.009128526784479618\n",
      "epoch: 17 step: 377, loss is 0.005199078936129808\n",
      "epoch: 17 step: 378, loss is 0.00726549606770277\n",
      "epoch: 17 step: 379, loss is 0.00041681306902319193\n",
      "epoch: 17 step: 380, loss is 0.0006801553536206484\n",
      "epoch: 17 step: 381, loss is 0.0011754595907405019\n",
      "epoch: 17 step: 382, loss is 0.09563350677490234\n",
      "epoch: 17 step: 383, loss is 0.055253420025110245\n",
      "epoch: 17 step: 384, loss is 0.06121531128883362\n",
      "epoch: 17 step: 385, loss is 0.0020004024263471365\n",
      "epoch: 17 step: 386, loss is 0.02330661192536354\n",
      "epoch: 17 step: 387, loss is 0.0017039874801412225\n",
      "epoch: 17 step: 388, loss is 0.009687700308859348\n",
      "epoch: 17 step: 389, loss is 0.021147526800632477\n",
      "epoch: 17 step: 390, loss is 0.03934646025300026\n",
      "epoch: 17 step: 391, loss is 0.012118021957576275\n",
      "epoch: 17 step: 392, loss is 0.0018596315057948232\n",
      "epoch: 17 step: 393, loss is 0.00023298295855056494\n",
      "epoch: 17 step: 394, loss is 0.020395291969180107\n",
      "epoch: 17 step: 395, loss is 0.020883165299892426\n",
      "epoch: 17 step: 396, loss is 0.007278793025761843\n",
      "epoch: 17 step: 397, loss is 0.019605500623583794\n",
      "epoch: 17 step: 398, loss is 0.0030058426782488823\n",
      "epoch: 17 step: 399, loss is 0.01901300437748432\n",
      "epoch: 17 step: 400, loss is 0.0007535935728810728\n",
      "epoch: 17 step: 401, loss is 0.035308193415403366\n",
      "epoch: 17 step: 402, loss is 0.004079787991940975\n",
      "epoch: 17 step: 403, loss is 0.00801183469593525\n",
      "epoch: 17 step: 404, loss is 0.002190603408962488\n",
      "epoch: 17 step: 405, loss is 0.0013491527643054724\n",
      "epoch: 17 step: 406, loss is 0.0014280664036050439\n",
      "epoch: 17 step: 407, loss is 0.017990197986364365\n",
      "epoch: 17 step: 408, loss is 0.008646493777632713\n",
      "epoch: 17 step: 409, loss is 0.03147988021373749\n",
      "epoch: 17 step: 410, loss is 0.049531638622283936\n",
      "epoch: 17 step: 411, loss is 0.007828649133443832\n",
      "epoch: 17 step: 412, loss is 0.0231162142008543\n",
      "epoch: 17 step: 413, loss is 0.04700401797890663\n",
      "epoch: 17 step: 414, loss is 0.011485090479254723\n",
      "epoch: 17 step: 415, loss is 0.009200001135468483\n",
      "epoch: 17 step: 416, loss is 0.0006689294241368771\n",
      "epoch: 17 step: 417, loss is 0.0015835361555218697\n",
      "epoch: 17 step: 418, loss is 0.0009308272274211049\n",
      "epoch: 17 step: 419, loss is 0.006411591544747353\n",
      "epoch: 17 step: 420, loss is 0.053977757692337036\n",
      "epoch: 17 step: 421, loss is 0.0015939378645271063\n",
      "epoch: 17 step: 422, loss is 0.0023389963898807764\n",
      "epoch: 17 step: 423, loss is 0.004586752504110336\n",
      "epoch: 17 step: 424, loss is 0.0028191544115543365\n",
      "epoch: 17 step: 425, loss is 0.013981221243739128\n",
      "epoch: 17 step: 426, loss is 0.0033724873792380095\n",
      "epoch: 17 step: 427, loss is 0.0006243977695703506\n",
      "epoch: 17 step: 428, loss is 0.013680203817784786\n",
      "epoch: 17 step: 429, loss is 0.057432398200035095\n",
      "epoch: 17 step: 430, loss is 0.01863247901201248\n",
      "epoch: 17 step: 431, loss is 0.021883174777030945\n",
      "epoch: 17 step: 432, loss is 0.00848218984901905\n",
      "epoch: 17 step: 433, loss is 0.012214896269142628\n",
      "epoch: 17 step: 434, loss is 0.006952736061066389\n",
      "epoch: 17 step: 435, loss is 0.002533958526328206\n",
      "epoch: 17 step: 436, loss is 0.0011560035636648536\n",
      "epoch: 17 step: 437, loss is 0.009633061476051807\n",
      "epoch: 17 step: 438, loss is 0.007322710007429123\n",
      "epoch: 17 step: 439, loss is 0.00280199758708477\n",
      "epoch: 17 step: 440, loss is 0.022285813465714455\n",
      "epoch: 17 step: 441, loss is 0.018427323549985886\n",
      "epoch: 17 step: 442, loss is 0.0017362602520734072\n",
      "epoch: 17 step: 443, loss is 0.048561397939920425\n",
      "epoch: 17 step: 444, loss is 0.005869345273822546\n",
      "epoch: 17 step: 445, loss is 0.033929768949747086\n",
      "epoch: 17 step: 446, loss is 0.009720581583678722\n",
      "epoch: 17 step: 447, loss is 0.04167095571756363\n",
      "epoch: 17 step: 448, loss is 0.008755098097026348\n",
      "epoch: 17 step: 449, loss is 0.0014345139497891068\n",
      "epoch: 17 step: 450, loss is 0.017236437648534775\n",
      "epoch: 17 step: 451, loss is 0.006212143227458\n",
      "epoch: 17 step: 452, loss is 0.003781111678108573\n",
      "epoch: 17 step: 453, loss is 0.00043819480924867094\n",
      "epoch: 17 step: 454, loss is 0.0014800606295466423\n",
      "epoch: 17 step: 455, loss is 0.10422821342945099\n",
      "epoch: 17 step: 456, loss is 0.028365004807710648\n",
      "epoch: 17 step: 457, loss is 0.01880207657814026\n",
      "epoch: 17 step: 458, loss is 0.009018277749419212\n",
      "epoch: 17 step: 459, loss is 0.048223212361335754\n",
      "epoch: 17 step: 460, loss is 0.0016604617703706026\n",
      "epoch: 17 step: 461, loss is 0.0023953444324433804\n",
      "epoch: 17 step: 462, loss is 0.039821162819862366\n",
      "epoch: 17 step: 463, loss is 0.05042093247175217\n",
      "epoch: 17 step: 464, loss is 0.0006825241143815219\n",
      "epoch: 17 step: 465, loss is 0.0014256598660722375\n",
      "epoch: 17 step: 466, loss is 0.011787882074713707\n",
      "epoch: 17 step: 467, loss is 0.03363567590713501\n",
      "epoch: 17 step: 468, loss is 0.00039133973768912256\n",
      "epoch: 17 step: 469, loss is 0.030914029106497765\n",
      "epoch: 17 step: 470, loss is 0.05150777846574783\n",
      "epoch: 17 step: 471, loss is 0.007435838226228952\n",
      "epoch: 17 step: 472, loss is 0.029401931911706924\n",
      "epoch: 17 step: 473, loss is 0.0076484885066747665\n",
      "epoch: 17 step: 474, loss is 0.005191416013985872\n",
      "epoch: 17 step: 475, loss is 0.004641251172870398\n",
      "epoch: 17 step: 476, loss is 0.05727998539805412\n",
      "epoch: 17 step: 477, loss is 0.0020577670074999332\n",
      "epoch: 17 step: 478, loss is 0.003961389884352684\n",
      "epoch: 17 step: 479, loss is 0.00516324769705534\n",
      "epoch: 17 step: 480, loss is 0.003934555687010288\n",
      "epoch: 17 step: 481, loss is 0.010547134093940258\n",
      "epoch: 17 step: 482, loss is 0.004950477741658688\n",
      "epoch: 17 step: 483, loss is 0.0035644567105919123\n",
      "epoch: 17 step: 484, loss is 0.10151531547307968\n",
      "epoch: 17 step: 485, loss is 0.02927444688975811\n",
      "epoch: 17 step: 486, loss is 0.04749157652258873\n",
      "epoch: 17 step: 487, loss is 0.023306015878915787\n",
      "epoch: 17 step: 488, loss is 0.008867631666362286\n",
      "epoch: 17 step: 489, loss is 0.0004410503897815943\n",
      "epoch: 17 step: 490, loss is 0.07951022684574127\n",
      "epoch: 17 step: 491, loss is 0.006540769245475531\n",
      "epoch: 17 step: 492, loss is 0.000444227596744895\n",
      "epoch: 17 step: 493, loss is 0.0045385234989225864\n",
      "epoch: 17 step: 494, loss is 0.007873513735830784\n",
      "epoch: 17 step: 495, loss is 0.00958478543907404\n",
      "epoch: 17 step: 496, loss is 0.004690701141953468\n",
      "epoch: 17 step: 497, loss is 0.05522611737251282\n",
      "epoch: 17 step: 498, loss is 0.019160082563757896\n",
      "epoch: 17 step: 499, loss is 0.000280905282124877\n",
      "epoch: 17 step: 500, loss is 0.0035751252435147762\n",
      "epoch: 17 step: 501, loss is 0.0018362367991358042\n",
      "epoch: 17 step: 502, loss is 0.0029509549494832754\n",
      "epoch: 17 step: 503, loss is 0.00540721882134676\n",
      "epoch: 17 step: 504, loss is 0.04637065529823303\n",
      "epoch: 17 step: 505, loss is 0.014179905876517296\n",
      "epoch: 17 step: 506, loss is 0.001544221886433661\n",
      "epoch: 17 step: 507, loss is 0.0019489063415676355\n",
      "epoch: 17 step: 508, loss is 0.006905444897711277\n",
      "epoch: 17 step: 509, loss is 0.0018854155205190182\n",
      "epoch: 17 step: 510, loss is 0.012776559218764305\n",
      "epoch: 17 step: 511, loss is 0.00027493684319779277\n",
      "epoch: 17 step: 512, loss is 0.016367468982934952\n",
      "epoch: 17 step: 513, loss is 0.0015994374407455325\n",
      "epoch: 17 step: 514, loss is 0.009514308534562588\n",
      "epoch: 17 step: 515, loss is 0.00048755586612969637\n",
      "epoch: 17 step: 516, loss is 0.0037371485959738493\n",
      "epoch: 17 step: 517, loss is 0.12394586950540543\n",
      "epoch: 17 step: 518, loss is 0.007837200537323952\n",
      "epoch: 17 step: 519, loss is 0.002154372166842222\n",
      "epoch: 17 step: 520, loss is 0.0017970724729821086\n",
      "epoch: 17 step: 521, loss is 0.02511243149638176\n",
      "epoch: 17 step: 522, loss is 0.0013372059911489487\n",
      "epoch: 17 step: 523, loss is 0.01647643931210041\n",
      "epoch: 17 step: 524, loss is 0.0026913576293736696\n",
      "epoch: 17 step: 525, loss is 0.0031818069983273745\n",
      "epoch: 17 step: 526, loss is 0.004865163005888462\n",
      "epoch: 17 step: 527, loss is 0.019753454253077507\n",
      "epoch: 17 step: 528, loss is 0.005590787623077631\n",
      "epoch: 17 step: 529, loss is 0.015669988468289375\n",
      "epoch: 17 step: 530, loss is 0.0046470919623970985\n",
      "epoch: 17 step: 531, loss is 0.006702288519591093\n",
      "epoch: 17 step: 532, loss is 0.04449836537241936\n",
      "epoch: 17 step: 533, loss is 0.0020903709810227156\n",
      "epoch: 17 step: 534, loss is 0.0035412670113146305\n",
      "epoch: 17 step: 535, loss is 0.0049570281989872456\n",
      "epoch: 17 step: 536, loss is 0.029253020882606506\n",
      "epoch: 17 step: 537, loss is 0.0008127355249598622\n",
      "epoch: 17 step: 538, loss is 0.002994048409163952\n",
      "epoch: 17 step: 539, loss is 0.005046763923019171\n",
      "epoch: 17 step: 540, loss is 0.0001986486604437232\n",
      "epoch: 17 step: 541, loss is 0.007418336346745491\n",
      "epoch: 17 step: 542, loss is 0.1681121289730072\n",
      "epoch: 17 step: 543, loss is 0.044195257127285004\n",
      "epoch: 17 step: 544, loss is 0.009202023968100548\n",
      "epoch: 17 step: 545, loss is 0.006688529625535011\n",
      "epoch: 17 step: 546, loss is 0.0012154760770499706\n",
      "epoch: 17 step: 547, loss is 0.002523055300116539\n",
      "epoch: 17 step: 548, loss is 0.02276550978422165\n",
      "epoch: 17 step: 549, loss is 0.05740213766694069\n",
      "epoch: 17 step: 550, loss is 0.031569719314575195\n",
      "epoch: 17 step: 551, loss is 0.019131679087877274\n",
      "epoch: 17 step: 552, loss is 0.0398578904569149\n",
      "epoch: 17 step: 553, loss is 0.005341182928532362\n",
      "epoch: 17 step: 554, loss is 0.0012272068997845054\n",
      "epoch: 17 step: 555, loss is 0.018486235290765762\n",
      "epoch: 17 step: 556, loss is 0.07572142034769058\n",
      "epoch: 17 step: 557, loss is 0.012421851977705956\n",
      "epoch: 17 step: 558, loss is 0.019984779879450798\n",
      "epoch: 17 step: 559, loss is 0.015989553183317184\n",
      "epoch: 17 step: 560, loss is 0.02337080053985119\n",
      "epoch: 17 step: 561, loss is 0.060226839035749435\n",
      "epoch: 17 step: 562, loss is 0.06788847595453262\n",
      "epoch: 17 step: 563, loss is 0.007977510802447796\n",
      "epoch: 17 step: 564, loss is 0.0041680289432406425\n",
      "epoch: 17 step: 565, loss is 0.004672549664974213\n",
      "epoch: 17 step: 566, loss is 0.02005688287317753\n",
      "epoch: 17 step: 567, loss is 0.018295077607035637\n",
      "epoch: 17 step: 568, loss is 0.02369922585785389\n",
      "epoch: 17 step: 569, loss is 0.002163041615858674\n",
      "epoch: 17 step: 570, loss is 0.180143803358078\n",
      "epoch: 17 step: 571, loss is 0.020936323329806328\n",
      "epoch: 17 step: 572, loss is 0.01596401445567608\n",
      "epoch: 17 step: 573, loss is 0.07296524941921234\n",
      "epoch: 17 step: 574, loss is 0.0019418414449319243\n",
      "epoch: 17 step: 575, loss is 0.014162890613079071\n",
      "epoch: 17 step: 576, loss is 0.006460090167820454\n",
      "epoch: 17 step: 577, loss is 0.01099843718111515\n",
      "epoch: 17 step: 578, loss is 0.038214217871427536\n",
      "epoch: 17 step: 579, loss is 0.036412738263607025\n",
      "epoch: 17 step: 580, loss is 0.0012388507602736354\n",
      "epoch: 17 step: 581, loss is 0.00804449524730444\n",
      "epoch: 17 step: 582, loss is 0.036626897752285004\n",
      "epoch: 17 step: 583, loss is 0.00019621074898168445\n",
      "epoch: 17 step: 584, loss is 0.0572754368185997\n",
      "epoch: 17 step: 585, loss is 0.016835886985063553\n",
      "epoch: 17 step: 586, loss is 0.003682460868731141\n",
      "epoch: 17 step: 587, loss is 0.0037451290991157293\n",
      "epoch: 17 step: 588, loss is 0.024459069594740868\n",
      "epoch: 17 step: 589, loss is 0.007826302200555801\n",
      "epoch: 17 step: 590, loss is 0.0033945059403777122\n",
      "epoch: 17 step: 591, loss is 0.004518141038715839\n",
      "epoch: 17 step: 592, loss is 0.002267527859658003\n",
      "epoch: 17 step: 593, loss is 0.0034825196489691734\n",
      "epoch: 17 step: 594, loss is 0.0014469772577285767\n",
      "epoch: 17 step: 595, loss is 0.06233827397227287\n",
      "epoch: 17 step: 596, loss is 0.003771251067519188\n",
      "epoch: 17 step: 597, loss is 0.016657833009958267\n",
      "epoch: 17 step: 598, loss is 0.09408707916736603\n",
      "epoch: 17 step: 599, loss is 0.04016567021608353\n",
      "epoch: 17 step: 600, loss is 0.0917818620800972\n",
      "epoch: 17 step: 601, loss is 0.017733603715896606\n",
      "epoch: 17 step: 602, loss is 0.007463371381163597\n",
      "epoch: 17 step: 603, loss is 0.0023548093158751726\n",
      "epoch: 17 step: 604, loss is 0.000755322864279151\n",
      "epoch: 17 step: 605, loss is 0.0026878986973315477\n",
      "epoch: 17 step: 606, loss is 0.03124134987592697\n",
      "epoch: 17 step: 607, loss is 0.0010349573567509651\n",
      "epoch: 17 step: 608, loss is 0.06252829730510712\n",
      "epoch: 17 step: 609, loss is 0.14635184407234192\n",
      "epoch: 17 step: 610, loss is 0.0011200240114703774\n",
      "epoch: 17 step: 611, loss is 0.07003211975097656\n",
      "epoch: 17 step: 612, loss is 0.023346079513430595\n",
      "epoch: 17 step: 613, loss is 0.0014379778876900673\n",
      "epoch: 17 step: 614, loss is 0.026122258976101875\n",
      "epoch: 17 step: 615, loss is 0.023019777610898018\n",
      "epoch: 17 step: 616, loss is 0.005841962527483702\n",
      "epoch: 17 step: 617, loss is 0.0019770918879657984\n",
      "epoch: 17 step: 618, loss is 0.015207578428089619\n",
      "epoch: 17 step: 619, loss is 0.028603622689843178\n",
      "epoch: 17 step: 620, loss is 0.003195009892806411\n",
      "epoch: 17 step: 621, loss is 0.005048827733844519\n",
      "epoch: 17 step: 622, loss is 0.008388003334403038\n",
      "epoch: 17 step: 623, loss is 0.0010886391391977668\n",
      "epoch: 17 step: 624, loss is 0.11468882858753204\n",
      "epoch: 17 step: 625, loss is 0.04363120719790459\n",
      "epoch: 17 step: 626, loss is 0.010287823155522346\n",
      "epoch: 17 step: 627, loss is 0.04178357124328613\n",
      "epoch: 17 step: 628, loss is 0.003954180516302586\n",
      "epoch: 17 step: 629, loss is 0.0006963511696085334\n",
      "epoch: 17 step: 630, loss is 0.004827525466680527\n",
      "epoch: 17 step: 631, loss is 0.012963751330971718\n",
      "epoch: 17 step: 632, loss is 0.028383396565914154\n",
      "epoch: 17 step: 633, loss is 0.01499288622289896\n",
      "epoch: 17 step: 634, loss is 0.027589309960603714\n",
      "epoch: 17 step: 635, loss is 0.0025319461710751057\n",
      "epoch: 17 step: 636, loss is 0.0029800632037222385\n",
      "epoch: 17 step: 637, loss is 0.056478582322597504\n",
      "epoch: 17 step: 638, loss is 0.0013316723052412271\n",
      "epoch: 17 step: 639, loss is 0.007167025003582239\n",
      "epoch: 17 step: 640, loss is 0.029736964032053947\n",
      "epoch: 17 step: 641, loss is 0.18060849606990814\n",
      "epoch: 17 step: 642, loss is 0.13441552221775055\n",
      "epoch: 17 step: 643, loss is 0.04919189214706421\n",
      "epoch: 17 step: 644, loss is 0.001883380115032196\n",
      "epoch: 17 step: 645, loss is 0.029707038775086403\n",
      "epoch: 17 step: 646, loss is 0.0028928702231496572\n",
      "epoch: 17 step: 647, loss is 0.028644448146224022\n",
      "epoch: 17 step: 648, loss is 0.015234933234751225\n",
      "epoch: 17 step: 649, loss is 0.01994875818490982\n",
      "epoch: 17 step: 650, loss is 0.09319958835840225\n",
      "epoch: 17 step: 651, loss is 0.003588079009205103\n",
      "epoch: 17 step: 652, loss is 0.024376165121793747\n",
      "epoch: 17 step: 653, loss is 0.02979760244488716\n",
      "epoch: 17 step: 654, loss is 0.041281554847955704\n",
      "epoch: 17 step: 655, loss is 0.07150638103485107\n",
      "epoch: 17 step: 656, loss is 0.003328729188069701\n",
      "epoch: 17 step: 657, loss is 0.060014933347702026\n",
      "epoch: 17 step: 658, loss is 0.10455423593521118\n",
      "epoch: 17 step: 659, loss is 0.007388876285403967\n",
      "epoch: 17 step: 660, loss is 0.012984436936676502\n",
      "epoch: 17 step: 661, loss is 0.010928446426987648\n",
      "epoch: 17 step: 662, loss is 0.0470917783677578\n",
      "epoch: 17 step: 663, loss is 0.004829826299101114\n",
      "epoch: 17 step: 664, loss is 0.00014412619930226356\n",
      "epoch: 17 step: 665, loss is 0.025118272751569748\n",
      "epoch: 17 step: 666, loss is 0.01649629697203636\n",
      "epoch: 17 step: 667, loss is 0.09071137756109238\n",
      "epoch: 17 step: 668, loss is 0.008918454870581627\n",
      "epoch: 17 step: 669, loss is 0.03626307100057602\n",
      "epoch: 17 step: 670, loss is 0.022459587082266808\n",
      "epoch: 17 step: 671, loss is 0.01916622370481491\n",
      "epoch: 17 step: 672, loss is 0.0024767564609646797\n",
      "epoch: 17 step: 673, loss is 0.021104544401168823\n",
      "epoch: 17 step: 674, loss is 0.11183376610279083\n",
      "epoch: 17 step: 675, loss is 0.037664663046598434\n",
      "epoch: 17 step: 676, loss is 0.10971499234437943\n",
      "epoch: 17 step: 677, loss is 0.049468766897916794\n",
      "epoch: 17 step: 678, loss is 0.00902897771447897\n",
      "epoch: 17 step: 679, loss is 0.12274064123630524\n",
      "epoch: 17 step: 680, loss is 0.06354329735040665\n",
      "epoch: 17 step: 681, loss is 0.06863655149936676\n",
      "epoch: 17 step: 682, loss is 0.026692718267440796\n",
      "epoch: 17 step: 683, loss is 0.09165599197149277\n",
      "epoch: 17 step: 684, loss is 0.017672138288617134\n",
      "epoch: 17 step: 685, loss is 0.016044171527028084\n",
      "epoch: 17 step: 686, loss is 0.0018636486493051052\n",
      "epoch: 17 step: 687, loss is 0.01511648204177618\n",
      "epoch: 17 step: 688, loss is 0.005270528141409159\n",
      "epoch: 17 step: 689, loss is 0.003204852342605591\n",
      "epoch: 17 step: 690, loss is 0.004543543793261051\n",
      "epoch: 17 step: 691, loss is 0.028322279453277588\n",
      "epoch: 17 step: 692, loss is 0.02680389955639839\n",
      "epoch: 17 step: 693, loss is 0.021849961951375008\n",
      "epoch: 17 step: 694, loss is 0.01938835345208645\n",
      "epoch: 17 step: 695, loss is 0.015454273670911789\n",
      "epoch: 17 step: 696, loss is 0.023287944495677948\n",
      "epoch: 17 step: 697, loss is 0.0011295139556750655\n",
      "epoch: 17 step: 698, loss is 0.003600343596190214\n",
      "epoch: 17 step: 699, loss is 0.0023739670868963003\n",
      "epoch: 17 step: 700, loss is 0.018574541434645653\n",
      "epoch: 17 step: 701, loss is 0.013014878146350384\n",
      "epoch: 17 step: 702, loss is 0.03047303669154644\n",
      "epoch: 17 step: 703, loss is 0.004583165515214205\n",
      "epoch: 17 step: 704, loss is 0.011814229190349579\n",
      "epoch: 17 step: 705, loss is 0.09460753202438354\n",
      "epoch: 17 step: 706, loss is 0.0007119090878404677\n",
      "epoch: 17 step: 707, loss is 0.002700239885598421\n",
      "epoch: 17 step: 708, loss is 0.0071027809754014015\n",
      "epoch: 17 step: 709, loss is 0.0029084370471537113\n",
      "epoch: 17 step: 710, loss is 0.0030862612184137106\n",
      "epoch: 17 step: 711, loss is 0.05193118005990982\n",
      "epoch: 17 step: 712, loss is 0.003975504543632269\n",
      "epoch: 17 step: 713, loss is 0.001461657346226275\n",
      "epoch: 17 step: 714, loss is 0.018236152827739716\n",
      "epoch: 17 step: 715, loss is 0.08573196828365326\n",
      "epoch: 17 step: 716, loss is 0.003353478852659464\n",
      "epoch: 17 step: 717, loss is 0.0007985513657331467\n",
      "epoch: 17 step: 718, loss is 0.022631026804447174\n",
      "epoch: 17 step: 719, loss is 0.0063974205404520035\n",
      "epoch: 17 step: 720, loss is 0.049742817878723145\n",
      "epoch: 17 step: 721, loss is 0.0023974478244781494\n",
      "epoch: 17 step: 722, loss is 0.04832330718636513\n",
      "epoch: 17 step: 723, loss is 0.01189250499010086\n",
      "epoch: 17 step: 724, loss is 0.01368927862495184\n",
      "epoch: 17 step: 725, loss is 0.003460794221609831\n",
      "epoch: 17 step: 726, loss is 0.023837151005864143\n",
      "epoch: 17 step: 727, loss is 0.0031282424461096525\n",
      "epoch: 17 step: 728, loss is 0.02525194175541401\n",
      "epoch: 17 step: 729, loss is 0.012128304690122604\n",
      "epoch: 17 step: 730, loss is 0.005472699645906687\n",
      "epoch: 17 step: 731, loss is 0.03415560722351074\n",
      "epoch: 17 step: 732, loss is 0.02046716772019863\n",
      "epoch: 17 step: 733, loss is 0.02274274453520775\n",
      "epoch: 17 step: 734, loss is 0.003412543796002865\n",
      "epoch: 17 step: 735, loss is 0.0039787390269339085\n",
      "epoch: 17 step: 736, loss is 0.03404805436730385\n",
      "epoch: 17 step: 737, loss is 0.06676525622606277\n",
      "epoch: 17 step: 738, loss is 0.0006889431970193982\n",
      "epoch: 17 step: 739, loss is 0.0018738338258117437\n",
      "epoch: 17 step: 740, loss is 0.05560542643070221\n",
      "epoch: 17 step: 741, loss is 0.006600525695830584\n",
      "epoch: 17 step: 742, loss is 0.07972941547632217\n",
      "epoch: 17 step: 743, loss is 0.03878609836101532\n",
      "epoch: 17 step: 744, loss is 0.007938815280795097\n",
      "epoch: 17 step: 745, loss is 0.01708034798502922\n",
      "epoch: 17 step: 746, loss is 0.015283236280083656\n",
      "epoch: 17 step: 747, loss is 0.05508904159069061\n",
      "epoch: 17 step: 748, loss is 0.002906291512772441\n",
      "epoch: 17 step: 749, loss is 0.009204398840665817\n",
      "epoch: 17 step: 750, loss is 0.01184140332043171\n",
      "epoch: 17 step: 751, loss is 0.028530428186058998\n",
      "epoch: 17 step: 752, loss is 0.017665034160017967\n",
      "epoch: 17 step: 753, loss is 0.08751621842384338\n",
      "epoch: 17 step: 754, loss is 0.03787749260663986\n",
      "epoch: 17 step: 755, loss is 0.028443871065974236\n",
      "epoch: 17 step: 756, loss is 0.002263330854475498\n",
      "epoch: 17 step: 757, loss is 0.010677528567612171\n",
      "epoch: 17 step: 758, loss is 0.05833497643470764\n",
      "epoch: 17 step: 759, loss is 0.04115656390786171\n",
      "epoch: 17 step: 760, loss is 0.017743857577443123\n",
      "epoch: 17 step: 761, loss is 0.0163438580930233\n",
      "epoch: 17 step: 762, loss is 0.02562791481614113\n",
      "epoch: 17 step: 763, loss is 0.10020250827074051\n",
      "epoch: 17 step: 764, loss is 0.05613786354660988\n",
      "epoch: 17 step: 765, loss is 0.0420476570725441\n",
      "epoch: 17 step: 766, loss is 0.026953130960464478\n",
      "epoch: 17 step: 767, loss is 0.08090795576572418\n",
      "epoch: 17 step: 768, loss is 0.0798473060131073\n",
      "epoch: 17 step: 769, loss is 0.0027830610051751137\n",
      "epoch: 17 step: 770, loss is 0.0930090919137001\n",
      "epoch: 17 step: 771, loss is 0.08060402423143387\n",
      "epoch: 17 step: 772, loss is 0.037244729697704315\n",
      "epoch: 17 step: 773, loss is 0.01957949623465538\n",
      "epoch: 17 step: 774, loss is 0.015034967102110386\n",
      "epoch: 17 step: 775, loss is 0.0016434466233476996\n",
      "epoch: 17 step: 776, loss is 0.015396938659250736\n",
      "epoch: 17 step: 777, loss is 0.09628776460886002\n",
      "epoch: 17 step: 778, loss is 0.018011920154094696\n",
      "epoch: 17 step: 779, loss is 0.05626997351646423\n",
      "epoch: 17 step: 780, loss is 0.03183261677622795\n",
      "epoch: 17 step: 781, loss is 0.0018643656512722373\n",
      "epoch: 17 step: 782, loss is 0.0011186529882252216\n",
      "epoch: 17 step: 783, loss is 0.02272561378777027\n",
      "epoch: 17 step: 784, loss is 0.004120759200304747\n",
      "epoch: 17 step: 785, loss is 0.026270173490047455\n",
      "epoch: 17 step: 786, loss is 0.17314283549785614\n",
      "epoch: 17 step: 787, loss is 0.005958402995020151\n",
      "epoch: 17 step: 788, loss is 0.03025699406862259\n",
      "epoch: 17 step: 789, loss is 0.05973751097917557\n",
      "epoch: 17 step: 790, loss is 0.035773616284132004\n",
      "epoch: 17 step: 791, loss is 0.043096475303173065\n",
      "epoch: 17 step: 792, loss is 0.07093384116888046\n",
      "epoch: 17 step: 793, loss is 0.01090610958635807\n",
      "epoch: 17 step: 794, loss is 0.041473355144262314\n",
      "epoch: 17 step: 795, loss is 0.05364368483424187\n",
      "epoch: 17 step: 796, loss is 0.037795908749103546\n",
      "epoch: 17 step: 797, loss is 0.025359025225043297\n",
      "epoch: 17 step: 798, loss is 0.0022196120116859674\n",
      "epoch: 17 step: 799, loss is 0.021331632509827614\n",
      "epoch: 17 step: 800, loss is 0.008236213587224483\n",
      "epoch: 17 step: 801, loss is 0.067830890417099\n",
      "epoch: 17 step: 802, loss is 0.027892371639609337\n",
      "epoch: 17 step: 803, loss is 0.012064335867762566\n",
      "epoch: 17 step: 804, loss is 0.08962807804346085\n",
      "epoch: 17 step: 805, loss is 0.012210971675813198\n",
      "epoch: 17 step: 806, loss is 0.003655168227851391\n",
      "epoch: 17 step: 807, loss is 0.0189649797976017\n",
      "epoch: 17 step: 808, loss is 0.015292054042220116\n",
      "epoch: 17 step: 809, loss is 0.014697735197842121\n",
      "epoch: 17 step: 810, loss is 0.02390556037425995\n",
      "epoch: 17 step: 811, loss is 0.006332398392260075\n",
      "epoch: 17 step: 812, loss is 0.027192121371626854\n",
      "epoch: 17 step: 813, loss is 0.06662200391292572\n",
      "epoch: 17 step: 814, loss is 0.022590868175029755\n",
      "epoch: 17 step: 815, loss is 0.10463555157184601\n",
      "epoch: 17 step: 816, loss is 0.09567961096763611\n",
      "epoch: 17 step: 817, loss is 0.026870472356677055\n",
      "epoch: 17 step: 818, loss is 0.12047954648733139\n",
      "epoch: 17 step: 819, loss is 0.005685827694833279\n",
      "epoch: 17 step: 820, loss is 0.01059672050178051\n",
      "epoch: 17 step: 821, loss is 0.003161687171086669\n",
      "epoch: 17 step: 822, loss is 0.032466188073158264\n",
      "epoch: 17 step: 823, loss is 0.005670818034559488\n",
      "epoch: 17 step: 824, loss is 0.014255201444029808\n",
      "epoch: 17 step: 825, loss is 0.016973674297332764\n",
      "epoch: 17 step: 826, loss is 0.033709440380334854\n",
      "epoch: 17 step: 827, loss is 0.0384659506380558\n",
      "epoch: 17 step: 828, loss is 0.0011537460377439857\n",
      "epoch: 17 step: 829, loss is 0.012455472722649574\n",
      "epoch: 17 step: 830, loss is 0.003002006094902754\n",
      "epoch: 17 step: 831, loss is 0.01833319291472435\n",
      "epoch: 17 step: 832, loss is 0.022664416581392288\n",
      "epoch: 17 step: 833, loss is 0.006494848057627678\n",
      "epoch: 17 step: 834, loss is 0.00010618057422107086\n",
      "epoch: 17 step: 835, loss is 0.04290167614817619\n",
      "epoch: 17 step: 836, loss is 0.007942739874124527\n",
      "epoch: 17 step: 837, loss is 0.005251509603112936\n",
      "epoch: 17 step: 838, loss is 0.021864579990506172\n",
      "epoch: 17 step: 839, loss is 0.03276357054710388\n",
      "epoch: 17 step: 840, loss is 0.04052913933992386\n",
      "epoch: 17 step: 841, loss is 0.07235044240951538\n",
      "epoch: 17 step: 842, loss is 0.022412048652768135\n",
      "epoch: 17 step: 843, loss is 0.009021399542689323\n",
      "epoch: 17 step: 844, loss is 0.000964554725214839\n",
      "epoch: 17 step: 845, loss is 0.0074570211581885815\n",
      "epoch: 17 step: 846, loss is 0.04475441202521324\n",
      "epoch: 17 step: 847, loss is 0.09828060120344162\n",
      "epoch: 17 step: 848, loss is 0.003619676223024726\n",
      "epoch: 17 step: 849, loss is 0.0049484893679618835\n",
      "epoch: 17 step: 850, loss is 0.1154332160949707\n",
      "epoch: 17 step: 851, loss is 0.005897032096982002\n",
      "epoch: 17 step: 852, loss is 0.010738549754023552\n",
      "epoch: 17 step: 853, loss is 0.015056300908327103\n",
      "epoch: 17 step: 854, loss is 0.0025154065806418657\n",
      "epoch: 17 step: 855, loss is 0.008264483883976936\n",
      "epoch: 17 step: 856, loss is 0.05969499796628952\n",
      "epoch: 17 step: 857, loss is 0.02782673016190529\n",
      "epoch: 17 step: 858, loss is 0.00413738377392292\n",
      "epoch: 17 step: 859, loss is 0.058428581804037094\n",
      "epoch: 17 step: 860, loss is 0.017126917839050293\n",
      "epoch: 17 step: 861, loss is 0.11532874405384064\n",
      "epoch: 17 step: 862, loss is 0.009759489446878433\n",
      "epoch: 17 step: 863, loss is 0.008547993376851082\n",
      "epoch: 17 step: 864, loss is 0.013653465546667576\n",
      "epoch: 17 step: 865, loss is 0.06150124967098236\n",
      "epoch: 17 step: 866, loss is 0.008810291066765785\n",
      "epoch: 17 step: 867, loss is 0.022927945479750633\n",
      "epoch: 17 step: 868, loss is 0.00818194355815649\n",
      "epoch: 17 step: 869, loss is 0.3812561333179474\n",
      "epoch: 17 step: 870, loss is 0.07031424343585968\n",
      "epoch: 17 step: 871, loss is 0.0006390200578607619\n",
      "epoch: 17 step: 872, loss is 0.00468797842040658\n",
      "epoch: 17 step: 873, loss is 0.060878537595272064\n",
      "epoch: 17 step: 874, loss is 0.01697601191699505\n",
      "epoch: 17 step: 875, loss is 0.007920954376459122\n",
      "epoch: 17 step: 876, loss is 0.06685832887887955\n",
      "epoch: 17 step: 877, loss is 0.09781379997730255\n",
      "epoch: 17 step: 878, loss is 0.05425737425684929\n",
      "epoch: 17 step: 879, loss is 0.0024777885992079973\n",
      "epoch: 17 step: 880, loss is 0.08167769759893417\n",
      "epoch: 17 step: 881, loss is 0.06690164655447006\n",
      "epoch: 17 step: 882, loss is 0.01981344074010849\n",
      "epoch: 17 step: 883, loss is 0.00913456454873085\n",
      "epoch: 17 step: 884, loss is 0.054970882833004\n",
      "epoch: 17 step: 885, loss is 0.014162464067339897\n",
      "epoch: 17 step: 886, loss is 0.0027521364390850067\n",
      "epoch: 17 step: 887, loss is 0.02504820004105568\n",
      "epoch: 17 step: 888, loss is 0.0777304396033287\n",
      "epoch: 17 step: 889, loss is 0.00208605476655066\n",
      "epoch: 17 step: 890, loss is 0.06478609144687653\n",
      "epoch: 17 step: 891, loss is 0.003737999591976404\n",
      "epoch: 17 step: 892, loss is 0.16045938432216644\n",
      "epoch: 17 step: 893, loss is 0.05595748871564865\n",
      "epoch: 17 step: 894, loss is 0.009828182868659496\n",
      "epoch: 17 step: 895, loss is 0.009129409678280354\n",
      "epoch: 17 step: 896, loss is 0.012498429045081139\n",
      "epoch: 17 step: 897, loss is 0.014566712081432343\n",
      "epoch: 17 step: 898, loss is 0.04508614167571068\n",
      "epoch: 17 step: 899, loss is 0.002797828521579504\n",
      "epoch: 17 step: 900, loss is 0.07829378545284271\n",
      "epoch: 17 step: 901, loss is 0.019396672025322914\n",
      "epoch: 17 step: 902, loss is 0.057372938841581345\n",
      "epoch: 17 step: 903, loss is 0.0019224373390898108\n",
      "epoch: 17 step: 904, loss is 0.03212076425552368\n",
      "epoch: 17 step: 905, loss is 0.01117045059800148\n",
      "epoch: 17 step: 906, loss is 0.021743295714259148\n",
      "epoch: 17 step: 907, loss is 0.0002598656283225864\n",
      "epoch: 17 step: 908, loss is 0.0283825621008873\n",
      "epoch: 17 step: 909, loss is 0.026258045807480812\n",
      "epoch: 17 step: 910, loss is 0.10485285520553589\n",
      "epoch: 17 step: 911, loss is 0.008944669738411903\n",
      "epoch: 17 step: 912, loss is 0.008761369623243809\n",
      "epoch: 17 step: 913, loss is 0.008131339214742184\n",
      "epoch: 17 step: 914, loss is 0.0003587098326534033\n",
      "epoch: 17 step: 915, loss is 0.02006600797176361\n",
      "epoch: 17 step: 916, loss is 0.00795000046491623\n",
      "epoch: 17 step: 917, loss is 0.0037072994746267796\n",
      "epoch: 17 step: 918, loss is 0.010279227048158646\n",
      "epoch: 17 step: 919, loss is 0.0721028670668602\n",
      "epoch: 17 step: 920, loss is 0.007872054353356361\n",
      "epoch: 17 step: 921, loss is 0.05608284845948219\n",
      "epoch: 17 step: 922, loss is 0.050988759845495224\n",
      "epoch: 17 step: 923, loss is 0.0038773331325501204\n",
      "epoch: 17 step: 924, loss is 0.01620384491980076\n",
      "epoch: 17 step: 925, loss is 0.05950632691383362\n",
      "epoch: 17 step: 926, loss is 0.022534668445587158\n",
      "epoch: 17 step: 927, loss is 0.009463883005082607\n",
      "epoch: 17 step: 928, loss is 0.04952113330364227\n",
      "epoch: 17 step: 929, loss is 0.023984810337424278\n",
      "epoch: 17 step: 930, loss is 0.00829686876386404\n",
      "epoch: 17 step: 931, loss is 0.0022329536732286215\n",
      "epoch: 17 step: 932, loss is 0.012479042634367943\n",
      "epoch: 17 step: 933, loss is 0.07605350762605667\n",
      "epoch: 17 step: 934, loss is 0.021005922928452492\n",
      "epoch: 17 step: 935, loss is 0.014952647499740124\n",
      "epoch: 17 step: 936, loss is 0.006444710772484541\n",
      "epoch: 17 step: 937, loss is 0.002151718595996499\n",
      "epoch: 18 step: 1, loss is 0.0028296259697526693\n",
      "epoch: 18 step: 2, loss is 0.006028329487890005\n",
      "epoch: 18 step: 3, loss is 0.08131467550992966\n",
      "epoch: 18 step: 4, loss is 0.005359617993235588\n",
      "epoch: 18 step: 5, loss is 0.037300802767276764\n",
      "epoch: 18 step: 6, loss is 0.04363824054598808\n",
      "epoch: 18 step: 7, loss is 0.01690581440925598\n",
      "epoch: 18 step: 8, loss is 0.0015348452143371105\n",
      "epoch: 18 step: 9, loss is 0.027311526238918304\n",
      "epoch: 18 step: 10, loss is 0.011151566170156002\n",
      "epoch: 18 step: 11, loss is 0.06673764437437057\n",
      "epoch: 18 step: 12, loss is 0.03982752561569214\n",
      "epoch: 18 step: 13, loss is 0.07629536092281342\n",
      "epoch: 18 step: 14, loss is 0.002583334455266595\n",
      "epoch: 18 step: 15, loss is 0.01750205084681511\n",
      "epoch: 18 step: 16, loss is 0.02895945869386196\n",
      "epoch: 18 step: 17, loss is 0.0021690295543521643\n",
      "epoch: 18 step: 18, loss is 0.006603149697184563\n",
      "epoch: 18 step: 19, loss is 0.00978251826018095\n",
      "epoch: 18 step: 20, loss is 0.011307883076369762\n",
      "epoch: 18 step: 21, loss is 0.006440304219722748\n",
      "epoch: 18 step: 22, loss is 0.001899332390166819\n",
      "epoch: 18 step: 23, loss is 0.01695893704891205\n",
      "epoch: 18 step: 24, loss is 0.015542713925242424\n",
      "epoch: 18 step: 25, loss is 0.005259210243821144\n",
      "epoch: 18 step: 26, loss is 0.026570672169327736\n",
      "epoch: 18 step: 27, loss is 0.0009226227412000299\n",
      "epoch: 18 step: 28, loss is 0.01592175103724003\n",
      "epoch: 18 step: 29, loss is 0.004698767326772213\n",
      "epoch: 18 step: 30, loss is 0.011517683044075966\n",
      "epoch: 18 step: 31, loss is 0.0036972076632082462\n",
      "epoch: 18 step: 32, loss is 0.0029472883325070143\n",
      "epoch: 18 step: 33, loss is 0.02345050685107708\n",
      "epoch: 18 step: 34, loss is 0.010257760994136333\n",
      "epoch: 18 step: 35, loss is 0.007934117689728737\n",
      "epoch: 18 step: 36, loss is 0.03848624229431152\n",
      "epoch: 18 step: 37, loss is 0.002409581560641527\n",
      "epoch: 18 step: 38, loss is 0.0011154039530083537\n",
      "epoch: 18 step: 39, loss is 0.019315339624881744\n",
      "epoch: 18 step: 40, loss is 0.01946682669222355\n",
      "epoch: 18 step: 41, loss is 0.00477246381342411\n",
      "epoch: 18 step: 42, loss is 0.04196015000343323\n",
      "epoch: 18 step: 43, loss is 0.021762028336524963\n",
      "epoch: 18 step: 44, loss is 0.01056321244686842\n",
      "epoch: 18 step: 45, loss is 0.027935313060879707\n",
      "epoch: 18 step: 46, loss is 0.01165392342954874\n",
      "epoch: 18 step: 47, loss is 0.002994136419147253\n",
      "epoch: 18 step: 48, loss is 0.004903565160930157\n",
      "epoch: 18 step: 49, loss is 0.01829580031335354\n",
      "epoch: 18 step: 50, loss is 0.00310347112827003\n",
      "epoch: 18 step: 51, loss is 0.037571981549263\n",
      "epoch: 18 step: 52, loss is 0.04889053478837013\n",
      "epoch: 18 step: 53, loss is 0.0033260895870625973\n",
      "epoch: 18 step: 54, loss is 0.002247462747618556\n",
      "epoch: 18 step: 55, loss is 0.011579960584640503\n",
      "epoch: 18 step: 56, loss is 0.006988782435655594\n",
      "epoch: 18 step: 57, loss is 0.014303500764071941\n",
      "epoch: 18 step: 58, loss is 0.09588172286748886\n",
      "epoch: 18 step: 59, loss is 0.0014611480291932821\n",
      "epoch: 18 step: 60, loss is 0.00490749953314662\n",
      "epoch: 18 step: 61, loss is 0.01016340870410204\n",
      "epoch: 18 step: 62, loss is 0.051928386092185974\n",
      "epoch: 18 step: 63, loss is 0.0008289992692880332\n",
      "epoch: 18 step: 64, loss is 0.003033296437934041\n",
      "epoch: 18 step: 65, loss is 0.05513660982251167\n",
      "epoch: 18 step: 66, loss is 0.0024543635081499815\n",
      "epoch: 18 step: 67, loss is 0.0043592085130512714\n",
      "epoch: 18 step: 68, loss is 0.0010236026719212532\n",
      "epoch: 18 step: 69, loss is 0.021937457844614983\n",
      "epoch: 18 step: 70, loss is 0.004762678407132626\n",
      "epoch: 18 step: 71, loss is 0.007541458588093519\n",
      "epoch: 18 step: 72, loss is 0.008658356964588165\n",
      "epoch: 18 step: 73, loss is 0.004116286523640156\n",
      "epoch: 18 step: 74, loss is 0.016529686748981476\n",
      "epoch: 18 step: 75, loss is 0.000772222934756428\n",
      "epoch: 18 step: 76, loss is 0.020277872681617737\n",
      "epoch: 18 step: 77, loss is 0.054196540266275406\n",
      "epoch: 18 step: 78, loss is 0.007122788578271866\n",
      "epoch: 18 step: 79, loss is 0.008230030536651611\n",
      "epoch: 18 step: 80, loss is 0.004553455859422684\n",
      "epoch: 18 step: 81, loss is 0.001962255919352174\n",
      "epoch: 18 step: 82, loss is 0.013094204477965832\n",
      "epoch: 18 step: 83, loss is 0.017824746668338776\n",
      "epoch: 18 step: 84, loss is 0.02973574958741665\n",
      "epoch: 18 step: 85, loss is 0.0046463534235954285\n",
      "epoch: 18 step: 86, loss is 0.0059407297521829605\n",
      "epoch: 18 step: 87, loss is 0.014343660324811935\n",
      "epoch: 18 step: 88, loss is 0.015552843920886517\n",
      "epoch: 18 step: 89, loss is 0.0021409967448562384\n",
      "epoch: 18 step: 90, loss is 0.004605211783200502\n",
      "epoch: 18 step: 91, loss is 0.004066613037139177\n",
      "epoch: 18 step: 92, loss is 0.007053636014461517\n",
      "epoch: 18 step: 93, loss is 0.0023819496855139732\n",
      "epoch: 18 step: 94, loss is 0.0012765699066221714\n",
      "epoch: 18 step: 95, loss is 0.0010940557112917304\n",
      "epoch: 18 step: 96, loss is 0.00450532091781497\n",
      "epoch: 18 step: 97, loss is 0.0023839124478399754\n",
      "epoch: 18 step: 98, loss is 0.026249397546052933\n",
      "epoch: 18 step: 99, loss is 0.007545748725533485\n",
      "epoch: 18 step: 100, loss is 0.013487993739545345\n",
      "epoch: 18 step: 101, loss is 0.0005382138187997043\n",
      "epoch: 18 step: 102, loss is 0.06576430797576904\n",
      "epoch: 18 step: 103, loss is 0.02828257717192173\n",
      "epoch: 18 step: 104, loss is 0.0030807810835540295\n",
      "epoch: 18 step: 105, loss is 0.027926256880164146\n",
      "epoch: 18 step: 106, loss is 0.004561122041195631\n",
      "epoch: 18 step: 107, loss is 0.008365216664969921\n",
      "epoch: 18 step: 108, loss is 0.015446530655026436\n",
      "epoch: 18 step: 109, loss is 0.0061750710010528564\n",
      "epoch: 18 step: 110, loss is 0.029575766995549202\n",
      "epoch: 18 step: 111, loss is 0.017693111672997475\n",
      "epoch: 18 step: 112, loss is 0.018673241138458252\n",
      "epoch: 18 step: 113, loss is 0.025258909910917282\n",
      "epoch: 18 step: 114, loss is 0.00410017604008317\n",
      "epoch: 18 step: 115, loss is 0.0028127713594585657\n",
      "epoch: 18 step: 116, loss is 0.00257774256169796\n",
      "epoch: 18 step: 117, loss is 0.0077826716005802155\n",
      "epoch: 18 step: 118, loss is 0.003420749679207802\n",
      "epoch: 18 step: 119, loss is 0.026354342699050903\n",
      "epoch: 18 step: 120, loss is 0.000766269862651825\n",
      "epoch: 18 step: 121, loss is 0.00023880143999122083\n",
      "epoch: 18 step: 122, loss is 0.05939221754670143\n",
      "epoch: 18 step: 123, loss is 0.015716761350631714\n",
      "epoch: 18 step: 124, loss is 0.007603644393384457\n",
      "epoch: 18 step: 125, loss is 0.001026300829835236\n",
      "epoch: 18 step: 126, loss is 0.0030226099770516157\n",
      "epoch: 18 step: 127, loss is 0.006691519636660814\n",
      "epoch: 18 step: 128, loss is 0.036403968930244446\n",
      "epoch: 18 step: 129, loss is 0.018647322431206703\n",
      "epoch: 18 step: 130, loss is 0.007746631745249033\n",
      "epoch: 18 step: 131, loss is 0.00805666483938694\n",
      "epoch: 18 step: 132, loss is 0.049916598945856094\n",
      "epoch: 18 step: 133, loss is 0.002677402226254344\n",
      "epoch: 18 step: 134, loss is 0.014536992646753788\n",
      "epoch: 18 step: 135, loss is 0.03872726857662201\n",
      "epoch: 18 step: 136, loss is 0.012269357219338417\n",
      "epoch: 18 step: 137, loss is 0.001563127152621746\n",
      "epoch: 18 step: 138, loss is 0.0018236773321405053\n",
      "epoch: 18 step: 139, loss is 0.06277015060186386\n",
      "epoch: 18 step: 140, loss is 0.0049827853217720985\n",
      "epoch: 18 step: 141, loss is 0.002279148669913411\n",
      "epoch: 18 step: 142, loss is 0.03875034675002098\n",
      "epoch: 18 step: 143, loss is 0.0041331946849823\n",
      "epoch: 18 step: 144, loss is 0.0006494676345027983\n",
      "epoch: 18 step: 145, loss is 0.0025942556094378233\n",
      "epoch: 18 step: 146, loss is 0.000464303680928424\n",
      "epoch: 18 step: 147, loss is 0.02034950628876686\n",
      "epoch: 18 step: 148, loss is 0.04469643905758858\n",
      "epoch: 18 step: 149, loss is 0.0020746891386806965\n",
      "epoch: 18 step: 150, loss is 0.0024204731453210115\n",
      "epoch: 18 step: 151, loss is 0.0072924173437058926\n",
      "epoch: 18 step: 152, loss is 0.0018023514421656728\n",
      "epoch: 18 step: 153, loss is 0.012282098643481731\n",
      "epoch: 18 step: 154, loss is 0.0017600592691451311\n",
      "epoch: 18 step: 155, loss is 0.015379590913653374\n",
      "epoch: 18 step: 156, loss is 0.006623314693570137\n",
      "epoch: 18 step: 157, loss is 0.012414543889462948\n",
      "epoch: 18 step: 158, loss is 0.002683544298633933\n",
      "epoch: 18 step: 159, loss is 0.0008760556811466813\n",
      "epoch: 18 step: 160, loss is 0.002164467005059123\n",
      "epoch: 18 step: 161, loss is 0.031419020146131516\n",
      "epoch: 18 step: 162, loss is 0.0002784959215205163\n",
      "epoch: 18 step: 163, loss is 0.0037789023481309414\n",
      "epoch: 18 step: 164, loss is 0.0011903485283255577\n",
      "epoch: 18 step: 165, loss is 0.09545496106147766\n",
      "epoch: 18 step: 166, loss is 0.0020191732328385115\n",
      "epoch: 18 step: 167, loss is 0.023004893213510513\n",
      "epoch: 18 step: 168, loss is 0.0013370538363233209\n",
      "epoch: 18 step: 169, loss is 0.001967937685549259\n",
      "epoch: 18 step: 170, loss is 0.035628512501716614\n",
      "epoch: 18 step: 171, loss is 0.01558829378336668\n",
      "epoch: 18 step: 172, loss is 0.0024943759199231863\n",
      "epoch: 18 step: 173, loss is 0.0033945259638130665\n",
      "epoch: 18 step: 174, loss is 0.02108123153448105\n",
      "epoch: 18 step: 175, loss is 0.002474119421094656\n",
      "epoch: 18 step: 176, loss is 0.004575653932988644\n",
      "epoch: 18 step: 177, loss is 0.005411697085946798\n",
      "epoch: 18 step: 178, loss is 0.012855341657996178\n",
      "epoch: 18 step: 179, loss is 0.01203590165823698\n",
      "epoch: 18 step: 180, loss is 0.02912742644548416\n",
      "epoch: 18 step: 181, loss is 0.017210956662893295\n",
      "epoch: 18 step: 182, loss is 0.011320213787257671\n",
      "epoch: 18 step: 183, loss is 0.00284240092150867\n",
      "epoch: 18 step: 184, loss is 0.012258549220860004\n",
      "epoch: 18 step: 185, loss is 0.020548777654767036\n",
      "epoch: 18 step: 186, loss is 0.01197829470038414\n",
      "epoch: 18 step: 187, loss is 0.029719488695263863\n",
      "epoch: 18 step: 188, loss is 0.0016895376611500978\n",
      "epoch: 18 step: 189, loss is 0.003223482519388199\n",
      "epoch: 18 step: 190, loss is 0.005183025728911161\n",
      "epoch: 18 step: 191, loss is 0.022786755114793777\n",
      "epoch: 18 step: 192, loss is 0.026267262175679207\n",
      "epoch: 18 step: 193, loss is 0.0066734906286001205\n",
      "epoch: 18 step: 194, loss is 0.0074998424388468266\n",
      "epoch: 18 step: 195, loss is 0.006088537164032459\n",
      "epoch: 18 step: 196, loss is 0.0046309554018080235\n",
      "epoch: 18 step: 197, loss is 0.16157527267932892\n",
      "epoch: 18 step: 198, loss is 0.00560950068756938\n",
      "epoch: 18 step: 199, loss is 0.025776898488402367\n",
      "epoch: 18 step: 200, loss is 0.002376773627474904\n",
      "epoch: 18 step: 201, loss is 0.0008202973403967917\n",
      "epoch: 18 step: 202, loss is 0.0035839576739817858\n",
      "epoch: 18 step: 203, loss is 0.01329943910241127\n",
      "epoch: 18 step: 204, loss is 0.0007751430384814739\n",
      "epoch: 18 step: 205, loss is 0.0005969750927761197\n",
      "epoch: 18 step: 206, loss is 0.0062363469041883945\n",
      "epoch: 18 step: 207, loss is 0.008584646508097649\n",
      "epoch: 18 step: 208, loss is 0.0009615825256332755\n",
      "epoch: 18 step: 209, loss is 0.011542920023202896\n",
      "epoch: 18 step: 210, loss is 0.0033292239531874657\n",
      "epoch: 18 step: 211, loss is 0.03491215407848358\n",
      "epoch: 18 step: 212, loss is 0.0012009837664663792\n",
      "epoch: 18 step: 213, loss is 0.008101021870970726\n",
      "epoch: 18 step: 214, loss is 0.043545905500650406\n",
      "epoch: 18 step: 215, loss is 0.009332322515547276\n",
      "epoch: 18 step: 216, loss is 0.006617571227252483\n",
      "epoch: 18 step: 217, loss is 0.000732636486645788\n",
      "epoch: 18 step: 218, loss is 0.002812541089951992\n",
      "epoch: 18 step: 219, loss is 0.029978537932038307\n",
      "epoch: 18 step: 220, loss is 0.012079807929694653\n",
      "epoch: 18 step: 221, loss is 0.008940293453633785\n",
      "epoch: 18 step: 222, loss is 0.0036917501129209995\n",
      "epoch: 18 step: 223, loss is 0.006151934619992971\n",
      "epoch: 18 step: 224, loss is 0.0037275287322700024\n",
      "epoch: 18 step: 225, loss is 0.023501964285969734\n",
      "epoch: 18 step: 226, loss is 0.0005805265973322093\n",
      "epoch: 18 step: 227, loss is 0.06784304976463318\n",
      "epoch: 18 step: 228, loss is 0.005262466613203287\n",
      "epoch: 18 step: 229, loss is 0.02643466927111149\n",
      "epoch: 18 step: 230, loss is 0.0014913028571754694\n",
      "epoch: 18 step: 231, loss is 0.0008822432719171047\n",
      "epoch: 18 step: 232, loss is 0.011988261714577675\n",
      "epoch: 18 step: 233, loss is 0.0010705298045650125\n",
      "epoch: 18 step: 234, loss is 0.0007649983745068312\n",
      "epoch: 18 step: 235, loss is 0.044237371534109116\n",
      "epoch: 18 step: 236, loss is 0.016528863459825516\n",
      "epoch: 18 step: 237, loss is 0.0220811665058136\n",
      "epoch: 18 step: 238, loss is 0.0007640605908818543\n",
      "epoch: 18 step: 239, loss is 0.0026476485654711723\n",
      "epoch: 18 step: 240, loss is 0.006033763755112886\n",
      "epoch: 18 step: 241, loss is 0.005507740657776594\n",
      "epoch: 18 step: 242, loss is 0.0062112449668347836\n",
      "epoch: 18 step: 243, loss is 0.001991824945434928\n",
      "epoch: 18 step: 244, loss is 0.0014437086647376418\n",
      "epoch: 18 step: 245, loss is 0.050576575100421906\n",
      "epoch: 18 step: 246, loss is 0.00970187596976757\n",
      "epoch: 18 step: 247, loss is 0.009705173783004284\n",
      "epoch: 18 step: 248, loss is 0.0011160166468471289\n",
      "epoch: 18 step: 249, loss is 0.018472732976078987\n",
      "epoch: 18 step: 250, loss is 0.000498577079270035\n",
      "epoch: 18 step: 251, loss is 0.06926318258047104\n",
      "epoch: 18 step: 252, loss is 0.02857786975800991\n",
      "epoch: 18 step: 253, loss is 0.014754479750990868\n",
      "epoch: 18 step: 254, loss is 0.0017499151872470975\n",
      "epoch: 18 step: 255, loss is 0.00023711432004347444\n",
      "epoch: 18 step: 256, loss is 0.014412760734558105\n",
      "epoch: 18 step: 257, loss is 0.0016992625314742327\n",
      "epoch: 18 step: 258, loss is 0.01166216004639864\n",
      "epoch: 18 step: 259, loss is 0.00015180780610535294\n",
      "epoch: 18 step: 260, loss is 0.003074588254094124\n",
      "epoch: 18 step: 261, loss is 0.0010153462644666433\n",
      "epoch: 18 step: 262, loss is 0.003036510432139039\n",
      "epoch: 18 step: 263, loss is 0.0009515167330391705\n",
      "epoch: 18 step: 264, loss is 0.02997272089123726\n",
      "epoch: 18 step: 265, loss is 0.004418327007442713\n",
      "epoch: 18 step: 266, loss is 0.002539797918871045\n",
      "epoch: 18 step: 267, loss is 0.03852776437997818\n",
      "epoch: 18 step: 268, loss is 0.0006194470915943384\n",
      "epoch: 18 step: 269, loss is 0.03761318698525429\n",
      "epoch: 18 step: 270, loss is 0.0051278723403811455\n",
      "epoch: 18 step: 271, loss is 0.0021632020361721516\n",
      "epoch: 18 step: 272, loss is 0.005117976572364569\n",
      "epoch: 18 step: 273, loss is 0.001327107660472393\n",
      "epoch: 18 step: 274, loss is 0.04958821088075638\n",
      "epoch: 18 step: 275, loss is 0.012361869215965271\n",
      "epoch: 18 step: 276, loss is 0.004708280321210623\n",
      "epoch: 18 step: 277, loss is 0.0016356598353013396\n",
      "epoch: 18 step: 278, loss is 0.01250413991510868\n",
      "epoch: 18 step: 279, loss is 0.005134097766131163\n",
      "epoch: 18 step: 280, loss is 0.004102547653019428\n",
      "epoch: 18 step: 281, loss is 0.004805394448339939\n",
      "epoch: 18 step: 282, loss is 0.004857463296502829\n",
      "epoch: 18 step: 283, loss is 0.0074849361553788185\n",
      "epoch: 18 step: 284, loss is 0.08071916550397873\n",
      "epoch: 18 step: 285, loss is 0.1041809618473053\n",
      "epoch: 18 step: 286, loss is 0.00017380251665599644\n",
      "epoch: 18 step: 287, loss is 0.0004470034036785364\n",
      "epoch: 18 step: 288, loss is 0.027833426371216774\n",
      "epoch: 18 step: 289, loss is 0.0021286322735249996\n",
      "epoch: 18 step: 290, loss is 0.0059167686849832535\n",
      "epoch: 18 step: 291, loss is 0.009420011192560196\n",
      "epoch: 18 step: 292, loss is 0.027056463062763214\n",
      "epoch: 18 step: 293, loss is 0.0019385340856388211\n",
      "epoch: 18 step: 294, loss is 0.021326692774891853\n",
      "epoch: 18 step: 295, loss is 0.001668249606154859\n",
      "epoch: 18 step: 296, loss is 0.0007646927260793746\n",
      "epoch: 18 step: 297, loss is 0.010890844278037548\n",
      "epoch: 18 step: 298, loss is 0.09092198312282562\n",
      "epoch: 18 step: 299, loss is 0.00135523802600801\n",
      "epoch: 18 step: 300, loss is 0.0004034920420963317\n",
      "epoch: 18 step: 301, loss is 0.0024124246556311846\n",
      "epoch: 18 step: 302, loss is 0.03207048773765564\n",
      "epoch: 18 step: 303, loss is 0.01098359003663063\n",
      "epoch: 18 step: 304, loss is 0.004743873607367277\n",
      "epoch: 18 step: 305, loss is 0.16178040206432343\n",
      "epoch: 18 step: 306, loss is 0.017269844189286232\n",
      "epoch: 18 step: 307, loss is 0.009495860897004604\n",
      "epoch: 18 step: 308, loss is 0.003153215628117323\n",
      "epoch: 18 step: 309, loss is 0.0011833885218948126\n",
      "epoch: 18 step: 310, loss is 0.016808170825242996\n",
      "epoch: 18 step: 311, loss is 0.0006294316845014691\n",
      "epoch: 18 step: 312, loss is 0.003594236448407173\n",
      "epoch: 18 step: 313, loss is 0.0028450540266931057\n",
      "epoch: 18 step: 314, loss is 0.04930251091718674\n",
      "epoch: 18 step: 315, loss is 0.006496858317404985\n",
      "epoch: 18 step: 316, loss is 0.0018662151414901018\n",
      "epoch: 18 step: 317, loss is 0.05852165445685387\n",
      "epoch: 18 step: 318, loss is 0.0004386742948554456\n",
      "epoch: 18 step: 319, loss is 0.0013917505275458097\n",
      "epoch: 18 step: 320, loss is 0.03403705358505249\n",
      "epoch: 18 step: 321, loss is 0.044865239411592484\n",
      "epoch: 18 step: 322, loss is 0.03750915080308914\n",
      "epoch: 18 step: 323, loss is 0.00506112864241004\n",
      "epoch: 18 step: 324, loss is 0.0027272040024399757\n",
      "epoch: 18 step: 325, loss is 0.03387289494276047\n",
      "epoch: 18 step: 326, loss is 0.0032070425804704428\n",
      "epoch: 18 step: 327, loss is 0.0016945977695286274\n",
      "epoch: 18 step: 328, loss is 3.8140815377118997e-06\n",
      "epoch: 18 step: 329, loss is 0.12433647364377975\n",
      "epoch: 18 step: 330, loss is 0.009681853465735912\n",
      "epoch: 18 step: 331, loss is 0.01009008102118969\n",
      "epoch: 18 step: 332, loss is 0.00046837134868837893\n",
      "epoch: 18 step: 333, loss is 0.0027398148085922003\n",
      "epoch: 18 step: 334, loss is 0.0007729096105322242\n",
      "epoch: 18 step: 335, loss is 0.0034477082081139088\n",
      "epoch: 18 step: 336, loss is 0.1466710865497589\n",
      "epoch: 18 step: 337, loss is 0.0020366180688142776\n",
      "epoch: 18 step: 338, loss is 0.04865773767232895\n",
      "epoch: 18 step: 339, loss is 0.08471563458442688\n",
      "epoch: 18 step: 340, loss is 0.010625992901623249\n",
      "epoch: 18 step: 341, loss is 0.0005442448309622705\n",
      "epoch: 18 step: 342, loss is 0.006931624840945005\n",
      "epoch: 18 step: 343, loss is 0.010884109884500504\n",
      "epoch: 18 step: 344, loss is 0.03188140690326691\n",
      "epoch: 18 step: 345, loss is 0.010619676671922207\n",
      "epoch: 18 step: 346, loss is 0.002069098874926567\n",
      "epoch: 18 step: 347, loss is 0.013314565643668175\n",
      "epoch: 18 step: 348, loss is 0.016073482111096382\n",
      "epoch: 18 step: 349, loss is 0.003107859753072262\n",
      "epoch: 18 step: 350, loss is 0.011191463097929955\n",
      "epoch: 18 step: 351, loss is 0.0038217336405068636\n",
      "epoch: 18 step: 352, loss is 0.0001621949195396155\n",
      "epoch: 18 step: 353, loss is 0.007881366647779942\n",
      "epoch: 18 step: 354, loss is 0.017187759280204773\n",
      "epoch: 18 step: 355, loss is 0.009457032196223736\n",
      "epoch: 18 step: 356, loss is 0.004057940095663071\n",
      "epoch: 18 step: 357, loss is 0.004888125695288181\n",
      "epoch: 18 step: 358, loss is 0.014552155509591103\n",
      "epoch: 18 step: 359, loss is 0.0036477467510849237\n",
      "epoch: 18 step: 360, loss is 0.006902644410729408\n",
      "epoch: 18 step: 361, loss is 0.0012148652458563447\n",
      "epoch: 18 step: 362, loss is 0.0037260428071022034\n",
      "epoch: 18 step: 363, loss is 0.015580616891384125\n",
      "epoch: 18 step: 364, loss is 0.0640207901597023\n",
      "epoch: 18 step: 365, loss is 0.0009467158233746886\n",
      "epoch: 18 step: 366, loss is 0.004609077237546444\n",
      "epoch: 18 step: 367, loss is 0.04164033383131027\n",
      "epoch: 18 step: 368, loss is 0.0021773874759674072\n",
      "epoch: 18 step: 369, loss is 0.0035772358532994986\n",
      "epoch: 18 step: 370, loss is 0.005488909315317869\n",
      "epoch: 18 step: 371, loss is 0.033467669039964676\n",
      "epoch: 18 step: 372, loss is 0.010361546650528908\n",
      "epoch: 18 step: 373, loss is 0.0938602089881897\n",
      "epoch: 18 step: 374, loss is 0.002049621194601059\n",
      "epoch: 18 step: 375, loss is 0.03313327953219414\n",
      "epoch: 18 step: 376, loss is 0.0004065177636221051\n",
      "epoch: 18 step: 377, loss is 0.005968439392745495\n",
      "epoch: 18 step: 378, loss is 0.03364766016602516\n",
      "epoch: 18 step: 379, loss is 0.007494020275771618\n",
      "epoch: 18 step: 380, loss is 0.00033840807736851275\n",
      "epoch: 18 step: 381, loss is 0.01990487053990364\n",
      "epoch: 18 step: 382, loss is 0.003649473888799548\n",
      "epoch: 18 step: 383, loss is 0.054494891315698624\n",
      "epoch: 18 step: 384, loss is 0.07313290983438492\n",
      "epoch: 18 step: 385, loss is 2.149917600036133e-05\n",
      "epoch: 18 step: 386, loss is 0.03754536807537079\n",
      "epoch: 18 step: 387, loss is 0.03249745815992355\n",
      "epoch: 18 step: 388, loss is 0.0014811964938417077\n",
      "epoch: 18 step: 389, loss is 0.01977454498410225\n",
      "epoch: 18 step: 390, loss is 0.000339584075845778\n",
      "epoch: 18 step: 391, loss is 0.0030804078560322523\n",
      "epoch: 18 step: 392, loss is 0.0004170685715507716\n",
      "epoch: 18 step: 393, loss is 0.00666769640520215\n",
      "epoch: 18 step: 394, loss is 0.0738355964422226\n",
      "epoch: 18 step: 395, loss is 0.040671367198228836\n",
      "epoch: 18 step: 396, loss is 0.018852325156331062\n",
      "epoch: 18 step: 397, loss is 0.06612802296876907\n",
      "epoch: 18 step: 398, loss is 0.01063997857272625\n",
      "epoch: 18 step: 399, loss is 0.0023832523729652166\n",
      "epoch: 18 step: 400, loss is 0.040317755192518234\n",
      "epoch: 18 step: 401, loss is 0.014451762661337852\n",
      "epoch: 18 step: 402, loss is 0.0008721665944904089\n",
      "epoch: 18 step: 403, loss is 0.02155819535255432\n",
      "epoch: 18 step: 404, loss is 0.022499989718198776\n",
      "epoch: 18 step: 405, loss is 0.03318753093481064\n",
      "epoch: 18 step: 406, loss is 0.04663645103573799\n",
      "epoch: 18 step: 407, loss is 0.026969488710165024\n",
      "epoch: 18 step: 408, loss is 0.00534425862133503\n",
      "epoch: 18 step: 409, loss is 0.007655343506485224\n",
      "epoch: 18 step: 410, loss is 0.0005842493264935911\n",
      "epoch: 18 step: 411, loss is 0.0008366417023353279\n",
      "epoch: 18 step: 412, loss is 0.014140517450869083\n",
      "epoch: 18 step: 413, loss is 0.013389901258051395\n",
      "epoch: 18 step: 414, loss is 0.022425096482038498\n",
      "epoch: 18 step: 415, loss is 0.0021518580615520477\n",
      "epoch: 18 step: 416, loss is 0.0337434783577919\n",
      "epoch: 18 step: 417, loss is 0.08909312635660172\n",
      "epoch: 18 step: 418, loss is 0.005683696363121271\n",
      "epoch: 18 step: 419, loss is 0.0007762778550386429\n",
      "epoch: 18 step: 420, loss is 0.02530035562813282\n",
      "epoch: 18 step: 421, loss is 0.006434423848986626\n",
      "epoch: 18 step: 422, loss is 0.06594894826412201\n",
      "epoch: 18 step: 423, loss is 0.004065110348165035\n",
      "epoch: 18 step: 424, loss is 0.006934888660907745\n",
      "epoch: 18 step: 425, loss is 0.1266261339187622\n",
      "epoch: 18 step: 426, loss is 0.030734330415725708\n",
      "epoch: 18 step: 427, loss is 0.046507492661476135\n",
      "epoch: 18 step: 428, loss is 0.017628781497478485\n",
      "epoch: 18 step: 429, loss is 0.047892820090055466\n",
      "epoch: 18 step: 430, loss is 0.015898598358035088\n",
      "epoch: 18 step: 431, loss is 0.1652180403470993\n",
      "epoch: 18 step: 432, loss is 0.029230546206235886\n",
      "epoch: 18 step: 433, loss is 0.03262319788336754\n",
      "epoch: 18 step: 434, loss is 0.02136509120464325\n",
      "epoch: 18 step: 435, loss is 0.0004663283471018076\n",
      "epoch: 18 step: 436, loss is 0.009696261025965214\n",
      "epoch: 18 step: 437, loss is 0.01215158961713314\n",
      "epoch: 18 step: 438, loss is 0.03144121170043945\n",
      "epoch: 18 step: 439, loss is 0.001729926560074091\n",
      "epoch: 18 step: 440, loss is 0.04712827876210213\n",
      "epoch: 18 step: 441, loss is 0.16634409129619598\n",
      "epoch: 18 step: 442, loss is 0.1605948954820633\n",
      "epoch: 18 step: 443, loss is 0.001360033405944705\n",
      "epoch: 18 step: 444, loss is 0.041676849126815796\n",
      "epoch: 18 step: 445, loss is 0.0140508022159338\n",
      "epoch: 18 step: 446, loss is 0.0055446927435696125\n",
      "epoch: 18 step: 447, loss is 0.011534412391483784\n",
      "epoch: 18 step: 448, loss is 0.0033254872541874647\n",
      "epoch: 18 step: 449, loss is 0.0023343199864029884\n",
      "epoch: 18 step: 450, loss is 0.024203700944781303\n",
      "epoch: 18 step: 451, loss is 0.02367977984249592\n",
      "epoch: 18 step: 452, loss is 0.013299301266670227\n",
      "epoch: 18 step: 453, loss is 0.015864679589867592\n",
      "epoch: 18 step: 454, loss is 0.0011763962684199214\n",
      "epoch: 18 step: 455, loss is 0.013395768590271473\n",
      "epoch: 18 step: 456, loss is 0.012273604981601238\n",
      "epoch: 18 step: 457, loss is 0.001262515434063971\n",
      "epoch: 18 step: 458, loss is 0.048012033104896545\n",
      "epoch: 18 step: 459, loss is 0.007699004840105772\n",
      "epoch: 18 step: 460, loss is 0.0018464880995452404\n",
      "epoch: 18 step: 461, loss is 0.011116289533674717\n",
      "epoch: 18 step: 462, loss is 0.015036934055387974\n",
      "epoch: 18 step: 463, loss is 0.03624621033668518\n",
      "epoch: 18 step: 464, loss is 0.004568881820887327\n",
      "epoch: 18 step: 465, loss is 0.015921296551823616\n",
      "epoch: 18 step: 466, loss is 0.08591751009225845\n",
      "epoch: 18 step: 467, loss is 0.002426060615107417\n",
      "epoch: 18 step: 468, loss is 0.03289330005645752\n",
      "epoch: 18 step: 469, loss is 0.006324644200503826\n",
      "epoch: 18 step: 470, loss is 0.001489406917244196\n",
      "epoch: 18 step: 471, loss is 0.0022544702515006065\n",
      "epoch: 18 step: 472, loss is 0.007186200004070997\n",
      "epoch: 18 step: 473, loss is 0.0032406237442046404\n",
      "epoch: 18 step: 474, loss is 0.060539305210113525\n",
      "epoch: 18 step: 475, loss is 0.050872668623924255\n",
      "epoch: 18 step: 476, loss is 0.011882498860359192\n",
      "epoch: 18 step: 477, loss is 0.008821511641144753\n",
      "epoch: 18 step: 478, loss is 0.03675556182861328\n",
      "epoch: 18 step: 479, loss is 0.0003732625627890229\n",
      "epoch: 18 step: 480, loss is 0.01866006664931774\n",
      "epoch: 18 step: 481, loss is 0.015044177882373333\n",
      "epoch: 18 step: 482, loss is 0.004661154467612505\n",
      "epoch: 18 step: 483, loss is 0.0005641119787469506\n",
      "epoch: 18 step: 484, loss is 0.007405813317745924\n",
      "epoch: 18 step: 485, loss is 0.0018132944824174047\n",
      "epoch: 18 step: 486, loss is 0.010876880027353764\n",
      "epoch: 18 step: 487, loss is 0.014802567660808563\n",
      "epoch: 18 step: 488, loss is 0.007876106537878513\n",
      "epoch: 18 step: 489, loss is 0.01569797284901142\n",
      "epoch: 18 step: 490, loss is 0.019220279529690742\n",
      "epoch: 18 step: 491, loss is 0.0036176342982798815\n",
      "epoch: 18 step: 492, loss is 0.0995250791311264\n",
      "epoch: 18 step: 493, loss is 0.004348730202764273\n",
      "epoch: 18 step: 494, loss is 0.03331257402896881\n",
      "epoch: 18 step: 495, loss is 0.03748800605535507\n",
      "epoch: 18 step: 496, loss is 0.003715724218636751\n",
      "epoch: 18 step: 497, loss is 0.009772765450179577\n",
      "epoch: 18 step: 498, loss is 0.03565593808889389\n",
      "epoch: 18 step: 499, loss is 0.009318131022155285\n",
      "epoch: 18 step: 500, loss is 0.0016802994068711996\n",
      "epoch: 18 step: 501, loss is 0.0011576517717912793\n",
      "epoch: 18 step: 502, loss is 0.009374111890792847\n",
      "epoch: 18 step: 503, loss is 0.0013010390102863312\n",
      "epoch: 18 step: 504, loss is 0.0012331762118265033\n",
      "epoch: 18 step: 505, loss is 0.002513144165277481\n",
      "epoch: 18 step: 506, loss is 0.06281477212905884\n",
      "epoch: 18 step: 507, loss is 0.09735053032636642\n",
      "epoch: 18 step: 508, loss is 0.016465891152620316\n",
      "epoch: 18 step: 509, loss is 0.004366463981568813\n",
      "epoch: 18 step: 510, loss is 0.0073854196816682816\n",
      "epoch: 18 step: 511, loss is 0.09107300639152527\n",
      "epoch: 18 step: 512, loss is 0.05887051299214363\n",
      "epoch: 18 step: 513, loss is 0.0059007746167480946\n",
      "epoch: 18 step: 514, loss is 0.0013993082102388144\n",
      "epoch: 18 step: 515, loss is 0.002955249510705471\n",
      "epoch: 18 step: 516, loss is 0.00044828353566117585\n",
      "epoch: 18 step: 517, loss is 0.01094322931021452\n",
      "epoch: 18 step: 518, loss is 0.05893273651599884\n",
      "epoch: 18 step: 519, loss is 0.05356288701295853\n",
      "epoch: 18 step: 520, loss is 0.010337259620428085\n",
      "epoch: 18 step: 521, loss is 0.031535010784864426\n",
      "epoch: 18 step: 522, loss is 0.04682805389165878\n",
      "epoch: 18 step: 523, loss is 0.1069769486784935\n",
      "epoch: 18 step: 524, loss is 0.012295322492718697\n",
      "epoch: 18 step: 525, loss is 0.03185700252652168\n",
      "epoch: 18 step: 526, loss is 0.0004789195372723043\n",
      "epoch: 18 step: 527, loss is 0.01845933310687542\n",
      "epoch: 18 step: 528, loss is 0.024565981701016426\n",
      "epoch: 18 step: 529, loss is 0.01582011952996254\n",
      "epoch: 18 step: 530, loss is 0.008414055220782757\n",
      "epoch: 18 step: 531, loss is 0.00804921891540289\n",
      "epoch: 18 step: 532, loss is 0.0077848718501627445\n",
      "epoch: 18 step: 533, loss is 0.024034399539232254\n",
      "epoch: 18 step: 534, loss is 0.008397094905376434\n",
      "epoch: 18 step: 535, loss is 0.0015808513853698969\n",
      "epoch: 18 step: 536, loss is 0.051227010786533356\n",
      "epoch: 18 step: 537, loss is 0.01011801976710558\n",
      "epoch: 18 step: 538, loss is 0.022748958319425583\n",
      "epoch: 18 step: 539, loss is 0.00757393566891551\n",
      "epoch: 18 step: 540, loss is 0.03697557747364044\n",
      "epoch: 18 step: 541, loss is 0.06776705384254456\n",
      "epoch: 18 step: 542, loss is 0.002379985060542822\n",
      "epoch: 18 step: 543, loss is 0.02817477285861969\n",
      "epoch: 18 step: 544, loss is 0.058077916502952576\n",
      "epoch: 18 step: 545, loss is 0.052415430545806885\n",
      "epoch: 18 step: 546, loss is 0.017597787082195282\n",
      "epoch: 18 step: 547, loss is 0.030887862667441368\n",
      "epoch: 18 step: 548, loss is 0.03162640333175659\n",
      "epoch: 18 step: 549, loss is 0.018559779971837997\n",
      "epoch: 18 step: 550, loss is 0.02511667273938656\n",
      "epoch: 18 step: 551, loss is 0.11193088442087173\n",
      "epoch: 18 step: 552, loss is 0.012858022004365921\n",
      "epoch: 18 step: 553, loss is 0.027701061218976974\n",
      "epoch: 18 step: 554, loss is 0.006053554359823465\n",
      "epoch: 18 step: 555, loss is 0.01745249517261982\n",
      "epoch: 18 step: 556, loss is 0.03573073819279671\n",
      "epoch: 18 step: 557, loss is 0.10396026074886322\n",
      "epoch: 18 step: 558, loss is 0.00901811383664608\n",
      "epoch: 18 step: 559, loss is 0.004123492632061243\n",
      "epoch: 18 step: 560, loss is 0.001975041814148426\n",
      "epoch: 18 step: 561, loss is 0.006424307823181152\n",
      "epoch: 18 step: 562, loss is 0.004097592085599899\n",
      "epoch: 18 step: 563, loss is 0.020108655095100403\n",
      "epoch: 18 step: 564, loss is 0.002910605166107416\n",
      "epoch: 18 step: 565, loss is 0.022940343245863914\n",
      "epoch: 18 step: 566, loss is 0.0024891234934329987\n",
      "epoch: 18 step: 567, loss is 0.02460312470793724\n",
      "epoch: 18 step: 568, loss is 0.004160696640610695\n",
      "epoch: 18 step: 569, loss is 0.057133354246616364\n",
      "epoch: 18 step: 570, loss is 0.00749686174094677\n",
      "epoch: 18 step: 571, loss is 0.015067494474351406\n",
      "epoch: 18 step: 572, loss is 0.03571927547454834\n",
      "epoch: 18 step: 573, loss is 0.030113328248262405\n",
      "epoch: 18 step: 574, loss is 0.0012335141655057669\n",
      "epoch: 18 step: 575, loss is 0.0020834002643823624\n",
      "epoch: 18 step: 576, loss is 0.022707587108016014\n",
      "epoch: 18 step: 577, loss is 0.005697511602193117\n",
      "epoch: 18 step: 578, loss is 0.008198462426662445\n",
      "epoch: 18 step: 579, loss is 0.0017253410769626498\n",
      "epoch: 18 step: 580, loss is 0.06414173543453217\n",
      "epoch: 18 step: 581, loss is 0.007254281546920538\n",
      "epoch: 18 step: 582, loss is 0.00037566234823316336\n",
      "epoch: 18 step: 583, loss is 0.04347958788275719\n",
      "epoch: 18 step: 584, loss is 0.06988595426082611\n",
      "epoch: 18 step: 585, loss is 0.01922408863902092\n",
      "epoch: 18 step: 586, loss is 0.00035767065128311515\n",
      "epoch: 18 step: 587, loss is 0.026196375489234924\n",
      "epoch: 18 step: 588, loss is 0.07017815113067627\n",
      "epoch: 18 step: 589, loss is 0.004135995637625456\n",
      "epoch: 18 step: 590, loss is 0.005004847422242165\n",
      "epoch: 18 step: 591, loss is 0.022845948114991188\n",
      "epoch: 18 step: 592, loss is 0.0011598053388297558\n",
      "epoch: 18 step: 593, loss is 0.02308378741145134\n",
      "epoch: 18 step: 594, loss is 0.0013074225280433893\n",
      "epoch: 18 step: 595, loss is 0.0016483186045661569\n",
      "epoch: 18 step: 596, loss is 0.0005256918375380337\n",
      "epoch: 18 step: 597, loss is 0.0005735097802244127\n",
      "epoch: 18 step: 598, loss is 0.001300961710512638\n",
      "epoch: 18 step: 599, loss is 0.003937111236155033\n",
      "epoch: 18 step: 600, loss is 0.02387353591620922\n",
      "epoch: 18 step: 601, loss is 0.05267529562115669\n",
      "epoch: 18 step: 602, loss is 0.0012956546852365136\n",
      "epoch: 18 step: 603, loss is 0.020762424916028976\n",
      "epoch: 18 step: 604, loss is 0.0027572030667215586\n",
      "epoch: 18 step: 605, loss is 0.013084069825708866\n",
      "epoch: 18 step: 606, loss is 0.1719161570072174\n",
      "epoch: 18 step: 607, loss is 0.020513994619250298\n",
      "epoch: 18 step: 608, loss is 0.03594508394598961\n",
      "epoch: 18 step: 609, loss is 0.013289395719766617\n",
      "epoch: 18 step: 610, loss is 0.04403110593557358\n",
      "epoch: 18 step: 611, loss is 0.047741539776325226\n",
      "epoch: 18 step: 612, loss is 0.008948390372097492\n",
      "epoch: 18 step: 613, loss is 0.01682450622320175\n",
      "epoch: 18 step: 614, loss is 0.046046651899814606\n",
      "epoch: 18 step: 615, loss is 0.018683193251490593\n",
      "epoch: 18 step: 616, loss is 0.0386338010430336\n",
      "epoch: 18 step: 617, loss is 0.023087969049811363\n",
      "epoch: 18 step: 618, loss is 0.007078979164361954\n",
      "epoch: 18 step: 619, loss is 0.052013132721185684\n",
      "epoch: 18 step: 620, loss is 0.047049615532159805\n",
      "epoch: 18 step: 621, loss is 0.04597157984972\n",
      "epoch: 18 step: 622, loss is 0.0029819789342582226\n",
      "epoch: 18 step: 623, loss is 0.006428116001188755\n",
      "epoch: 18 step: 624, loss is 0.0008030423196032643\n",
      "epoch: 18 step: 625, loss is 0.004073502495884895\n",
      "epoch: 18 step: 626, loss is 0.0023580240085721016\n",
      "epoch: 18 step: 627, loss is 0.006293501239269972\n",
      "epoch: 18 step: 628, loss is 0.0076770787127316\n",
      "epoch: 18 step: 629, loss is 0.0011017119977623224\n",
      "epoch: 18 step: 630, loss is 0.003509378060698509\n",
      "epoch: 18 step: 631, loss is 0.0012513662222772837\n",
      "epoch: 18 step: 632, loss is 0.007838308811187744\n",
      "epoch: 18 step: 633, loss is 0.024347543716430664\n",
      "epoch: 18 step: 634, loss is 0.012216102331876755\n",
      "epoch: 18 step: 635, loss is 0.012993928045034409\n",
      "epoch: 18 step: 636, loss is 0.005413006525486708\n",
      "epoch: 18 step: 637, loss is 0.008129972964525223\n",
      "epoch: 18 step: 638, loss is 0.0015589894028380513\n",
      "epoch: 18 step: 639, loss is 0.058995652943849564\n",
      "epoch: 18 step: 640, loss is 0.008521664887666702\n",
      "epoch: 18 step: 641, loss is 0.03084622696042061\n",
      "epoch: 18 step: 642, loss is 0.009844141080975533\n",
      "epoch: 18 step: 643, loss is 0.003933501895517111\n",
      "epoch: 18 step: 644, loss is 0.010611608624458313\n",
      "epoch: 18 step: 645, loss is 0.02834418974816799\n",
      "epoch: 18 step: 646, loss is 0.005162159446626902\n",
      "epoch: 18 step: 647, loss is 0.00013266033784020692\n",
      "epoch: 18 step: 648, loss is 0.09435099363327026\n",
      "epoch: 18 step: 649, loss is 0.005006211809813976\n",
      "epoch: 18 step: 650, loss is 0.0009187264367938042\n",
      "epoch: 18 step: 651, loss is 0.006707730703055859\n",
      "epoch: 18 step: 652, loss is 0.017298640683293343\n",
      "epoch: 18 step: 653, loss is 0.02409164048731327\n",
      "epoch: 18 step: 654, loss is 0.012717430479824543\n",
      "epoch: 18 step: 655, loss is 0.017795680090785027\n",
      "epoch: 18 step: 656, loss is 0.030553258955478668\n",
      "epoch: 18 step: 657, loss is 0.04333971068263054\n",
      "epoch: 18 step: 658, loss is 0.00021598197054117918\n",
      "epoch: 18 step: 659, loss is 0.0022539643105119467\n",
      "epoch: 18 step: 660, loss is 0.005380901508033276\n",
      "epoch: 18 step: 661, loss is 0.005219696555286646\n",
      "epoch: 18 step: 662, loss is 0.022584648802876472\n",
      "epoch: 18 step: 663, loss is 0.004912929609417915\n",
      "epoch: 18 step: 664, loss is 0.009872705675661564\n",
      "epoch: 18 step: 665, loss is 0.004923216067254543\n",
      "epoch: 18 step: 666, loss is 0.03166951984167099\n",
      "epoch: 18 step: 667, loss is 0.006845405325293541\n",
      "epoch: 18 step: 668, loss is 0.0007894004811532795\n",
      "epoch: 18 step: 669, loss is 0.0915570929646492\n",
      "epoch: 18 step: 670, loss is 0.06722458451986313\n",
      "epoch: 18 step: 671, loss is 0.02341507188975811\n",
      "epoch: 18 step: 672, loss is 0.010238911025226116\n",
      "epoch: 18 step: 673, loss is 0.0010776816634461284\n",
      "epoch: 18 step: 674, loss is 0.013309841975569725\n",
      "epoch: 18 step: 675, loss is 0.0010202631819993258\n",
      "epoch: 18 step: 676, loss is 0.007221888285130262\n",
      "epoch: 18 step: 677, loss is 0.013001175597310066\n",
      "epoch: 18 step: 678, loss is 0.00275133247487247\n",
      "epoch: 18 step: 679, loss is 0.04739570990204811\n",
      "epoch: 18 step: 680, loss is 0.039137303829193115\n",
      "epoch: 18 step: 681, loss is 0.0007103364914655685\n",
      "epoch: 18 step: 682, loss is 0.06040944531559944\n",
      "epoch: 18 step: 683, loss is 0.00454464228823781\n",
      "epoch: 18 step: 684, loss is 0.00865933671593666\n",
      "epoch: 18 step: 685, loss is 0.00899410992860794\n",
      "epoch: 18 step: 686, loss is 0.023861562833189964\n",
      "epoch: 18 step: 687, loss is 0.009734371677041054\n",
      "epoch: 18 step: 688, loss is 0.016771730035543442\n",
      "epoch: 18 step: 689, loss is 0.0566113106906414\n",
      "epoch: 18 step: 690, loss is 0.02102859504520893\n",
      "epoch: 18 step: 691, loss is 0.01636539027094841\n",
      "epoch: 18 step: 692, loss is 0.02591843344271183\n",
      "epoch: 18 step: 693, loss is 0.0003511476388666779\n",
      "epoch: 18 step: 694, loss is 0.01656321994960308\n",
      "epoch: 18 step: 695, loss is 0.021319637075066566\n",
      "epoch: 18 step: 696, loss is 0.06047339364886284\n",
      "epoch: 18 step: 697, loss is 0.09995575249195099\n",
      "epoch: 18 step: 698, loss is 0.01326545886695385\n",
      "epoch: 18 step: 699, loss is 0.002847987227141857\n",
      "epoch: 18 step: 700, loss is 0.003052014159038663\n",
      "epoch: 18 step: 701, loss is 0.00522437272593379\n",
      "epoch: 18 step: 702, loss is 0.12562455236911774\n",
      "epoch: 18 step: 703, loss is 0.03131909668445587\n",
      "epoch: 18 step: 704, loss is 0.0018541795434430242\n",
      "epoch: 18 step: 705, loss is 0.008168983273208141\n",
      "epoch: 18 step: 706, loss is 0.00557855935767293\n",
      "epoch: 18 step: 707, loss is 0.018975844606757164\n",
      "epoch: 18 step: 708, loss is 0.00031298361136578023\n",
      "epoch: 18 step: 709, loss is 0.020701145753264427\n",
      "epoch: 18 step: 710, loss is 0.0022056763991713524\n",
      "epoch: 18 step: 711, loss is 0.027528604492545128\n",
      "epoch: 18 step: 712, loss is 0.011954590678215027\n",
      "epoch: 18 step: 713, loss is 0.0010147104039788246\n",
      "epoch: 18 step: 714, loss is 0.014748016372323036\n",
      "epoch: 18 step: 715, loss is 0.0033067110925912857\n",
      "epoch: 18 step: 716, loss is 0.006039814092218876\n",
      "epoch: 18 step: 717, loss is 0.008488341234624386\n",
      "epoch: 18 step: 718, loss is 0.027267059311270714\n",
      "epoch: 18 step: 719, loss is 0.011068059131503105\n",
      "epoch: 18 step: 720, loss is 0.019956067204475403\n",
      "epoch: 18 step: 721, loss is 0.049971938133239746\n",
      "epoch: 18 step: 722, loss is 0.011101018637418747\n",
      "epoch: 18 step: 723, loss is 0.0016973861493170261\n",
      "epoch: 18 step: 724, loss is 0.00018167895905207843\n",
      "epoch: 18 step: 725, loss is 0.009692464023828506\n",
      "epoch: 18 step: 726, loss is 0.002360930200666189\n",
      "epoch: 18 step: 727, loss is 0.0012068824144080281\n",
      "epoch: 18 step: 728, loss is 0.13445694744586945\n",
      "epoch: 18 step: 729, loss is 0.0045389155857264996\n",
      "epoch: 18 step: 730, loss is 0.17410339415073395\n",
      "epoch: 18 step: 731, loss is 0.001133730635046959\n",
      "epoch: 18 step: 732, loss is 0.0034049316309392452\n",
      "epoch: 18 step: 733, loss is 0.007130045909434557\n",
      "epoch: 18 step: 734, loss is 0.052334703505039215\n",
      "epoch: 18 step: 735, loss is 0.0029306444339454174\n",
      "epoch: 18 step: 736, loss is 0.0004932535230182111\n",
      "epoch: 18 step: 737, loss is 0.00345670897513628\n",
      "epoch: 18 step: 738, loss is 0.0030459780246019363\n",
      "epoch: 18 step: 739, loss is 0.003957545850425959\n",
      "epoch: 18 step: 740, loss is 0.06373471766710281\n",
      "epoch: 18 step: 741, loss is 0.013443002477288246\n",
      "epoch: 18 step: 742, loss is 0.027717068791389465\n",
      "epoch: 18 step: 743, loss is 0.01716889813542366\n",
      "epoch: 18 step: 744, loss is 0.000875480764079839\n",
      "epoch: 18 step: 745, loss is 0.0005157025880180299\n",
      "epoch: 18 step: 746, loss is 0.0065086642280220985\n",
      "epoch: 18 step: 747, loss is 0.024284640327095985\n",
      "epoch: 18 step: 748, loss is 0.004248219542205334\n",
      "epoch: 18 step: 749, loss is 0.00941428542137146\n",
      "epoch: 18 step: 750, loss is 0.0013224000576883554\n",
      "epoch: 18 step: 751, loss is 0.02145448699593544\n",
      "epoch: 18 step: 752, loss is 0.028514821082353592\n",
      "epoch: 18 step: 753, loss is 0.04136097803711891\n",
      "epoch: 18 step: 754, loss is 0.12896974384784698\n",
      "epoch: 18 step: 755, loss is 0.036722876131534576\n",
      "epoch: 18 step: 756, loss is 0.004675781354308128\n",
      "epoch: 18 step: 757, loss is 0.00025869300588965416\n",
      "epoch: 18 step: 758, loss is 0.0018620818154886365\n",
      "epoch: 18 step: 759, loss is 0.01962251402437687\n",
      "epoch: 18 step: 760, loss is 0.003427556250244379\n",
      "epoch: 18 step: 761, loss is 0.006777751259505749\n",
      "epoch: 18 step: 762, loss is 0.03752567619085312\n",
      "epoch: 18 step: 763, loss is 0.05969949811697006\n",
      "epoch: 18 step: 764, loss is 0.04277349263429642\n",
      "epoch: 18 step: 765, loss is 0.010999917052686214\n",
      "epoch: 18 step: 766, loss is 0.02712206356227398\n",
      "epoch: 18 step: 767, loss is 0.05490661785006523\n",
      "epoch: 18 step: 768, loss is 0.05250134319067001\n",
      "epoch: 18 step: 769, loss is 0.05446399748325348\n",
      "epoch: 18 step: 770, loss is 0.06665200740098953\n",
      "epoch: 18 step: 771, loss is 0.055778395384550095\n",
      "epoch: 18 step: 772, loss is 0.0437965951859951\n",
      "epoch: 18 step: 773, loss is 0.05182131379842758\n",
      "epoch: 18 step: 774, loss is 0.0012630859855562449\n",
      "epoch: 18 step: 775, loss is 0.0018430508207529783\n",
      "epoch: 18 step: 776, loss is 0.03323584049940109\n",
      "epoch: 18 step: 777, loss is 0.0009264384862035513\n",
      "epoch: 18 step: 778, loss is 0.024175070226192474\n",
      "epoch: 18 step: 779, loss is 0.06203983724117279\n",
      "epoch: 18 step: 780, loss is 0.03937886282801628\n",
      "epoch: 18 step: 781, loss is 0.012079439125955105\n",
      "epoch: 18 step: 782, loss is 0.13271059095859528\n",
      "epoch: 18 step: 783, loss is 0.008090263232588768\n",
      "epoch: 18 step: 784, loss is 0.09543491899967194\n",
      "epoch: 18 step: 785, loss is 0.017610035836696625\n",
      "epoch: 18 step: 786, loss is 0.001024494762532413\n",
      "epoch: 18 step: 787, loss is 0.017975732684135437\n",
      "epoch: 18 step: 788, loss is 0.12236512452363968\n",
      "epoch: 18 step: 789, loss is 0.02103404887020588\n",
      "epoch: 18 step: 790, loss is 0.005965278018265963\n",
      "epoch: 18 step: 791, loss is 0.13185736536979675\n",
      "epoch: 18 step: 792, loss is 0.06311953067779541\n",
      "epoch: 18 step: 793, loss is 0.09746240079402924\n",
      "epoch: 18 step: 794, loss is 0.009183483198285103\n",
      "epoch: 18 step: 795, loss is 0.0037380456924438477\n",
      "epoch: 18 step: 796, loss is 0.06275459378957748\n",
      "epoch: 18 step: 797, loss is 0.006965568754822016\n",
      "epoch: 18 step: 798, loss is 0.008003049530088902\n",
      "epoch: 18 step: 799, loss is 0.0028349016793072224\n",
      "epoch: 18 step: 800, loss is 0.050320930778980255\n",
      "epoch: 18 step: 801, loss is 0.021738767623901367\n",
      "epoch: 18 step: 802, loss is 0.02331732027232647\n",
      "epoch: 18 step: 803, loss is 0.030046703293919563\n",
      "epoch: 18 step: 804, loss is 0.0035206745378673077\n",
      "epoch: 18 step: 805, loss is 0.20016059279441833\n",
      "epoch: 18 step: 806, loss is 0.0021878951229155064\n",
      "epoch: 18 step: 807, loss is 0.02938029170036316\n",
      "epoch: 18 step: 808, loss is 0.009564850479364395\n",
      "epoch: 18 step: 809, loss is 0.1251538246870041\n",
      "epoch: 18 step: 810, loss is 0.07858256250619888\n",
      "epoch: 18 step: 811, loss is 0.005655381828546524\n",
      "epoch: 18 step: 812, loss is 0.001975281862542033\n",
      "epoch: 18 step: 813, loss is 0.010171579197049141\n",
      "epoch: 18 step: 814, loss is 0.005263759288936853\n",
      "epoch: 18 step: 815, loss is 0.08658495545387268\n",
      "epoch: 18 step: 816, loss is 0.03215546905994415\n",
      "epoch: 18 step: 817, loss is 0.056337740272283554\n",
      "epoch: 18 step: 818, loss is 0.0013944165548309684\n",
      "epoch: 18 step: 819, loss is 0.025673672556877136\n",
      "epoch: 18 step: 820, loss is 0.022588269785046577\n",
      "epoch: 18 step: 821, loss is 0.12661731243133545\n",
      "epoch: 18 step: 822, loss is 0.024731451645493507\n",
      "epoch: 18 step: 823, loss is 0.07724647969007492\n",
      "epoch: 18 step: 824, loss is 0.013341203331947327\n",
      "epoch: 18 step: 825, loss is 0.002169194631278515\n",
      "epoch: 18 step: 826, loss is 0.04292206093668938\n",
      "epoch: 18 step: 827, loss is 0.01800239086151123\n",
      "epoch: 18 step: 828, loss is 0.006453603971749544\n",
      "epoch: 18 step: 829, loss is 0.04401535168290138\n",
      "epoch: 18 step: 830, loss is 0.045967474579811096\n",
      "epoch: 18 step: 831, loss is 0.002723094541579485\n",
      "epoch: 18 step: 832, loss is 0.007175604347139597\n",
      "epoch: 18 step: 833, loss is 0.001409200020134449\n",
      "epoch: 18 step: 834, loss is 0.1293954998254776\n",
      "epoch: 18 step: 835, loss is 0.12509562075138092\n",
      "epoch: 18 step: 836, loss is 0.030241187661886215\n",
      "epoch: 18 step: 837, loss is 0.04649348184466362\n",
      "epoch: 18 step: 838, loss is 0.01349161472171545\n",
      "epoch: 18 step: 839, loss is 0.02956615388393402\n",
      "epoch: 18 step: 840, loss is 0.06976727396249771\n",
      "epoch: 18 step: 841, loss is 0.061667922884225845\n",
      "epoch: 18 step: 842, loss is 0.010368098504841328\n",
      "epoch: 18 step: 843, loss is 0.025243015959858894\n",
      "epoch: 18 step: 844, loss is 0.040242958813905716\n",
      "epoch: 18 step: 845, loss is 0.010688768699765205\n",
      "epoch: 18 step: 846, loss is 0.015419112518429756\n",
      "epoch: 18 step: 847, loss is 0.002622139174491167\n",
      "epoch: 18 step: 848, loss is 0.007703314535319805\n",
      "epoch: 18 step: 849, loss is 0.006489743012934923\n",
      "epoch: 18 step: 850, loss is 0.005201894324272871\n",
      "epoch: 18 step: 851, loss is 0.012073823250830173\n",
      "epoch: 18 step: 852, loss is 0.07830295711755753\n",
      "epoch: 18 step: 853, loss is 0.01122492365539074\n",
      "epoch: 18 step: 854, loss is 0.10993712395429611\n",
      "epoch: 18 step: 855, loss is 0.06201782450079918\n",
      "epoch: 18 step: 856, loss is 0.022170888260006905\n",
      "epoch: 18 step: 857, loss is 0.01247746404260397\n",
      "epoch: 18 step: 858, loss is 0.05369243770837784\n",
      "epoch: 18 step: 859, loss is 0.0029209223575890064\n",
      "epoch: 18 step: 860, loss is 0.011698046699166298\n",
      "epoch: 18 step: 861, loss is 0.012188127264380455\n",
      "epoch: 18 step: 862, loss is 0.011283599771559238\n",
      "epoch: 18 step: 863, loss is 0.11523780226707458\n",
      "epoch: 18 step: 864, loss is 0.11202453076839447\n",
      "epoch: 18 step: 865, loss is 0.009136715903878212\n",
      "epoch: 18 step: 866, loss is 0.05756084620952606\n",
      "epoch: 18 step: 867, loss is 0.006837422493845224\n",
      "epoch: 18 step: 868, loss is 0.03611215949058533\n",
      "epoch: 18 step: 869, loss is 0.07010438293218613\n",
      "epoch: 18 step: 870, loss is 0.09077668190002441\n",
      "epoch: 18 step: 871, loss is 0.04779716208577156\n",
      "epoch: 18 step: 872, loss is 0.06717585027217865\n",
      "epoch: 18 step: 873, loss is 0.028366275131702423\n",
      "epoch: 18 step: 874, loss is 0.0014098731335252523\n",
      "epoch: 18 step: 875, loss is 0.08870711177587509\n",
      "epoch: 18 step: 876, loss is 0.011469739489257336\n",
      "epoch: 18 step: 877, loss is 0.031011046841740608\n",
      "epoch: 18 step: 878, loss is 0.0018878777045756578\n",
      "epoch: 18 step: 879, loss is 0.017015580087900162\n",
      "epoch: 18 step: 880, loss is 0.017076244577765465\n",
      "epoch: 18 step: 881, loss is 0.03590603172779083\n",
      "epoch: 18 step: 882, loss is 0.0545123815536499\n",
      "epoch: 18 step: 883, loss is 0.006961761508136988\n",
      "epoch: 18 step: 884, loss is 0.0004636677331291139\n",
      "epoch: 18 step: 885, loss is 0.012515573762357235\n",
      "epoch: 18 step: 886, loss is 0.01327489409595728\n",
      "epoch: 18 step: 887, loss is 0.010170613415539265\n",
      "epoch: 18 step: 888, loss is 0.08405183255672455\n",
      "epoch: 18 step: 889, loss is 0.044631361961364746\n",
      "epoch: 18 step: 890, loss is 0.05285346135497093\n",
      "epoch: 18 step: 891, loss is 0.10473940521478653\n",
      "epoch: 18 step: 892, loss is 0.01063730102032423\n",
      "epoch: 18 step: 893, loss is 0.05974351987242699\n",
      "epoch: 18 step: 894, loss is 0.10088025033473969\n",
      "epoch: 18 step: 895, loss is 0.1055627167224884\n",
      "epoch: 18 step: 896, loss is 0.016921045258641243\n",
      "epoch: 18 step: 897, loss is 0.022557809948921204\n",
      "epoch: 18 step: 898, loss is 0.08625631779432297\n",
      "epoch: 18 step: 899, loss is 0.023314960300922394\n",
      "epoch: 18 step: 900, loss is 0.02217291295528412\n",
      "epoch: 18 step: 901, loss is 0.08316362649202347\n",
      "epoch: 18 step: 902, loss is 0.004495678003877401\n",
      "epoch: 18 step: 903, loss is 0.008041055873036385\n",
      "epoch: 18 step: 904, loss is 0.061203498393297195\n",
      "epoch: 18 step: 905, loss is 0.02045264095067978\n",
      "epoch: 18 step: 906, loss is 0.058881934732198715\n",
      "epoch: 18 step: 907, loss is 0.1083100438117981\n",
      "epoch: 18 step: 908, loss is 0.008411193266510963\n",
      "epoch: 18 step: 909, loss is 0.11631577461957932\n",
      "epoch: 18 step: 910, loss is 0.002887646434828639\n",
      "epoch: 18 step: 911, loss is 0.019258785992860794\n",
      "epoch: 18 step: 912, loss is 0.036788683384656906\n",
      "epoch: 18 step: 913, loss is 0.03840414062142372\n",
      "epoch: 18 step: 914, loss is 0.014390561729669571\n",
      "epoch: 18 step: 915, loss is 0.011707451194524765\n",
      "epoch: 18 step: 916, loss is 0.043945468962192535\n",
      "epoch: 18 step: 917, loss is 0.0438532680273056\n",
      "epoch: 18 step: 918, loss is 0.015870312228798866\n",
      "epoch: 18 step: 919, loss is 0.015762044116854668\n",
      "epoch: 18 step: 920, loss is 0.008144608698785305\n",
      "epoch: 18 step: 921, loss is 0.0054236953146755695\n",
      "epoch: 18 step: 922, loss is 0.054185397922992706\n",
      "epoch: 18 step: 923, loss is 0.0023305185604840517\n",
      "epoch: 18 step: 924, loss is 0.04238360375165939\n",
      "epoch: 18 step: 925, loss is 0.07110735774040222\n",
      "epoch: 18 step: 926, loss is 0.05879455432295799\n",
      "epoch: 18 step: 927, loss is 0.03541569411754608\n",
      "epoch: 18 step: 928, loss is 0.060250140726566315\n",
      "epoch: 18 step: 929, loss is 0.008235721848905087\n",
      "epoch: 18 step: 930, loss is 0.0023626305628567934\n",
      "epoch: 18 step: 931, loss is 0.0027621742337942123\n",
      "epoch: 18 step: 932, loss is 0.004695613868534565\n",
      "epoch: 18 step: 933, loss is 0.060981396585702896\n",
      "epoch: 18 step: 934, loss is 0.022957492619752884\n",
      "epoch: 18 step: 935, loss is 0.06804023683071136\n",
      "epoch: 18 step: 936, loss is 0.30068686604499817\n",
      "epoch: 18 step: 937, loss is 0.013857799582183361\n",
      "epoch: 19 step: 1, loss is 0.0027343733236193657\n",
      "epoch: 19 step: 2, loss is 0.06913716346025467\n",
      "epoch: 19 step: 3, loss is 0.018293172121047974\n",
      "epoch: 19 step: 4, loss is 0.01850118860602379\n",
      "epoch: 19 step: 5, loss is 0.015381474047899246\n",
      "epoch: 19 step: 6, loss is 0.009402456693351269\n",
      "epoch: 19 step: 7, loss is 0.010224329307675362\n",
      "epoch: 19 step: 8, loss is 0.009532819502055645\n",
      "epoch: 19 step: 9, loss is 0.009886765852570534\n",
      "epoch: 19 step: 10, loss is 0.0009111501276493073\n",
      "epoch: 19 step: 11, loss is 0.0074185458943247795\n",
      "epoch: 19 step: 12, loss is 0.06314460933208466\n",
      "epoch: 19 step: 13, loss is 0.00794556736946106\n",
      "epoch: 19 step: 14, loss is 0.048834748566150665\n",
      "epoch: 19 step: 15, loss is 0.02815951034426689\n",
      "epoch: 19 step: 16, loss is 0.10498849302530289\n",
      "epoch: 19 step: 17, loss is 0.007176733575761318\n",
      "epoch: 19 step: 18, loss is 0.05275283008813858\n",
      "epoch: 19 step: 19, loss is 0.008318363688886166\n",
      "epoch: 19 step: 20, loss is 0.0050587039440870285\n",
      "epoch: 19 step: 21, loss is 0.0686454251408577\n",
      "epoch: 19 step: 22, loss is 0.015633324161171913\n",
      "epoch: 19 step: 23, loss is 0.013251018710434437\n",
      "epoch: 19 step: 24, loss is 0.002000626875087619\n",
      "epoch: 19 step: 25, loss is 0.006322070024907589\n",
      "epoch: 19 step: 26, loss is 0.023759296163916588\n",
      "epoch: 19 step: 27, loss is 0.014518100768327713\n",
      "epoch: 19 step: 28, loss is 0.07704552263021469\n",
      "epoch: 19 step: 29, loss is 0.014885487966239452\n",
      "epoch: 19 step: 30, loss is 0.0006729370797984302\n",
      "epoch: 19 step: 31, loss is 0.009756706655025482\n",
      "epoch: 19 step: 32, loss is 0.02855994738638401\n",
      "epoch: 19 step: 33, loss is 0.036928314715623856\n",
      "epoch: 19 step: 34, loss is 0.11361322551965714\n",
      "epoch: 19 step: 35, loss is 0.04000510647892952\n",
      "epoch: 19 step: 36, loss is 0.027442796155810356\n",
      "epoch: 19 step: 37, loss is 0.021486692130565643\n",
      "epoch: 19 step: 38, loss is 0.006486560218036175\n",
      "epoch: 19 step: 39, loss is 0.0023175955284386873\n",
      "epoch: 19 step: 40, loss is 0.03310295566916466\n",
      "epoch: 19 step: 41, loss is 0.0017771669663488865\n",
      "epoch: 19 step: 42, loss is 0.015800492838025093\n",
      "epoch: 19 step: 43, loss is 0.00930653978139162\n",
      "epoch: 19 step: 44, loss is 0.16980740427970886\n",
      "epoch: 19 step: 45, loss is 0.005424412898719311\n",
      "epoch: 19 step: 46, loss is 0.003208317095413804\n",
      "epoch: 19 step: 47, loss is 0.0043232375755906105\n",
      "epoch: 19 step: 48, loss is 0.0033426512964069843\n",
      "epoch: 19 step: 49, loss is 0.003750559175387025\n",
      "epoch: 19 step: 50, loss is 0.005860426463186741\n",
      "epoch: 19 step: 51, loss is 0.004346783272922039\n",
      "epoch: 19 step: 52, loss is 0.0005156304687261581\n",
      "epoch: 19 step: 53, loss is 0.011730868369340897\n",
      "epoch: 19 step: 54, loss is 0.12276904284954071\n",
      "epoch: 19 step: 55, loss is 0.00021078542340546846\n",
      "epoch: 19 step: 56, loss is 0.03323483094573021\n",
      "epoch: 19 step: 57, loss is 0.009971070103347301\n",
      "epoch: 19 step: 58, loss is 0.05519182235002518\n",
      "epoch: 19 step: 59, loss is 0.017602110281586647\n",
      "epoch: 19 step: 60, loss is 0.0004334575787652284\n",
      "epoch: 19 step: 61, loss is 0.0008181866724044085\n",
      "epoch: 19 step: 62, loss is 0.0020324173383414745\n",
      "epoch: 19 step: 63, loss is 0.0058401054702699184\n",
      "epoch: 19 step: 64, loss is 0.018233755603432655\n",
      "epoch: 19 step: 65, loss is 0.002003663219511509\n",
      "epoch: 19 step: 66, loss is 0.007688309997320175\n",
      "epoch: 19 step: 67, loss is 0.011436494998633862\n",
      "epoch: 19 step: 68, loss is 0.020751826465129852\n",
      "epoch: 19 step: 69, loss is 0.0006985758664086461\n",
      "epoch: 19 step: 70, loss is 0.001080275047570467\n",
      "epoch: 19 step: 71, loss is 0.035725779831409454\n",
      "epoch: 19 step: 72, loss is 0.046665776520967484\n",
      "epoch: 19 step: 73, loss is 0.029403043910861015\n",
      "epoch: 19 step: 74, loss is 0.0005383518291637301\n",
      "epoch: 19 step: 75, loss is 0.03793465718626976\n",
      "epoch: 19 step: 76, loss is 0.012895777821540833\n",
      "epoch: 19 step: 77, loss is 0.0014852171298116446\n",
      "epoch: 19 step: 78, loss is 0.006376081611961126\n",
      "epoch: 19 step: 79, loss is 0.0004083334933966398\n",
      "epoch: 19 step: 80, loss is 0.000679587887134403\n",
      "epoch: 19 step: 81, loss is 0.0004089631838724017\n",
      "epoch: 19 step: 82, loss is 0.03933173418045044\n",
      "epoch: 19 step: 83, loss is 0.0016428621020168066\n",
      "epoch: 19 step: 84, loss is 0.004227038007229567\n",
      "epoch: 19 step: 85, loss is 0.0008304042275995016\n",
      "epoch: 19 step: 86, loss is 0.041020750999450684\n",
      "epoch: 19 step: 87, loss is 0.0007239631959237158\n",
      "epoch: 19 step: 88, loss is 0.024709559977054596\n",
      "epoch: 19 step: 89, loss is 0.005768412258476019\n",
      "epoch: 19 step: 90, loss is 0.13109923899173737\n",
      "epoch: 19 step: 91, loss is 0.014093366451561451\n",
      "epoch: 19 step: 92, loss is 0.0001704931928543374\n",
      "epoch: 19 step: 93, loss is 0.004310302436351776\n",
      "epoch: 19 step: 94, loss is 0.0047219316475093365\n",
      "epoch: 19 step: 95, loss is 0.0070087602362036705\n",
      "epoch: 19 step: 96, loss is 0.029296888038516045\n",
      "epoch: 19 step: 97, loss is 0.021349046379327774\n",
      "epoch: 19 step: 98, loss is 0.010512877255678177\n",
      "epoch: 19 step: 99, loss is 0.002237452892586589\n",
      "epoch: 19 step: 100, loss is 0.004223607014864683\n",
      "epoch: 19 step: 101, loss is 0.01608171872794628\n",
      "epoch: 19 step: 102, loss is 0.007158167660236359\n",
      "epoch: 19 step: 103, loss is 0.025387480854988098\n",
      "epoch: 19 step: 104, loss is 0.010255773551762104\n",
      "epoch: 19 step: 105, loss is 0.007114178501069546\n",
      "epoch: 19 step: 106, loss is 0.007723176386207342\n",
      "epoch: 19 step: 107, loss is 0.04300157353281975\n",
      "epoch: 19 step: 108, loss is 0.004903203342109919\n",
      "epoch: 19 step: 109, loss is 0.0375639870762825\n",
      "epoch: 19 step: 110, loss is 0.00892648659646511\n",
      "epoch: 19 step: 111, loss is 0.013169347308576107\n",
      "epoch: 19 step: 112, loss is 0.00446689547970891\n",
      "epoch: 19 step: 113, loss is 0.0022810697555541992\n",
      "epoch: 19 step: 114, loss is 0.007237120531499386\n",
      "epoch: 19 step: 115, loss is 0.02618630789220333\n",
      "epoch: 19 step: 116, loss is 0.023979106917977333\n",
      "epoch: 19 step: 117, loss is 0.039194412529468536\n",
      "epoch: 19 step: 118, loss is 0.007762959226965904\n",
      "epoch: 19 step: 119, loss is 0.00011264839849900454\n",
      "epoch: 19 step: 120, loss is 0.001621044473722577\n",
      "epoch: 19 step: 121, loss is 0.03029368817806244\n",
      "epoch: 19 step: 122, loss is 0.021410688757896423\n",
      "epoch: 19 step: 123, loss is 0.01647278293967247\n",
      "epoch: 19 step: 124, loss is 0.005015188362449408\n",
      "epoch: 19 step: 125, loss is 0.0051024905405938625\n",
      "epoch: 19 step: 126, loss is 0.007603025995194912\n",
      "epoch: 19 step: 127, loss is 0.0038556386716663837\n",
      "epoch: 19 step: 128, loss is 0.004651343449950218\n",
      "epoch: 19 step: 129, loss is 0.0012407007161527872\n",
      "epoch: 19 step: 130, loss is 0.012706832960247993\n",
      "epoch: 19 step: 131, loss is 0.005598375573754311\n",
      "epoch: 19 step: 132, loss is 0.002282561734318733\n",
      "epoch: 19 step: 133, loss is 0.0004300673899706453\n",
      "epoch: 19 step: 134, loss is 0.0009801383130252361\n",
      "epoch: 19 step: 135, loss is 0.0007308672647923231\n",
      "epoch: 19 step: 136, loss is 0.04976729303598404\n",
      "epoch: 19 step: 137, loss is 0.00032138096867129207\n",
      "epoch: 19 step: 138, loss is 0.07985099405050278\n",
      "epoch: 19 step: 139, loss is 0.0044213649816811085\n",
      "epoch: 19 step: 140, loss is 0.01563877798616886\n",
      "epoch: 19 step: 141, loss is 0.024199388921260834\n",
      "epoch: 19 step: 142, loss is 0.0011401968076825142\n",
      "epoch: 19 step: 143, loss is 0.13757942616939545\n",
      "epoch: 19 step: 144, loss is 0.03784337267279625\n",
      "epoch: 19 step: 145, loss is 0.002042203675955534\n",
      "epoch: 19 step: 146, loss is 0.002430037362501025\n",
      "epoch: 19 step: 147, loss is 0.0025596783962100744\n",
      "epoch: 19 step: 148, loss is 0.00623463885858655\n",
      "epoch: 19 step: 149, loss is 0.0013223385903984308\n",
      "epoch: 19 step: 150, loss is 0.001582828350365162\n",
      "epoch: 19 step: 151, loss is 0.005030818283557892\n",
      "epoch: 19 step: 152, loss is 0.004342707805335522\n",
      "epoch: 19 step: 153, loss is 0.012314154766499996\n",
      "epoch: 19 step: 154, loss is 0.011994395405054092\n",
      "epoch: 19 step: 155, loss is 0.01748545654118061\n",
      "epoch: 19 step: 156, loss is 0.0017288395902141929\n",
      "epoch: 19 step: 157, loss is 0.0027237734757363796\n",
      "epoch: 19 step: 158, loss is 0.005275473929941654\n",
      "epoch: 19 step: 159, loss is 0.0009885824983939528\n",
      "epoch: 19 step: 160, loss is 0.025392884388566017\n",
      "epoch: 19 step: 161, loss is 0.062218885868787766\n",
      "epoch: 19 step: 162, loss is 0.015925757586956024\n",
      "epoch: 19 step: 163, loss is 0.0027042098809033632\n",
      "epoch: 19 step: 164, loss is 0.027098674327135086\n",
      "epoch: 19 step: 165, loss is 0.035704124718904495\n",
      "epoch: 19 step: 166, loss is 0.049002308398485184\n",
      "epoch: 19 step: 167, loss is 0.003365890821442008\n",
      "epoch: 19 step: 168, loss is 0.004495932720601559\n",
      "epoch: 19 step: 169, loss is 0.01064771693199873\n",
      "epoch: 19 step: 170, loss is 0.012760910205543041\n",
      "epoch: 19 step: 171, loss is 0.0018741851672530174\n",
      "epoch: 19 step: 172, loss is 0.0013262098655104637\n",
      "epoch: 19 step: 173, loss is 0.0011402576928958297\n",
      "epoch: 19 step: 174, loss is 0.003691737772896886\n",
      "epoch: 19 step: 175, loss is 0.0033484422601759434\n",
      "epoch: 19 step: 176, loss is 0.05084305629134178\n",
      "epoch: 19 step: 177, loss is 0.10681174695491791\n",
      "epoch: 19 step: 178, loss is 0.017951196059584618\n",
      "epoch: 19 step: 179, loss is 0.00033350737066939473\n",
      "epoch: 19 step: 180, loss is 0.0021129422821104527\n",
      "epoch: 19 step: 181, loss is 0.01695106364786625\n",
      "epoch: 19 step: 182, loss is 0.008600224740803242\n",
      "epoch: 19 step: 183, loss is 0.005598788615316153\n",
      "epoch: 19 step: 184, loss is 0.0008166476618498564\n",
      "epoch: 19 step: 185, loss is 0.032164618372917175\n",
      "epoch: 19 step: 186, loss is 0.0007740440778434277\n",
      "epoch: 19 step: 187, loss is 0.009079639799892902\n",
      "epoch: 19 step: 188, loss is 0.014689093455672264\n",
      "epoch: 19 step: 189, loss is 0.006674718111753464\n",
      "epoch: 19 step: 190, loss is 0.03327994793653488\n",
      "epoch: 19 step: 191, loss is 0.0014022266259416938\n",
      "epoch: 19 step: 192, loss is 0.00640009855851531\n",
      "epoch: 19 step: 193, loss is 0.021337473765015602\n",
      "epoch: 19 step: 194, loss is 0.0029361392371356487\n",
      "epoch: 19 step: 195, loss is 0.013333682902157307\n",
      "epoch: 19 step: 196, loss is 0.0033005368895828724\n",
      "epoch: 19 step: 197, loss is 0.0029532143380492926\n",
      "epoch: 19 step: 198, loss is 0.0016505330568179488\n",
      "epoch: 19 step: 199, loss is 0.00261877104640007\n",
      "epoch: 19 step: 200, loss is 0.009277863427996635\n",
      "epoch: 19 step: 201, loss is 0.008630997501313686\n",
      "epoch: 19 step: 202, loss is 0.007893847301602364\n",
      "epoch: 19 step: 203, loss is 0.026851417496800423\n",
      "epoch: 19 step: 204, loss is 0.0029187770560383797\n",
      "epoch: 19 step: 205, loss is 0.0006960020400583744\n",
      "epoch: 19 step: 206, loss is 0.0015064111212268472\n",
      "epoch: 19 step: 207, loss is 0.0013967284467071295\n",
      "epoch: 19 step: 208, loss is 0.13761980831623077\n",
      "epoch: 19 step: 209, loss is 0.014311734586954117\n",
      "epoch: 19 step: 210, loss is 0.0011649868683889508\n",
      "epoch: 19 step: 211, loss is 0.0003644320531748235\n",
      "epoch: 19 step: 212, loss is 0.013809346593916416\n",
      "epoch: 19 step: 213, loss is 0.00026286259526386857\n",
      "epoch: 19 step: 214, loss is 0.016168270260095596\n",
      "epoch: 19 step: 215, loss is 0.019599322229623795\n",
      "epoch: 19 step: 216, loss is 0.004874737933278084\n",
      "epoch: 19 step: 217, loss is 0.006573068909347057\n",
      "epoch: 19 step: 218, loss is 0.020694121718406677\n",
      "epoch: 19 step: 219, loss is 0.04083041101694107\n",
      "epoch: 19 step: 220, loss is 0.0008036190993152559\n",
      "epoch: 19 step: 221, loss is 0.029074328020215034\n",
      "epoch: 19 step: 222, loss is 0.003977893386036158\n",
      "epoch: 19 step: 223, loss is 0.029748907312750816\n",
      "epoch: 19 step: 224, loss is 0.0007493004668504\n",
      "epoch: 19 step: 225, loss is 0.004485271405428648\n",
      "epoch: 19 step: 226, loss is 0.007843690924346447\n",
      "epoch: 19 step: 227, loss is 0.0013467895332723856\n",
      "epoch: 19 step: 228, loss is 0.02254878729581833\n",
      "epoch: 19 step: 229, loss is 0.00779248122125864\n",
      "epoch: 19 step: 230, loss is 0.006030023097991943\n",
      "epoch: 19 step: 231, loss is 0.0022199300583451986\n",
      "epoch: 19 step: 232, loss is 0.035770952701568604\n",
      "epoch: 19 step: 233, loss is 0.0015601844061166048\n",
      "epoch: 19 step: 234, loss is 0.003042674856260419\n",
      "epoch: 19 step: 235, loss is 0.023269670084118843\n",
      "epoch: 19 step: 236, loss is 0.013057833537459373\n",
      "epoch: 19 step: 237, loss is 0.003292430890724063\n",
      "epoch: 19 step: 238, loss is 0.07511681318283081\n",
      "epoch: 19 step: 239, loss is 0.0027288736309856176\n",
      "epoch: 19 step: 240, loss is 0.09119778126478195\n",
      "epoch: 19 step: 241, loss is 0.0006307237781584263\n",
      "epoch: 19 step: 242, loss is 0.03907798230648041\n",
      "epoch: 19 step: 243, loss is 0.015188333578407764\n",
      "epoch: 19 step: 244, loss is 0.006776466034352779\n",
      "epoch: 19 step: 245, loss is 0.0006095398566685617\n",
      "epoch: 19 step: 246, loss is 0.007786192931234837\n",
      "epoch: 19 step: 247, loss is 0.008138825185596943\n",
      "epoch: 19 step: 248, loss is 0.005648517049849033\n",
      "epoch: 19 step: 249, loss is 0.03328980877995491\n",
      "epoch: 19 step: 250, loss is 0.001200865488499403\n",
      "epoch: 19 step: 251, loss is 0.00020687437790911645\n",
      "epoch: 19 step: 252, loss is 0.09920494258403778\n",
      "epoch: 19 step: 253, loss is 0.0019714441150426865\n",
      "epoch: 19 step: 254, loss is 0.0035049915313720703\n",
      "epoch: 19 step: 255, loss is 0.000631951552350074\n",
      "epoch: 19 step: 256, loss is 0.0026649506762623787\n",
      "epoch: 19 step: 257, loss is 0.0011509624309837818\n",
      "epoch: 19 step: 258, loss is 0.004381428472697735\n",
      "epoch: 19 step: 259, loss is 0.0021920811850577593\n",
      "epoch: 19 step: 260, loss is 0.008098945021629333\n",
      "epoch: 19 step: 261, loss is 0.01344288606196642\n",
      "epoch: 19 step: 262, loss is 0.0015629320405423641\n",
      "epoch: 19 step: 263, loss is 0.0002836895000655204\n",
      "epoch: 19 step: 264, loss is 0.00010599424422252923\n",
      "epoch: 19 step: 265, loss is 0.0006949023809283972\n",
      "epoch: 19 step: 266, loss is 0.022317076101899147\n",
      "epoch: 19 step: 267, loss is 0.03371167182922363\n",
      "epoch: 19 step: 268, loss is 0.02270798198878765\n",
      "epoch: 19 step: 269, loss is 0.0072541311383247375\n",
      "epoch: 19 step: 270, loss is 0.052928414195775986\n",
      "epoch: 19 step: 271, loss is 0.004327807575464249\n",
      "epoch: 19 step: 272, loss is 0.005586856044828892\n",
      "epoch: 19 step: 273, loss is 0.014624333940446377\n",
      "epoch: 19 step: 274, loss is 0.0002753061125986278\n",
      "epoch: 19 step: 275, loss is 0.0045625451020896435\n",
      "epoch: 19 step: 276, loss is 0.025733977556228638\n",
      "epoch: 19 step: 277, loss is 0.057173505425453186\n",
      "epoch: 19 step: 278, loss is 0.034232091158628464\n",
      "epoch: 19 step: 279, loss is 0.015714354813098907\n",
      "epoch: 19 step: 280, loss is 0.05453942343592644\n",
      "epoch: 19 step: 281, loss is 0.0024274366442114115\n",
      "epoch: 19 step: 282, loss is 0.004064156673848629\n",
      "epoch: 19 step: 283, loss is 0.024227704852819443\n",
      "epoch: 19 step: 284, loss is 0.0021310090087354183\n",
      "epoch: 19 step: 285, loss is 0.021624747663736343\n",
      "epoch: 19 step: 286, loss is 0.06356184929609299\n",
      "epoch: 19 step: 287, loss is 0.028411148115992546\n",
      "epoch: 19 step: 288, loss is 0.060115568339824677\n",
      "epoch: 19 step: 289, loss is 0.015741167590022087\n",
      "epoch: 19 step: 290, loss is 0.016505908221006393\n",
      "epoch: 19 step: 291, loss is 0.00932888500392437\n",
      "epoch: 19 step: 292, loss is 0.15005403757095337\n",
      "epoch: 19 step: 293, loss is 0.003949505742639303\n",
      "epoch: 19 step: 294, loss is 0.029383478686213493\n",
      "epoch: 19 step: 295, loss is 0.0027406583540141582\n",
      "epoch: 19 step: 296, loss is 0.05672883614897728\n",
      "epoch: 19 step: 297, loss is 0.0012000089045614004\n",
      "epoch: 19 step: 298, loss is 0.0039311484433710575\n",
      "epoch: 19 step: 299, loss is 0.0008681461331434548\n",
      "epoch: 19 step: 300, loss is 0.02395285665988922\n",
      "epoch: 19 step: 301, loss is 0.0042894319631159306\n",
      "epoch: 19 step: 302, loss is 0.026379665359854698\n",
      "epoch: 19 step: 303, loss is 0.008396180346608162\n",
      "epoch: 19 step: 304, loss is 0.01623823679983616\n",
      "epoch: 19 step: 305, loss is 0.016202639788389206\n",
      "epoch: 19 step: 306, loss is 0.002147804945707321\n",
      "epoch: 19 step: 307, loss is 0.005476477090269327\n",
      "epoch: 19 step: 308, loss is 0.002058669924736023\n",
      "epoch: 19 step: 309, loss is 0.06840828061103821\n",
      "epoch: 19 step: 310, loss is 0.0019220288377255201\n",
      "epoch: 19 step: 311, loss is 0.002516478067263961\n",
      "epoch: 19 step: 312, loss is 0.037502389401197433\n",
      "epoch: 19 step: 313, loss is 0.07550498843193054\n",
      "epoch: 19 step: 314, loss is 0.026290584355592728\n",
      "epoch: 19 step: 315, loss is 0.004474271088838577\n",
      "epoch: 19 step: 316, loss is 0.08846411854028702\n",
      "epoch: 19 step: 317, loss is 0.03469880670309067\n",
      "epoch: 19 step: 318, loss is 0.022701697424054146\n",
      "epoch: 19 step: 319, loss is 0.046525612473487854\n",
      "epoch: 19 step: 320, loss is 0.006571615114808083\n",
      "epoch: 19 step: 321, loss is 0.0038398762699216604\n",
      "epoch: 19 step: 322, loss is 0.021547727286815643\n",
      "epoch: 19 step: 323, loss is 0.08877445012331009\n",
      "epoch: 19 step: 324, loss is 0.025112057104706764\n",
      "epoch: 19 step: 325, loss is 0.013229716569185257\n",
      "epoch: 19 step: 326, loss is 0.000662642065435648\n",
      "epoch: 19 step: 327, loss is 0.0035055421758443117\n",
      "epoch: 19 step: 328, loss is 0.023869745433330536\n",
      "epoch: 19 step: 329, loss is 0.021124498918652534\n",
      "epoch: 19 step: 330, loss is 0.005954778753221035\n",
      "epoch: 19 step: 331, loss is 0.011485974304378033\n",
      "epoch: 19 step: 332, loss is 0.010009185411036015\n",
      "epoch: 19 step: 333, loss is 0.005149771925061941\n",
      "epoch: 19 step: 334, loss is 0.0024204084184020758\n",
      "epoch: 19 step: 335, loss is 0.003271555295214057\n",
      "epoch: 19 step: 336, loss is 0.0688391625881195\n",
      "epoch: 19 step: 337, loss is 0.0027310822624713182\n",
      "epoch: 19 step: 338, loss is 0.0051771411672234535\n",
      "epoch: 19 step: 339, loss is 0.0032796349842101336\n",
      "epoch: 19 step: 340, loss is 0.013549275696277618\n",
      "epoch: 19 step: 341, loss is 0.0003044006589334458\n",
      "epoch: 19 step: 342, loss is 0.015466505661606789\n",
      "epoch: 19 step: 343, loss is 0.012095868587493896\n",
      "epoch: 19 step: 344, loss is 0.00995911005884409\n",
      "epoch: 19 step: 345, loss is 0.0014431077288463712\n",
      "epoch: 19 step: 346, loss is 0.001676842337474227\n",
      "epoch: 19 step: 347, loss is 0.008516197092831135\n",
      "epoch: 19 step: 348, loss is 0.010094053111970425\n",
      "epoch: 19 step: 349, loss is 0.0005356493638828397\n",
      "epoch: 19 step: 350, loss is 0.028416158631443977\n",
      "epoch: 19 step: 351, loss is 0.013265705667436123\n",
      "epoch: 19 step: 352, loss is 0.0026144871953874826\n",
      "epoch: 19 step: 353, loss is 0.0024986176285892725\n",
      "epoch: 19 step: 354, loss is 0.0052095986902713776\n",
      "epoch: 19 step: 355, loss is 0.02386355586349964\n",
      "epoch: 19 step: 356, loss is 0.0022143577225506306\n",
      "epoch: 19 step: 357, loss is 0.017491837963461876\n",
      "epoch: 19 step: 358, loss is 0.006545305252075195\n",
      "epoch: 19 step: 359, loss is 0.00022259991965256631\n",
      "epoch: 19 step: 360, loss is 0.027474189177155495\n",
      "epoch: 19 step: 361, loss is 0.017495492473244667\n",
      "epoch: 19 step: 362, loss is 0.014303136616945267\n",
      "epoch: 19 step: 363, loss is 0.015653643757104874\n",
      "epoch: 19 step: 364, loss is 0.017661303281784058\n",
      "epoch: 19 step: 365, loss is 0.0016849810490384698\n",
      "epoch: 19 step: 366, loss is 0.0028231244068592787\n",
      "epoch: 19 step: 367, loss is 0.022049307823181152\n",
      "epoch: 19 step: 368, loss is 0.0032018376514315605\n",
      "epoch: 19 step: 369, loss is 0.02136744000017643\n",
      "epoch: 19 step: 370, loss is 0.0005783434608019888\n",
      "epoch: 19 step: 371, loss is 0.00083458999870345\n",
      "epoch: 19 step: 372, loss is 0.00124629947822541\n",
      "epoch: 19 step: 373, loss is 0.0006921730237081647\n",
      "epoch: 19 step: 374, loss is 0.0020836801268160343\n",
      "epoch: 19 step: 375, loss is 0.010816192254424095\n",
      "epoch: 19 step: 376, loss is 0.0033685783855617046\n",
      "epoch: 19 step: 377, loss is 0.0027378075756132603\n",
      "epoch: 19 step: 378, loss is 0.06934674084186554\n",
      "epoch: 19 step: 379, loss is 0.007731505669653416\n",
      "epoch: 19 step: 380, loss is 0.006352391559630632\n",
      "epoch: 19 step: 381, loss is 0.01367887295782566\n",
      "epoch: 19 step: 382, loss is 0.006272641941905022\n",
      "epoch: 19 step: 383, loss is 0.0011204363545402884\n",
      "epoch: 19 step: 384, loss is 0.0011426635319367051\n",
      "epoch: 19 step: 385, loss is 0.0005727141979150474\n",
      "epoch: 19 step: 386, loss is 0.003160242922604084\n",
      "epoch: 19 step: 387, loss is 0.001265413244254887\n",
      "epoch: 19 step: 388, loss is 0.015395266935229301\n",
      "epoch: 19 step: 389, loss is 0.007930951192975044\n",
      "epoch: 19 step: 390, loss is 0.01893210969865322\n",
      "epoch: 19 step: 391, loss is 0.0007529364083893597\n",
      "epoch: 19 step: 392, loss is 0.004510766826570034\n",
      "epoch: 19 step: 393, loss is 0.0009047579951584339\n",
      "epoch: 19 step: 394, loss is 0.004036303143948317\n",
      "epoch: 19 step: 395, loss is 0.002770190592855215\n",
      "epoch: 19 step: 396, loss is 0.01747465506196022\n",
      "epoch: 19 step: 397, loss is 0.003806591499596834\n",
      "epoch: 19 step: 398, loss is 0.0030931851360946894\n",
      "epoch: 19 step: 399, loss is 0.007753600366413593\n",
      "epoch: 19 step: 400, loss is 0.009514939971268177\n",
      "epoch: 19 step: 401, loss is 0.002297723898664117\n",
      "epoch: 19 step: 402, loss is 0.02094469778239727\n",
      "epoch: 19 step: 403, loss is 0.023741576820611954\n",
      "epoch: 19 step: 404, loss is 0.00449048075824976\n",
      "epoch: 19 step: 405, loss is 0.06126488000154495\n",
      "epoch: 19 step: 406, loss is 0.001430787262506783\n",
      "epoch: 19 step: 407, loss is 0.0008755994495004416\n",
      "epoch: 19 step: 408, loss is 0.0013977654743939638\n",
      "epoch: 19 step: 409, loss is 0.0005087088793516159\n",
      "epoch: 19 step: 410, loss is 0.0009607091196812689\n",
      "epoch: 19 step: 411, loss is 0.01467530895024538\n",
      "epoch: 19 step: 412, loss is 0.0019672338385134935\n",
      "epoch: 19 step: 413, loss is 9.383347060065717e-05\n",
      "epoch: 19 step: 414, loss is 0.0728844553232193\n",
      "epoch: 19 step: 415, loss is 0.004465458448976278\n",
      "epoch: 19 step: 416, loss is 0.0023930075112730265\n",
      "epoch: 19 step: 417, loss is 0.039427753537893295\n",
      "epoch: 19 step: 418, loss is 6.496161950053647e-05\n",
      "epoch: 19 step: 419, loss is 0.10916481912136078\n",
      "epoch: 19 step: 420, loss is 0.006537390407174826\n",
      "epoch: 19 step: 421, loss is 0.044771671295166016\n",
      "epoch: 19 step: 422, loss is 0.020633848384022713\n",
      "epoch: 19 step: 423, loss is 0.008121168240904808\n",
      "epoch: 19 step: 424, loss is 0.011102018877863884\n",
      "epoch: 19 step: 425, loss is 0.0016308135818690062\n",
      "epoch: 19 step: 426, loss is 0.03036602959036827\n",
      "epoch: 19 step: 427, loss is 0.025812599807977676\n",
      "epoch: 19 step: 428, loss is 0.0027642936911433935\n",
      "epoch: 19 step: 429, loss is 0.008830825798213482\n",
      "epoch: 19 step: 430, loss is 0.013028489425778389\n",
      "epoch: 19 step: 431, loss is 0.006578761152923107\n",
      "epoch: 19 step: 432, loss is 0.009387579746544361\n",
      "epoch: 19 step: 433, loss is 0.00028462643967941403\n",
      "epoch: 19 step: 434, loss is 0.01615055464208126\n",
      "epoch: 19 step: 435, loss is 0.013282991014420986\n",
      "epoch: 19 step: 436, loss is 0.011382881551980972\n",
      "epoch: 19 step: 437, loss is 0.002694924594834447\n",
      "epoch: 19 step: 438, loss is 0.004693773575127125\n",
      "epoch: 19 step: 439, loss is 0.004492075182497501\n",
      "epoch: 19 step: 440, loss is 0.011807594448328018\n",
      "epoch: 19 step: 441, loss is 0.014609236270189285\n",
      "epoch: 19 step: 442, loss is 0.004810456186532974\n",
      "epoch: 19 step: 443, loss is 0.000403458543587476\n",
      "epoch: 19 step: 444, loss is 0.01432530302554369\n",
      "epoch: 19 step: 445, loss is 0.0004920946666970849\n",
      "epoch: 19 step: 446, loss is 0.000635678181424737\n",
      "epoch: 19 step: 447, loss is 0.013762121088802814\n",
      "epoch: 19 step: 448, loss is 0.0022338437847793102\n",
      "epoch: 19 step: 449, loss is 0.028134696185588837\n",
      "epoch: 19 step: 450, loss is 0.020078210160136223\n",
      "epoch: 19 step: 451, loss is 0.008834482170641422\n",
      "epoch: 19 step: 452, loss is 0.0011810512514784932\n",
      "epoch: 19 step: 453, loss is 0.002972188638523221\n",
      "epoch: 19 step: 454, loss is 0.004684545565396547\n",
      "epoch: 19 step: 455, loss is 0.0010850499384105206\n",
      "epoch: 19 step: 456, loss is 0.002966588130220771\n",
      "epoch: 19 step: 457, loss is 0.004047764930874109\n",
      "epoch: 19 step: 458, loss is 0.0032463043462485075\n",
      "epoch: 19 step: 459, loss is 0.008247305639088154\n",
      "epoch: 19 step: 460, loss is 0.003111794590950012\n",
      "epoch: 19 step: 461, loss is 0.0006864379975013435\n",
      "epoch: 19 step: 462, loss is 0.0021379098761826754\n",
      "epoch: 19 step: 463, loss is 0.02297917753458023\n",
      "epoch: 19 step: 464, loss is 0.015858249738812447\n",
      "epoch: 19 step: 465, loss is 0.010946018621325493\n",
      "epoch: 19 step: 466, loss is 0.0027460053097456694\n",
      "epoch: 19 step: 467, loss is 0.007142959628254175\n",
      "epoch: 19 step: 468, loss is 0.00033261594944633543\n",
      "epoch: 19 step: 469, loss is 0.0007787516224198043\n",
      "epoch: 19 step: 470, loss is 0.0030876214150339365\n",
      "epoch: 19 step: 471, loss is 0.005885274149477482\n",
      "epoch: 19 step: 472, loss is 0.005901511292904615\n",
      "epoch: 19 step: 473, loss is 0.112956203520298\n",
      "epoch: 19 step: 474, loss is 0.005789963528513908\n",
      "epoch: 19 step: 475, loss is 0.0029269875958561897\n",
      "epoch: 19 step: 476, loss is 0.0007777563878335059\n",
      "epoch: 19 step: 477, loss is 0.005402553826570511\n",
      "epoch: 19 step: 478, loss is 0.002423335565254092\n",
      "epoch: 19 step: 479, loss is 0.025761503726243973\n",
      "epoch: 19 step: 480, loss is 0.0010440309997648\n",
      "epoch: 19 step: 481, loss is 0.0022898053284734488\n",
      "epoch: 19 step: 482, loss is 0.05590714141726494\n",
      "epoch: 19 step: 483, loss is 0.005668574012815952\n",
      "epoch: 19 step: 484, loss is 0.018747376278042793\n",
      "epoch: 19 step: 485, loss is 0.0022393737453967333\n",
      "epoch: 19 step: 486, loss is 0.007425378076732159\n",
      "epoch: 19 step: 487, loss is 0.0006857823464088142\n",
      "epoch: 19 step: 488, loss is 9.017928823595867e-05\n",
      "epoch: 19 step: 489, loss is 0.008322895504534245\n",
      "epoch: 19 step: 490, loss is 0.038748230785131454\n",
      "epoch: 19 step: 491, loss is 0.00038243550807237625\n",
      "epoch: 19 step: 492, loss is 3.747004666365683e-05\n",
      "epoch: 19 step: 493, loss is 0.0010408431990072131\n",
      "epoch: 19 step: 494, loss is 0.0067976536229252815\n",
      "epoch: 19 step: 495, loss is 0.003892884124070406\n",
      "epoch: 19 step: 496, loss is 0.011170231737196445\n",
      "epoch: 19 step: 497, loss is 0.026721037924289703\n",
      "epoch: 19 step: 498, loss is 0.010835227556526661\n",
      "epoch: 19 step: 499, loss is 0.02017637901008129\n",
      "epoch: 19 step: 500, loss is 0.04437996447086334\n",
      "epoch: 19 step: 501, loss is 0.06209811568260193\n",
      "epoch: 19 step: 502, loss is 0.002036005724221468\n",
      "epoch: 19 step: 503, loss is 0.03270464017987251\n",
      "epoch: 19 step: 504, loss is 0.001755729434080422\n",
      "epoch: 19 step: 505, loss is 0.03776383027434349\n",
      "epoch: 19 step: 506, loss is 0.02602440118789673\n",
      "epoch: 19 step: 507, loss is 0.002480453345924616\n",
      "epoch: 19 step: 508, loss is 0.0024748605210334063\n",
      "epoch: 19 step: 509, loss is 0.004115784540772438\n",
      "epoch: 19 step: 510, loss is 0.0076720090582966805\n",
      "epoch: 19 step: 511, loss is 0.0030540055595338345\n",
      "epoch: 19 step: 512, loss is 0.0007943774689920247\n",
      "epoch: 19 step: 513, loss is 0.0010277391411364079\n",
      "epoch: 19 step: 514, loss is 0.0004966745036654174\n",
      "epoch: 19 step: 515, loss is 0.002128393156453967\n",
      "epoch: 19 step: 516, loss is 0.005791044794023037\n",
      "epoch: 19 step: 517, loss is 0.01526029221713543\n",
      "epoch: 19 step: 518, loss is 0.07622282952070236\n",
      "epoch: 19 step: 519, loss is 0.0011316736927255988\n",
      "epoch: 19 step: 520, loss is 0.06292035430669785\n",
      "epoch: 19 step: 521, loss is 0.003794523188844323\n",
      "epoch: 19 step: 522, loss is 0.06250850111246109\n",
      "epoch: 19 step: 523, loss is 0.01857234723865986\n",
      "epoch: 19 step: 524, loss is 0.0014441069215536118\n",
      "epoch: 19 step: 525, loss is 0.0006535821594297886\n",
      "epoch: 19 step: 526, loss is 0.07465207576751709\n",
      "epoch: 19 step: 527, loss is 0.007978261448442936\n",
      "epoch: 19 step: 528, loss is 0.007749773096293211\n",
      "epoch: 19 step: 529, loss is 0.0019832884427160025\n",
      "epoch: 19 step: 530, loss is 0.025792114436626434\n",
      "epoch: 19 step: 531, loss is 0.011804576963186264\n",
      "epoch: 19 step: 532, loss is 0.005436811130493879\n",
      "epoch: 19 step: 533, loss is 0.015227758325636387\n",
      "epoch: 19 step: 534, loss is 0.023599635809659958\n",
      "epoch: 19 step: 535, loss is 0.021023616194725037\n",
      "epoch: 19 step: 536, loss is 0.02319599874317646\n",
      "epoch: 19 step: 537, loss is 0.00337681220844388\n",
      "epoch: 19 step: 538, loss is 0.11910991370677948\n",
      "epoch: 19 step: 539, loss is 0.0008765275706537068\n",
      "epoch: 19 step: 540, loss is 0.018992830067873\n",
      "epoch: 19 step: 541, loss is 0.07254388928413391\n",
      "epoch: 19 step: 542, loss is 0.06273302435874939\n",
      "epoch: 19 step: 543, loss is 0.012603064998984337\n",
      "epoch: 19 step: 544, loss is 0.05329896882176399\n",
      "epoch: 19 step: 545, loss is 0.030723121017217636\n",
      "epoch: 19 step: 546, loss is 0.015507559292018414\n",
      "epoch: 19 step: 547, loss is 0.0005815124022774398\n",
      "epoch: 19 step: 548, loss is 0.011789203621447086\n",
      "epoch: 19 step: 549, loss is 0.0041435882449150085\n",
      "epoch: 19 step: 550, loss is 0.022240763530135155\n",
      "epoch: 19 step: 551, loss is 0.06896314024925232\n",
      "epoch: 19 step: 552, loss is 0.008867503143846989\n",
      "epoch: 19 step: 553, loss is 0.0032966905273497105\n",
      "epoch: 19 step: 554, loss is 0.005337900947779417\n",
      "epoch: 19 step: 555, loss is 0.0009151392732746899\n",
      "epoch: 19 step: 556, loss is 0.0059007517993450165\n",
      "epoch: 19 step: 557, loss is 0.006913782563060522\n",
      "epoch: 19 step: 558, loss is 0.02083010971546173\n",
      "epoch: 19 step: 559, loss is 0.001522132195532322\n",
      "epoch: 19 step: 560, loss is 0.0050156270153820515\n",
      "epoch: 19 step: 561, loss is 0.009249359369277954\n",
      "epoch: 19 step: 562, loss is 0.0758734792470932\n",
      "epoch: 19 step: 563, loss is 0.02468237280845642\n",
      "epoch: 19 step: 564, loss is 0.06479259580373764\n",
      "epoch: 19 step: 565, loss is 0.00783696211874485\n",
      "epoch: 19 step: 566, loss is 0.03402848169207573\n",
      "epoch: 19 step: 567, loss is 0.07288599014282227\n",
      "epoch: 19 step: 568, loss is 0.057826075702905655\n",
      "epoch: 19 step: 569, loss is 0.02887561358511448\n",
      "epoch: 19 step: 570, loss is 0.001451625837944448\n",
      "epoch: 19 step: 571, loss is 0.0025666479486972094\n",
      "epoch: 19 step: 572, loss is 0.03404831513762474\n",
      "epoch: 19 step: 573, loss is 0.009276073426008224\n",
      "epoch: 19 step: 574, loss is 0.045709382742643356\n",
      "epoch: 19 step: 575, loss is 0.007880398072302341\n",
      "epoch: 19 step: 576, loss is 0.007515819743275642\n",
      "epoch: 19 step: 577, loss is 0.11240781843662262\n",
      "epoch: 19 step: 578, loss is 0.02099822275340557\n",
      "epoch: 19 step: 579, loss is 0.0032084386330097914\n",
      "epoch: 19 step: 580, loss is 0.0034626717679202557\n",
      "epoch: 19 step: 581, loss is 0.012699495069682598\n",
      "epoch: 19 step: 582, loss is 0.09372050315141678\n",
      "epoch: 19 step: 583, loss is 0.004314913880079985\n",
      "epoch: 19 step: 584, loss is 0.0018043789314106107\n",
      "epoch: 19 step: 585, loss is 0.007469263859093189\n",
      "epoch: 19 step: 586, loss is 0.02626935951411724\n",
      "epoch: 19 step: 587, loss is 0.008692609146237373\n",
      "epoch: 19 step: 588, loss is 0.029340237379074097\n",
      "epoch: 19 step: 589, loss is 0.021911732852458954\n",
      "epoch: 19 step: 590, loss is 0.015255372039973736\n",
      "epoch: 19 step: 591, loss is 0.01422827783972025\n",
      "epoch: 19 step: 592, loss is 0.0004190137842670083\n",
      "epoch: 19 step: 593, loss is 0.011934636160731316\n",
      "epoch: 19 step: 594, loss is 0.0019985195249319077\n",
      "epoch: 19 step: 595, loss is 0.05835195630788803\n",
      "epoch: 19 step: 596, loss is 0.0024974269326776266\n",
      "epoch: 19 step: 597, loss is 0.015113519504666328\n",
      "epoch: 19 step: 598, loss is 0.05607948824763298\n",
      "epoch: 19 step: 599, loss is 0.03107334114611149\n",
      "epoch: 19 step: 600, loss is 0.13695579767227173\n",
      "epoch: 19 step: 601, loss is 0.010770714841783047\n",
      "epoch: 19 step: 602, loss is 0.003472596639767289\n",
      "epoch: 19 step: 603, loss is 0.014455920085310936\n",
      "epoch: 19 step: 604, loss is 0.00864897109568119\n",
      "epoch: 19 step: 605, loss is 0.0011650390224531293\n",
      "epoch: 19 step: 606, loss is 0.005305801052600145\n",
      "epoch: 19 step: 607, loss is 0.014906502328813076\n",
      "epoch: 19 step: 608, loss is 0.026050658896565437\n",
      "epoch: 19 step: 609, loss is 0.05051422119140625\n",
      "epoch: 19 step: 610, loss is 0.0005848570726811886\n",
      "epoch: 19 step: 611, loss is 0.1918001025915146\n",
      "epoch: 19 step: 612, loss is 0.024461321532726288\n",
      "epoch: 19 step: 613, loss is 0.045909930020570755\n",
      "epoch: 19 step: 614, loss is 0.008115834556519985\n",
      "epoch: 19 step: 615, loss is 0.005426603369414806\n",
      "epoch: 19 step: 616, loss is 0.07639207690954208\n",
      "epoch: 19 step: 617, loss is 0.0010821386240422726\n",
      "epoch: 19 step: 618, loss is 0.0957457646727562\n",
      "epoch: 19 step: 619, loss is 0.0956992581486702\n",
      "epoch: 19 step: 620, loss is 0.002726564649492502\n",
      "epoch: 19 step: 621, loss is 0.02414950355887413\n",
      "epoch: 19 step: 622, loss is 0.02156250923871994\n",
      "epoch: 19 step: 623, loss is 0.001373238512314856\n",
      "epoch: 19 step: 624, loss is 0.07111161947250366\n",
      "epoch: 19 step: 625, loss is 0.01172017864882946\n",
      "epoch: 19 step: 626, loss is 0.009948726743459702\n",
      "epoch: 19 step: 627, loss is 0.0013396157883107662\n",
      "epoch: 19 step: 628, loss is 0.001395340426824987\n",
      "epoch: 19 step: 629, loss is 0.02100154012441635\n",
      "epoch: 19 step: 630, loss is 0.009601494297385216\n",
      "epoch: 19 step: 631, loss is 0.01173087116330862\n",
      "epoch: 19 step: 632, loss is 0.031197723001241684\n",
      "epoch: 19 step: 633, loss is 0.046290718019008636\n",
      "epoch: 19 step: 634, loss is 0.19778239727020264\n",
      "epoch: 19 step: 635, loss is 0.00602219020947814\n",
      "epoch: 19 step: 636, loss is 0.04092482104897499\n",
      "epoch: 19 step: 637, loss is 0.0010597099317237735\n",
      "epoch: 19 step: 638, loss is 0.01112433336675167\n",
      "epoch: 19 step: 639, loss is 0.008052583783864975\n",
      "epoch: 19 step: 640, loss is 0.015156783163547516\n",
      "epoch: 19 step: 641, loss is 0.01380996871739626\n",
      "epoch: 19 step: 642, loss is 0.0003724282141774893\n",
      "epoch: 19 step: 643, loss is 0.00302313594147563\n",
      "epoch: 19 step: 644, loss is 0.010804387740790844\n",
      "epoch: 19 step: 645, loss is 0.010337511077523232\n",
      "epoch: 19 step: 646, loss is 0.021439265459775925\n",
      "epoch: 19 step: 647, loss is 0.003989649936556816\n",
      "epoch: 19 step: 648, loss is 0.02629420906305313\n",
      "epoch: 19 step: 649, loss is 0.014137670397758484\n",
      "epoch: 19 step: 650, loss is 0.0584234744310379\n",
      "epoch: 19 step: 651, loss is 0.0003605986130423844\n",
      "epoch: 19 step: 652, loss is 0.039196401834487915\n",
      "epoch: 19 step: 653, loss is 0.010741861537098885\n",
      "epoch: 19 step: 654, loss is 0.017337705940008163\n",
      "epoch: 19 step: 655, loss is 0.0017175794346258044\n",
      "epoch: 19 step: 656, loss is 0.0018103312468156219\n",
      "epoch: 19 step: 657, loss is 0.0026688077487051487\n",
      "epoch: 19 step: 658, loss is 0.15308153629302979\n",
      "epoch: 19 step: 659, loss is 0.001762067899107933\n",
      "epoch: 19 step: 660, loss is 0.017265377566218376\n",
      "epoch: 19 step: 661, loss is 0.005978288594633341\n",
      "epoch: 19 step: 662, loss is 0.039530593901872635\n",
      "epoch: 19 step: 663, loss is 0.05528491735458374\n",
      "epoch: 19 step: 664, loss is 0.012374678626656532\n",
      "epoch: 19 step: 665, loss is 0.002611509058624506\n",
      "epoch: 19 step: 666, loss is 0.0024294203612953424\n",
      "epoch: 19 step: 667, loss is 0.03708858788013458\n",
      "epoch: 19 step: 668, loss is 0.001921219052746892\n",
      "epoch: 19 step: 669, loss is 0.0068749613128602505\n",
      "epoch: 19 step: 670, loss is 0.07589828968048096\n",
      "epoch: 19 step: 671, loss is 0.003791345749050379\n",
      "epoch: 19 step: 672, loss is 0.0008648571674712002\n",
      "epoch: 19 step: 673, loss is 0.0030640223994851112\n",
      "epoch: 19 step: 674, loss is 0.003463946282863617\n",
      "epoch: 19 step: 675, loss is 0.0016610907623544335\n",
      "epoch: 19 step: 676, loss is 0.014944602735340595\n",
      "epoch: 19 step: 677, loss is 0.015399478375911713\n",
      "epoch: 19 step: 678, loss is 0.01591787487268448\n",
      "epoch: 19 step: 679, loss is 0.0344773530960083\n",
      "epoch: 19 step: 680, loss is 0.008262835443019867\n",
      "epoch: 19 step: 681, loss is 0.002135609509423375\n",
      "epoch: 19 step: 682, loss is 0.011947709135711193\n",
      "epoch: 19 step: 683, loss is 0.05764191597700119\n",
      "epoch: 19 step: 684, loss is 0.03200089558959007\n",
      "epoch: 19 step: 685, loss is 0.0038277972489595413\n",
      "epoch: 19 step: 686, loss is 0.017056770622730255\n",
      "epoch: 19 step: 687, loss is 0.10825039446353912\n",
      "epoch: 19 step: 688, loss is 0.003978592809289694\n",
      "epoch: 19 step: 689, loss is 0.01312208827584982\n",
      "epoch: 19 step: 690, loss is 0.007405699230730534\n",
      "epoch: 19 step: 691, loss is 0.0075428602285683155\n",
      "epoch: 19 step: 692, loss is 0.0008751411223784089\n",
      "epoch: 19 step: 693, loss is 0.09395598620176315\n",
      "epoch: 19 step: 694, loss is 0.021713273599743843\n",
      "epoch: 19 step: 695, loss is 0.0013101663207635283\n",
      "epoch: 19 step: 696, loss is 0.031105630099773407\n",
      "epoch: 19 step: 697, loss is 0.04869101196527481\n",
      "epoch: 19 step: 698, loss is 0.014085013419389725\n",
      "epoch: 19 step: 699, loss is 0.0017743492498993874\n",
      "epoch: 19 step: 700, loss is 0.004880520515143871\n",
      "epoch: 19 step: 701, loss is 0.003491921816021204\n",
      "epoch: 19 step: 702, loss is 0.000390762957977131\n",
      "epoch: 19 step: 703, loss is 0.04704294726252556\n",
      "epoch: 19 step: 704, loss is 0.013381772674620152\n",
      "epoch: 19 step: 705, loss is 0.024925198405981064\n",
      "epoch: 19 step: 706, loss is 0.004714967682957649\n",
      "epoch: 19 step: 707, loss is 0.11377083510160446\n",
      "epoch: 19 step: 708, loss is 0.054067324846982956\n",
      "epoch: 19 step: 709, loss is 0.007618861272931099\n",
      "epoch: 19 step: 710, loss is 0.027206527069211006\n",
      "epoch: 19 step: 711, loss is 0.00268749101087451\n",
      "epoch: 19 step: 712, loss is 0.02165324240922928\n",
      "epoch: 19 step: 713, loss is 0.04474647343158722\n",
      "epoch: 19 step: 714, loss is 0.03970380499958992\n",
      "epoch: 19 step: 715, loss is 0.1798398792743683\n",
      "epoch: 19 step: 716, loss is 0.07716615498065948\n",
      "epoch: 19 step: 717, loss is 0.0630323737859726\n",
      "epoch: 19 step: 718, loss is 0.019784381613135338\n",
      "epoch: 19 step: 719, loss is 0.016009068116545677\n",
      "epoch: 19 step: 720, loss is 0.016084490343928337\n",
      "epoch: 19 step: 721, loss is 0.04778914153575897\n",
      "epoch: 19 step: 722, loss is 0.03949381411075592\n",
      "epoch: 19 step: 723, loss is 0.17872096598148346\n",
      "epoch: 19 step: 724, loss is 0.15896916389465332\n",
      "epoch: 19 step: 725, loss is 0.008322345092892647\n",
      "epoch: 19 step: 726, loss is 0.2121478021144867\n",
      "epoch: 19 step: 727, loss is 0.02551155909895897\n",
      "epoch: 19 step: 728, loss is 0.1076197400689125\n",
      "epoch: 19 step: 729, loss is 0.05164477601647377\n",
      "epoch: 19 step: 730, loss is 0.019090844318270683\n",
      "epoch: 19 step: 731, loss is 0.0017130557680502534\n",
      "epoch: 19 step: 732, loss is 0.01724679209291935\n",
      "epoch: 19 step: 733, loss is 0.026281971484422684\n",
      "epoch: 19 step: 734, loss is 0.0030242970678955317\n",
      "epoch: 19 step: 735, loss is 0.07929926365613937\n",
      "epoch: 19 step: 736, loss is 0.15799272060394287\n",
      "epoch: 19 step: 737, loss is 0.04357893764972687\n",
      "epoch: 19 step: 738, loss is 0.064666748046875\n",
      "epoch: 19 step: 739, loss is 0.15710052847862244\n",
      "epoch: 19 step: 740, loss is 0.01852310448884964\n",
      "epoch: 19 step: 741, loss is 0.01925286091864109\n",
      "epoch: 19 step: 742, loss is 0.13479936122894287\n",
      "epoch: 19 step: 743, loss is 0.012901546433568\n",
      "epoch: 19 step: 744, loss is 0.002919337712228298\n",
      "epoch: 19 step: 745, loss is 0.03208918124437332\n",
      "epoch: 19 step: 746, loss is 0.0006183166988193989\n",
      "epoch: 19 step: 747, loss is 0.02439199760556221\n",
      "epoch: 19 step: 748, loss is 0.009147468954324722\n",
      "epoch: 19 step: 749, loss is 0.011834435164928436\n",
      "epoch: 19 step: 750, loss is 0.014012551866471767\n",
      "epoch: 19 step: 751, loss is 0.0042970916256308556\n",
      "epoch: 19 step: 752, loss is 0.01982930488884449\n",
      "epoch: 19 step: 753, loss is 0.07875876128673553\n",
      "epoch: 19 step: 754, loss is 0.05083632096648216\n",
      "epoch: 19 step: 755, loss is 0.0518810972571373\n",
      "epoch: 19 step: 756, loss is 0.00240384042263031\n",
      "epoch: 19 step: 757, loss is 0.12702560424804688\n",
      "epoch: 19 step: 758, loss is 0.046087682247161865\n",
      "epoch: 19 step: 759, loss is 0.0374591164290905\n",
      "epoch: 19 step: 760, loss is 0.0423145517706871\n",
      "epoch: 19 step: 761, loss is 0.019136598333716393\n",
      "epoch: 19 step: 762, loss is 0.009307327680289745\n",
      "epoch: 19 step: 763, loss is 0.0016946857795119286\n",
      "epoch: 19 step: 764, loss is 0.007283372338861227\n",
      "epoch: 19 step: 765, loss is 0.04158390685915947\n",
      "epoch: 19 step: 766, loss is 0.009813649579882622\n",
      "epoch: 19 step: 767, loss is 0.002310846233740449\n",
      "epoch: 19 step: 768, loss is 0.08966310322284698\n",
      "epoch: 19 step: 769, loss is 0.0334964357316494\n",
      "epoch: 19 step: 770, loss is 0.019043603911995888\n",
      "epoch: 19 step: 771, loss is 0.013813963159918785\n",
      "epoch: 19 step: 772, loss is 0.015460600145161152\n",
      "epoch: 19 step: 773, loss is 0.09747152030467987\n",
      "epoch: 19 step: 774, loss is 0.005511457100510597\n",
      "epoch: 19 step: 775, loss is 0.053708333522081375\n",
      "epoch: 19 step: 776, loss is 0.0010796216083690524\n",
      "epoch: 19 step: 777, loss is 0.041419122368097305\n",
      "epoch: 19 step: 778, loss is 0.006899898871779442\n",
      "epoch: 19 step: 779, loss is 0.007175454869866371\n",
      "epoch: 19 step: 780, loss is 0.016603603959083557\n",
      "epoch: 19 step: 781, loss is 0.012833199463784695\n",
      "epoch: 19 step: 782, loss is 0.002338656922802329\n",
      "epoch: 19 step: 783, loss is 0.04731202870607376\n",
      "epoch: 19 step: 784, loss is 0.06047373265028\n",
      "epoch: 19 step: 785, loss is 0.036898914724588394\n",
      "epoch: 19 step: 786, loss is 0.004369075875729322\n",
      "epoch: 19 step: 787, loss is 0.0016211210750043392\n",
      "epoch: 19 step: 788, loss is 0.019285688176751137\n",
      "epoch: 19 step: 789, loss is 0.0033341979142278433\n",
      "epoch: 19 step: 790, loss is 0.04209553450345993\n",
      "epoch: 19 step: 791, loss is 0.03859017416834831\n",
      "epoch: 19 step: 792, loss is 0.022369571030139923\n",
      "epoch: 19 step: 793, loss is 0.027517585083842278\n",
      "epoch: 19 step: 794, loss is 0.0033100147265940905\n",
      "epoch: 19 step: 795, loss is 0.2046549767255783\n",
      "epoch: 19 step: 796, loss is 0.001245963736437261\n",
      "epoch: 19 step: 797, loss is 0.007529290392994881\n",
      "epoch: 19 step: 798, loss is 0.09602643549442291\n",
      "epoch: 19 step: 799, loss is 0.048938650637865067\n",
      "epoch: 19 step: 800, loss is 0.10440679639577866\n",
      "epoch: 19 step: 801, loss is 0.0038178330287337303\n",
      "epoch: 19 step: 802, loss is 0.017251791432499886\n",
      "epoch: 19 step: 803, loss is 0.062425464391708374\n",
      "epoch: 19 step: 804, loss is 0.01295643113553524\n",
      "epoch: 19 step: 805, loss is 0.0023833028972148895\n",
      "epoch: 19 step: 806, loss is 0.002178012626245618\n",
      "epoch: 19 step: 807, loss is 0.10149548947811127\n",
      "epoch: 19 step: 808, loss is 0.016048841178417206\n",
      "epoch: 19 step: 809, loss is 0.010563111864030361\n",
      "epoch: 19 step: 810, loss is 0.03429895639419556\n",
      "epoch: 19 step: 811, loss is 0.01633748784661293\n",
      "epoch: 19 step: 812, loss is 0.013529781252145767\n",
      "epoch: 19 step: 813, loss is 0.17143140733242035\n",
      "epoch: 19 step: 814, loss is 0.023455360904335976\n",
      "epoch: 19 step: 815, loss is 0.037299565970897675\n",
      "epoch: 19 step: 816, loss is 0.030752072110772133\n",
      "epoch: 19 step: 817, loss is 0.06503991037607193\n",
      "epoch: 19 step: 818, loss is 0.078336201608181\n",
      "epoch: 19 step: 819, loss is 0.06476590037345886\n",
      "epoch: 19 step: 820, loss is 0.0018448764458298683\n",
      "epoch: 19 step: 821, loss is 0.023081185296177864\n",
      "epoch: 19 step: 822, loss is 0.02041064016520977\n",
      "epoch: 19 step: 823, loss is 0.04356995224952698\n",
      "epoch: 19 step: 824, loss is 0.03874952718615532\n",
      "epoch: 19 step: 825, loss is 0.15254737436771393\n",
      "epoch: 19 step: 826, loss is 0.02356608584523201\n",
      "epoch: 19 step: 827, loss is 0.10763037204742432\n",
      "epoch: 19 step: 828, loss is 0.017097337171435356\n",
      "epoch: 19 step: 829, loss is 0.020430514588952065\n",
      "epoch: 19 step: 830, loss is 0.033230263739824295\n",
      "epoch: 19 step: 831, loss is 0.017629574984312057\n",
      "epoch: 19 step: 832, loss is 0.0037244390696287155\n",
      "epoch: 19 step: 833, loss is 0.021697096526622772\n",
      "epoch: 19 step: 834, loss is 0.01877533458173275\n",
      "epoch: 19 step: 835, loss is 0.0004739153082482517\n",
      "epoch: 19 step: 836, loss is 0.039535947144031525\n",
      "epoch: 19 step: 837, loss is 0.005039699841290712\n",
      "epoch: 19 step: 838, loss is 0.04803244397044182\n",
      "epoch: 19 step: 839, loss is 0.10528521239757538\n",
      "epoch: 19 step: 840, loss is 0.008686528541147709\n",
      "epoch: 19 step: 841, loss is 0.03867514804005623\n",
      "epoch: 19 step: 842, loss is 0.009693946689367294\n",
      "epoch: 19 step: 843, loss is 0.021025096997618675\n",
      "epoch: 19 step: 844, loss is 0.030478816479444504\n",
      "epoch: 19 step: 845, loss is 0.0343913659453392\n",
      "epoch: 19 step: 846, loss is 0.009157013148069382\n",
      "epoch: 19 step: 847, loss is 0.03292771801352501\n",
      "epoch: 19 step: 848, loss is 0.016343817114830017\n",
      "epoch: 19 step: 849, loss is 0.05843352526426315\n",
      "epoch: 19 step: 850, loss is 0.014128553681075573\n",
      "epoch: 19 step: 851, loss is 0.0028097685426473618\n",
      "epoch: 19 step: 852, loss is 0.053938958793878555\n",
      "epoch: 19 step: 853, loss is 0.06206929683685303\n",
      "epoch: 19 step: 854, loss is 0.049576714634895325\n",
      "epoch: 19 step: 855, loss is 0.0027546242345124483\n",
      "epoch: 19 step: 856, loss is 0.04669902101159096\n",
      "epoch: 19 step: 857, loss is 0.07377725094556808\n",
      "epoch: 19 step: 858, loss is 0.006224852055311203\n",
      "epoch: 19 step: 859, loss is 0.0022923178039491177\n",
      "epoch: 19 step: 860, loss is 0.0514775812625885\n",
      "epoch: 19 step: 861, loss is 0.0008348352275788784\n",
      "epoch: 19 step: 862, loss is 0.027707932516932487\n",
      "epoch: 19 step: 863, loss is 0.09316336363554001\n",
      "epoch: 19 step: 864, loss is 0.041300609707832336\n",
      "epoch: 19 step: 865, loss is 0.042615845799446106\n",
      "epoch: 19 step: 866, loss is 0.0037361755967140198\n",
      "epoch: 19 step: 867, loss is 0.0016695077065378428\n",
      "epoch: 19 step: 868, loss is 0.011472105979919434\n",
      "epoch: 19 step: 869, loss is 0.017712900415062904\n",
      "epoch: 19 step: 870, loss is 0.04895060881972313\n",
      "epoch: 19 step: 871, loss is 0.05499463900923729\n",
      "epoch: 19 step: 872, loss is 0.19048239290714264\n",
      "epoch: 19 step: 873, loss is 0.002990373643115163\n",
      "epoch: 19 step: 874, loss is 0.052147410809993744\n",
      "epoch: 19 step: 875, loss is 0.02630665898323059\n",
      "epoch: 19 step: 876, loss is 0.015445772558450699\n",
      "epoch: 19 step: 877, loss is 0.00484151765704155\n",
      "epoch: 19 step: 878, loss is 0.003374929539859295\n",
      "epoch: 19 step: 879, loss is 0.01238273736089468\n",
      "epoch: 19 step: 880, loss is 0.011903556995093822\n",
      "epoch: 19 step: 881, loss is 0.021789006888866425\n",
      "epoch: 19 step: 882, loss is 0.0032956001814454794\n",
      "epoch: 19 step: 883, loss is 0.005508063361048698\n",
      "epoch: 19 step: 884, loss is 0.0236060731112957\n",
      "epoch: 19 step: 885, loss is 0.0036700922064483166\n",
      "epoch: 19 step: 886, loss is 0.06720750033855438\n",
      "epoch: 19 step: 887, loss is 0.005078929476439953\n",
      "epoch: 19 step: 888, loss is 0.038732241839170456\n",
      "epoch: 19 step: 889, loss is 0.007934789173305035\n",
      "epoch: 19 step: 890, loss is 0.04149765148758888\n",
      "epoch: 19 step: 891, loss is 0.018842656165361404\n",
      "epoch: 19 step: 892, loss is 0.03289049118757248\n",
      "epoch: 19 step: 893, loss is 0.09040897339582443\n",
      "epoch: 19 step: 894, loss is 0.03322646766901016\n",
      "epoch: 19 step: 895, loss is 0.010643178597092628\n",
      "epoch: 19 step: 896, loss is 0.0337824672460556\n",
      "epoch: 19 step: 897, loss is 0.0733167976140976\n",
      "epoch: 19 step: 898, loss is 0.0028954334557056427\n",
      "epoch: 19 step: 899, loss is 0.06522980332374573\n",
      "epoch: 19 step: 900, loss is 0.057275161147117615\n",
      "epoch: 19 step: 901, loss is 0.0263667069375515\n",
      "epoch: 19 step: 902, loss is 0.0037438846193253994\n",
      "epoch: 19 step: 903, loss is 0.0026752417907118797\n",
      "epoch: 19 step: 904, loss is 0.018965305760502815\n",
      "epoch: 19 step: 905, loss is 0.024384407326579094\n",
      "epoch: 19 step: 906, loss is 0.01499263197183609\n",
      "epoch: 19 step: 907, loss is 0.0019211404724046588\n",
      "epoch: 19 step: 908, loss is 0.0025425513740628958\n",
      "epoch: 19 step: 909, loss is 0.050467655062675476\n",
      "epoch: 19 step: 910, loss is 0.03386112302541733\n",
      "epoch: 19 step: 911, loss is 0.001063999254256487\n",
      "epoch: 19 step: 912, loss is 0.01828002743422985\n",
      "epoch: 19 step: 913, loss is 0.0124478405341506\n",
      "epoch: 19 step: 914, loss is 0.003335799789056182\n",
      "epoch: 19 step: 915, loss is 0.016428012400865555\n",
      "epoch: 19 step: 916, loss is 0.0075490777380764484\n",
      "epoch: 19 step: 917, loss is 0.012797514908015728\n",
      "epoch: 19 step: 918, loss is 0.017803482711315155\n",
      "epoch: 19 step: 919, loss is 0.012886746786534786\n",
      "epoch: 19 step: 920, loss is 0.018571771681308746\n",
      "epoch: 19 step: 921, loss is 0.09085886180400848\n",
      "epoch: 19 step: 922, loss is 0.01660650037229061\n",
      "epoch: 19 step: 923, loss is 0.00011989472841378301\n",
      "epoch: 19 step: 924, loss is 0.030853131785988808\n",
      "epoch: 19 step: 925, loss is 0.00264289160259068\n",
      "epoch: 19 step: 926, loss is 0.0040228222496807575\n",
      "epoch: 19 step: 927, loss is 0.0022679355461150408\n",
      "epoch: 19 step: 928, loss is 0.009325801394879818\n",
      "epoch: 19 step: 929, loss is 0.007270296569913626\n",
      "epoch: 19 step: 930, loss is 0.0005395901971496642\n",
      "epoch: 19 step: 931, loss is 0.05438327044248581\n",
      "epoch: 19 step: 932, loss is 0.0002705385268200189\n",
      "epoch: 19 step: 933, loss is 0.003665569704025984\n",
      "epoch: 19 step: 934, loss is 0.005674812477082014\n",
      "epoch: 19 step: 935, loss is 0.08053666353225708\n",
      "epoch: 19 step: 936, loss is 0.0014396815095096827\n",
      "epoch: 19 step: 937, loss is 0.0175635926425457\n",
      "epoch: 20 step: 1, loss is 0.0009065329213626683\n",
      "epoch: 20 step: 2, loss is 0.0060074208304286\n",
      "epoch: 20 step: 3, loss is 0.031954340636730194\n",
      "epoch: 20 step: 4, loss is 0.004941763821989298\n",
      "epoch: 20 step: 5, loss is 0.0033020421396940947\n",
      "epoch: 20 step: 6, loss is 0.05991562083363533\n",
      "epoch: 20 step: 7, loss is 0.0030834858771413565\n",
      "epoch: 20 step: 8, loss is 0.041729651391506195\n",
      "epoch: 20 step: 9, loss is 0.0006238503847271204\n",
      "epoch: 20 step: 10, loss is 0.019074711948633194\n",
      "epoch: 20 step: 11, loss is 0.0006609471747651696\n",
      "epoch: 20 step: 12, loss is 0.003184066852554679\n",
      "epoch: 20 step: 13, loss is 0.006371700204908848\n",
      "epoch: 20 step: 14, loss is 0.011273507960140705\n",
      "epoch: 20 step: 15, loss is 0.0005281281191855669\n",
      "epoch: 20 step: 16, loss is 0.008219826966524124\n",
      "epoch: 20 step: 17, loss is 0.011266415007412434\n",
      "epoch: 20 step: 18, loss is 0.021570777520537376\n",
      "epoch: 20 step: 19, loss is 0.001087769167497754\n",
      "epoch: 20 step: 20, loss is 0.00034798207343555987\n",
      "epoch: 20 step: 21, loss is 0.0017003815155476332\n",
      "epoch: 20 step: 22, loss is 0.004011495970189571\n",
      "epoch: 20 step: 23, loss is 0.03340601921081543\n",
      "epoch: 20 step: 24, loss is 0.030329786241054535\n",
      "epoch: 20 step: 25, loss is 0.004625319968909025\n",
      "epoch: 20 step: 26, loss is 0.0015636221505701542\n",
      "epoch: 20 step: 27, loss is 0.0072644371539354324\n",
      "epoch: 20 step: 28, loss is 0.014044553972780704\n",
      "epoch: 20 step: 29, loss is 0.0018026508623734117\n",
      "epoch: 20 step: 30, loss is 0.0022370952647179365\n",
      "epoch: 20 step: 31, loss is 0.02463528886437416\n",
      "epoch: 20 step: 32, loss is 0.003788383910432458\n",
      "epoch: 20 step: 33, loss is 0.021124979481101036\n",
      "epoch: 20 step: 34, loss is 0.0023699412122368813\n",
      "epoch: 20 step: 35, loss is 0.0359465666115284\n",
      "epoch: 20 step: 36, loss is 0.0009669286664575338\n",
      "epoch: 20 step: 37, loss is 0.009864810854196548\n",
      "epoch: 20 step: 38, loss is 0.010626643896102905\n",
      "epoch: 20 step: 39, loss is 0.012996084988117218\n",
      "epoch: 20 step: 40, loss is 0.005809531547129154\n",
      "epoch: 20 step: 41, loss is 0.021463776007294655\n",
      "epoch: 20 step: 42, loss is 0.011907924897968769\n",
      "epoch: 20 step: 43, loss is 0.00038644555024802685\n",
      "epoch: 20 step: 44, loss is 0.03307517617940903\n",
      "epoch: 20 step: 45, loss is 0.00035977037623524666\n",
      "epoch: 20 step: 46, loss is 0.009833177551627159\n",
      "epoch: 20 step: 47, loss is 7.111476588761434e-05\n",
      "epoch: 20 step: 48, loss is 0.0022315841633826494\n",
      "epoch: 20 step: 49, loss is 0.027132485061883926\n",
      "epoch: 20 step: 50, loss is 0.0033252995926886797\n",
      "epoch: 20 step: 51, loss is 0.0010860576294362545\n",
      "epoch: 20 step: 52, loss is 9.827112808125094e-05\n",
      "epoch: 20 step: 53, loss is 0.0027390269096940756\n",
      "epoch: 20 step: 54, loss is 0.006210938096046448\n",
      "epoch: 20 step: 55, loss is 0.0006091725663281977\n",
      "epoch: 20 step: 56, loss is 0.002175422152504325\n",
      "epoch: 20 step: 57, loss is 0.021794114261865616\n",
      "epoch: 20 step: 58, loss is 0.0038779734168201685\n",
      "epoch: 20 step: 59, loss is 0.004621951375156641\n",
      "epoch: 20 step: 60, loss is 0.002773084444925189\n",
      "epoch: 20 step: 61, loss is 0.13583211600780487\n",
      "epoch: 20 step: 62, loss is 0.002148647326976061\n",
      "epoch: 20 step: 63, loss is 0.002543778158724308\n",
      "epoch: 20 step: 64, loss is 0.0017877594800665975\n",
      "epoch: 20 step: 65, loss is 0.004375415854156017\n",
      "epoch: 20 step: 66, loss is 0.007322681602090597\n",
      "epoch: 20 step: 67, loss is 0.00036280095810070634\n",
      "epoch: 20 step: 68, loss is 0.022424818947911263\n",
      "epoch: 20 step: 69, loss is 0.002652976429089904\n",
      "epoch: 20 step: 70, loss is 0.035819876939058304\n",
      "epoch: 20 step: 71, loss is 0.023994820192456245\n",
      "epoch: 20 step: 72, loss is 0.00528023112565279\n",
      "epoch: 20 step: 73, loss is 0.007851739414036274\n",
      "epoch: 20 step: 74, loss is 0.0014937958912923932\n",
      "epoch: 20 step: 75, loss is 0.006303939037024975\n",
      "epoch: 20 step: 76, loss is 0.007429464720189571\n",
      "epoch: 20 step: 77, loss is 0.005060423631221056\n",
      "epoch: 20 step: 78, loss is 0.05230662226676941\n",
      "epoch: 20 step: 79, loss is 0.0020599719136953354\n",
      "epoch: 20 step: 80, loss is 0.006236041896045208\n",
      "epoch: 20 step: 81, loss is 0.032479897141456604\n",
      "epoch: 20 step: 82, loss is 0.00029456400079652667\n",
      "epoch: 20 step: 83, loss is 0.019205598160624504\n",
      "epoch: 20 step: 84, loss is 0.01786358654499054\n",
      "epoch: 20 step: 85, loss is 0.012986571528017521\n",
      "epoch: 20 step: 86, loss is 0.000274593970971182\n",
      "epoch: 20 step: 87, loss is 0.0008465895080007613\n",
      "epoch: 20 step: 88, loss is 0.024006500840187073\n",
      "epoch: 20 step: 89, loss is 0.002641534898430109\n",
      "epoch: 20 step: 90, loss is 0.001842966885305941\n",
      "epoch: 20 step: 91, loss is 0.019294125959277153\n",
      "epoch: 20 step: 92, loss is 0.0008358954801224172\n",
      "epoch: 20 step: 93, loss is 5.7865065173245966e-05\n",
      "epoch: 20 step: 94, loss is 0.00010276949615217745\n",
      "epoch: 20 step: 95, loss is 0.0012774133356288075\n",
      "epoch: 20 step: 96, loss is 0.011231567710638046\n",
      "epoch: 20 step: 97, loss is 0.08625620603561401\n",
      "epoch: 20 step: 98, loss is 0.015479084104299545\n",
      "epoch: 20 step: 99, loss is 0.03402107208967209\n",
      "epoch: 20 step: 100, loss is 0.004736031871289015\n",
      "epoch: 20 step: 101, loss is 0.0014021372189745307\n",
      "epoch: 20 step: 102, loss is 0.00014283115160651505\n",
      "epoch: 20 step: 103, loss is 0.0011405370896682143\n",
      "epoch: 20 step: 104, loss is 0.023640593513846397\n",
      "epoch: 20 step: 105, loss is 0.0034408357460051775\n",
      "epoch: 20 step: 106, loss is 0.006241242866963148\n",
      "epoch: 20 step: 107, loss is 0.002254986437037587\n",
      "epoch: 20 step: 108, loss is 0.025531090795993805\n",
      "epoch: 20 step: 109, loss is 0.0009289483423344791\n",
      "epoch: 20 step: 110, loss is 0.017346028238534927\n",
      "epoch: 20 step: 111, loss is 0.0023344503715634346\n",
      "epoch: 20 step: 112, loss is 0.03812107816338539\n",
      "epoch: 20 step: 113, loss is 0.006814706604927778\n",
      "epoch: 20 step: 114, loss is 0.021462073549628258\n",
      "epoch: 20 step: 115, loss is 0.013716667890548706\n",
      "epoch: 20 step: 116, loss is 0.0034387048799544573\n",
      "epoch: 20 step: 117, loss is 0.004594400059431791\n",
      "epoch: 20 step: 118, loss is 0.006252381484955549\n",
      "epoch: 20 step: 119, loss is 0.006448693107813597\n",
      "epoch: 20 step: 120, loss is 0.005709273274987936\n",
      "epoch: 20 step: 121, loss is 0.0014936556108295918\n",
      "epoch: 20 step: 122, loss is 0.055208880454301834\n",
      "epoch: 20 step: 123, loss is 0.004579378757625818\n",
      "epoch: 20 step: 124, loss is 0.00763124180957675\n",
      "epoch: 20 step: 125, loss is 0.060832299292087555\n",
      "epoch: 20 step: 126, loss is 0.0009367169113829732\n",
      "epoch: 20 step: 127, loss is 0.0006731639732606709\n",
      "epoch: 20 step: 128, loss is 0.0022867913357913494\n",
      "epoch: 20 step: 129, loss is 0.002918535377830267\n",
      "epoch: 20 step: 130, loss is 0.0002813494938891381\n",
      "epoch: 20 step: 131, loss is 0.006137842312455177\n",
      "epoch: 20 step: 132, loss is 0.002009045099839568\n",
      "epoch: 20 step: 133, loss is 0.006786798592656851\n",
      "epoch: 20 step: 134, loss is 0.001523630111478269\n",
      "epoch: 20 step: 135, loss is 0.008261742070317268\n",
      "epoch: 20 step: 136, loss is 0.0005520528648048639\n",
      "epoch: 20 step: 137, loss is 0.00026003358652815223\n",
      "epoch: 20 step: 138, loss is 0.0009064064361155033\n",
      "epoch: 20 step: 139, loss is 0.009338699281215668\n",
      "epoch: 20 step: 140, loss is 0.0007467059185728431\n",
      "epoch: 20 step: 141, loss is 0.020349491387605667\n",
      "epoch: 20 step: 142, loss is 0.09954161942005157\n",
      "epoch: 20 step: 143, loss is 0.0007179747335612774\n",
      "epoch: 20 step: 144, loss is 0.0006693063187412918\n",
      "epoch: 20 step: 145, loss is 0.02469337359070778\n",
      "epoch: 20 step: 146, loss is 0.0011995314853265882\n",
      "epoch: 20 step: 147, loss is 0.011177420616149902\n",
      "epoch: 20 step: 148, loss is 0.007720278110355139\n",
      "epoch: 20 step: 149, loss is 0.004221966955810785\n",
      "epoch: 20 step: 150, loss is 0.00628281757235527\n",
      "epoch: 20 step: 151, loss is 0.00415283115580678\n",
      "epoch: 20 step: 152, loss is 0.01839565858244896\n",
      "epoch: 20 step: 153, loss is 0.001062931609340012\n",
      "epoch: 20 step: 154, loss is 0.0037508399691432714\n",
      "epoch: 20 step: 155, loss is 0.00013345706975087523\n",
      "epoch: 20 step: 156, loss is 0.020170575007796288\n",
      "epoch: 20 step: 157, loss is 0.005833828821778297\n",
      "epoch: 20 step: 158, loss is 0.002450274070724845\n",
      "epoch: 20 step: 159, loss is 0.02942793443799019\n",
      "epoch: 20 step: 160, loss is 0.0010538495844230056\n",
      "epoch: 20 step: 161, loss is 0.04240451380610466\n",
      "epoch: 20 step: 162, loss is 0.022275039926171303\n",
      "epoch: 20 step: 163, loss is 0.0031726870220154524\n",
      "epoch: 20 step: 164, loss is 0.0006298227235674858\n",
      "epoch: 20 step: 165, loss is 0.08669087290763855\n",
      "epoch: 20 step: 166, loss is 0.025786468759179115\n",
      "epoch: 20 step: 167, loss is 0.0023434837348759174\n",
      "epoch: 20 step: 168, loss is 0.0009044916369020939\n",
      "epoch: 20 step: 169, loss is 0.001267247716896236\n",
      "epoch: 20 step: 170, loss is 0.0035377575550228357\n",
      "epoch: 20 step: 171, loss is 0.00028336222749203444\n",
      "epoch: 20 step: 172, loss is 0.008119897916913033\n",
      "epoch: 20 step: 173, loss is 0.00466154282912612\n",
      "epoch: 20 step: 174, loss is 0.0018404665170237422\n",
      "epoch: 20 step: 175, loss is 0.0003813887306023389\n",
      "epoch: 20 step: 176, loss is 0.0027529855724424124\n",
      "epoch: 20 step: 177, loss is 0.004485229030251503\n",
      "epoch: 20 step: 178, loss is 0.020982889458537102\n",
      "epoch: 20 step: 179, loss is 0.001001439755782485\n",
      "epoch: 20 step: 180, loss is 0.0059213098138570786\n",
      "epoch: 20 step: 181, loss is 0.05418553575873375\n",
      "epoch: 20 step: 182, loss is 0.003182454500347376\n",
      "epoch: 20 step: 183, loss is 0.004966504871845245\n",
      "epoch: 20 step: 184, loss is 0.005915787070989609\n",
      "epoch: 20 step: 185, loss is 0.18148958683013916\n",
      "epoch: 20 step: 186, loss is 0.0025143628008663654\n",
      "epoch: 20 step: 187, loss is 0.006225171033293009\n",
      "epoch: 20 step: 188, loss is 0.00028201050008647144\n",
      "epoch: 20 step: 189, loss is 0.0006681203376501799\n",
      "epoch: 20 step: 190, loss is 0.0017284753266721964\n",
      "epoch: 20 step: 191, loss is 0.000517528154887259\n",
      "epoch: 20 step: 192, loss is 0.0024540438316762447\n",
      "epoch: 20 step: 193, loss is 0.0014034723863005638\n",
      "epoch: 20 step: 194, loss is 0.0009554610005579889\n",
      "epoch: 20 step: 195, loss is 0.00019764820171985775\n",
      "epoch: 20 step: 196, loss is 8.374860772164539e-05\n",
      "epoch: 20 step: 197, loss is 0.0029215191025286913\n",
      "epoch: 20 step: 198, loss is 0.006347704213112593\n",
      "epoch: 20 step: 199, loss is 0.023460805416107178\n",
      "epoch: 20 step: 200, loss is 0.010616755113005638\n",
      "epoch: 20 step: 201, loss is 0.004849782679229975\n",
      "epoch: 20 step: 202, loss is 0.0005186116904951632\n",
      "epoch: 20 step: 203, loss is 0.004003251902759075\n",
      "epoch: 20 step: 204, loss is 0.00041904981480911374\n",
      "epoch: 20 step: 205, loss is 0.0005542851868085563\n",
      "epoch: 20 step: 206, loss is 0.003509481670334935\n",
      "epoch: 20 step: 207, loss is 0.0014719750033691525\n",
      "epoch: 20 step: 208, loss is 0.002936818404123187\n",
      "epoch: 20 step: 209, loss is 0.0020683715119957924\n",
      "epoch: 20 step: 210, loss is 0.00012257414346095175\n",
      "epoch: 20 step: 211, loss is 0.011667097918689251\n",
      "epoch: 20 step: 212, loss is 0.0030436632223427296\n",
      "epoch: 20 step: 213, loss is 0.024015571922063828\n",
      "epoch: 20 step: 214, loss is 0.0014907929580658674\n",
      "epoch: 20 step: 215, loss is 0.0008572477381676435\n",
      "epoch: 20 step: 216, loss is 0.0015674505848437548\n",
      "epoch: 20 step: 217, loss is 0.01436237059533596\n",
      "epoch: 20 step: 218, loss is 0.0005543335573747754\n",
      "epoch: 20 step: 219, loss is 0.0922493189573288\n",
      "epoch: 20 step: 220, loss is 0.02020268514752388\n",
      "epoch: 20 step: 221, loss is 0.0010376552818343043\n",
      "epoch: 20 step: 222, loss is 0.00016996494377963245\n",
      "epoch: 20 step: 223, loss is 0.008161722682416439\n",
      "epoch: 20 step: 224, loss is 0.018098091706633568\n",
      "epoch: 20 step: 225, loss is 0.009490296244621277\n",
      "epoch: 20 step: 226, loss is 0.006050942000001669\n",
      "epoch: 20 step: 227, loss is 0.0007517999038100243\n",
      "epoch: 20 step: 228, loss is 4.683282895712182e-05\n",
      "epoch: 20 step: 229, loss is 0.007343375589698553\n",
      "epoch: 20 step: 230, loss is 0.013397440314292908\n",
      "epoch: 20 step: 231, loss is 0.010808510705828667\n",
      "epoch: 20 step: 232, loss is 0.03050421178340912\n",
      "epoch: 20 step: 233, loss is 0.00950973853468895\n",
      "epoch: 20 step: 234, loss is 0.0006812990177422762\n",
      "epoch: 20 step: 235, loss is 0.00030778374639339745\n",
      "epoch: 20 step: 236, loss is 0.0003741877153515816\n",
      "epoch: 20 step: 237, loss is 0.0012256428599357605\n",
      "epoch: 20 step: 238, loss is 0.001054992782883346\n",
      "epoch: 20 step: 239, loss is 0.00249749724753201\n",
      "epoch: 20 step: 240, loss is 0.0008428702130913734\n",
      "epoch: 20 step: 241, loss is 0.0005786926485598087\n",
      "epoch: 20 step: 242, loss is 0.010103900916874409\n",
      "epoch: 20 step: 243, loss is 0.00048007769510149956\n",
      "epoch: 20 step: 244, loss is 0.0004281788133084774\n",
      "epoch: 20 step: 245, loss is 0.0012316984357312322\n",
      "epoch: 20 step: 246, loss is 0.0365946926176548\n",
      "epoch: 20 step: 247, loss is 0.010817673988640308\n",
      "epoch: 20 step: 248, loss is 0.01878809928894043\n",
      "epoch: 20 step: 249, loss is 0.0021948390640318394\n",
      "epoch: 20 step: 250, loss is 0.007470362354069948\n",
      "epoch: 20 step: 251, loss is 0.0001549972512293607\n",
      "epoch: 20 step: 252, loss is 0.0005637514404952526\n",
      "epoch: 20 step: 253, loss is 0.06239412724971771\n",
      "epoch: 20 step: 254, loss is 0.001100393244996667\n",
      "epoch: 20 step: 255, loss is 0.0020269271917641163\n",
      "epoch: 20 step: 256, loss is 0.004965746309608221\n",
      "epoch: 20 step: 257, loss is 0.03141295537352562\n",
      "epoch: 20 step: 258, loss is 0.0017992735374718904\n",
      "epoch: 20 step: 259, loss is 0.0035964269191026688\n",
      "epoch: 20 step: 260, loss is 0.029959235340356827\n",
      "epoch: 20 step: 261, loss is 0.0011667932849377394\n",
      "epoch: 20 step: 262, loss is 0.004638936370611191\n",
      "epoch: 20 step: 263, loss is 0.019134659320116043\n",
      "epoch: 20 step: 264, loss is 0.012499834410846233\n",
      "epoch: 20 step: 265, loss is 0.00043438674765639007\n",
      "epoch: 20 step: 266, loss is 0.0019725956954061985\n",
      "epoch: 20 step: 267, loss is 0.002859830856323242\n",
      "epoch: 20 step: 268, loss is 0.0035515062045305967\n",
      "epoch: 20 step: 269, loss is 0.031893495470285416\n",
      "epoch: 20 step: 270, loss is 0.0006896253325976431\n",
      "epoch: 20 step: 271, loss is 0.03303183242678642\n",
      "epoch: 20 step: 272, loss is 0.061044711619615555\n",
      "epoch: 20 step: 273, loss is 0.025060895830392838\n",
      "epoch: 20 step: 274, loss is 0.026481345295906067\n",
      "epoch: 20 step: 275, loss is 0.017650511115789413\n",
      "epoch: 20 step: 276, loss is 0.010146143846213818\n",
      "epoch: 20 step: 277, loss is 0.02869972214102745\n",
      "epoch: 20 step: 278, loss is 0.010363081470131874\n",
      "epoch: 20 step: 279, loss is 0.0010536041809245944\n",
      "epoch: 20 step: 280, loss is 0.039752569049596786\n",
      "epoch: 20 step: 281, loss is 0.010343963280320168\n",
      "epoch: 20 step: 282, loss is 0.00029739533783867955\n",
      "epoch: 20 step: 283, loss is 0.005929171107709408\n",
      "epoch: 20 step: 284, loss is 0.0029214441310614347\n",
      "epoch: 20 step: 285, loss is 0.0031168926507234573\n",
      "epoch: 20 step: 286, loss is 0.00445950124412775\n",
      "epoch: 20 step: 287, loss is 0.018091829493641853\n",
      "epoch: 20 step: 288, loss is 1.528635220893193e-05\n",
      "epoch: 20 step: 289, loss is 0.031769607216119766\n",
      "epoch: 20 step: 290, loss is 0.008329148404300213\n",
      "epoch: 20 step: 291, loss is 4.7371660912176594e-05\n",
      "epoch: 20 step: 292, loss is 0.021758688613772392\n",
      "epoch: 20 step: 293, loss is 9.66723746387288e-05\n",
      "epoch: 20 step: 294, loss is 0.02412373200058937\n",
      "epoch: 20 step: 295, loss is 0.008351389318704605\n",
      "epoch: 20 step: 296, loss is 0.005601285956799984\n",
      "epoch: 20 step: 297, loss is 0.0023751056287437677\n",
      "epoch: 20 step: 298, loss is 0.0036961662117391825\n",
      "epoch: 20 step: 299, loss is 0.004122884012758732\n",
      "epoch: 20 step: 300, loss is 0.0008543133735656738\n",
      "epoch: 20 step: 301, loss is 0.000703070720192045\n",
      "epoch: 20 step: 302, loss is 0.018629616126418114\n",
      "epoch: 20 step: 303, loss is 0.004609812516719103\n",
      "epoch: 20 step: 304, loss is 0.023126270622015\n",
      "epoch: 20 step: 305, loss is 0.0005546543980017304\n",
      "epoch: 20 step: 306, loss is 0.00733396178111434\n",
      "epoch: 20 step: 307, loss is 0.009315810166299343\n",
      "epoch: 20 step: 308, loss is 0.022048775106668472\n",
      "epoch: 20 step: 309, loss is 0.0006344480207189918\n",
      "epoch: 20 step: 310, loss is 0.05595037713646889\n",
      "epoch: 20 step: 311, loss is 0.08583202958106995\n",
      "epoch: 20 step: 312, loss is 0.0010736746480688453\n",
      "epoch: 20 step: 313, loss is 0.007424668408930302\n",
      "epoch: 20 step: 314, loss is 0.001956212567165494\n",
      "epoch: 20 step: 315, loss is 0.009936539456248283\n",
      "epoch: 20 step: 316, loss is 0.009228957816958427\n",
      "epoch: 20 step: 317, loss is 0.0006244826945476234\n",
      "epoch: 20 step: 318, loss is 0.0018314728513360023\n",
      "epoch: 20 step: 319, loss is 0.006905929185450077\n",
      "epoch: 20 step: 320, loss is 0.0024111515376716852\n",
      "epoch: 20 step: 321, loss is 0.001347773359157145\n",
      "epoch: 20 step: 322, loss is 0.0008730882545933127\n",
      "epoch: 20 step: 323, loss is 0.0010484304511919618\n",
      "epoch: 20 step: 324, loss is 0.00022502723732031882\n",
      "epoch: 20 step: 325, loss is 0.0010165758430957794\n",
      "epoch: 20 step: 326, loss is 0.03580756112933159\n",
      "epoch: 20 step: 327, loss is 0.008703992702066898\n",
      "epoch: 20 step: 328, loss is 0.009704743511974812\n",
      "epoch: 20 step: 329, loss is 0.037388965487480164\n",
      "epoch: 20 step: 330, loss is 0.008013393729925156\n",
      "epoch: 20 step: 331, loss is 0.001896096859127283\n",
      "epoch: 20 step: 332, loss is 0.03237525746226311\n",
      "epoch: 20 step: 333, loss is 0.00536433095112443\n",
      "epoch: 20 step: 334, loss is 0.004598002880811691\n",
      "epoch: 20 step: 335, loss is 0.0003662138478830457\n",
      "epoch: 20 step: 336, loss is 0.00026076086214743555\n",
      "epoch: 20 step: 337, loss is 0.004780041519552469\n",
      "epoch: 20 step: 338, loss is 0.04539526253938675\n",
      "epoch: 20 step: 339, loss is 0.009894433431327343\n",
      "epoch: 20 step: 340, loss is 0.009266817010939121\n",
      "epoch: 20 step: 341, loss is 0.005639197770506144\n",
      "epoch: 20 step: 342, loss is 0.0016919038025662303\n",
      "epoch: 20 step: 343, loss is 0.00028149227728135884\n",
      "epoch: 20 step: 344, loss is 0.010263239964842796\n",
      "epoch: 20 step: 345, loss is 0.03340793773531914\n",
      "epoch: 20 step: 346, loss is 0.0014013212639838457\n",
      "epoch: 20 step: 347, loss is 0.030263390392065048\n",
      "epoch: 20 step: 348, loss is 0.003501311643049121\n",
      "epoch: 20 step: 349, loss is 0.007411821745336056\n",
      "epoch: 20 step: 350, loss is 0.008767860941588879\n",
      "epoch: 20 step: 351, loss is 0.01367829367518425\n",
      "epoch: 20 step: 352, loss is 0.010717242024838924\n",
      "epoch: 20 step: 353, loss is 0.019708093255758286\n",
      "epoch: 20 step: 354, loss is 0.0007513079326599836\n",
      "epoch: 20 step: 355, loss is 0.010553045198321342\n",
      "epoch: 20 step: 356, loss is 0.00022711741621606052\n",
      "epoch: 20 step: 357, loss is 0.02035636268556118\n",
      "epoch: 20 step: 358, loss is 0.0009640182252041996\n",
      "epoch: 20 step: 359, loss is 0.018994947895407677\n",
      "epoch: 20 step: 360, loss is 0.03430754318833351\n",
      "epoch: 20 step: 361, loss is 0.004556730389595032\n",
      "epoch: 20 step: 362, loss is 0.0005499911494553089\n",
      "epoch: 20 step: 363, loss is 0.03743370622396469\n",
      "epoch: 20 step: 364, loss is 0.019734498113393784\n",
      "epoch: 20 step: 365, loss is 0.0002158809220418334\n",
      "epoch: 20 step: 366, loss is 0.14261740446090698\n",
      "epoch: 20 step: 367, loss is 0.004685533232986927\n",
      "epoch: 20 step: 368, loss is 0.0010270607890561223\n",
      "epoch: 20 step: 369, loss is 0.0009174293372780085\n",
      "epoch: 20 step: 370, loss is 0.002462799893692136\n",
      "epoch: 20 step: 371, loss is 0.0005387915298342705\n",
      "epoch: 20 step: 372, loss is 0.00010476030729478225\n",
      "epoch: 20 step: 373, loss is 0.019924132153391838\n",
      "epoch: 20 step: 374, loss is 0.0032623345032334328\n",
      "epoch: 20 step: 375, loss is 0.00039281437057070434\n",
      "epoch: 20 step: 376, loss is 0.00742257758975029\n",
      "epoch: 20 step: 377, loss is 0.0011280508479103446\n",
      "epoch: 20 step: 378, loss is 0.0014863474061712623\n",
      "epoch: 20 step: 379, loss is 0.0017871452728286386\n",
      "epoch: 20 step: 380, loss is 0.01902768760919571\n",
      "epoch: 20 step: 381, loss is 0.01309836097061634\n",
      "epoch: 20 step: 382, loss is 0.01309222262352705\n",
      "epoch: 20 step: 383, loss is 0.005470999516546726\n",
      "epoch: 20 step: 384, loss is 0.05451875180006027\n",
      "epoch: 20 step: 385, loss is 0.0005341125652194023\n",
      "epoch: 20 step: 386, loss is 0.00032944203121587634\n",
      "epoch: 20 step: 387, loss is 0.013644789345562458\n",
      "epoch: 20 step: 388, loss is 0.021234670653939247\n",
      "epoch: 20 step: 389, loss is 0.010163159109652042\n",
      "epoch: 20 step: 390, loss is 0.017470957711338997\n",
      "epoch: 20 step: 391, loss is 0.014498456381261349\n",
      "epoch: 20 step: 392, loss is 0.013241486623883247\n",
      "epoch: 20 step: 393, loss is 0.019574856385588646\n",
      "epoch: 20 step: 394, loss is 0.0022826013155281544\n",
      "epoch: 20 step: 395, loss is 0.08905656635761261\n",
      "epoch: 20 step: 396, loss is 0.0032762959599494934\n",
      "epoch: 20 step: 397, loss is 0.09783019125461578\n",
      "epoch: 20 step: 398, loss is 0.0003514577110763639\n",
      "epoch: 20 step: 399, loss is 0.003222205676138401\n",
      "epoch: 20 step: 400, loss is 0.0016011017141863704\n",
      "epoch: 20 step: 401, loss is 0.0015873574884608388\n",
      "epoch: 20 step: 402, loss is 0.00021000161359552294\n",
      "epoch: 20 step: 403, loss is 0.00011894798808498308\n",
      "epoch: 20 step: 404, loss is 0.0013566229026764631\n",
      "epoch: 20 step: 405, loss is 0.045146647840738297\n",
      "epoch: 20 step: 406, loss is 0.0001253474474651739\n",
      "epoch: 20 step: 407, loss is 0.0018463815795257688\n",
      "epoch: 20 step: 408, loss is 0.017077844589948654\n",
      "epoch: 20 step: 409, loss is 0.0027029213961213827\n",
      "epoch: 20 step: 410, loss is 0.062164802104234695\n",
      "epoch: 20 step: 411, loss is 0.03599401190876961\n",
      "epoch: 20 step: 412, loss is 0.0019261111738160253\n",
      "epoch: 20 step: 413, loss is 0.00010938549530692399\n",
      "epoch: 20 step: 414, loss is 0.01327725499868393\n",
      "epoch: 20 step: 415, loss is 0.0018926598131656647\n",
      "epoch: 20 step: 416, loss is 0.0010065434034913778\n",
      "epoch: 20 step: 417, loss is 0.03718327358365059\n",
      "epoch: 20 step: 418, loss is 0.0035885744728147984\n",
      "epoch: 20 step: 419, loss is 0.07069884985685349\n",
      "epoch: 20 step: 420, loss is 0.006849975790828466\n",
      "epoch: 20 step: 421, loss is 0.007047854829579592\n",
      "epoch: 20 step: 422, loss is 0.047541894018650055\n",
      "epoch: 20 step: 423, loss is 0.1367710828781128\n",
      "epoch: 20 step: 424, loss is 0.045292507857084274\n",
      "epoch: 20 step: 425, loss is 0.01228468120098114\n",
      "epoch: 20 step: 426, loss is 0.017111996188759804\n",
      "epoch: 20 step: 427, loss is 0.025309013202786446\n",
      "epoch: 20 step: 428, loss is 0.02131878398358822\n",
      "epoch: 20 step: 429, loss is 0.006039069499820471\n",
      "epoch: 20 step: 430, loss is 0.006262240931391716\n",
      "epoch: 20 step: 431, loss is 0.001235312782227993\n",
      "epoch: 20 step: 432, loss is 0.009155450388789177\n",
      "epoch: 20 step: 433, loss is 0.02914360910654068\n",
      "epoch: 20 step: 434, loss is 0.04491457715630531\n",
      "epoch: 20 step: 435, loss is 0.0013495663879439235\n",
      "epoch: 20 step: 436, loss is 0.009379266761243343\n",
      "epoch: 20 step: 437, loss is 0.01259163673967123\n",
      "epoch: 20 step: 438, loss is 0.0014068996533751488\n",
      "epoch: 20 step: 439, loss is 0.005054547917097807\n",
      "epoch: 20 step: 440, loss is 0.1387210339307785\n",
      "epoch: 20 step: 441, loss is 0.04145544394850731\n",
      "epoch: 20 step: 442, loss is 0.003044955898076296\n",
      "epoch: 20 step: 443, loss is 0.07817913591861725\n",
      "epoch: 20 step: 444, loss is 0.08505846560001373\n",
      "epoch: 20 step: 445, loss is 0.002077871933579445\n",
      "epoch: 20 step: 446, loss is 0.09838317334651947\n",
      "epoch: 20 step: 447, loss is 0.00428821612149477\n",
      "epoch: 20 step: 448, loss is 0.06446027755737305\n",
      "epoch: 20 step: 449, loss is 0.011326040141284466\n",
      "epoch: 20 step: 450, loss is 0.07529337704181671\n",
      "epoch: 20 step: 451, loss is 0.0012550447136163712\n",
      "epoch: 20 step: 452, loss is 0.011539874598383904\n",
      "epoch: 20 step: 453, loss is 0.09586852043867111\n",
      "epoch: 20 step: 454, loss is 0.006425073370337486\n",
      "epoch: 20 step: 455, loss is 0.030503051355481148\n",
      "epoch: 20 step: 456, loss is 0.0438755638897419\n",
      "epoch: 20 step: 457, loss is 0.0026874281466007233\n",
      "epoch: 20 step: 458, loss is 0.01402882020920515\n",
      "epoch: 20 step: 459, loss is 0.0033072566147893667\n",
      "epoch: 20 step: 460, loss is 0.013727720826864243\n",
      "epoch: 20 step: 461, loss is 0.01878250576555729\n",
      "epoch: 20 step: 462, loss is 0.006087035872042179\n",
      "epoch: 20 step: 463, loss is 0.004388181492686272\n",
      "epoch: 20 step: 464, loss is 0.03624063730239868\n",
      "epoch: 20 step: 465, loss is 0.0031172276940196753\n",
      "epoch: 20 step: 466, loss is 0.012478194199502468\n",
      "epoch: 20 step: 467, loss is 0.019770314916968346\n",
      "epoch: 20 step: 468, loss is 0.02741825580596924\n",
      "epoch: 20 step: 469, loss is 0.1225956380367279\n",
      "epoch: 20 step: 470, loss is 0.036737699061632156\n",
      "epoch: 20 step: 471, loss is 0.0018447651527822018\n",
      "epoch: 20 step: 472, loss is 0.008694200776517391\n",
      "epoch: 20 step: 473, loss is 0.002615587320178747\n",
      "epoch: 20 step: 474, loss is 0.0008448144653812051\n",
      "epoch: 20 step: 475, loss is 0.014285231940448284\n",
      "epoch: 20 step: 476, loss is 0.005170316435396671\n",
      "epoch: 20 step: 477, loss is 0.006274565123021603\n",
      "epoch: 20 step: 478, loss is 0.06713832914829254\n",
      "epoch: 20 step: 479, loss is 0.007816513068974018\n",
      "epoch: 20 step: 480, loss is 0.01178003940731287\n",
      "epoch: 20 step: 481, loss is 0.022329231724143028\n",
      "epoch: 20 step: 482, loss is 0.08830281347036362\n",
      "epoch: 20 step: 483, loss is 0.0008701690821908414\n",
      "epoch: 20 step: 484, loss is 0.005091570783406496\n",
      "epoch: 20 step: 485, loss is 0.0009428451885469258\n",
      "epoch: 20 step: 486, loss is 0.006482353899627924\n",
      "epoch: 20 step: 487, loss is 0.010707632638514042\n",
      "epoch: 20 step: 488, loss is 0.0017901126993820071\n",
      "epoch: 20 step: 489, loss is 0.03226453810930252\n",
      "epoch: 20 step: 490, loss is 0.04425167664885521\n",
      "epoch: 20 step: 491, loss is 0.04668382182717323\n",
      "epoch: 20 step: 492, loss is 0.0010178314987570047\n",
      "epoch: 20 step: 493, loss is 0.000876840902492404\n",
      "epoch: 20 step: 494, loss is 0.034835781902074814\n",
      "epoch: 20 step: 495, loss is 0.09377541393041611\n",
      "epoch: 20 step: 496, loss is 0.007784465327858925\n",
      "epoch: 20 step: 497, loss is 0.07876082509756088\n",
      "epoch: 20 step: 498, loss is 0.00477593345567584\n",
      "epoch: 20 step: 499, loss is 0.0023392250295728445\n",
      "epoch: 20 step: 500, loss is 0.0008155663963407278\n",
      "epoch: 20 step: 501, loss is 0.00217698747292161\n",
      "epoch: 20 step: 502, loss is 0.010031082667410374\n",
      "epoch: 20 step: 503, loss is 0.011639663949608803\n",
      "epoch: 20 step: 504, loss is 0.007368010003119707\n",
      "epoch: 20 step: 505, loss is 0.030483270063996315\n",
      "epoch: 20 step: 506, loss is 0.09662777185440063\n",
      "epoch: 20 step: 507, loss is 0.0118502052500844\n",
      "epoch: 20 step: 508, loss is 0.0030018168035894632\n",
      "epoch: 20 step: 509, loss is 0.010826637037098408\n",
      "epoch: 20 step: 510, loss is 0.023592982441186905\n",
      "epoch: 20 step: 511, loss is 0.049913935363292694\n",
      "epoch: 20 step: 512, loss is 0.006301680579781532\n",
      "epoch: 20 step: 513, loss is 0.0032696062698960304\n",
      "epoch: 20 step: 514, loss is 0.05844337120652199\n",
      "epoch: 20 step: 515, loss is 0.001522699254564941\n",
      "epoch: 20 step: 516, loss is 0.031072378158569336\n",
      "epoch: 20 step: 517, loss is 0.003399855690076947\n",
      "epoch: 20 step: 518, loss is 0.01685078628361225\n",
      "epoch: 20 step: 519, loss is 0.00918820220977068\n",
      "epoch: 20 step: 520, loss is 0.0008798045455478132\n",
      "epoch: 20 step: 521, loss is 0.0012487559579312801\n",
      "epoch: 20 step: 522, loss is 0.010745503939688206\n",
      "epoch: 20 step: 523, loss is 0.0193187166005373\n",
      "epoch: 20 step: 524, loss is 0.0045746504329144955\n",
      "epoch: 20 step: 525, loss is 0.03798584267497063\n",
      "epoch: 20 step: 526, loss is 0.005498326383531094\n",
      "epoch: 20 step: 527, loss is 0.0014144673477858305\n",
      "epoch: 20 step: 528, loss is 0.03654075413942337\n",
      "epoch: 20 step: 529, loss is 0.008270623162388802\n",
      "epoch: 20 step: 530, loss is 0.03554543852806091\n",
      "epoch: 20 step: 531, loss is 0.019004324451088905\n",
      "epoch: 20 step: 532, loss is 0.0021495544351637363\n",
      "epoch: 20 step: 533, loss is 0.02984292432665825\n",
      "epoch: 20 step: 534, loss is 0.003981995861977339\n",
      "epoch: 20 step: 535, loss is 0.03919566050171852\n",
      "epoch: 20 step: 536, loss is 0.12596821784973145\n",
      "epoch: 20 step: 537, loss is 0.06479451805353165\n",
      "epoch: 20 step: 538, loss is 0.0056540570221841335\n",
      "epoch: 20 step: 539, loss is 0.02273046039044857\n",
      "epoch: 20 step: 540, loss is 0.01563381403684616\n",
      "epoch: 20 step: 541, loss is 0.001028575235977769\n",
      "epoch: 20 step: 542, loss is 0.004169122781604528\n",
      "epoch: 20 step: 543, loss is 0.010903547517955303\n",
      "epoch: 20 step: 544, loss is 0.012994823046028614\n",
      "epoch: 20 step: 545, loss is 0.02362016960978508\n",
      "epoch: 20 step: 546, loss is 0.058586057275533676\n",
      "epoch: 20 step: 547, loss is 0.0009852942312136292\n",
      "epoch: 20 step: 548, loss is 0.00798594020307064\n",
      "epoch: 20 step: 549, loss is 0.0009004202438518405\n",
      "epoch: 20 step: 550, loss is 0.04251997917890549\n",
      "epoch: 20 step: 551, loss is 0.0053121550008654594\n",
      "epoch: 20 step: 552, loss is 0.00016463265637867153\n",
      "epoch: 20 step: 553, loss is 0.057448189705610275\n",
      "epoch: 20 step: 554, loss is 0.03510965034365654\n",
      "epoch: 20 step: 555, loss is 0.005399560555815697\n",
      "epoch: 20 step: 556, loss is 0.013180640526115894\n",
      "epoch: 20 step: 557, loss is 0.0514226108789444\n",
      "epoch: 20 step: 558, loss is 0.014318464323878288\n",
      "epoch: 20 step: 559, loss is 0.00969365518540144\n",
      "epoch: 20 step: 560, loss is 0.03454539552330971\n",
      "epoch: 20 step: 561, loss is 0.01957421377301216\n",
      "epoch: 20 step: 562, loss is 0.0006022563902661204\n",
      "epoch: 20 step: 563, loss is 0.01328575424849987\n",
      "epoch: 20 step: 564, loss is 0.001429074676707387\n",
      "epoch: 20 step: 565, loss is 0.04551311209797859\n",
      "epoch: 20 step: 566, loss is 0.004126823507249355\n",
      "epoch: 20 step: 567, loss is 0.012028494849801064\n",
      "epoch: 20 step: 568, loss is 0.0648016706109047\n",
      "epoch: 20 step: 569, loss is 0.013301325961947441\n",
      "epoch: 20 step: 570, loss is 0.006037943065166473\n",
      "epoch: 20 step: 571, loss is 0.01873358152806759\n",
      "epoch: 20 step: 572, loss is 0.0025638758670538664\n",
      "epoch: 20 step: 573, loss is 0.010266035795211792\n",
      "epoch: 20 step: 574, loss is 0.04317255690693855\n",
      "epoch: 20 step: 575, loss is 0.0033623813651502132\n",
      "epoch: 20 step: 576, loss is 0.009480012580752373\n",
      "epoch: 20 step: 577, loss is 0.08342929184436798\n",
      "epoch: 20 step: 578, loss is 0.1083081066608429\n",
      "epoch: 20 step: 579, loss is 0.07845934480428696\n",
      "epoch: 20 step: 580, loss is 0.053512055426836014\n",
      "epoch: 20 step: 581, loss is 0.10686007887125015\n",
      "epoch: 20 step: 582, loss is 0.0022010132670402527\n",
      "epoch: 20 step: 583, loss is 0.0007505530957132578\n",
      "epoch: 20 step: 584, loss is 0.019144732505083084\n",
      "epoch: 20 step: 585, loss is 0.0002352087467443198\n",
      "epoch: 20 step: 586, loss is 0.001820922363549471\n",
      "epoch: 20 step: 587, loss is 0.007067279890179634\n",
      "epoch: 20 step: 588, loss is 0.0007503438391722739\n",
      "epoch: 20 step: 589, loss is 0.002121072495356202\n",
      "epoch: 20 step: 590, loss is 0.07421628385782242\n",
      "epoch: 20 step: 591, loss is 0.08393118530511856\n",
      "epoch: 20 step: 592, loss is 0.0043850550428032875\n",
      "epoch: 20 step: 593, loss is 0.058887988328933716\n",
      "epoch: 20 step: 594, loss is 0.03642596676945686\n",
      "epoch: 20 step: 595, loss is 0.0009009150671772659\n",
      "epoch: 20 step: 596, loss is 0.0012742670951411128\n",
      "epoch: 20 step: 597, loss is 0.0024326909333467484\n",
      "epoch: 20 step: 598, loss is 0.004545008298009634\n",
      "epoch: 20 step: 599, loss is 0.0019737016409635544\n",
      "epoch: 20 step: 600, loss is 0.01774468459188938\n",
      "epoch: 20 step: 601, loss is 0.023196039721369743\n",
      "epoch: 20 step: 602, loss is 0.020616963505744934\n",
      "epoch: 20 step: 603, loss is 0.021526919677853584\n",
      "epoch: 20 step: 604, loss is 0.028372596949338913\n",
      "epoch: 20 step: 605, loss is 0.04190734773874283\n",
      "epoch: 20 step: 606, loss is 0.053616635501384735\n",
      "epoch: 20 step: 607, loss is 0.001936068874783814\n",
      "epoch: 20 step: 608, loss is 0.0059323557652533054\n",
      "epoch: 20 step: 609, loss is 0.02766832895576954\n",
      "epoch: 20 step: 610, loss is 0.0014141090214252472\n",
      "epoch: 20 step: 611, loss is 0.0004299499560147524\n",
      "epoch: 20 step: 612, loss is 0.01707332208752632\n",
      "epoch: 20 step: 613, loss is 0.15536247193813324\n",
      "epoch: 20 step: 614, loss is 0.05708342790603638\n",
      "epoch: 20 step: 615, loss is 0.08675722032785416\n",
      "epoch: 20 step: 616, loss is 0.0575614869594574\n",
      "epoch: 20 step: 617, loss is 0.0008422688697464764\n",
      "epoch: 20 step: 618, loss is 0.0005744160735048354\n",
      "epoch: 20 step: 619, loss is 0.0008738399483263493\n",
      "epoch: 20 step: 620, loss is 0.04456306993961334\n",
      "epoch: 20 step: 621, loss is 0.0068844822235405445\n",
      "epoch: 20 step: 622, loss is 0.06419427692890167\n",
      "epoch: 20 step: 623, loss is 0.0737370178103447\n",
      "epoch: 20 step: 624, loss is 0.0358038991689682\n",
      "epoch: 20 step: 625, loss is 0.02964896894991398\n",
      "epoch: 20 step: 626, loss is 0.0013250337215140462\n",
      "epoch: 20 step: 627, loss is 0.02990366518497467\n",
      "epoch: 20 step: 628, loss is 0.023470234125852585\n",
      "epoch: 20 step: 629, loss is 0.00205535558052361\n",
      "epoch: 20 step: 630, loss is 0.00103115220554173\n",
      "epoch: 20 step: 631, loss is 0.03630915284156799\n",
      "epoch: 20 step: 632, loss is 0.13796503841876984\n",
      "epoch: 20 step: 633, loss is 0.09723840653896332\n",
      "epoch: 20 step: 634, loss is 0.01005006767809391\n",
      "epoch: 20 step: 635, loss is 0.00597479660063982\n",
      "epoch: 20 step: 636, loss is 0.024865705519914627\n",
      "epoch: 20 step: 637, loss is 0.013419119641184807\n",
      "epoch: 20 step: 638, loss is 0.029907984659075737\n",
      "epoch: 20 step: 639, loss is 0.006981133949011564\n",
      "epoch: 20 step: 640, loss is 0.011945671401917934\n",
      "epoch: 20 step: 641, loss is 0.01646992191672325\n",
      "epoch: 20 step: 642, loss is 0.11564251035451889\n",
      "epoch: 20 step: 643, loss is 0.005265349522233009\n",
      "epoch: 20 step: 644, loss is 0.04820232093334198\n",
      "epoch: 20 step: 645, loss is 0.00016677659004926682\n",
      "epoch: 20 step: 646, loss is 0.005259335041046143\n",
      "epoch: 20 step: 647, loss is 0.02380170300602913\n",
      "epoch: 20 step: 648, loss is 0.007087391801178455\n",
      "epoch: 20 step: 649, loss is 0.009823736734688282\n",
      "epoch: 20 step: 650, loss is 0.009054651483893394\n",
      "epoch: 20 step: 651, loss is 0.003459041705355048\n",
      "epoch: 20 step: 652, loss is 0.028357069939374924\n",
      "epoch: 20 step: 653, loss is 0.005599145777523518\n",
      "epoch: 20 step: 654, loss is 0.044594235718250275\n",
      "epoch: 20 step: 655, loss is 0.03650009632110596\n",
      "epoch: 20 step: 656, loss is 0.03004608489573002\n",
      "epoch: 20 step: 657, loss is 0.01541116088628769\n",
      "epoch: 20 step: 658, loss is 0.1753716915845871\n",
      "epoch: 20 step: 659, loss is 0.0031259956303983927\n",
      "epoch: 20 step: 660, loss is 0.002461651572957635\n",
      "epoch: 20 step: 661, loss is 0.02610842138528824\n",
      "epoch: 20 step: 662, loss is 0.007167688105255365\n",
      "epoch: 20 step: 663, loss is 0.0029363655485212803\n",
      "epoch: 20 step: 664, loss is 0.024652596563100815\n",
      "epoch: 20 step: 665, loss is 0.04824623093008995\n",
      "epoch: 20 step: 666, loss is 0.13175995647907257\n",
      "epoch: 20 step: 667, loss is 0.000914190779440105\n",
      "epoch: 20 step: 668, loss is 0.009836511686444283\n",
      "epoch: 20 step: 669, loss is 0.001421653083525598\n",
      "epoch: 20 step: 670, loss is 0.005159958731383085\n",
      "epoch: 20 step: 671, loss is 0.031110303476452827\n",
      "epoch: 20 step: 672, loss is 0.0033296041656285524\n",
      "epoch: 20 step: 673, loss is 0.016588620841503143\n",
      "epoch: 20 step: 674, loss is 0.09214504063129425\n",
      "epoch: 20 step: 675, loss is 0.07388859987258911\n",
      "epoch: 20 step: 676, loss is 0.00623296620324254\n",
      "epoch: 20 step: 677, loss is 0.010529957711696625\n",
      "epoch: 20 step: 678, loss is 0.003613707609474659\n",
      "epoch: 20 step: 679, loss is 0.108002670109272\n",
      "epoch: 20 step: 680, loss is 0.04012920707464218\n",
      "epoch: 20 step: 681, loss is 0.0003506364009808749\n",
      "epoch: 20 step: 682, loss is 0.009598626755177975\n",
      "epoch: 20 step: 683, loss is 0.061013370752334595\n",
      "epoch: 20 step: 684, loss is 0.004759046249091625\n",
      "epoch: 20 step: 685, loss is 0.007294148672372103\n",
      "epoch: 20 step: 686, loss is 0.003452178556472063\n",
      "epoch: 20 step: 687, loss is 0.0025985820684581995\n",
      "epoch: 20 step: 688, loss is 0.020565209910273552\n",
      "epoch: 20 step: 689, loss is 0.009391422383487225\n",
      "epoch: 20 step: 690, loss is 0.03480635583400726\n",
      "epoch: 20 step: 691, loss is 0.016094360500574112\n",
      "epoch: 20 step: 692, loss is 0.012003936804831028\n",
      "epoch: 20 step: 693, loss is 0.023628491908311844\n",
      "epoch: 20 step: 694, loss is 0.03222644329071045\n",
      "epoch: 20 step: 695, loss is 0.04511219635605812\n",
      "epoch: 20 step: 696, loss is 0.003052906133234501\n",
      "epoch: 20 step: 697, loss is 0.1260586977005005\n",
      "epoch: 20 step: 698, loss is 0.05486598238348961\n",
      "epoch: 20 step: 699, loss is 0.015173465013504028\n",
      "epoch: 20 step: 700, loss is 0.02601022832095623\n",
      "epoch: 20 step: 701, loss is 0.0017800637288019061\n",
      "epoch: 20 step: 702, loss is 0.00533945020288229\n",
      "epoch: 20 step: 703, loss is 0.008890915662050247\n",
      "epoch: 20 step: 704, loss is 0.013837427832186222\n",
      "epoch: 20 step: 705, loss is 0.03279564157128334\n",
      "epoch: 20 step: 706, loss is 0.019493330270051956\n",
      "epoch: 20 step: 707, loss is 0.013084681704640388\n",
      "epoch: 20 step: 708, loss is 0.003768959315493703\n",
      "epoch: 20 step: 709, loss is 0.01402956247329712\n",
      "epoch: 20 step: 710, loss is 0.008593154139816761\n",
      "epoch: 20 step: 711, loss is 0.003558556782081723\n",
      "epoch: 20 step: 712, loss is 0.005861023440957069\n",
      "epoch: 20 step: 713, loss is 0.00021519938309211284\n",
      "epoch: 20 step: 714, loss is 0.005495880730450153\n",
      "epoch: 20 step: 715, loss is 0.013532237149775028\n",
      "epoch: 20 step: 716, loss is 0.04683544114232063\n",
      "epoch: 20 step: 717, loss is 0.009698161855340004\n",
      "epoch: 20 step: 718, loss is 0.006854079198092222\n",
      "epoch: 20 step: 719, loss is 0.012635777704417706\n",
      "epoch: 20 step: 720, loss is 0.004472404718399048\n",
      "epoch: 20 step: 721, loss is 0.009074266999959946\n",
      "epoch: 20 step: 722, loss is 0.037105221301317215\n",
      "epoch: 20 step: 723, loss is 0.005404213909059763\n",
      "epoch: 20 step: 724, loss is 0.0009296067291870713\n",
      "epoch: 20 step: 725, loss is 0.1372610330581665\n",
      "epoch: 20 step: 726, loss is 0.08253291249275208\n",
      "epoch: 20 step: 727, loss is 0.0008916524820961058\n",
      "epoch: 20 step: 728, loss is 0.0071711973287165165\n",
      "epoch: 20 step: 729, loss is 0.006819426082074642\n",
      "epoch: 20 step: 730, loss is 0.0027614980936050415\n",
      "epoch: 20 step: 731, loss is 0.0009618299081921577\n",
      "epoch: 20 step: 732, loss is 0.03586217015981674\n",
      "epoch: 20 step: 733, loss is 0.03749712556600571\n",
      "epoch: 20 step: 734, loss is 0.004326194990426302\n",
      "epoch: 20 step: 735, loss is 0.0005677371518686414\n",
      "epoch: 20 step: 736, loss is 0.012103517539799213\n",
      "epoch: 20 step: 737, loss is 0.0003508790396153927\n",
      "epoch: 20 step: 738, loss is 0.009542245417833328\n",
      "epoch: 20 step: 739, loss is 0.0013646043371409178\n",
      "epoch: 20 step: 740, loss is 0.003508917987346649\n",
      "epoch: 20 step: 741, loss is 0.033105529844760895\n",
      "epoch: 20 step: 742, loss is 0.0008876360370777547\n",
      "epoch: 20 step: 743, loss is 0.019973115995526314\n",
      "epoch: 20 step: 744, loss is 0.07007309049367905\n",
      "epoch: 20 step: 745, loss is 0.059756800532341\n",
      "epoch: 20 step: 746, loss is 0.05731221288442612\n",
      "epoch: 20 step: 747, loss is 0.0021372358314692974\n",
      "epoch: 20 step: 748, loss is 0.04029083997011185\n",
      "epoch: 20 step: 749, loss is 0.0015003533335402608\n",
      "epoch: 20 step: 750, loss is 0.01318463496863842\n",
      "epoch: 20 step: 751, loss is 0.00874454714357853\n",
      "epoch: 20 step: 752, loss is 0.0008192489622160792\n",
      "epoch: 20 step: 753, loss is 0.002960055135190487\n",
      "epoch: 20 step: 754, loss is 0.0021572140976786613\n",
      "epoch: 20 step: 755, loss is 0.00495291780680418\n",
      "epoch: 20 step: 756, loss is 0.009703206829726696\n",
      "epoch: 20 step: 757, loss is 0.020328961312770844\n",
      "epoch: 20 step: 758, loss is 0.008838774636387825\n",
      "epoch: 20 step: 759, loss is 0.00872727856040001\n",
      "epoch: 20 step: 760, loss is 0.05141410604119301\n",
      "epoch: 20 step: 761, loss is 0.0563596673309803\n",
      "epoch: 20 step: 762, loss is 0.004676549695432186\n",
      "epoch: 20 step: 763, loss is 0.004207709338515997\n",
      "epoch: 20 step: 764, loss is 0.010229509323835373\n",
      "epoch: 20 step: 765, loss is 0.04193614795804024\n",
      "epoch: 20 step: 766, loss is 0.0772145614027977\n",
      "epoch: 20 step: 767, loss is 0.0063177673146128654\n",
      "epoch: 20 step: 768, loss is 0.009222171269357204\n",
      "epoch: 20 step: 769, loss is 0.05100923404097557\n",
      "epoch: 20 step: 770, loss is 0.0006515950080938637\n",
      "epoch: 20 step: 771, loss is 0.0008416534983552992\n",
      "epoch: 20 step: 772, loss is 0.0034501939080655575\n",
      "epoch: 20 step: 773, loss is 0.000378301803721115\n",
      "epoch: 20 step: 774, loss is 0.00901357177644968\n",
      "epoch: 20 step: 775, loss is 0.0016534352907910943\n",
      "epoch: 20 step: 776, loss is 0.007716421503573656\n",
      "epoch: 20 step: 777, loss is 0.002403517020866275\n",
      "epoch: 20 step: 778, loss is 0.011786230839788914\n",
      "epoch: 20 step: 779, loss is 0.008625322952866554\n",
      "epoch: 20 step: 780, loss is 0.0008263486670330167\n",
      "epoch: 20 step: 781, loss is 0.012152542360126972\n",
      "epoch: 20 step: 782, loss is 0.03562890738248825\n",
      "epoch: 20 step: 783, loss is 0.013169566169381142\n",
      "epoch: 20 step: 784, loss is 0.03305106610059738\n",
      "epoch: 20 step: 785, loss is 0.04393472895026207\n",
      "epoch: 20 step: 786, loss is 0.05806152895092964\n",
      "epoch: 20 step: 787, loss is 0.003433498088270426\n",
      "epoch: 20 step: 788, loss is 0.03897661343216896\n",
      "epoch: 20 step: 789, loss is 0.004007823299616575\n",
      "epoch: 20 step: 790, loss is 0.0022379925940185785\n",
      "epoch: 20 step: 791, loss is 0.03197376802563667\n",
      "epoch: 20 step: 792, loss is 0.04610735550522804\n",
      "epoch: 20 step: 793, loss is 0.024333050474524498\n",
      "epoch: 20 step: 794, loss is 0.004938934929668903\n",
      "epoch: 20 step: 795, loss is 0.0014633432729169726\n",
      "epoch: 20 step: 796, loss is 0.04002039507031441\n",
      "epoch: 20 step: 797, loss is 0.042955540120601654\n",
      "epoch: 20 step: 798, loss is 0.00038355131982825696\n",
      "epoch: 20 step: 799, loss is 0.00402581924572587\n",
      "epoch: 20 step: 800, loss is 0.0026810506824404\n",
      "epoch: 20 step: 801, loss is 0.044548310339450836\n",
      "epoch: 20 step: 802, loss is 0.020184433087706566\n",
      "epoch: 20 step: 803, loss is 0.04069526866078377\n",
      "epoch: 20 step: 804, loss is 0.04220517352223396\n",
      "epoch: 20 step: 805, loss is 0.0016436398727819324\n",
      "epoch: 20 step: 806, loss is 0.011649099178612232\n",
      "epoch: 20 step: 807, loss is 0.0012614132137969136\n",
      "epoch: 20 step: 808, loss is 0.003438008949160576\n",
      "epoch: 20 step: 809, loss is 0.06876374781131744\n",
      "epoch: 20 step: 810, loss is 0.037152960896492004\n",
      "epoch: 20 step: 811, loss is 0.00019219932437408715\n",
      "epoch: 20 step: 812, loss is 0.012042691931128502\n",
      "epoch: 20 step: 813, loss is 0.02399502508342266\n",
      "epoch: 20 step: 814, loss is 0.08836304396390915\n",
      "epoch: 20 step: 815, loss is 0.005356296431273222\n",
      "epoch: 20 step: 816, loss is 0.0035270291846245527\n",
      "epoch: 20 step: 817, loss is 0.023324033245444298\n",
      "epoch: 20 step: 818, loss is 0.0007583597325719893\n",
      "epoch: 20 step: 819, loss is 0.004931197967380285\n",
      "epoch: 20 step: 820, loss is 0.048863671720027924\n",
      "epoch: 20 step: 821, loss is 0.015702985227108\n",
      "epoch: 20 step: 822, loss is 0.0007270773057825863\n",
      "epoch: 20 step: 823, loss is 0.12618298828601837\n",
      "epoch: 20 step: 824, loss is 0.0776197761297226\n",
      "epoch: 20 step: 825, loss is 0.005566958338022232\n",
      "epoch: 20 step: 826, loss is 0.0018892482621595263\n",
      "epoch: 20 step: 827, loss is 0.014950798824429512\n",
      "epoch: 20 step: 828, loss is 0.0013170287711545825\n",
      "epoch: 20 step: 829, loss is 0.008068529888987541\n",
      "epoch: 20 step: 830, loss is 0.0028818107675760984\n",
      "epoch: 20 step: 831, loss is 0.0280753206461668\n",
      "epoch: 20 step: 832, loss is 0.0008388774585910141\n",
      "epoch: 20 step: 833, loss is 0.021841824054718018\n",
      "epoch: 20 step: 834, loss is 0.002872379496693611\n",
      "epoch: 20 step: 835, loss is 0.028395047411322594\n",
      "epoch: 20 step: 836, loss is 0.07212651520967484\n",
      "epoch: 20 step: 837, loss is 0.059273868799209595\n",
      "epoch: 20 step: 838, loss is 0.02947094477713108\n",
      "epoch: 20 step: 839, loss is 0.0232631154358387\n",
      "epoch: 20 step: 840, loss is 0.01682777889072895\n",
      "epoch: 20 step: 841, loss is 0.005383919924497604\n",
      "epoch: 20 step: 842, loss is 0.0022642379626631737\n",
      "epoch: 20 step: 843, loss is 0.0850229412317276\n",
      "epoch: 20 step: 844, loss is 0.012793010100722313\n",
      "epoch: 20 step: 845, loss is 0.01982901059091091\n",
      "epoch: 20 step: 846, loss is 0.006633152719587088\n",
      "epoch: 20 step: 847, loss is 0.0018908502534031868\n",
      "epoch: 20 step: 848, loss is 0.007215488236397505\n",
      "epoch: 20 step: 849, loss is 0.014180068857967854\n",
      "epoch: 20 step: 850, loss is 0.0634133368730545\n",
      "epoch: 20 step: 851, loss is 0.024403681978583336\n",
      "epoch: 20 step: 852, loss is 0.09320351481437683\n",
      "epoch: 20 step: 853, loss is 0.0059443628415465355\n",
      "epoch: 20 step: 854, loss is 0.049036119133234024\n",
      "epoch: 20 step: 855, loss is 0.1022610142827034\n",
      "epoch: 20 step: 856, loss is 0.04223741590976715\n",
      "epoch: 20 step: 857, loss is 0.01766357384622097\n",
      "epoch: 20 step: 858, loss is 0.006253849249333143\n",
      "epoch: 20 step: 859, loss is 0.08432573825120926\n",
      "epoch: 20 step: 860, loss is 0.003847552463412285\n",
      "epoch: 20 step: 861, loss is 0.011951462365686893\n",
      "epoch: 20 step: 862, loss is 0.0023667446803301573\n",
      "epoch: 20 step: 863, loss is 0.10341089963912964\n",
      "epoch: 20 step: 864, loss is 0.0026977320667356253\n",
      "epoch: 20 step: 865, loss is 0.016549982130527496\n",
      "epoch: 20 step: 866, loss is 0.031799111515283585\n",
      "epoch: 20 step: 867, loss is 0.08870842307806015\n",
      "epoch: 20 step: 868, loss is 0.0013982286909595132\n",
      "epoch: 20 step: 869, loss is 0.08783009648323059\n",
      "epoch: 20 step: 870, loss is 0.03679719567298889\n",
      "epoch: 20 step: 871, loss is 0.03362612798810005\n",
      "epoch: 20 step: 872, loss is 0.04800322651863098\n",
      "epoch: 20 step: 873, loss is 0.036139026284217834\n",
      "epoch: 20 step: 874, loss is 0.006161596160382032\n",
      "epoch: 20 step: 875, loss is 0.03862705081701279\n",
      "epoch: 20 step: 876, loss is 0.013993162661790848\n",
      "epoch: 20 step: 877, loss is 0.03501249477267265\n",
      "epoch: 20 step: 878, loss is 0.030327141284942627\n",
      "epoch: 20 step: 879, loss is 0.0023525089491158724\n",
      "epoch: 20 step: 880, loss is 0.11401144415140152\n",
      "epoch: 20 step: 881, loss is 0.039884090423583984\n",
      "epoch: 20 step: 882, loss is 0.0015703202225267887\n",
      "epoch: 20 step: 883, loss is 0.0869210883975029\n",
      "epoch: 20 step: 884, loss is 0.0020756328012794256\n",
      "epoch: 20 step: 885, loss is 0.023961864411830902\n",
      "epoch: 20 step: 886, loss is 0.025751793757081032\n",
      "epoch: 20 step: 887, loss is 0.004457138944417238\n",
      "epoch: 20 step: 888, loss is 0.11147674918174744\n",
      "epoch: 20 step: 889, loss is 0.048875775188207626\n",
      "epoch: 20 step: 890, loss is 0.079934261739254\n",
      "epoch: 20 step: 891, loss is 0.05165845528244972\n",
      "epoch: 20 step: 892, loss is 0.002441050484776497\n",
      "epoch: 20 step: 893, loss is 0.015018900856375694\n",
      "epoch: 20 step: 894, loss is 0.01602785289287567\n",
      "epoch: 20 step: 895, loss is 0.02535695768892765\n",
      "epoch: 20 step: 896, loss is 0.013649952597916126\n",
      "epoch: 20 step: 897, loss is 0.0062971035949885845\n",
      "epoch: 20 step: 898, loss is 0.005243940278887749\n",
      "epoch: 20 step: 899, loss is 0.0008413750329054892\n",
      "epoch: 20 step: 900, loss is 0.0027974925469607115\n",
      "epoch: 20 step: 901, loss is 0.08004161715507507\n",
      "epoch: 20 step: 902, loss is 0.003105411771684885\n",
      "epoch: 20 step: 903, loss is 0.0047009410336613655\n",
      "epoch: 20 step: 904, loss is 0.013175737112760544\n",
      "epoch: 20 step: 905, loss is 0.014211730100214481\n",
      "epoch: 20 step: 906, loss is 0.015709256753325462\n",
      "epoch: 20 step: 907, loss is 0.010576574131846428\n",
      "epoch: 20 step: 908, loss is 0.001621456234715879\n",
      "epoch: 20 step: 909, loss is 0.001120694330893457\n",
      "epoch: 20 step: 910, loss is 0.0004781709285452962\n",
      "epoch: 20 step: 911, loss is 0.04879176989197731\n",
      "epoch: 20 step: 912, loss is 0.023731114342808723\n",
      "epoch: 20 step: 913, loss is 0.013145503588020802\n",
      "epoch: 20 step: 914, loss is 0.005598841235041618\n",
      "epoch: 20 step: 915, loss is 0.0017510231118649244\n",
      "epoch: 20 step: 916, loss is 0.002085349755361676\n",
      "epoch: 20 step: 917, loss is 0.003398170229047537\n",
      "epoch: 20 step: 918, loss is 0.00913296639919281\n",
      "epoch: 20 step: 919, loss is 0.00714010838419199\n",
      "epoch: 20 step: 920, loss is 0.08609272539615631\n",
      "epoch: 20 step: 921, loss is 0.03347088769078255\n",
      "epoch: 20 step: 922, loss is 0.0030253410805016756\n",
      "epoch: 20 step: 923, loss is 0.002078212331980467\n",
      "epoch: 20 step: 924, loss is 0.014960073865950108\n",
      "epoch: 20 step: 925, loss is 0.01665964350104332\n",
      "epoch: 20 step: 926, loss is 0.010443495586514473\n",
      "epoch: 20 step: 927, loss is 0.0008049600874073803\n",
      "epoch: 20 step: 928, loss is 0.023920226842164993\n",
      "epoch: 20 step: 929, loss is 0.0012588923564180732\n",
      "epoch: 20 step: 930, loss is 0.00492558628320694\n",
      "epoch: 20 step: 931, loss is 0.02750454843044281\n",
      "epoch: 20 step: 932, loss is 0.00022399562294594944\n",
      "epoch: 20 step: 933, loss is 0.0030761989764869213\n",
      "epoch: 20 step: 934, loss is 0.05020097643136978\n",
      "epoch: 20 step: 935, loss is 0.024312688037753105\n",
      "epoch: 20 step: 936, loss is 0.0009881482692435384\n",
      "epoch: 20 step: 937, loss is 0.03388552740216255\n",
      "epoch: 21 step: 1, loss is 0.0014127613976597786\n",
      "epoch: 21 step: 2, loss is 0.004011429380625486\n",
      "epoch: 21 step: 3, loss is 0.00031692683114670217\n",
      "epoch: 21 step: 4, loss is 0.0006200910429470241\n",
      "epoch: 21 step: 5, loss is 0.003379254834726453\n",
      "epoch: 21 step: 6, loss is 0.004303179681301117\n",
      "epoch: 21 step: 7, loss is 0.002303116489201784\n",
      "epoch: 21 step: 8, loss is 0.0005451947217807174\n",
      "epoch: 21 step: 9, loss is 0.0014188345521688461\n",
      "epoch: 21 step: 10, loss is 0.030718110501766205\n",
      "epoch: 21 step: 11, loss is 0.0046777743846178055\n",
      "epoch: 21 step: 12, loss is 0.07786373794078827\n",
      "epoch: 21 step: 13, loss is 0.00381631962954998\n",
      "epoch: 21 step: 14, loss is 0.01789705641567707\n",
      "epoch: 21 step: 15, loss is 0.0041954913176596165\n",
      "epoch: 21 step: 16, loss is 5.691889600711875e-05\n",
      "epoch: 21 step: 17, loss is 6.265802949201316e-05\n",
      "epoch: 21 step: 18, loss is 0.0009737847140058875\n",
      "epoch: 21 step: 19, loss is 0.020895959809422493\n",
      "epoch: 21 step: 20, loss is 0.05187491327524185\n",
      "epoch: 21 step: 21, loss is 0.013252150267362595\n",
      "epoch: 21 step: 22, loss is 0.005798635073006153\n",
      "epoch: 21 step: 23, loss is 0.0012239842908456922\n",
      "epoch: 21 step: 24, loss is 0.009414718486368656\n",
      "epoch: 21 step: 25, loss is 0.0024659803602844477\n",
      "epoch: 21 step: 26, loss is 0.0056180027313530445\n",
      "epoch: 21 step: 27, loss is 0.0008149425266310573\n",
      "epoch: 21 step: 28, loss is 0.006442961283028126\n",
      "epoch: 21 step: 29, loss is 0.15833036601543427\n",
      "epoch: 21 step: 30, loss is 0.003165353089570999\n",
      "epoch: 21 step: 31, loss is 0.01018176693469286\n",
      "epoch: 21 step: 32, loss is 6.994386058067903e-05\n",
      "epoch: 21 step: 33, loss is 0.0005287356907501817\n",
      "epoch: 21 step: 34, loss is 0.004590120166540146\n",
      "epoch: 21 step: 35, loss is 0.01759764738380909\n",
      "epoch: 21 step: 36, loss is 0.009358346462249756\n",
      "epoch: 21 step: 37, loss is 0.002518458990380168\n",
      "epoch: 21 step: 38, loss is 0.01607000082731247\n",
      "epoch: 21 step: 39, loss is 0.0038957176730036736\n",
      "epoch: 21 step: 40, loss is 0.028468403965234756\n",
      "epoch: 21 step: 41, loss is 0.003281613113358617\n",
      "epoch: 21 step: 42, loss is 0.006354509852826595\n",
      "epoch: 21 step: 43, loss is 0.05422697588801384\n",
      "epoch: 21 step: 44, loss is 6.402135477401316e-05\n",
      "epoch: 21 step: 45, loss is 0.0028504019137471914\n",
      "epoch: 21 step: 46, loss is 0.010477378964424133\n",
      "epoch: 21 step: 47, loss is 0.0012300090165808797\n",
      "epoch: 21 step: 48, loss is 0.022477520629763603\n",
      "epoch: 21 step: 49, loss is 0.0001283948076888919\n",
      "epoch: 21 step: 50, loss is 0.00021683552768081427\n",
      "epoch: 21 step: 51, loss is 0.011399813927710056\n",
      "epoch: 21 step: 52, loss is 0.04007117077708244\n",
      "epoch: 21 step: 53, loss is 0.005652422085404396\n",
      "epoch: 21 step: 54, loss is 0.0020413657184690237\n",
      "epoch: 21 step: 55, loss is 0.005922324024140835\n",
      "epoch: 21 step: 56, loss is 0.00027964587206952274\n",
      "epoch: 21 step: 57, loss is 0.030513646081089973\n",
      "epoch: 21 step: 58, loss is 0.009439419023692608\n",
      "epoch: 21 step: 59, loss is 0.009807751514017582\n",
      "epoch: 21 step: 60, loss is 0.055388763546943665\n",
      "epoch: 21 step: 61, loss is 0.00016893871361389756\n",
      "epoch: 21 step: 62, loss is 0.006957139819860458\n",
      "epoch: 21 step: 63, loss is 0.0019465353107079864\n",
      "epoch: 21 step: 64, loss is 0.011079942807555199\n",
      "epoch: 21 step: 65, loss is 0.02791924960911274\n",
      "epoch: 21 step: 66, loss is 0.00028476797160692513\n",
      "epoch: 21 step: 67, loss is 0.018926510587334633\n",
      "epoch: 21 step: 68, loss is 0.03882259130477905\n",
      "epoch: 21 step: 69, loss is 0.04882702976465225\n",
      "epoch: 21 step: 70, loss is 0.000885327288415283\n",
      "epoch: 21 step: 71, loss is 0.00023001540102995932\n",
      "epoch: 21 step: 72, loss is 0.0004639071994461119\n",
      "epoch: 21 step: 73, loss is 0.006478311959654093\n",
      "epoch: 21 step: 74, loss is 0.0029917180072516203\n",
      "epoch: 21 step: 75, loss is 0.0259593203663826\n",
      "epoch: 21 step: 76, loss is 0.013791937381029129\n",
      "epoch: 21 step: 77, loss is 0.0025398980360478163\n",
      "epoch: 21 step: 78, loss is 0.0009894861141219735\n",
      "epoch: 21 step: 79, loss is 0.005225670523941517\n",
      "epoch: 21 step: 80, loss is 0.00024348797160200775\n",
      "epoch: 21 step: 81, loss is 0.03160512447357178\n",
      "epoch: 21 step: 82, loss is 0.0005636164569295943\n",
      "epoch: 21 step: 83, loss is 0.0020528624299913645\n",
      "epoch: 21 step: 84, loss is 0.00031770046916790307\n",
      "epoch: 21 step: 85, loss is 0.00012329762103036046\n",
      "epoch: 21 step: 86, loss is 0.012883228249847889\n",
      "epoch: 21 step: 87, loss is 0.002070169895887375\n",
      "epoch: 21 step: 88, loss is 0.013337153941392899\n",
      "epoch: 21 step: 89, loss is 0.007310317363590002\n",
      "epoch: 21 step: 90, loss is 0.010153194889426231\n",
      "epoch: 21 step: 91, loss is 0.0007228412432596087\n",
      "epoch: 21 step: 92, loss is 0.004091088194400072\n",
      "epoch: 21 step: 93, loss is 0.004132264293730259\n",
      "epoch: 21 step: 94, loss is 0.029837576672434807\n",
      "epoch: 21 step: 95, loss is 0.012347452342510223\n",
      "epoch: 21 step: 96, loss is 0.003314343746751547\n",
      "epoch: 21 step: 97, loss is 0.0007641853298991919\n",
      "epoch: 21 step: 98, loss is 0.016580473631620407\n",
      "epoch: 21 step: 99, loss is 0.009486204013228416\n",
      "epoch: 21 step: 100, loss is 0.008520698174834251\n",
      "epoch: 21 step: 101, loss is 0.0004796589491888881\n",
      "epoch: 21 step: 102, loss is 0.0022055450826883316\n",
      "epoch: 21 step: 103, loss is 0.0005107795586809516\n",
      "epoch: 21 step: 104, loss is 0.0007405683863908052\n",
      "epoch: 21 step: 105, loss is 0.004254304338246584\n",
      "epoch: 21 step: 106, loss is 0.006239727139472961\n",
      "epoch: 21 step: 107, loss is 0.004328888840973377\n",
      "epoch: 21 step: 108, loss is 0.002543181413784623\n",
      "epoch: 21 step: 109, loss is 0.0007439131732098758\n",
      "epoch: 21 step: 110, loss is 0.0004147420695517212\n",
      "epoch: 21 step: 111, loss is 0.003975024446845055\n",
      "epoch: 21 step: 112, loss is 0.009951678104698658\n",
      "epoch: 21 step: 113, loss is 0.0007718777633272111\n",
      "epoch: 21 step: 114, loss is 0.0033957716077566147\n",
      "epoch: 21 step: 115, loss is 0.0013269816990941763\n",
      "epoch: 21 step: 116, loss is 0.07243742793798447\n",
      "epoch: 21 step: 117, loss is 0.007551542483270168\n",
      "epoch: 21 step: 118, loss is 0.007971511222422123\n",
      "epoch: 21 step: 119, loss is 0.0007830326212570071\n",
      "epoch: 21 step: 120, loss is 0.027167972177267075\n",
      "epoch: 21 step: 121, loss is 0.001564108650200069\n",
      "epoch: 21 step: 122, loss is 0.003842335194349289\n",
      "epoch: 21 step: 123, loss is 0.007641658652573824\n",
      "epoch: 21 step: 124, loss is 0.003307001432403922\n",
      "epoch: 21 step: 125, loss is 0.0002551387879066169\n",
      "epoch: 21 step: 126, loss is 0.0004990965244360268\n",
      "epoch: 21 step: 127, loss is 0.1336163580417633\n",
      "epoch: 21 step: 128, loss is 0.0258298609405756\n",
      "epoch: 21 step: 129, loss is 0.007133550941944122\n",
      "epoch: 21 step: 130, loss is 0.00043311112676747143\n",
      "epoch: 21 step: 131, loss is 0.004365330561995506\n",
      "epoch: 21 step: 132, loss is 0.005387807264924049\n",
      "epoch: 21 step: 133, loss is 0.004421071149408817\n",
      "epoch: 21 step: 134, loss is 0.0001356149441562593\n",
      "epoch: 21 step: 135, loss is 0.00030689817504025996\n",
      "epoch: 21 step: 136, loss is 0.0012355446815490723\n",
      "epoch: 21 step: 137, loss is 0.005907033104449511\n",
      "epoch: 21 step: 138, loss is 0.012212874367833138\n",
      "epoch: 21 step: 139, loss is 0.0031402891036123037\n",
      "epoch: 21 step: 140, loss is 0.0012299033114686608\n",
      "epoch: 21 step: 141, loss is 0.020344283431768417\n",
      "epoch: 21 step: 142, loss is 0.028156330808997154\n",
      "epoch: 21 step: 143, loss is 0.004295585211366415\n",
      "epoch: 21 step: 144, loss is 0.018711073324084282\n",
      "epoch: 21 step: 145, loss is 0.016821153461933136\n",
      "epoch: 21 step: 146, loss is 0.006642473395913839\n",
      "epoch: 21 step: 147, loss is 0.0012018836569041014\n",
      "epoch: 21 step: 148, loss is 0.04141726717352867\n",
      "epoch: 21 step: 149, loss is 0.0573982335627079\n",
      "epoch: 21 step: 150, loss is 0.009275649674236774\n",
      "epoch: 21 step: 151, loss is 0.0028117697220295668\n",
      "epoch: 21 step: 152, loss is 0.023949816823005676\n",
      "epoch: 21 step: 153, loss is 0.005516062956303358\n",
      "epoch: 21 step: 154, loss is 0.00802395585924387\n",
      "epoch: 21 step: 155, loss is 0.0032224066089838743\n",
      "epoch: 21 step: 156, loss is 0.012292108498513699\n",
      "epoch: 21 step: 157, loss is 0.0024289931170642376\n",
      "epoch: 21 step: 158, loss is 0.008532298728823662\n",
      "epoch: 21 step: 159, loss is 0.021534105762839317\n",
      "epoch: 21 step: 160, loss is 0.004602314438670874\n",
      "epoch: 21 step: 161, loss is 0.00037872360553592443\n",
      "epoch: 21 step: 162, loss is 0.00031879564630798995\n",
      "epoch: 21 step: 163, loss is 0.0008202456519939005\n",
      "epoch: 21 step: 164, loss is 0.003887507598847151\n",
      "epoch: 21 step: 165, loss is 0.10529740899801254\n",
      "epoch: 21 step: 166, loss is 0.023281170055270195\n",
      "epoch: 21 step: 167, loss is 0.005991064943373203\n",
      "epoch: 21 step: 168, loss is 0.017966795712709427\n",
      "epoch: 21 step: 169, loss is 0.00582030788064003\n",
      "epoch: 21 step: 170, loss is 0.000711901462636888\n",
      "epoch: 21 step: 171, loss is 0.0016892263665795326\n",
      "epoch: 21 step: 172, loss is 0.08357878774404526\n",
      "epoch: 21 step: 173, loss is 0.016598600894212723\n",
      "epoch: 21 step: 174, loss is 0.003985521849244833\n",
      "epoch: 21 step: 175, loss is 0.0429534874856472\n",
      "epoch: 21 step: 176, loss is 0.013084834441542625\n",
      "epoch: 21 step: 177, loss is 0.0010880716145038605\n",
      "epoch: 21 step: 178, loss is 0.017552489414811134\n",
      "epoch: 21 step: 179, loss is 0.00017416408809367567\n",
      "epoch: 21 step: 180, loss is 0.000524930830579251\n",
      "epoch: 21 step: 181, loss is 0.032526154071092606\n",
      "epoch: 21 step: 182, loss is 0.005124813411384821\n",
      "epoch: 21 step: 183, loss is 0.002967739710584283\n",
      "epoch: 21 step: 184, loss is 0.0004834405262954533\n",
      "epoch: 21 step: 185, loss is 0.0030508427880704403\n",
      "epoch: 21 step: 186, loss is 0.007894791662693024\n",
      "epoch: 21 step: 187, loss is 0.0008179872529581189\n",
      "epoch: 21 step: 188, loss is 0.045740704983472824\n",
      "epoch: 21 step: 189, loss is 0.03962085023522377\n",
      "epoch: 21 step: 190, loss is 0.0008053939673118293\n",
      "epoch: 21 step: 191, loss is 0.02221640944480896\n",
      "epoch: 21 step: 192, loss is 0.001426728325895965\n",
      "epoch: 21 step: 193, loss is 0.00023772830900270492\n",
      "epoch: 21 step: 194, loss is 0.022559193894267082\n",
      "epoch: 21 step: 195, loss is 0.017258213832974434\n",
      "epoch: 21 step: 196, loss is 0.002837400883436203\n",
      "epoch: 21 step: 197, loss is 0.0005924423458054662\n",
      "epoch: 21 step: 198, loss is 0.05695308744907379\n",
      "epoch: 21 step: 199, loss is 0.0006845044554211199\n",
      "epoch: 21 step: 200, loss is 0.014159095473587513\n",
      "epoch: 21 step: 201, loss is 0.0004730532527901232\n",
      "epoch: 21 step: 202, loss is 0.009812002070248127\n",
      "epoch: 21 step: 203, loss is 0.006452521309256554\n",
      "epoch: 21 step: 204, loss is 0.0009043814497999847\n",
      "epoch: 21 step: 205, loss is 0.05273200571537018\n",
      "epoch: 21 step: 206, loss is 0.0005391054437495768\n",
      "epoch: 21 step: 207, loss is 0.001025979407131672\n",
      "epoch: 21 step: 208, loss is 0.000711542961653322\n",
      "epoch: 21 step: 209, loss is 0.00020150805357843637\n",
      "epoch: 21 step: 210, loss is 0.00510774552822113\n",
      "epoch: 21 step: 211, loss is 0.0666777715086937\n",
      "epoch: 21 step: 212, loss is 0.003404628485441208\n",
      "epoch: 21 step: 213, loss is 0.025593582540750504\n",
      "epoch: 21 step: 214, loss is 0.0015530233504250646\n",
      "epoch: 21 step: 215, loss is 0.030461762100458145\n",
      "epoch: 21 step: 216, loss is 0.00915516633540392\n",
      "epoch: 21 step: 217, loss is 0.005573014263063669\n",
      "epoch: 21 step: 218, loss is 0.026084404438734055\n",
      "epoch: 21 step: 219, loss is 0.009429416619241238\n",
      "epoch: 21 step: 220, loss is 0.08886902779340744\n",
      "epoch: 21 step: 221, loss is 0.0034320629201829433\n",
      "epoch: 21 step: 222, loss is 0.03684081509709358\n",
      "epoch: 21 step: 223, loss is 8.955342491390184e-05\n",
      "epoch: 21 step: 224, loss is 0.00017836542974691838\n",
      "epoch: 21 step: 225, loss is 0.0002289734547957778\n",
      "epoch: 21 step: 226, loss is 0.1309307962656021\n",
      "epoch: 21 step: 227, loss is 0.0182126946747303\n",
      "epoch: 21 step: 228, loss is 0.005345130804926157\n",
      "epoch: 21 step: 229, loss is 0.0005059958784841001\n",
      "epoch: 21 step: 230, loss is 0.0007339526200667024\n",
      "epoch: 21 step: 231, loss is 0.0064291199669241905\n",
      "epoch: 21 step: 232, loss is 0.0007611413020640612\n",
      "epoch: 21 step: 233, loss is 0.009260570630431175\n",
      "epoch: 21 step: 234, loss is 0.0056014168076217175\n",
      "epoch: 21 step: 235, loss is 0.025281138718128204\n",
      "epoch: 21 step: 236, loss is 0.0003349154721945524\n",
      "epoch: 21 step: 237, loss is 0.004000817891210318\n",
      "epoch: 21 step: 238, loss is 0.0042144786566495895\n",
      "epoch: 21 step: 239, loss is 0.0021081629674881697\n",
      "epoch: 21 step: 240, loss is 0.01005928497761488\n",
      "epoch: 21 step: 241, loss is 0.009432013146579266\n",
      "epoch: 21 step: 242, loss is 0.005875738337635994\n",
      "epoch: 21 step: 243, loss is 0.0007370811072178185\n",
      "epoch: 21 step: 244, loss is 0.0013847824884578586\n",
      "epoch: 21 step: 245, loss is 0.0005360969225876033\n",
      "epoch: 21 step: 246, loss is 0.0030436026863753796\n",
      "epoch: 21 step: 247, loss is 0.0015842028660699725\n",
      "epoch: 21 step: 248, loss is 0.006253334693610668\n",
      "epoch: 21 step: 249, loss is 0.0039321305230259895\n",
      "epoch: 21 step: 250, loss is 0.008583450689911842\n",
      "epoch: 21 step: 251, loss is 0.003794882446527481\n",
      "epoch: 21 step: 252, loss is 0.0005941198905929923\n",
      "epoch: 21 step: 253, loss is 0.006936846300959587\n",
      "epoch: 21 step: 254, loss is 0.009795681573450565\n",
      "epoch: 21 step: 255, loss is 0.019611556082963943\n",
      "epoch: 21 step: 256, loss is 0.004279776941984892\n",
      "epoch: 21 step: 257, loss is 0.0026018477510660887\n",
      "epoch: 21 step: 258, loss is 0.004751046188175678\n",
      "epoch: 21 step: 259, loss is 0.008028450421988964\n",
      "epoch: 21 step: 260, loss is 0.002687329426407814\n",
      "epoch: 21 step: 261, loss is 0.002163500525057316\n",
      "epoch: 21 step: 262, loss is 0.00031481956830248237\n",
      "epoch: 21 step: 263, loss is 0.0002603449684102088\n",
      "epoch: 21 step: 264, loss is 0.00882358755916357\n",
      "epoch: 21 step: 265, loss is 0.0003575347363948822\n",
      "epoch: 21 step: 266, loss is 0.03733468055725098\n",
      "epoch: 21 step: 267, loss is 0.0005283195059746504\n",
      "epoch: 21 step: 268, loss is 0.03754584118723869\n",
      "epoch: 21 step: 269, loss is 0.00018887233454734087\n",
      "epoch: 21 step: 270, loss is 0.0007095166947692633\n",
      "epoch: 21 step: 271, loss is 0.0017863521352410316\n",
      "epoch: 21 step: 272, loss is 0.017543643712997437\n",
      "epoch: 21 step: 273, loss is 0.049576520919799805\n",
      "epoch: 21 step: 274, loss is 0.024831116199493408\n",
      "epoch: 21 step: 275, loss is 0.0558944046497345\n",
      "epoch: 21 step: 276, loss is 0.0040420652367174625\n",
      "epoch: 21 step: 277, loss is 0.0052709621377289295\n",
      "epoch: 21 step: 278, loss is 0.012882966548204422\n",
      "epoch: 21 step: 279, loss is 0.006258825771510601\n",
      "epoch: 21 step: 280, loss is 0.001070466241799295\n",
      "epoch: 21 step: 281, loss is 0.0066667161881923676\n",
      "epoch: 21 step: 282, loss is 0.006927869748324156\n",
      "epoch: 21 step: 283, loss is 0.016093704849481583\n",
      "epoch: 21 step: 284, loss is 6.145114457467571e-05\n",
      "epoch: 21 step: 285, loss is 0.003096391446888447\n",
      "epoch: 21 step: 286, loss is 0.014422026462852955\n",
      "epoch: 21 step: 287, loss is 0.009291693568229675\n",
      "epoch: 21 step: 288, loss is 0.023567384108901024\n",
      "epoch: 21 step: 289, loss is 0.016064094379544258\n",
      "epoch: 21 step: 290, loss is 0.00024272690643556416\n",
      "epoch: 21 step: 291, loss is 0.003452504752203822\n",
      "epoch: 21 step: 292, loss is 0.0013647542800754309\n",
      "epoch: 21 step: 293, loss is 0.003695855149999261\n",
      "epoch: 21 step: 294, loss is 0.011578579433262348\n",
      "epoch: 21 step: 295, loss is 0.0016092960722744465\n",
      "epoch: 21 step: 296, loss is 0.06908883154392242\n",
      "epoch: 21 step: 297, loss is 0.0075574107468128204\n",
      "epoch: 21 step: 298, loss is 0.0005310742999427021\n",
      "epoch: 21 step: 299, loss is 0.007641422096639872\n",
      "epoch: 21 step: 300, loss is 0.013656841591000557\n",
      "epoch: 21 step: 301, loss is 0.0012862137518823147\n",
      "epoch: 21 step: 302, loss is 0.002286674687638879\n",
      "epoch: 21 step: 303, loss is 0.001085601281374693\n",
      "epoch: 21 step: 304, loss is 6.91160312271677e-05\n",
      "epoch: 21 step: 305, loss is 0.012341034598648548\n",
      "epoch: 21 step: 306, loss is 0.005403236020356417\n",
      "epoch: 21 step: 307, loss is 0.006637352053076029\n",
      "epoch: 21 step: 308, loss is 8.883441478246823e-05\n",
      "epoch: 21 step: 309, loss is 0.0011875752825289965\n",
      "epoch: 21 step: 310, loss is 0.003402548609301448\n",
      "epoch: 21 step: 311, loss is 0.050431422889232635\n",
      "epoch: 21 step: 312, loss is 0.0016903101932257414\n",
      "epoch: 21 step: 313, loss is 0.01048978604376316\n",
      "epoch: 21 step: 314, loss is 0.041522085666656494\n",
      "epoch: 21 step: 315, loss is 0.18448461592197418\n",
      "epoch: 21 step: 316, loss is 0.0042734029702842236\n",
      "epoch: 21 step: 317, loss is 0.05847727134823799\n",
      "epoch: 21 step: 318, loss is 0.019059963524341583\n",
      "epoch: 21 step: 319, loss is 0.015812229365110397\n",
      "epoch: 21 step: 320, loss is 0.0031815078109502792\n",
      "epoch: 21 step: 321, loss is 0.02145250514149666\n",
      "epoch: 21 step: 322, loss is 0.056443896144628525\n",
      "epoch: 21 step: 323, loss is 0.009577690623700619\n",
      "epoch: 21 step: 324, loss is 0.00955204013735056\n",
      "epoch: 21 step: 325, loss is 0.03963184356689453\n",
      "epoch: 21 step: 326, loss is 0.1102525070309639\n",
      "epoch: 21 step: 327, loss is 0.0015480377478525043\n",
      "epoch: 21 step: 328, loss is 0.014147876761853695\n",
      "epoch: 21 step: 329, loss is 0.011621018871665001\n",
      "epoch: 21 step: 330, loss is 0.00034527157549746335\n",
      "epoch: 21 step: 331, loss is 0.012905293144285679\n",
      "epoch: 21 step: 332, loss is 0.012880360707640648\n",
      "epoch: 21 step: 333, loss is 0.025758203119039536\n",
      "epoch: 21 step: 334, loss is 0.06950743496417999\n",
      "epoch: 21 step: 335, loss is 0.009568395093083382\n",
      "epoch: 21 step: 336, loss is 0.021917082369327545\n",
      "epoch: 21 step: 337, loss is 0.03604070842266083\n",
      "epoch: 21 step: 338, loss is 0.04012195020914078\n",
      "epoch: 21 step: 339, loss is 0.004012324381619692\n",
      "epoch: 21 step: 340, loss is 0.008368970826268196\n",
      "epoch: 21 step: 341, loss is 0.0014825521502643824\n",
      "epoch: 21 step: 342, loss is 0.004214445129036903\n",
      "epoch: 21 step: 343, loss is 0.07809678465127945\n",
      "epoch: 21 step: 344, loss is 0.02291492372751236\n",
      "epoch: 21 step: 345, loss is 0.016930844634771347\n",
      "epoch: 21 step: 346, loss is 6.938016304047778e-05\n",
      "epoch: 21 step: 347, loss is 0.03712514415383339\n",
      "epoch: 21 step: 348, loss is 0.0001554270857013762\n",
      "epoch: 21 step: 349, loss is 0.0021528301294893026\n",
      "epoch: 21 step: 350, loss is 0.0018107494106516242\n",
      "epoch: 21 step: 351, loss is 0.025830602273344994\n",
      "epoch: 21 step: 352, loss is 0.024047954007983208\n",
      "epoch: 21 step: 353, loss is 0.0557960607111454\n",
      "epoch: 21 step: 354, loss is 0.0014960659900680184\n",
      "epoch: 21 step: 355, loss is 0.0010957616614177823\n",
      "epoch: 21 step: 356, loss is 0.012191787362098694\n",
      "epoch: 21 step: 357, loss is 0.0006035282858647406\n",
      "epoch: 21 step: 358, loss is 0.014591687358915806\n",
      "epoch: 21 step: 359, loss is 0.004073574673384428\n",
      "epoch: 21 step: 360, loss is 0.0008555742679163814\n",
      "epoch: 21 step: 361, loss is 0.0012875484535470605\n",
      "epoch: 21 step: 362, loss is 0.000826879171654582\n",
      "epoch: 21 step: 363, loss is 0.0014763803919777274\n",
      "epoch: 21 step: 364, loss is 0.005435453727841377\n",
      "epoch: 21 step: 365, loss is 0.009930054657161236\n",
      "epoch: 21 step: 366, loss is 0.052772752940654755\n",
      "epoch: 21 step: 367, loss is 0.002549957251176238\n",
      "epoch: 21 step: 368, loss is 0.0077557931654155254\n",
      "epoch: 21 step: 369, loss is 0.012310145422816277\n",
      "epoch: 21 step: 370, loss is 0.0005343991215340793\n",
      "epoch: 21 step: 371, loss is 0.009853942319750786\n",
      "epoch: 21 step: 372, loss is 0.08550818264484406\n",
      "epoch: 21 step: 373, loss is 0.03297996148467064\n",
      "epoch: 21 step: 374, loss is 0.007689868565648794\n",
      "epoch: 21 step: 375, loss is 0.0010125598637387156\n",
      "epoch: 21 step: 376, loss is 0.004864873364567757\n",
      "epoch: 21 step: 377, loss is 0.0071270279586315155\n",
      "epoch: 21 step: 378, loss is 0.0008156054536812007\n",
      "epoch: 21 step: 379, loss is 0.05565306171774864\n",
      "epoch: 21 step: 380, loss is 0.16098186373710632\n",
      "epoch: 21 step: 381, loss is 0.06795257329940796\n",
      "epoch: 21 step: 382, loss is 0.003043469740077853\n",
      "epoch: 21 step: 383, loss is 0.0007046760874800384\n",
      "epoch: 21 step: 384, loss is 0.07921770215034485\n",
      "epoch: 21 step: 385, loss is 0.0006073950207792222\n",
      "epoch: 21 step: 386, loss is 0.05602884665131569\n",
      "epoch: 21 step: 387, loss is 0.0015388684114441276\n",
      "epoch: 21 step: 388, loss is 0.01834200881421566\n",
      "epoch: 21 step: 389, loss is 0.00595482112839818\n",
      "epoch: 21 step: 390, loss is 0.004210890736430883\n",
      "epoch: 21 step: 391, loss is 0.00507813086733222\n",
      "epoch: 21 step: 392, loss is 0.037613075226545334\n",
      "epoch: 21 step: 393, loss is 0.0004940589424222708\n",
      "epoch: 21 step: 394, loss is 0.005231019109487534\n",
      "epoch: 21 step: 395, loss is 0.01829075627028942\n",
      "epoch: 21 step: 396, loss is 0.007752715609967709\n",
      "epoch: 21 step: 397, loss is 0.07485859096050262\n",
      "epoch: 21 step: 398, loss is 0.13210099935531616\n",
      "epoch: 21 step: 399, loss is 0.012735378928482533\n",
      "epoch: 21 step: 400, loss is 0.011277755722403526\n",
      "epoch: 21 step: 401, loss is 0.0036052085924893618\n",
      "epoch: 21 step: 402, loss is 0.0013799965381622314\n",
      "epoch: 21 step: 403, loss is 0.016953645274043083\n",
      "epoch: 21 step: 404, loss is 0.0011252627009525895\n",
      "epoch: 21 step: 405, loss is 0.030598927289247513\n",
      "epoch: 21 step: 406, loss is 0.0008260319591499865\n",
      "epoch: 21 step: 407, loss is 0.0034687009174376726\n",
      "epoch: 21 step: 408, loss is 0.000340718514053151\n",
      "epoch: 21 step: 409, loss is 0.002836846513673663\n",
      "epoch: 21 step: 410, loss is 0.007215301506221294\n",
      "epoch: 21 step: 411, loss is 0.023836415261030197\n",
      "epoch: 21 step: 412, loss is 0.010867662727832794\n",
      "epoch: 21 step: 413, loss is 0.0009293187176808715\n",
      "epoch: 21 step: 414, loss is 0.00109831802546978\n",
      "epoch: 21 step: 415, loss is 0.035961151123046875\n",
      "epoch: 21 step: 416, loss is 0.006119555328041315\n",
      "epoch: 21 step: 417, loss is 0.0003037588612642139\n",
      "epoch: 21 step: 418, loss is 0.01692073419690132\n",
      "epoch: 21 step: 419, loss is 0.07018890976905823\n",
      "epoch: 21 step: 420, loss is 0.040068067610263824\n",
      "epoch: 21 step: 421, loss is 0.005590916145592928\n",
      "epoch: 21 step: 422, loss is 0.007793571334332228\n",
      "epoch: 21 step: 423, loss is 0.0022906502708792686\n",
      "epoch: 21 step: 424, loss is 0.004274893086403608\n",
      "epoch: 21 step: 425, loss is 0.016560763120651245\n",
      "epoch: 21 step: 426, loss is 0.022870708256959915\n",
      "epoch: 21 step: 427, loss is 0.0005971695063635707\n",
      "epoch: 21 step: 428, loss is 0.016596298664808273\n",
      "epoch: 21 step: 429, loss is 0.0004666144959628582\n",
      "epoch: 21 step: 430, loss is 0.06115724518895149\n",
      "epoch: 21 step: 431, loss is 0.02115442417562008\n",
      "epoch: 21 step: 432, loss is 0.04928198456764221\n",
      "epoch: 21 step: 433, loss is 0.009997742250561714\n",
      "epoch: 21 step: 434, loss is 0.008005553856492043\n",
      "epoch: 21 step: 435, loss is 0.025678345933556557\n",
      "epoch: 21 step: 436, loss is 0.07276690751314163\n",
      "epoch: 21 step: 437, loss is 0.015898188576102257\n",
      "epoch: 21 step: 438, loss is 0.0655445009469986\n",
      "epoch: 21 step: 439, loss is 0.0016368855722248554\n",
      "epoch: 21 step: 440, loss is 0.009824554435908794\n",
      "epoch: 21 step: 441, loss is 0.0016304018208757043\n",
      "epoch: 21 step: 442, loss is 0.07857441157102585\n",
      "epoch: 21 step: 443, loss is 0.06745277345180511\n",
      "epoch: 21 step: 444, loss is 0.0213462021201849\n",
      "epoch: 21 step: 445, loss is 0.0001521861122455448\n",
      "epoch: 21 step: 446, loss is 0.00025624045520089567\n",
      "epoch: 21 step: 447, loss is 0.11006949841976166\n",
      "epoch: 21 step: 448, loss is 0.023547163233160973\n",
      "epoch: 21 step: 449, loss is 0.0029755523428320885\n",
      "epoch: 21 step: 450, loss is 0.03941452503204346\n",
      "epoch: 21 step: 451, loss is 0.04745601490139961\n",
      "epoch: 21 step: 452, loss is 0.0008957976242527366\n",
      "epoch: 21 step: 453, loss is 0.00041543692350387573\n",
      "epoch: 21 step: 454, loss is 0.0014127792092040181\n",
      "epoch: 21 step: 455, loss is 0.009869033470749855\n",
      "epoch: 21 step: 456, loss is 0.01810496672987938\n",
      "epoch: 21 step: 457, loss is 0.002754549030214548\n",
      "epoch: 21 step: 458, loss is 0.006120337173342705\n",
      "epoch: 21 step: 459, loss is 0.21012872457504272\n",
      "epoch: 21 step: 460, loss is 0.015074743889272213\n",
      "epoch: 21 step: 461, loss is 0.0023451538290828466\n",
      "epoch: 21 step: 462, loss is 0.005353044718503952\n",
      "epoch: 21 step: 463, loss is 0.006802310701459646\n",
      "epoch: 21 step: 464, loss is 0.002891024574637413\n",
      "epoch: 21 step: 465, loss is 0.003168397583067417\n",
      "epoch: 21 step: 466, loss is 0.015607140958309174\n",
      "epoch: 21 step: 467, loss is 0.09417715668678284\n",
      "epoch: 21 step: 468, loss is 0.0005379294161684811\n",
      "epoch: 21 step: 469, loss is 0.0073262769728899\n",
      "epoch: 21 step: 470, loss is 0.005060693249106407\n",
      "epoch: 21 step: 471, loss is 0.056692011654376984\n",
      "epoch: 21 step: 472, loss is 0.000509299396071583\n",
      "epoch: 21 step: 473, loss is 0.0028124183882027864\n",
      "epoch: 21 step: 474, loss is 0.0031117654871195555\n",
      "epoch: 21 step: 475, loss is 0.011855782940983772\n",
      "epoch: 21 step: 476, loss is 0.03911281377077103\n",
      "epoch: 21 step: 477, loss is 0.009746459312736988\n",
      "epoch: 21 step: 478, loss is 0.006305885035544634\n",
      "epoch: 21 step: 479, loss is 0.02048371359705925\n",
      "epoch: 21 step: 480, loss is 0.012462022714316845\n",
      "epoch: 21 step: 481, loss is 0.00046568975085392594\n",
      "epoch: 21 step: 482, loss is 0.00010628179734339938\n",
      "epoch: 21 step: 483, loss is 0.002110241912305355\n",
      "epoch: 21 step: 484, loss is 0.014786629006266594\n",
      "epoch: 21 step: 485, loss is 0.0008169525535777211\n",
      "epoch: 21 step: 486, loss is 0.00029078658553771675\n",
      "epoch: 21 step: 487, loss is 0.02666216529905796\n",
      "epoch: 21 step: 488, loss is 0.005256026051938534\n",
      "epoch: 21 step: 489, loss is 0.00112622429151088\n",
      "epoch: 21 step: 490, loss is 0.08014625310897827\n",
      "epoch: 21 step: 491, loss is 0.03552472963929176\n",
      "epoch: 21 step: 492, loss is 0.007477765437215567\n",
      "epoch: 21 step: 493, loss is 0.12406876683235168\n",
      "epoch: 21 step: 494, loss is 0.010203883051872253\n",
      "epoch: 21 step: 495, loss is 0.14732347428798676\n",
      "epoch: 21 step: 496, loss is 0.028311410918831825\n",
      "epoch: 21 step: 497, loss is 0.024576669558882713\n",
      "epoch: 21 step: 498, loss is 0.09441261738538742\n",
      "epoch: 21 step: 499, loss is 0.0056919194757938385\n",
      "epoch: 21 step: 500, loss is 0.0572671964764595\n",
      "epoch: 21 step: 501, loss is 0.020590918138623238\n",
      "epoch: 21 step: 502, loss is 0.02567772939801216\n",
      "epoch: 21 step: 503, loss is 0.0036688826512545347\n",
      "epoch: 21 step: 504, loss is 0.03943103551864624\n",
      "epoch: 21 step: 505, loss is 0.03245612606406212\n",
      "epoch: 21 step: 506, loss is 0.029551304876804352\n",
      "epoch: 21 step: 507, loss is 0.002338145859539509\n",
      "epoch: 21 step: 508, loss is 0.10630345344543457\n",
      "epoch: 21 step: 509, loss is 0.04134703800082207\n",
      "epoch: 21 step: 510, loss is 0.06115132197737694\n",
      "epoch: 21 step: 511, loss is 0.007848840206861496\n",
      "epoch: 21 step: 512, loss is 0.03730754181742668\n",
      "epoch: 21 step: 513, loss is 0.07248213142156601\n",
      "epoch: 21 step: 514, loss is 0.0013197680236771703\n",
      "epoch: 21 step: 515, loss is 0.0022161491215229034\n",
      "epoch: 21 step: 516, loss is 0.011696798726916313\n",
      "epoch: 21 step: 517, loss is 0.0071390545926988125\n",
      "epoch: 21 step: 518, loss is 0.05162288621068001\n",
      "epoch: 21 step: 519, loss is 0.04601460322737694\n",
      "epoch: 21 step: 520, loss is 0.003041267627850175\n",
      "epoch: 21 step: 521, loss is 0.005059401039034128\n",
      "epoch: 21 step: 522, loss is 0.013643663376569748\n",
      "epoch: 21 step: 523, loss is 0.009958709590137005\n",
      "epoch: 21 step: 524, loss is 0.08337283134460449\n",
      "epoch: 21 step: 525, loss is 0.016277199611067772\n",
      "epoch: 21 step: 526, loss is 0.00016415516438428313\n",
      "epoch: 21 step: 527, loss is 0.008957547135651112\n",
      "epoch: 21 step: 528, loss is 0.0018307355931028724\n",
      "epoch: 21 step: 529, loss is 0.06024295836687088\n",
      "epoch: 21 step: 530, loss is 0.02122952789068222\n",
      "epoch: 21 step: 531, loss is 0.014223975129425526\n",
      "epoch: 21 step: 532, loss is 0.006408976390957832\n",
      "epoch: 21 step: 533, loss is 0.0005265193176455796\n",
      "epoch: 21 step: 534, loss is 0.0005501657142303884\n",
      "epoch: 21 step: 535, loss is 0.07945635914802551\n",
      "epoch: 21 step: 536, loss is 0.03974882885813713\n",
      "epoch: 21 step: 537, loss is 0.005128037184476852\n",
      "epoch: 21 step: 538, loss is 0.050695132464170456\n",
      "epoch: 21 step: 539, loss is 0.046543557196855545\n",
      "epoch: 21 step: 540, loss is 0.0006625389796681702\n",
      "epoch: 21 step: 541, loss is 0.004132596775889397\n",
      "epoch: 21 step: 542, loss is 0.0019021763000637293\n",
      "epoch: 21 step: 543, loss is 0.0118971336632967\n",
      "epoch: 21 step: 544, loss is 0.04870825633406639\n",
      "epoch: 21 step: 545, loss is 0.004734840244054794\n",
      "epoch: 21 step: 546, loss is 0.007899520918726921\n",
      "epoch: 21 step: 547, loss is 2.8171116355224513e-05\n",
      "epoch: 21 step: 548, loss is 0.0024452977813780308\n",
      "epoch: 21 step: 549, loss is 0.0002746964164543897\n",
      "epoch: 21 step: 550, loss is 0.04437275975942612\n",
      "epoch: 21 step: 551, loss is 0.018421595916152\n",
      "epoch: 21 step: 552, loss is 0.015182781033217907\n",
      "epoch: 21 step: 553, loss is 0.1349700689315796\n",
      "epoch: 21 step: 554, loss is 0.003048211568966508\n",
      "epoch: 21 step: 555, loss is 0.0001144233756349422\n",
      "epoch: 21 step: 556, loss is 0.003862230572849512\n",
      "epoch: 21 step: 557, loss is 0.007147352676838636\n",
      "epoch: 21 step: 558, loss is 0.051217880100011826\n",
      "epoch: 21 step: 559, loss is 0.08289164304733276\n",
      "epoch: 21 step: 560, loss is 0.004607096780091524\n",
      "epoch: 21 step: 561, loss is 0.003969363868236542\n",
      "epoch: 21 step: 562, loss is 0.03156167268753052\n",
      "epoch: 21 step: 563, loss is 0.0107914749532938\n",
      "epoch: 21 step: 564, loss is 0.0004336463753134012\n",
      "epoch: 21 step: 565, loss is 0.0036259382031857967\n",
      "epoch: 21 step: 566, loss is 0.011411831714212894\n",
      "epoch: 21 step: 567, loss is 0.038895245641469955\n",
      "epoch: 21 step: 568, loss is 0.08420983701944351\n",
      "epoch: 21 step: 569, loss is 0.0009051369270309806\n",
      "epoch: 21 step: 570, loss is 0.003643254516646266\n",
      "epoch: 21 step: 571, loss is 0.010505801066756248\n",
      "epoch: 21 step: 572, loss is 0.0059669893234968185\n",
      "epoch: 21 step: 573, loss is 0.0025492871645838022\n",
      "epoch: 21 step: 574, loss is 0.003060431918129325\n",
      "epoch: 21 step: 575, loss is 0.04768140986561775\n",
      "epoch: 21 step: 576, loss is 0.003584801685065031\n",
      "epoch: 21 step: 577, loss is 0.16255038976669312\n",
      "epoch: 21 step: 578, loss is 0.014213437214493752\n",
      "epoch: 21 step: 579, loss is 0.0031683328561484814\n",
      "epoch: 21 step: 580, loss is 0.011908577755093575\n",
      "epoch: 21 step: 581, loss is 0.020916253328323364\n",
      "epoch: 21 step: 582, loss is 0.019144531339406967\n",
      "epoch: 21 step: 583, loss is 0.059346649795770645\n",
      "epoch: 21 step: 584, loss is 0.0183316171169281\n",
      "epoch: 21 step: 585, loss is 0.022764483466744423\n",
      "epoch: 21 step: 586, loss is 0.012759448029100895\n",
      "epoch: 21 step: 587, loss is 0.0006999266915954649\n",
      "epoch: 21 step: 588, loss is 0.0012433651136234403\n",
      "epoch: 21 step: 589, loss is 0.02396310307085514\n",
      "epoch: 21 step: 590, loss is 0.0020666439086198807\n",
      "epoch: 21 step: 591, loss is 0.016384558752179146\n",
      "epoch: 21 step: 592, loss is 0.021947160363197327\n",
      "epoch: 21 step: 593, loss is 0.0015111955581232905\n",
      "epoch: 21 step: 594, loss is 0.017356133088469505\n",
      "epoch: 21 step: 595, loss is 0.0008584933239035308\n",
      "epoch: 21 step: 596, loss is 0.0011229849187657237\n",
      "epoch: 21 step: 597, loss is 0.012979050166904926\n",
      "epoch: 21 step: 598, loss is 0.0034177065826952457\n",
      "epoch: 21 step: 599, loss is 0.006355637684464455\n",
      "epoch: 21 step: 600, loss is 0.00765195582062006\n",
      "epoch: 21 step: 601, loss is 0.015534505248069763\n",
      "epoch: 21 step: 602, loss is 0.011448342353105545\n",
      "epoch: 21 step: 603, loss is 0.11240910738706589\n",
      "epoch: 21 step: 604, loss is 0.006896704435348511\n",
      "epoch: 21 step: 605, loss is 0.0017578682163730264\n",
      "epoch: 21 step: 606, loss is 0.0006924723857082427\n",
      "epoch: 21 step: 607, loss is 0.0007324130856432021\n",
      "epoch: 21 step: 608, loss is 0.0007508226553909481\n",
      "epoch: 21 step: 609, loss is 0.0004365032073110342\n",
      "epoch: 21 step: 610, loss is 0.0005715476581826806\n",
      "epoch: 21 step: 611, loss is 0.00374991144053638\n",
      "epoch: 21 step: 612, loss is 0.00404533464461565\n",
      "epoch: 21 step: 613, loss is 0.0037366775795817375\n",
      "epoch: 21 step: 614, loss is 0.004344072192907333\n",
      "epoch: 21 step: 615, loss is 0.008401407860219479\n",
      "epoch: 21 step: 616, loss is 0.008256752975285053\n",
      "epoch: 21 step: 617, loss is 0.0026814164593815804\n",
      "epoch: 21 step: 618, loss is 0.018317969515919685\n",
      "epoch: 21 step: 619, loss is 0.00048504871665500104\n",
      "epoch: 21 step: 620, loss is 0.0004920006031170487\n",
      "epoch: 21 step: 621, loss is 0.017755309119820595\n",
      "epoch: 21 step: 622, loss is 0.00786572229117155\n",
      "epoch: 21 step: 623, loss is 0.0008363084052689373\n",
      "epoch: 21 step: 624, loss is 0.03258362039923668\n",
      "epoch: 21 step: 625, loss is 0.002928144298493862\n",
      "epoch: 21 step: 626, loss is 0.00204012100584805\n",
      "epoch: 21 step: 627, loss is 0.025184057652950287\n",
      "epoch: 21 step: 628, loss is 0.0001265045430045575\n",
      "epoch: 21 step: 629, loss is 0.001601440249942243\n",
      "epoch: 21 step: 630, loss is 0.0016124201938509941\n",
      "epoch: 21 step: 631, loss is 0.002040783641859889\n",
      "epoch: 21 step: 632, loss is 0.011010593734681606\n",
      "epoch: 21 step: 633, loss is 0.01391272060573101\n",
      "epoch: 21 step: 634, loss is 0.08662138879299164\n",
      "epoch: 21 step: 635, loss is 0.05711810290813446\n",
      "epoch: 21 step: 636, loss is 0.002046136185526848\n",
      "epoch: 21 step: 637, loss is 0.023467227816581726\n",
      "epoch: 21 step: 638, loss is 0.010707045905292034\n",
      "epoch: 21 step: 639, loss is 0.019803741946816444\n",
      "epoch: 21 step: 640, loss is 0.0014673848636448383\n",
      "epoch: 21 step: 641, loss is 0.002795148640871048\n",
      "epoch: 21 step: 642, loss is 0.012897772714495659\n",
      "epoch: 21 step: 643, loss is 0.02737114019691944\n",
      "epoch: 21 step: 644, loss is 0.0020814011804759502\n",
      "epoch: 21 step: 645, loss is 0.005545989144593477\n",
      "epoch: 21 step: 646, loss is 0.019942190498113632\n",
      "epoch: 21 step: 647, loss is 0.07651219516992569\n",
      "epoch: 21 step: 648, loss is 0.013009103946387768\n",
      "epoch: 21 step: 649, loss is 0.002756439847871661\n",
      "epoch: 21 step: 650, loss is 0.001941606868058443\n",
      "epoch: 21 step: 651, loss is 6.740080425515771e-05\n",
      "epoch: 21 step: 652, loss is 0.02015019953250885\n",
      "epoch: 21 step: 653, loss is 0.007927202619612217\n",
      "epoch: 21 step: 654, loss is 0.03409077972173691\n",
      "epoch: 21 step: 655, loss is 0.01218615472316742\n",
      "epoch: 21 step: 656, loss is 0.01610633172094822\n",
      "epoch: 21 step: 657, loss is 0.012548659928143024\n",
      "epoch: 21 step: 658, loss is 0.006777660921216011\n",
      "epoch: 21 step: 659, loss is 0.012982049025595188\n",
      "epoch: 21 step: 660, loss is 0.004906374495476484\n",
      "epoch: 21 step: 661, loss is 0.034837786108255386\n",
      "epoch: 21 step: 662, loss is 0.01203367579728365\n",
      "epoch: 21 step: 663, loss is 0.011975087225437164\n",
      "epoch: 21 step: 664, loss is 0.016619697213172913\n",
      "epoch: 21 step: 665, loss is 0.03164331987500191\n",
      "epoch: 21 step: 666, loss is 0.0036851775366812944\n",
      "epoch: 21 step: 667, loss is 0.008371461182832718\n",
      "epoch: 21 step: 668, loss is 0.004749971907585859\n",
      "epoch: 21 step: 669, loss is 0.0033047336619347334\n",
      "epoch: 21 step: 670, loss is 0.00685581611469388\n",
      "epoch: 21 step: 671, loss is 0.01192099042236805\n",
      "epoch: 21 step: 672, loss is 0.02955492213368416\n",
      "epoch: 21 step: 673, loss is 0.022322040051221848\n",
      "epoch: 21 step: 674, loss is 0.005457452964037657\n",
      "epoch: 21 step: 675, loss is 0.0001569445594213903\n",
      "epoch: 21 step: 676, loss is 0.01012351643294096\n",
      "epoch: 21 step: 677, loss is 0.0014956101076677442\n",
      "epoch: 21 step: 678, loss is 0.0952051654458046\n",
      "epoch: 21 step: 679, loss is 0.0017743411008268595\n",
      "epoch: 21 step: 680, loss is 0.02854594960808754\n",
      "epoch: 21 step: 681, loss is 9.90727567113936e-05\n",
      "epoch: 21 step: 682, loss is 0.0010746424086391926\n",
      "epoch: 21 step: 683, loss is 0.0002592006349004805\n",
      "epoch: 21 step: 684, loss is 0.0005864156992174685\n",
      "epoch: 21 step: 685, loss is 0.005114425905048847\n",
      "epoch: 21 step: 686, loss is 0.0019479129696264863\n",
      "epoch: 21 step: 687, loss is 0.006350690498948097\n",
      "epoch: 21 step: 688, loss is 0.021151868626475334\n",
      "epoch: 21 step: 689, loss is 0.002762489253655076\n",
      "epoch: 21 step: 690, loss is 0.0008114983211271465\n",
      "epoch: 21 step: 691, loss is 0.024164484813809395\n",
      "epoch: 21 step: 692, loss is 0.0067969695664942265\n",
      "epoch: 21 step: 693, loss is 0.0015828405739739537\n",
      "epoch: 21 step: 694, loss is 0.0011170352809131145\n",
      "epoch: 21 step: 695, loss is 0.004224244970828295\n",
      "epoch: 21 step: 696, loss is 0.0019225094001740217\n",
      "epoch: 21 step: 697, loss is 0.0008574113599024713\n",
      "epoch: 21 step: 698, loss is 0.0022012118715792894\n",
      "epoch: 21 step: 699, loss is 0.003722073510289192\n",
      "epoch: 21 step: 700, loss is 0.002577474107965827\n",
      "epoch: 21 step: 701, loss is 0.015860233455896378\n",
      "epoch: 21 step: 702, loss is 0.0008662615437060595\n",
      "epoch: 21 step: 703, loss is 0.014046942815184593\n",
      "epoch: 21 step: 704, loss is 0.04507283493876457\n",
      "epoch: 21 step: 705, loss is 0.011058888398110867\n",
      "epoch: 21 step: 706, loss is 0.0712323933839798\n",
      "epoch: 21 step: 707, loss is 0.0006817918620072305\n",
      "epoch: 21 step: 708, loss is 0.04080641269683838\n",
      "epoch: 21 step: 709, loss is 0.0002582180022727698\n",
      "epoch: 21 step: 710, loss is 0.024997539818286896\n",
      "epoch: 21 step: 711, loss is 0.0005251533002592623\n",
      "epoch: 21 step: 712, loss is 0.001189771923236549\n",
      "epoch: 21 step: 713, loss is 0.00019422470359131694\n",
      "epoch: 21 step: 714, loss is 0.0015747459838166833\n",
      "epoch: 21 step: 715, loss is 0.006669400725513697\n",
      "epoch: 21 step: 716, loss is 0.05191541835665703\n",
      "epoch: 21 step: 717, loss is 0.06729725003242493\n",
      "epoch: 21 step: 718, loss is 0.002936371834948659\n",
      "epoch: 21 step: 719, loss is 0.07410569489002228\n",
      "epoch: 21 step: 720, loss is 0.006475280504673719\n",
      "epoch: 21 step: 721, loss is 0.0013613887131214142\n",
      "epoch: 21 step: 722, loss is 0.04591841995716095\n",
      "epoch: 21 step: 723, loss is 0.00228193704970181\n",
      "epoch: 21 step: 724, loss is 0.035933561623096466\n",
      "epoch: 21 step: 725, loss is 0.0036158766597509384\n",
      "epoch: 21 step: 726, loss is 0.001649224548600614\n",
      "epoch: 21 step: 727, loss is 0.027786990627646446\n",
      "epoch: 21 step: 728, loss is 0.010791199281811714\n",
      "epoch: 21 step: 729, loss is 0.006749165244400501\n",
      "epoch: 21 step: 730, loss is 0.0012153337011113763\n",
      "epoch: 21 step: 731, loss is 0.03395720198750496\n",
      "epoch: 21 step: 732, loss is 0.00015176209853962064\n",
      "epoch: 21 step: 733, loss is 0.08103573322296143\n",
      "epoch: 21 step: 734, loss is 0.06261232495307922\n",
      "epoch: 21 step: 735, loss is 0.0006035848637111485\n",
      "epoch: 21 step: 736, loss is 0.002423431258648634\n",
      "epoch: 21 step: 737, loss is 0.001688301912508905\n",
      "epoch: 21 step: 738, loss is 0.028577173128724098\n",
      "epoch: 21 step: 739, loss is 0.004712597467005253\n",
      "epoch: 21 step: 740, loss is 0.0029159558471292257\n",
      "epoch: 21 step: 741, loss is 0.00021132545953150839\n",
      "epoch: 21 step: 742, loss is 0.0015421921852976084\n",
      "epoch: 21 step: 743, loss is 0.012225053273141384\n",
      "epoch: 21 step: 744, loss is 0.005754100624471903\n",
      "epoch: 21 step: 745, loss is 0.004354876931756735\n",
      "epoch: 21 step: 746, loss is 0.002335041295737028\n",
      "epoch: 21 step: 747, loss is 0.00047680287389084697\n",
      "epoch: 21 step: 748, loss is 0.007024015765637159\n",
      "epoch: 21 step: 749, loss is 0.0062462943606078625\n",
      "epoch: 21 step: 750, loss is 0.00018754653865471482\n",
      "epoch: 21 step: 751, loss is 0.01179032027721405\n",
      "epoch: 21 step: 752, loss is 0.0341499038040638\n",
      "epoch: 21 step: 753, loss is 0.0020236303098499775\n",
      "epoch: 21 step: 754, loss is 0.0028308939654380083\n",
      "epoch: 21 step: 755, loss is 0.0816708356142044\n",
      "epoch: 21 step: 756, loss is 0.09367985278367996\n",
      "epoch: 21 step: 757, loss is 0.007941145449876785\n",
      "epoch: 21 step: 758, loss is 0.0035884850658476353\n",
      "epoch: 21 step: 759, loss is 0.004488286562263966\n",
      "epoch: 21 step: 760, loss is 0.019705288112163544\n",
      "epoch: 21 step: 761, loss is 0.0003915891866199672\n",
      "epoch: 21 step: 762, loss is 0.0005371574661694467\n",
      "epoch: 21 step: 763, loss is 0.007756979204714298\n",
      "epoch: 21 step: 764, loss is 0.022805314511060715\n",
      "epoch: 21 step: 765, loss is 0.09566489607095718\n",
      "epoch: 21 step: 766, loss is 0.006842860020697117\n",
      "epoch: 21 step: 767, loss is 0.004379495978355408\n",
      "epoch: 21 step: 768, loss is 0.0021807020530104637\n",
      "epoch: 21 step: 769, loss is 0.049322858452796936\n",
      "epoch: 21 step: 770, loss is 0.0017325041117146611\n",
      "epoch: 21 step: 771, loss is 0.02210945449769497\n",
      "epoch: 21 step: 772, loss is 0.029564782977104187\n",
      "epoch: 21 step: 773, loss is 0.007942133583128452\n",
      "epoch: 21 step: 774, loss is 0.017475590109825134\n",
      "epoch: 21 step: 775, loss is 0.0003280550881754607\n",
      "epoch: 21 step: 776, loss is 0.022260889410972595\n",
      "epoch: 21 step: 777, loss is 0.002682851394638419\n",
      "epoch: 21 step: 778, loss is 0.02198314480483532\n",
      "epoch: 21 step: 779, loss is 0.0011924743885174394\n",
      "epoch: 21 step: 780, loss is 0.017094898968935013\n",
      "epoch: 21 step: 781, loss is 0.01701505109667778\n",
      "epoch: 21 step: 782, loss is 0.0007262551225721836\n",
      "epoch: 21 step: 783, loss is 0.023747356608510017\n",
      "epoch: 21 step: 784, loss is 0.05077729746699333\n",
      "epoch: 21 step: 785, loss is 0.004215607885271311\n",
      "epoch: 21 step: 786, loss is 0.018168052658438683\n",
      "epoch: 21 step: 787, loss is 0.0027602044865489006\n",
      "epoch: 21 step: 788, loss is 0.003475881414487958\n",
      "epoch: 21 step: 789, loss is 0.08539583534002304\n",
      "epoch: 21 step: 790, loss is 0.000610842602327466\n",
      "epoch: 21 step: 791, loss is 0.002560483291745186\n",
      "epoch: 21 step: 792, loss is 0.0011800974607467651\n",
      "epoch: 21 step: 793, loss is 0.00499976659193635\n",
      "epoch: 21 step: 794, loss is 0.001980055123567581\n",
      "epoch: 21 step: 795, loss is 0.016292130574584007\n",
      "epoch: 21 step: 796, loss is 0.0007857352029532194\n",
      "epoch: 21 step: 797, loss is 0.008671733550727367\n",
      "epoch: 21 step: 798, loss is 0.058255620300769806\n",
      "epoch: 21 step: 799, loss is 0.00923836324363947\n",
      "epoch: 21 step: 800, loss is 0.009951365180313587\n",
      "epoch: 21 step: 801, loss is 0.00969931110739708\n",
      "epoch: 21 step: 802, loss is 0.04519309476017952\n",
      "epoch: 21 step: 803, loss is 0.013047998771071434\n",
      "epoch: 21 step: 804, loss is 0.008788236416876316\n",
      "epoch: 21 step: 805, loss is 0.010737012140452862\n",
      "epoch: 21 step: 806, loss is 0.09310884028673172\n",
      "epoch: 21 step: 807, loss is 0.008212026208639145\n",
      "epoch: 21 step: 808, loss is 0.0031282000709325075\n",
      "epoch: 21 step: 809, loss is 0.0018339913804084063\n",
      "epoch: 21 step: 810, loss is 0.0038999607786536217\n",
      "epoch: 21 step: 811, loss is 0.04225381463766098\n",
      "epoch: 21 step: 812, loss is 0.0005888249725103378\n",
      "epoch: 21 step: 813, loss is 0.003918877802789211\n",
      "epoch: 21 step: 814, loss is 0.024024374783039093\n",
      "epoch: 21 step: 815, loss is 0.04881773144006729\n",
      "epoch: 21 step: 816, loss is 0.08377107232809067\n",
      "epoch: 21 step: 817, loss is 0.15714618563652039\n",
      "epoch: 21 step: 818, loss is 0.008892040699720383\n",
      "epoch: 21 step: 819, loss is 0.004307648167014122\n",
      "epoch: 21 step: 820, loss is 0.024228708818554878\n",
      "epoch: 21 step: 821, loss is 0.008383527398109436\n",
      "epoch: 21 step: 822, loss is 0.00395487854257226\n",
      "epoch: 21 step: 823, loss is 0.059934936463832855\n",
      "epoch: 21 step: 824, loss is 0.03444609045982361\n",
      "epoch: 21 step: 825, loss is 0.06365721672773361\n",
      "epoch: 21 step: 826, loss is 0.017954165115952492\n",
      "epoch: 21 step: 827, loss is 0.0031728302128612995\n",
      "epoch: 21 step: 828, loss is 0.03159794956445694\n",
      "epoch: 21 step: 829, loss is 0.004517305642366409\n",
      "epoch: 21 step: 830, loss is 0.017817961052060127\n",
      "epoch: 21 step: 831, loss is 0.00995567161589861\n",
      "epoch: 21 step: 832, loss is 0.003207745961844921\n",
      "epoch: 21 step: 833, loss is 0.035960689187049866\n",
      "epoch: 21 step: 834, loss is 0.004201909527182579\n",
      "epoch: 21 step: 835, loss is 0.009164531715214252\n",
      "epoch: 21 step: 836, loss is 0.0004872873250860721\n",
      "epoch: 21 step: 837, loss is 0.04350941628217697\n",
      "epoch: 21 step: 838, loss is 0.034574661403894424\n",
      "epoch: 21 step: 839, loss is 0.014149107970297337\n",
      "epoch: 21 step: 840, loss is 0.03906315192580223\n",
      "epoch: 21 step: 841, loss is 0.0009177410393022001\n",
      "epoch: 21 step: 842, loss is 0.002789702732115984\n",
      "epoch: 21 step: 843, loss is 0.001940090791322291\n",
      "epoch: 21 step: 844, loss is 0.03907905891537666\n",
      "epoch: 21 step: 845, loss is 0.04011660814285278\n",
      "epoch: 21 step: 846, loss is 0.023567546159029007\n",
      "epoch: 21 step: 847, loss is 0.011724335141479969\n",
      "epoch: 21 step: 848, loss is 0.0069456202909350395\n",
      "epoch: 21 step: 849, loss is 0.00650867959484458\n",
      "epoch: 21 step: 850, loss is 0.014947950839996338\n",
      "epoch: 21 step: 851, loss is 0.05190952494740486\n",
      "epoch: 21 step: 852, loss is 0.021103976294398308\n",
      "epoch: 21 step: 853, loss is 0.015378814190626144\n",
      "epoch: 21 step: 854, loss is 0.05105365812778473\n",
      "epoch: 21 step: 855, loss is 0.05736074596643448\n",
      "epoch: 21 step: 856, loss is 0.015949038788676262\n",
      "epoch: 21 step: 857, loss is 0.0760122612118721\n",
      "epoch: 21 step: 858, loss is 0.004628228489309549\n",
      "epoch: 21 step: 859, loss is 0.11206985265016556\n",
      "epoch: 21 step: 860, loss is 0.0035555653739720583\n",
      "epoch: 21 step: 861, loss is 0.010786346159875393\n",
      "epoch: 21 step: 862, loss is 0.007739522028714418\n",
      "epoch: 21 step: 863, loss is 0.008771946653723717\n",
      "epoch: 21 step: 864, loss is 0.20803049206733704\n",
      "epoch: 21 step: 865, loss is 0.010817654430866241\n",
      "epoch: 21 step: 866, loss is 0.11930731683969498\n",
      "epoch: 21 step: 867, loss is 0.007815735414624214\n",
      "epoch: 21 step: 868, loss is 0.008872997015714645\n",
      "epoch: 21 step: 869, loss is 0.00316407042555511\n",
      "epoch: 21 step: 870, loss is 0.01924986205995083\n",
      "epoch: 21 step: 871, loss is 0.026453008875250816\n",
      "epoch: 21 step: 872, loss is 0.11153693497180939\n",
      "epoch: 21 step: 873, loss is 0.00017548493633512408\n",
      "epoch: 21 step: 874, loss is 0.0010894667357206345\n",
      "epoch: 21 step: 875, loss is 0.0011119699338451028\n",
      "epoch: 21 step: 876, loss is 0.017463309690356255\n",
      "epoch: 21 step: 877, loss is 0.00902982335537672\n",
      "epoch: 21 step: 878, loss is 0.008628319948911667\n",
      "epoch: 21 step: 879, loss is 0.06371171027421951\n",
      "epoch: 21 step: 880, loss is 0.037867460399866104\n",
      "epoch: 21 step: 881, loss is 0.0008610397344455123\n",
      "epoch: 21 step: 882, loss is 0.0243974719196558\n",
      "epoch: 21 step: 883, loss is 0.01662619598209858\n",
      "epoch: 21 step: 884, loss is 0.051309723407030106\n",
      "epoch: 21 step: 885, loss is 0.039350174367427826\n",
      "epoch: 21 step: 886, loss is 0.016802605241537094\n",
      "epoch: 21 step: 887, loss is 0.016407091170549393\n",
      "epoch: 21 step: 888, loss is 0.15834535658359528\n",
      "epoch: 21 step: 889, loss is 0.0016604062402620912\n",
      "epoch: 21 step: 890, loss is 0.0539083294570446\n",
      "epoch: 21 step: 891, loss is 0.0012725635897368193\n",
      "epoch: 21 step: 892, loss is 0.027769189327955246\n",
      "epoch: 21 step: 893, loss is 0.0909397080540657\n",
      "epoch: 21 step: 894, loss is 0.00995669700205326\n",
      "epoch: 21 step: 895, loss is 0.008049502968788147\n",
      "epoch: 21 step: 896, loss is 0.0019312531221657991\n",
      "epoch: 21 step: 897, loss is 0.003341097617521882\n",
      "epoch: 21 step: 898, loss is 0.0038500623777508736\n",
      "epoch: 21 step: 899, loss is 0.2485952526330948\n",
      "epoch: 21 step: 900, loss is 0.0009037134004756808\n",
      "epoch: 21 step: 901, loss is 0.1012808158993721\n",
      "epoch: 21 step: 902, loss is 0.009590594097971916\n",
      "epoch: 21 step: 903, loss is 0.01291682943701744\n",
      "epoch: 21 step: 904, loss is 0.006605612579733133\n",
      "epoch: 21 step: 905, loss is 0.00828563328832388\n",
      "epoch: 21 step: 906, loss is 0.0060566323809325695\n",
      "epoch: 21 step: 907, loss is 0.05843083932995796\n",
      "epoch: 21 step: 908, loss is 0.0006157952593639493\n",
      "epoch: 21 step: 909, loss is 0.0041257468983531\n",
      "epoch: 21 step: 910, loss is 0.03044973313808441\n",
      "epoch: 21 step: 911, loss is 0.014677402563393116\n",
      "epoch: 21 step: 912, loss is 0.01104958076030016\n",
      "epoch: 21 step: 913, loss is 0.0008452323381789029\n",
      "epoch: 21 step: 914, loss is 0.00181580672506243\n",
      "epoch: 21 step: 915, loss is 0.007323081139475107\n",
      "epoch: 21 step: 916, loss is 0.0025624732952564955\n",
      "epoch: 21 step: 917, loss is 0.015619348734617233\n",
      "epoch: 21 step: 918, loss is 0.05585956200957298\n",
      "epoch: 21 step: 919, loss is 0.05225708335638046\n",
      "epoch: 21 step: 920, loss is 0.005549582187086344\n",
      "epoch: 21 step: 921, loss is 0.0020265073981136084\n",
      "epoch: 21 step: 922, loss is 0.02978336438536644\n",
      "epoch: 21 step: 923, loss is 0.0009702704264782369\n",
      "epoch: 21 step: 924, loss is 0.0018261601217091084\n",
      "epoch: 21 step: 925, loss is 0.010587772354483604\n",
      "epoch: 21 step: 926, loss is 0.01720472425222397\n",
      "epoch: 21 step: 927, loss is 0.025415126234292984\n",
      "epoch: 21 step: 928, loss is 0.005042078904807568\n",
      "epoch: 21 step: 929, loss is 0.004982804413884878\n",
      "epoch: 21 step: 930, loss is 0.05555461719632149\n",
      "epoch: 21 step: 931, loss is 0.001214679330587387\n",
      "epoch: 21 step: 932, loss is 0.0006398949772119522\n",
      "epoch: 21 step: 933, loss is 0.054602429270744324\n",
      "epoch: 21 step: 934, loss is 0.009331815876066685\n",
      "epoch: 21 step: 935, loss is 0.03144841268658638\n",
      "epoch: 21 step: 936, loss is 0.017630256712436676\n",
      "epoch: 21 step: 937, loss is 0.0020286659710109234\n",
      "epoch: 22 step: 1, loss is 0.015678811818361282\n",
      "epoch: 22 step: 2, loss is 0.001340968650765717\n",
      "epoch: 22 step: 3, loss is 0.0053702606819570065\n",
      "epoch: 22 step: 4, loss is 0.005323866382241249\n",
      "epoch: 22 step: 5, loss is 0.0005485442816279829\n",
      "epoch: 22 step: 6, loss is 0.008910401724278927\n",
      "epoch: 22 step: 7, loss is 0.00778171606361866\n",
      "epoch: 22 step: 8, loss is 0.0002328483242308721\n",
      "epoch: 22 step: 9, loss is 0.005679985508322716\n",
      "epoch: 22 step: 10, loss is 0.0024629789404571056\n",
      "epoch: 22 step: 11, loss is 0.00990129541605711\n",
      "epoch: 22 step: 12, loss is 0.0015819225227460265\n",
      "epoch: 22 step: 13, loss is 0.01043149083852768\n",
      "epoch: 22 step: 14, loss is 0.008198912255465984\n",
      "epoch: 22 step: 15, loss is 0.009604371152818203\n",
      "epoch: 22 step: 16, loss is 0.008590588346123695\n",
      "epoch: 22 step: 17, loss is 0.012295758351683617\n",
      "epoch: 22 step: 18, loss is 0.008327607065439224\n",
      "epoch: 22 step: 19, loss is 0.022601932287216187\n",
      "epoch: 22 step: 20, loss is 0.0006588938995264471\n",
      "epoch: 22 step: 21, loss is 0.0026600121054798365\n",
      "epoch: 22 step: 22, loss is 0.0014028889127075672\n",
      "epoch: 22 step: 23, loss is 0.012566923163831234\n",
      "epoch: 22 step: 24, loss is 0.0028313323855400085\n",
      "epoch: 22 step: 25, loss is 0.0016063475050032139\n",
      "epoch: 22 step: 26, loss is 0.030374260619282722\n",
      "epoch: 22 step: 27, loss is 0.006142672151327133\n",
      "epoch: 22 step: 28, loss is 0.021573470905423164\n",
      "epoch: 22 step: 29, loss is 0.002928926842287183\n",
      "epoch: 22 step: 30, loss is 0.0312763974070549\n",
      "epoch: 22 step: 31, loss is 0.0009383528376929462\n",
      "epoch: 22 step: 32, loss is 0.0004548929864540696\n",
      "epoch: 22 step: 33, loss is 0.0045797573402523994\n",
      "epoch: 22 step: 34, loss is 0.006400473415851593\n",
      "epoch: 22 step: 35, loss is 0.0028230156749486923\n",
      "epoch: 22 step: 36, loss is 0.0004336547863204032\n",
      "epoch: 22 step: 37, loss is 0.0010462659411132336\n",
      "epoch: 22 step: 38, loss is 0.008927402086555958\n",
      "epoch: 22 step: 39, loss is 0.034073393791913986\n",
      "epoch: 22 step: 40, loss is 0.0004545424599200487\n",
      "epoch: 22 step: 41, loss is 0.008173113688826561\n",
      "epoch: 22 step: 42, loss is 0.0014589125057682395\n",
      "epoch: 22 step: 43, loss is 0.0016297110123559833\n",
      "epoch: 22 step: 44, loss is 0.000121694800327532\n",
      "epoch: 22 step: 45, loss is 0.009679100476205349\n",
      "epoch: 22 step: 46, loss is 0.0030012433417141438\n",
      "epoch: 22 step: 47, loss is 0.0016906275413930416\n",
      "epoch: 22 step: 48, loss is 0.0017157996771857142\n",
      "epoch: 22 step: 49, loss is 0.0009461467852815986\n",
      "epoch: 22 step: 50, loss is 0.005182725377380848\n",
      "epoch: 22 step: 51, loss is 0.0006405902095139027\n",
      "epoch: 22 step: 52, loss is 0.006170495413243771\n",
      "epoch: 22 step: 53, loss is 0.016848579049110413\n",
      "epoch: 22 step: 54, loss is 0.023416725918650627\n",
      "epoch: 22 step: 55, loss is 0.15657927095890045\n",
      "epoch: 22 step: 56, loss is 0.0005359902279451489\n",
      "epoch: 22 step: 57, loss is 0.00012976842117495835\n",
      "epoch: 22 step: 58, loss is 0.01620485633611679\n",
      "epoch: 22 step: 59, loss is 0.009186971932649612\n",
      "epoch: 22 step: 60, loss is 0.002752998610958457\n",
      "epoch: 22 step: 61, loss is 0.007119069807231426\n",
      "epoch: 22 step: 62, loss is 0.020844465121626854\n",
      "epoch: 22 step: 63, loss is 0.0002754997694864869\n",
      "epoch: 22 step: 64, loss is 0.0002997656411025673\n",
      "epoch: 22 step: 65, loss is 0.000738385715521872\n",
      "epoch: 22 step: 66, loss is 0.006142196711152792\n",
      "epoch: 22 step: 67, loss is 0.032945312559604645\n",
      "epoch: 22 step: 68, loss is 0.00087482895469293\n",
      "epoch: 22 step: 69, loss is 0.005947559606283903\n",
      "epoch: 22 step: 70, loss is 0.004243276547640562\n",
      "epoch: 22 step: 71, loss is 0.0017624488100409508\n",
      "epoch: 22 step: 72, loss is 0.0005989722558297217\n",
      "epoch: 22 step: 73, loss is 0.004674064926803112\n",
      "epoch: 22 step: 74, loss is 0.0012152512790635228\n",
      "epoch: 22 step: 75, loss is 0.0035830815322697163\n",
      "epoch: 22 step: 76, loss is 0.002210622653365135\n",
      "epoch: 22 step: 77, loss is 0.00443114060908556\n",
      "epoch: 22 step: 78, loss is 0.00012979519669897854\n",
      "epoch: 22 step: 79, loss is 0.1607465296983719\n",
      "epoch: 22 step: 80, loss is 0.0007833464769646525\n",
      "epoch: 22 step: 81, loss is 5.886270446353592e-05\n",
      "epoch: 22 step: 82, loss is 0.0006653959862887859\n",
      "epoch: 22 step: 83, loss is 0.03068549558520317\n",
      "epoch: 22 step: 84, loss is 0.014398947358131409\n",
      "epoch: 22 step: 85, loss is 0.0019540826324373484\n",
      "epoch: 22 step: 86, loss is 0.0034412755630910397\n",
      "epoch: 22 step: 87, loss is 0.007374708075076342\n",
      "epoch: 22 step: 88, loss is 0.0005156442057341337\n",
      "epoch: 22 step: 89, loss is 0.0006269545410759747\n",
      "epoch: 22 step: 90, loss is 0.011039940640330315\n",
      "epoch: 22 step: 91, loss is 0.0016638289671391249\n",
      "epoch: 22 step: 92, loss is 0.0026960447430610657\n",
      "epoch: 22 step: 93, loss is 0.012759109027683735\n",
      "epoch: 22 step: 94, loss is 0.004083568230271339\n",
      "epoch: 22 step: 95, loss is 0.0004925005487166345\n",
      "epoch: 22 step: 96, loss is 0.03156065568327904\n",
      "epoch: 22 step: 97, loss is 0.002906554378569126\n",
      "epoch: 22 step: 98, loss is 0.017460115253925323\n",
      "epoch: 22 step: 99, loss is 0.004441957920789719\n",
      "epoch: 22 step: 100, loss is 0.09328304976224899\n",
      "epoch: 22 step: 101, loss is 0.04503121227025986\n",
      "epoch: 22 step: 102, loss is 0.007493439130485058\n",
      "epoch: 22 step: 103, loss is 0.009764106944203377\n",
      "epoch: 22 step: 104, loss is 0.011197191663086414\n",
      "epoch: 22 step: 105, loss is 0.004015743732452393\n",
      "epoch: 22 step: 106, loss is 0.00918915681540966\n",
      "epoch: 22 step: 107, loss is 0.0017533317441120744\n",
      "epoch: 22 step: 108, loss is 0.002251871395856142\n",
      "epoch: 22 step: 109, loss is 0.015941843390464783\n",
      "epoch: 22 step: 110, loss is 0.0026482376269996166\n",
      "epoch: 22 step: 111, loss is 0.0006245475960895419\n",
      "epoch: 22 step: 112, loss is 0.01137986034154892\n",
      "epoch: 22 step: 113, loss is 0.0036751460283994675\n",
      "epoch: 22 step: 114, loss is 0.0006537717999890447\n",
      "epoch: 22 step: 115, loss is 0.0023766816593706608\n",
      "epoch: 22 step: 116, loss is 0.0008800409268587828\n",
      "epoch: 22 step: 117, loss is 0.002336351666599512\n",
      "epoch: 22 step: 118, loss is 0.015742486342787743\n",
      "epoch: 22 step: 119, loss is 0.000737351831048727\n",
      "epoch: 22 step: 120, loss is 0.01602654531598091\n",
      "epoch: 22 step: 121, loss is 0.000981016899459064\n",
      "epoch: 22 step: 122, loss is 0.0360928513109684\n",
      "epoch: 22 step: 123, loss is 0.04070691391825676\n",
      "epoch: 22 step: 124, loss is 0.001069757156074047\n",
      "epoch: 22 step: 125, loss is 0.0059165433049201965\n",
      "epoch: 22 step: 126, loss is 0.020117787644267082\n",
      "epoch: 22 step: 127, loss is 0.037977080792188644\n",
      "epoch: 22 step: 128, loss is 0.13961833715438843\n",
      "epoch: 22 step: 129, loss is 0.004688432440161705\n",
      "epoch: 22 step: 130, loss is 0.0021131120156496763\n",
      "epoch: 22 step: 131, loss is 0.00021202971402090043\n",
      "epoch: 22 step: 132, loss is 0.0008630194352008402\n",
      "epoch: 22 step: 133, loss is 0.006635984871536493\n",
      "epoch: 22 step: 134, loss is 0.0018380916444584727\n",
      "epoch: 22 step: 135, loss is 0.020524337887763977\n",
      "epoch: 22 step: 136, loss is 0.0018189511029049754\n",
      "epoch: 22 step: 137, loss is 0.0004249392659403384\n",
      "epoch: 22 step: 138, loss is 0.004294941667467356\n",
      "epoch: 22 step: 139, loss is 0.04052897170186043\n",
      "epoch: 22 step: 140, loss is 0.003593949368223548\n",
      "epoch: 22 step: 141, loss is 0.01367422565817833\n",
      "epoch: 22 step: 142, loss is 0.030992427840828896\n",
      "epoch: 22 step: 143, loss is 0.023604683578014374\n",
      "epoch: 22 step: 144, loss is 0.00800293032079935\n",
      "epoch: 22 step: 145, loss is 0.0006629458512179554\n",
      "epoch: 22 step: 146, loss is 0.10308695584535599\n",
      "epoch: 22 step: 147, loss is 0.000599491992034018\n",
      "epoch: 22 step: 148, loss is 0.0232330784201622\n",
      "epoch: 22 step: 149, loss is 0.03067880868911743\n",
      "epoch: 22 step: 150, loss is 0.020434943959116936\n",
      "epoch: 22 step: 151, loss is 0.000465493620140478\n",
      "epoch: 22 step: 152, loss is 0.002650023903697729\n",
      "epoch: 22 step: 153, loss is 0.0014090448385104537\n",
      "epoch: 22 step: 154, loss is 0.0044918484054505825\n",
      "epoch: 22 step: 155, loss is 0.021472081542015076\n",
      "epoch: 22 step: 156, loss is 0.002774261636659503\n",
      "epoch: 22 step: 157, loss is 0.0008150830399245024\n",
      "epoch: 22 step: 158, loss is 0.000590107636526227\n",
      "epoch: 22 step: 159, loss is 0.0005594651447609067\n",
      "epoch: 22 step: 160, loss is 0.00011319516488583758\n",
      "epoch: 22 step: 161, loss is 0.009398362599313259\n",
      "epoch: 22 step: 162, loss is 0.07938036322593689\n",
      "epoch: 22 step: 163, loss is 0.0024978367146104574\n",
      "epoch: 22 step: 164, loss is 0.001043776050209999\n",
      "epoch: 22 step: 165, loss is 0.003879430703818798\n",
      "epoch: 22 step: 166, loss is 0.0064664618112146854\n",
      "epoch: 22 step: 167, loss is 0.018737727776169777\n",
      "epoch: 22 step: 168, loss is 0.021303337067365646\n",
      "epoch: 22 step: 169, loss is 0.012866785749793053\n",
      "epoch: 22 step: 170, loss is 0.016824880614876747\n",
      "epoch: 22 step: 171, loss is 0.008588356897234917\n",
      "epoch: 22 step: 172, loss is 0.06399553269147873\n",
      "epoch: 22 step: 173, loss is 0.02286555990576744\n",
      "epoch: 22 step: 174, loss is 0.004148585721850395\n",
      "epoch: 22 step: 175, loss is 0.03410672768950462\n",
      "epoch: 22 step: 176, loss is 0.004886202979832888\n",
      "epoch: 22 step: 177, loss is 0.0012061874149367213\n",
      "epoch: 22 step: 178, loss is 0.00014052054029889405\n",
      "epoch: 22 step: 179, loss is 0.0027479808777570724\n",
      "epoch: 22 step: 180, loss is 2.1539502995437942e-05\n",
      "epoch: 22 step: 181, loss is 0.00029893609462305903\n",
      "epoch: 22 step: 182, loss is 0.00034875425626523793\n",
      "epoch: 22 step: 183, loss is 0.011664770543575287\n",
      "epoch: 22 step: 184, loss is 0.006444576196372509\n",
      "epoch: 22 step: 185, loss is 0.006216684356331825\n",
      "epoch: 22 step: 186, loss is 0.0006332338671199977\n",
      "epoch: 22 step: 187, loss is 0.06171496957540512\n",
      "epoch: 22 step: 188, loss is 0.00673564150929451\n",
      "epoch: 22 step: 189, loss is 0.016658008098602295\n",
      "epoch: 22 step: 190, loss is 0.11067583411931992\n",
      "epoch: 22 step: 191, loss is 0.0007811433752067387\n",
      "epoch: 22 step: 192, loss is 0.002870969707146287\n",
      "epoch: 22 step: 193, loss is 0.008903616108000278\n",
      "epoch: 22 step: 194, loss is 0.0102932034060359\n",
      "epoch: 22 step: 195, loss is 0.03394472226500511\n",
      "epoch: 22 step: 196, loss is 0.0006603753427043557\n",
      "epoch: 22 step: 197, loss is 0.0018115590792149305\n",
      "epoch: 22 step: 198, loss is 0.0033834681380540133\n",
      "epoch: 22 step: 199, loss is 0.006238831207156181\n",
      "epoch: 22 step: 200, loss is 0.008039796724915504\n",
      "epoch: 22 step: 201, loss is 0.0007530219736509025\n",
      "epoch: 22 step: 202, loss is 0.0028459401801228523\n",
      "epoch: 22 step: 203, loss is 0.0082938801497221\n",
      "epoch: 22 step: 204, loss is 0.005616944283246994\n",
      "epoch: 22 step: 205, loss is 0.001610291888937354\n",
      "epoch: 22 step: 206, loss is 0.04573141038417816\n",
      "epoch: 22 step: 207, loss is 0.03042958676815033\n",
      "epoch: 22 step: 208, loss is 0.0001547352148918435\n",
      "epoch: 22 step: 209, loss is 0.006977272219955921\n",
      "epoch: 22 step: 210, loss is 0.0002395285409875214\n",
      "epoch: 22 step: 211, loss is 0.011672977358102798\n",
      "epoch: 22 step: 212, loss is 0.001072790939360857\n",
      "epoch: 22 step: 213, loss is 0.017895149067044258\n",
      "epoch: 22 step: 214, loss is 0.002165925456210971\n",
      "epoch: 22 step: 215, loss is 0.015146967954933643\n",
      "epoch: 22 step: 216, loss is 0.006464886479079723\n",
      "epoch: 22 step: 217, loss is 0.01788948103785515\n",
      "epoch: 22 step: 218, loss is 0.0008545754826627672\n",
      "epoch: 22 step: 219, loss is 0.028309637680649757\n",
      "epoch: 22 step: 220, loss is 0.003109574783593416\n",
      "epoch: 22 step: 221, loss is 0.06264353543519974\n",
      "epoch: 22 step: 222, loss is 0.01181863620877266\n",
      "epoch: 22 step: 223, loss is 0.0008035916835069656\n",
      "epoch: 22 step: 224, loss is 0.015953782945871353\n",
      "epoch: 22 step: 225, loss is 0.0023326657246798277\n",
      "epoch: 22 step: 226, loss is 0.001390071352943778\n",
      "epoch: 22 step: 227, loss is 0.020603012293577194\n",
      "epoch: 22 step: 228, loss is 0.024553963914513588\n",
      "epoch: 22 step: 229, loss is 0.012953871861100197\n",
      "epoch: 22 step: 230, loss is 0.0021480147261172533\n",
      "epoch: 22 step: 231, loss is 0.00022886156511958688\n",
      "epoch: 22 step: 232, loss is 0.000813064631074667\n",
      "epoch: 22 step: 233, loss is 0.0013773329555988312\n",
      "epoch: 22 step: 234, loss is 0.08511663228273392\n",
      "epoch: 22 step: 235, loss is 0.006241279188543558\n",
      "epoch: 22 step: 236, loss is 0.042786747217178345\n",
      "epoch: 22 step: 237, loss is 0.0006321991677395999\n",
      "epoch: 22 step: 238, loss is 0.004565514624118805\n",
      "epoch: 22 step: 239, loss is 0.06867078691720963\n",
      "epoch: 22 step: 240, loss is 0.001081212074495852\n",
      "epoch: 22 step: 241, loss is 0.03068210743367672\n",
      "epoch: 22 step: 242, loss is 0.0001338503061560914\n",
      "epoch: 22 step: 243, loss is 0.011880367062985897\n",
      "epoch: 22 step: 244, loss is 0.01481046061962843\n",
      "epoch: 22 step: 245, loss is 0.0010988846188411117\n",
      "epoch: 22 step: 246, loss is 0.009775016456842422\n",
      "epoch: 22 step: 247, loss is 0.019828515127301216\n",
      "epoch: 22 step: 248, loss is 0.007146432995796204\n",
      "epoch: 22 step: 249, loss is 0.0015391873894259334\n",
      "epoch: 22 step: 250, loss is 0.00032446818659082055\n",
      "epoch: 22 step: 251, loss is 0.02223760262131691\n",
      "epoch: 22 step: 252, loss is 0.003101835958659649\n",
      "epoch: 22 step: 253, loss is 0.06152147054672241\n",
      "epoch: 22 step: 254, loss is 0.006480214651674032\n",
      "epoch: 22 step: 255, loss is 0.010876831598579884\n",
      "epoch: 22 step: 256, loss is 0.00361364777199924\n",
      "epoch: 22 step: 257, loss is 0.01711970940232277\n",
      "epoch: 22 step: 258, loss is 0.015315786935389042\n",
      "epoch: 22 step: 259, loss is 0.051640961319208145\n",
      "epoch: 22 step: 260, loss is 0.08225618302822113\n",
      "epoch: 22 step: 261, loss is 0.0023983425926417112\n",
      "epoch: 22 step: 262, loss is 0.0034115200396627188\n",
      "epoch: 22 step: 263, loss is 0.004451055079698563\n",
      "epoch: 22 step: 264, loss is 0.023751869797706604\n",
      "epoch: 22 step: 265, loss is 0.022061118856072426\n",
      "epoch: 22 step: 266, loss is 0.0007058335468173027\n",
      "epoch: 22 step: 267, loss is 0.08891073614358902\n",
      "epoch: 22 step: 268, loss is 0.001167996320873499\n",
      "epoch: 22 step: 269, loss is 0.002557244850322604\n",
      "epoch: 22 step: 270, loss is 0.0275744516402483\n",
      "epoch: 22 step: 271, loss is 0.023321829736232758\n",
      "epoch: 22 step: 272, loss is 0.0035086486022919416\n",
      "epoch: 22 step: 273, loss is 0.003914189990609884\n",
      "epoch: 22 step: 274, loss is 0.01706884056329727\n",
      "epoch: 22 step: 275, loss is 0.008157584816217422\n",
      "epoch: 22 step: 276, loss is 0.019072704017162323\n",
      "epoch: 22 step: 277, loss is 0.004186195787042379\n",
      "epoch: 22 step: 278, loss is 0.008139602839946747\n",
      "epoch: 22 step: 279, loss is 0.004281147383153439\n",
      "epoch: 22 step: 280, loss is 0.0021568741649389267\n",
      "epoch: 22 step: 281, loss is 0.06686150282621384\n",
      "epoch: 22 step: 282, loss is 0.010803992860019207\n",
      "epoch: 22 step: 283, loss is 0.025018658488988876\n",
      "epoch: 22 step: 284, loss is 0.005638722330331802\n",
      "epoch: 22 step: 285, loss is 0.002916179597377777\n",
      "epoch: 22 step: 286, loss is 0.007475927006453276\n",
      "epoch: 22 step: 287, loss is 0.02347203530371189\n",
      "epoch: 22 step: 288, loss is 8.7678839918226e-05\n",
      "epoch: 22 step: 289, loss is 0.0009718576329760253\n",
      "epoch: 22 step: 290, loss is 0.0031782074365764856\n",
      "epoch: 22 step: 291, loss is 0.036614056676626205\n",
      "epoch: 22 step: 292, loss is 0.0020649777725338936\n",
      "epoch: 22 step: 293, loss is 0.00017080620455089957\n",
      "epoch: 22 step: 294, loss is 0.03525824472308159\n",
      "epoch: 22 step: 295, loss is 0.00026758306194096804\n",
      "epoch: 22 step: 296, loss is 0.0025346942711621523\n",
      "epoch: 22 step: 297, loss is 0.0039030713960528374\n",
      "epoch: 22 step: 298, loss is 0.05203882232308388\n",
      "epoch: 22 step: 299, loss is 0.0012900828151032329\n",
      "epoch: 22 step: 300, loss is 0.005553660448640585\n",
      "epoch: 22 step: 301, loss is 0.005403465125709772\n",
      "epoch: 22 step: 302, loss is 0.0011086920276284218\n",
      "epoch: 22 step: 303, loss is 0.003491046605631709\n",
      "epoch: 22 step: 304, loss is 0.0006115802680142224\n",
      "epoch: 22 step: 305, loss is 0.03451148420572281\n",
      "epoch: 22 step: 306, loss is 0.04508911073207855\n",
      "epoch: 22 step: 307, loss is 0.025332070887088776\n",
      "epoch: 22 step: 308, loss is 0.0008514260407537222\n",
      "epoch: 22 step: 309, loss is 0.0008561051217839122\n",
      "epoch: 22 step: 310, loss is 0.008706361055374146\n",
      "epoch: 22 step: 311, loss is 0.0008595940889790654\n",
      "epoch: 22 step: 312, loss is 0.05694344639778137\n",
      "epoch: 22 step: 313, loss is 0.001978919142857194\n",
      "epoch: 22 step: 314, loss is 0.0016491697169840336\n",
      "epoch: 22 step: 315, loss is 0.000547615869436413\n",
      "epoch: 22 step: 316, loss is 0.004203394521027803\n",
      "epoch: 22 step: 317, loss is 0.06173991784453392\n",
      "epoch: 22 step: 318, loss is 0.0022495982702821493\n",
      "epoch: 22 step: 319, loss is 0.05853539705276489\n",
      "epoch: 22 step: 320, loss is 0.0006884880713187158\n",
      "epoch: 22 step: 321, loss is 0.0019350677030161023\n",
      "epoch: 22 step: 322, loss is 0.008647027425467968\n",
      "epoch: 22 step: 323, loss is 0.00043664220720529556\n",
      "epoch: 22 step: 324, loss is 0.006142549216747284\n",
      "epoch: 22 step: 325, loss is 0.002848709002137184\n",
      "epoch: 22 step: 326, loss is 0.02227877825498581\n",
      "epoch: 22 step: 327, loss is 0.0016922835493460298\n",
      "epoch: 22 step: 328, loss is 0.0009787563467398286\n",
      "epoch: 22 step: 329, loss is 0.011354632675647736\n",
      "epoch: 22 step: 330, loss is 0.003926930017769337\n",
      "epoch: 22 step: 331, loss is 0.010043606162071228\n",
      "epoch: 22 step: 332, loss is 0.016922689974308014\n",
      "epoch: 22 step: 333, loss is 0.018699167296290398\n",
      "epoch: 22 step: 334, loss is 0.0049070012755692005\n",
      "epoch: 22 step: 335, loss is 0.0336575023829937\n",
      "epoch: 22 step: 336, loss is 0.0020105387084186077\n",
      "epoch: 22 step: 337, loss is 0.008411770686507225\n",
      "epoch: 22 step: 338, loss is 0.0014063281705603004\n",
      "epoch: 22 step: 339, loss is 0.0035797732416540384\n",
      "epoch: 22 step: 340, loss is 0.005150360055267811\n",
      "epoch: 22 step: 341, loss is 0.011405465193092823\n",
      "epoch: 22 step: 342, loss is 0.04456198960542679\n",
      "epoch: 22 step: 343, loss is 0.0007167300209403038\n",
      "epoch: 22 step: 344, loss is 0.000604243774432689\n",
      "epoch: 22 step: 345, loss is 0.0031029903329908848\n",
      "epoch: 22 step: 346, loss is 0.019902484491467476\n",
      "epoch: 22 step: 347, loss is 0.0023730131797492504\n",
      "epoch: 22 step: 348, loss is 0.005001408979296684\n",
      "epoch: 22 step: 349, loss is 0.00036663177888840437\n",
      "epoch: 22 step: 350, loss is 0.0004384977510198951\n",
      "epoch: 22 step: 351, loss is 0.00047271320363506675\n",
      "epoch: 22 step: 352, loss is 0.0020077358931303024\n",
      "epoch: 22 step: 353, loss is 0.003383972682058811\n",
      "epoch: 22 step: 354, loss is 0.0003259655204601586\n",
      "epoch: 22 step: 355, loss is 0.001427071401849389\n",
      "epoch: 22 step: 356, loss is 0.00010614588973112404\n",
      "epoch: 22 step: 357, loss is 0.010183151811361313\n",
      "epoch: 22 step: 358, loss is 0.009193776175379753\n",
      "epoch: 22 step: 359, loss is 0.013231268152594566\n",
      "epoch: 22 step: 360, loss is 0.003815488889813423\n",
      "epoch: 22 step: 361, loss is 0.002354200230911374\n",
      "epoch: 22 step: 362, loss is 0.0025829907972365618\n",
      "epoch: 22 step: 363, loss is 0.009815741330385208\n",
      "epoch: 22 step: 364, loss is 0.0008677206351421773\n",
      "epoch: 22 step: 365, loss is 0.05067863687872887\n",
      "epoch: 22 step: 366, loss is 0.00576211791485548\n",
      "epoch: 22 step: 367, loss is 0.005765390582382679\n",
      "epoch: 22 step: 368, loss is 0.031519703567028046\n",
      "epoch: 22 step: 369, loss is 0.006001383997499943\n",
      "epoch: 22 step: 370, loss is 0.01080065406858921\n",
      "epoch: 22 step: 371, loss is 0.0005495627992786467\n",
      "epoch: 22 step: 372, loss is 0.0010121988598257303\n",
      "epoch: 22 step: 373, loss is 3.270403976785019e-05\n",
      "epoch: 22 step: 374, loss is 0.00038560331449843943\n",
      "epoch: 22 step: 375, loss is 0.00012513592082541436\n",
      "epoch: 22 step: 376, loss is 0.0017606135224923491\n",
      "epoch: 22 step: 377, loss is 0.00011217252176720649\n",
      "epoch: 22 step: 378, loss is 0.005071133375167847\n",
      "epoch: 22 step: 379, loss is 0.010744336992502213\n",
      "epoch: 22 step: 380, loss is 0.0035584282595664263\n",
      "epoch: 22 step: 381, loss is 0.000421504199039191\n",
      "epoch: 22 step: 382, loss is 0.021950481459498405\n",
      "epoch: 22 step: 383, loss is 0.0019512688741087914\n",
      "epoch: 22 step: 384, loss is 0.004123907070606947\n",
      "epoch: 22 step: 385, loss is 0.0017362445360049605\n",
      "epoch: 22 step: 386, loss is 0.023333951830863953\n",
      "epoch: 22 step: 387, loss is 0.0009979342576116323\n",
      "epoch: 22 step: 388, loss is 0.0326833613216877\n",
      "epoch: 22 step: 389, loss is 0.006273009814321995\n",
      "epoch: 22 step: 390, loss is 0.0003803389554377645\n",
      "epoch: 22 step: 391, loss is 0.03927528113126755\n",
      "epoch: 22 step: 392, loss is 0.00013410237443167716\n",
      "epoch: 22 step: 393, loss is 0.0023775070440024137\n",
      "epoch: 22 step: 394, loss is 0.010604905895888805\n",
      "epoch: 22 step: 395, loss is 0.0005216230056248605\n",
      "epoch: 22 step: 396, loss is 0.00047392776468768716\n",
      "epoch: 22 step: 397, loss is 0.0032866783440113068\n",
      "epoch: 22 step: 398, loss is 0.00038319628220051527\n",
      "epoch: 22 step: 399, loss is 0.0021209309343248606\n",
      "epoch: 22 step: 400, loss is 0.007927218452095985\n",
      "epoch: 22 step: 401, loss is 0.0002870675816666335\n",
      "epoch: 22 step: 402, loss is 0.011615579016506672\n",
      "epoch: 22 step: 403, loss is 0.005553670693188906\n",
      "epoch: 22 step: 404, loss is 0.006171477492898703\n",
      "epoch: 22 step: 405, loss is 0.01957847736775875\n",
      "epoch: 22 step: 406, loss is 0.005794005934149027\n",
      "epoch: 22 step: 407, loss is 0.002990663517266512\n",
      "epoch: 22 step: 408, loss is 0.00524562131613493\n",
      "epoch: 22 step: 409, loss is 0.00971238687634468\n",
      "epoch: 22 step: 410, loss is 0.01490060891956091\n",
      "epoch: 22 step: 411, loss is 0.023174641653895378\n",
      "epoch: 22 step: 412, loss is 0.0009069633088074625\n",
      "epoch: 22 step: 413, loss is 0.002459213137626648\n",
      "epoch: 22 step: 414, loss is 0.0010616646613925695\n",
      "epoch: 22 step: 415, loss is 0.049655649811029434\n",
      "epoch: 22 step: 416, loss is 0.008079677820205688\n",
      "epoch: 22 step: 417, loss is 0.0002570705837570131\n",
      "epoch: 22 step: 418, loss is 0.00024114509869832546\n",
      "epoch: 22 step: 419, loss is 0.0005871012108400464\n",
      "epoch: 22 step: 420, loss is 0.0036242660135030746\n",
      "epoch: 22 step: 421, loss is 0.01540136057883501\n",
      "epoch: 22 step: 422, loss is 0.00031504101934842765\n",
      "epoch: 22 step: 423, loss is 0.0038609972689300776\n",
      "epoch: 22 step: 424, loss is 0.0066569289192557335\n",
      "epoch: 22 step: 425, loss is 0.0014130158815532923\n",
      "epoch: 22 step: 426, loss is 0.0019798213616013527\n",
      "epoch: 22 step: 427, loss is 0.0015611604321748018\n",
      "epoch: 22 step: 428, loss is 0.008985815569758415\n",
      "epoch: 22 step: 429, loss is 0.0017140660202130675\n",
      "epoch: 22 step: 430, loss is 0.001398588763549924\n",
      "epoch: 22 step: 431, loss is 0.0005387823912315071\n",
      "epoch: 22 step: 432, loss is 0.0018340623937547207\n",
      "epoch: 22 step: 433, loss is 0.024169618263840675\n",
      "epoch: 22 step: 434, loss is 0.0011592271039262414\n",
      "epoch: 22 step: 435, loss is 0.005454937927424908\n",
      "epoch: 22 step: 436, loss is 0.034759558737277985\n",
      "epoch: 22 step: 437, loss is 0.005291393958032131\n",
      "epoch: 22 step: 438, loss is 0.011996165849268436\n",
      "epoch: 22 step: 439, loss is 0.03167826682329178\n",
      "epoch: 22 step: 440, loss is 0.04188515990972519\n",
      "epoch: 22 step: 441, loss is 0.014528753235936165\n",
      "epoch: 22 step: 442, loss is 0.0007234090589918196\n",
      "epoch: 22 step: 443, loss is 0.04063878953456879\n",
      "epoch: 22 step: 444, loss is 0.005820718593895435\n",
      "epoch: 22 step: 445, loss is 0.002123401267454028\n",
      "epoch: 22 step: 446, loss is 0.001910664257593453\n",
      "epoch: 22 step: 447, loss is 0.0005588745116256177\n",
      "epoch: 22 step: 448, loss is 0.023489193990826607\n",
      "epoch: 22 step: 449, loss is 0.000666895299218595\n",
      "epoch: 22 step: 450, loss is 0.00041837451863102615\n",
      "epoch: 22 step: 451, loss is 0.0032071107998490334\n",
      "epoch: 22 step: 452, loss is 0.0008305013761855662\n",
      "epoch: 22 step: 453, loss is 0.08713416010141373\n",
      "epoch: 22 step: 454, loss is 0.009205046109855175\n",
      "epoch: 22 step: 455, loss is 0.0004357674333732575\n",
      "epoch: 22 step: 456, loss is 0.005960030015558004\n",
      "epoch: 22 step: 457, loss is 0.0010701973224058747\n",
      "epoch: 22 step: 458, loss is 0.0073309955187141895\n",
      "epoch: 22 step: 459, loss is 0.0008573659579269588\n",
      "epoch: 22 step: 460, loss is 0.08643197268247604\n",
      "epoch: 22 step: 461, loss is 0.0031567879486829042\n",
      "epoch: 22 step: 462, loss is 0.08723676204681396\n",
      "epoch: 22 step: 463, loss is 0.02063312567770481\n",
      "epoch: 22 step: 464, loss is 0.000659305602312088\n",
      "epoch: 22 step: 465, loss is 0.0031279847025871277\n",
      "epoch: 22 step: 466, loss is 0.0013314313255250454\n",
      "epoch: 22 step: 467, loss is 0.020193682983517647\n",
      "epoch: 22 step: 468, loss is 0.0019078755285590887\n",
      "epoch: 22 step: 469, loss is 0.015656642615795135\n",
      "epoch: 22 step: 470, loss is 0.003908757586032152\n",
      "epoch: 22 step: 471, loss is 0.001545475679449737\n",
      "epoch: 22 step: 472, loss is 0.0002859405358321965\n",
      "epoch: 22 step: 473, loss is 0.002730944659560919\n",
      "epoch: 22 step: 474, loss is 0.09198764711618423\n",
      "epoch: 22 step: 475, loss is 0.009874612092971802\n",
      "epoch: 22 step: 476, loss is 0.008990652859210968\n",
      "epoch: 22 step: 477, loss is 0.005797683726996183\n",
      "epoch: 22 step: 478, loss is 0.15636713802814484\n",
      "epoch: 22 step: 479, loss is 0.012382402084767818\n",
      "epoch: 22 step: 480, loss is 0.012271416373550892\n",
      "epoch: 22 step: 481, loss is 0.012049796059727669\n",
      "epoch: 22 step: 482, loss is 0.006270124111324549\n",
      "epoch: 22 step: 483, loss is 0.0005088459001854062\n",
      "epoch: 22 step: 484, loss is 0.013542287983000278\n",
      "epoch: 22 step: 485, loss is 0.0012028602650389075\n",
      "epoch: 22 step: 486, loss is 0.006351383402943611\n",
      "epoch: 22 step: 487, loss is 0.03517058491706848\n",
      "epoch: 22 step: 488, loss is 0.014728395268321037\n",
      "epoch: 22 step: 489, loss is 0.06922689825296402\n",
      "epoch: 22 step: 490, loss is 0.050649493932724\n",
      "epoch: 22 step: 491, loss is 0.0021593335550278425\n",
      "epoch: 22 step: 492, loss is 0.00032689704676158726\n",
      "epoch: 22 step: 493, loss is 0.0007644771249033511\n",
      "epoch: 22 step: 494, loss is 0.000630438094958663\n",
      "epoch: 22 step: 495, loss is 0.002571431454271078\n",
      "epoch: 22 step: 496, loss is 0.0009552129777148366\n",
      "epoch: 22 step: 497, loss is 0.0015837352257221937\n",
      "epoch: 22 step: 498, loss is 0.032006196677684784\n",
      "epoch: 22 step: 499, loss is 0.013349426910281181\n",
      "epoch: 22 step: 500, loss is 0.026860082522034645\n",
      "epoch: 22 step: 501, loss is 0.059216320514678955\n",
      "epoch: 22 step: 502, loss is 0.0011349559063091874\n",
      "epoch: 22 step: 503, loss is 0.0009499918087385595\n",
      "epoch: 22 step: 504, loss is 0.05635920539498329\n",
      "epoch: 22 step: 505, loss is 0.0006242426461540163\n",
      "epoch: 22 step: 506, loss is 0.052023932337760925\n",
      "epoch: 22 step: 507, loss is 0.00023304916976485401\n",
      "epoch: 22 step: 508, loss is 0.04573250189423561\n",
      "epoch: 22 step: 509, loss is 0.009250154718756676\n",
      "epoch: 22 step: 510, loss is 0.00833170022815466\n",
      "epoch: 22 step: 511, loss is 0.0038836977910250425\n",
      "epoch: 22 step: 512, loss is 0.013493012636899948\n",
      "epoch: 22 step: 513, loss is 0.033336322754621506\n",
      "epoch: 22 step: 514, loss is 0.0024136118590831757\n",
      "epoch: 22 step: 515, loss is 0.0010591191239655018\n",
      "epoch: 22 step: 516, loss is 0.0001353880943497643\n",
      "epoch: 22 step: 517, loss is 0.003277033334597945\n",
      "epoch: 22 step: 518, loss is 0.0026379073970019817\n",
      "epoch: 22 step: 519, loss is 0.0011738413013517857\n",
      "epoch: 22 step: 520, loss is 0.005674893967807293\n",
      "epoch: 22 step: 521, loss is 0.0004666252061724663\n",
      "epoch: 22 step: 522, loss is 0.0001498969941167161\n",
      "epoch: 22 step: 523, loss is 0.0023917669896036386\n",
      "epoch: 22 step: 524, loss is 0.00032728261430747807\n",
      "epoch: 22 step: 525, loss is 0.012241784483194351\n",
      "epoch: 22 step: 526, loss is 0.008164935745298862\n",
      "epoch: 22 step: 527, loss is 0.07359485328197479\n",
      "epoch: 22 step: 528, loss is 0.006094912998378277\n",
      "epoch: 22 step: 529, loss is 0.0004336967831477523\n",
      "epoch: 22 step: 530, loss is 0.01916801929473877\n",
      "epoch: 22 step: 531, loss is 0.0012738718651235104\n",
      "epoch: 22 step: 532, loss is 0.0001198568643303588\n",
      "epoch: 22 step: 533, loss is 0.003912676591426134\n",
      "epoch: 22 step: 534, loss is 0.0007387785008177161\n",
      "epoch: 22 step: 535, loss is 0.0002014729252550751\n",
      "epoch: 22 step: 536, loss is 0.0023518980015069246\n",
      "epoch: 22 step: 537, loss is 0.004609762690961361\n",
      "epoch: 22 step: 538, loss is 0.027725908905267715\n",
      "epoch: 22 step: 539, loss is 0.0066517675295472145\n",
      "epoch: 22 step: 540, loss is 0.14900904893875122\n",
      "epoch: 22 step: 541, loss is 0.01145021803677082\n",
      "epoch: 22 step: 542, loss is 0.014130337163805962\n",
      "epoch: 22 step: 543, loss is 0.04861658811569214\n",
      "epoch: 22 step: 544, loss is 0.0004295977996662259\n",
      "epoch: 22 step: 545, loss is 0.0001242326688952744\n",
      "epoch: 22 step: 546, loss is 0.02202838659286499\n",
      "epoch: 22 step: 547, loss is 0.005580989643931389\n",
      "epoch: 22 step: 548, loss is 0.009888007305562496\n",
      "epoch: 22 step: 549, loss is 0.0017786856042221189\n",
      "epoch: 22 step: 550, loss is 0.07720304280519485\n",
      "epoch: 22 step: 551, loss is 0.06440743058919907\n",
      "epoch: 22 step: 552, loss is 0.002507705008611083\n",
      "epoch: 22 step: 553, loss is 0.11093366891145706\n",
      "epoch: 22 step: 554, loss is 0.018906516954302788\n",
      "epoch: 22 step: 555, loss is 0.011251217685639858\n",
      "epoch: 22 step: 556, loss is 0.2229851484298706\n",
      "epoch: 22 step: 557, loss is 0.0031091675627976656\n",
      "epoch: 22 step: 558, loss is 0.000848598952870816\n",
      "epoch: 22 step: 559, loss is 0.013443570584058762\n",
      "epoch: 22 step: 560, loss is 0.09445676952600479\n",
      "epoch: 22 step: 561, loss is 0.007421535439789295\n",
      "epoch: 22 step: 562, loss is 0.13183864951133728\n",
      "epoch: 22 step: 563, loss is 0.0009927862556651235\n",
      "epoch: 22 step: 564, loss is 0.0011587621411308646\n",
      "epoch: 22 step: 565, loss is 0.0057252454571425915\n",
      "epoch: 22 step: 566, loss is 0.00011487554002087563\n",
      "epoch: 22 step: 567, loss is 0.02722383849322796\n",
      "epoch: 22 step: 568, loss is 0.0031423824839293957\n",
      "epoch: 22 step: 569, loss is 0.0962924063205719\n",
      "epoch: 22 step: 570, loss is 0.00278653297573328\n",
      "epoch: 22 step: 571, loss is 0.012911277823150158\n",
      "epoch: 22 step: 572, loss is 0.012039244174957275\n",
      "epoch: 22 step: 573, loss is 0.004770709201693535\n",
      "epoch: 22 step: 574, loss is 0.002637567464262247\n",
      "epoch: 22 step: 575, loss is 0.00910039059817791\n",
      "epoch: 22 step: 576, loss is 0.009951204061508179\n",
      "epoch: 22 step: 577, loss is 0.00351382396183908\n",
      "epoch: 22 step: 578, loss is 0.020079387351870537\n",
      "epoch: 22 step: 579, loss is 0.004128090105950832\n",
      "epoch: 22 step: 580, loss is 0.022873524576425552\n",
      "epoch: 22 step: 581, loss is 0.004871026147156954\n",
      "epoch: 22 step: 582, loss is 0.0013732617953792214\n",
      "epoch: 22 step: 583, loss is 0.002249752404168248\n",
      "epoch: 22 step: 584, loss is 0.011332057416439056\n",
      "epoch: 22 step: 585, loss is 0.02934505045413971\n",
      "epoch: 22 step: 586, loss is 0.0013530125143006444\n",
      "epoch: 22 step: 587, loss is 0.09361769258975983\n",
      "epoch: 22 step: 588, loss is 0.04644910991191864\n",
      "epoch: 22 step: 589, loss is 0.07763184607028961\n",
      "epoch: 22 step: 590, loss is 0.03525814414024353\n",
      "epoch: 22 step: 591, loss is 0.002672647824510932\n",
      "epoch: 22 step: 592, loss is 0.05425386503338814\n",
      "epoch: 22 step: 593, loss is 0.009740573354065418\n",
      "epoch: 22 step: 594, loss is 0.0004693649825640023\n",
      "epoch: 22 step: 595, loss is 0.0006140076438896358\n",
      "epoch: 22 step: 596, loss is 0.002144953003153205\n",
      "epoch: 22 step: 597, loss is 0.020995311439037323\n",
      "epoch: 22 step: 598, loss is 0.0039011500775814056\n",
      "epoch: 22 step: 599, loss is 0.0012066933559253812\n",
      "epoch: 22 step: 600, loss is 0.04761688783764839\n",
      "epoch: 22 step: 601, loss is 0.00394185446202755\n",
      "epoch: 22 step: 602, loss is 0.04615400359034538\n",
      "epoch: 22 step: 603, loss is 6.092760668252595e-05\n",
      "epoch: 22 step: 604, loss is 0.009421690367162228\n",
      "epoch: 22 step: 605, loss is 0.010852930136024952\n",
      "epoch: 22 step: 606, loss is 0.003281682962551713\n",
      "epoch: 22 step: 607, loss is 0.04967339709401131\n",
      "epoch: 22 step: 608, loss is 0.013660662807524204\n",
      "epoch: 22 step: 609, loss is 0.004820801317691803\n",
      "epoch: 22 step: 610, loss is 0.0024679522030055523\n",
      "epoch: 22 step: 611, loss is 0.0006150640547275543\n",
      "epoch: 22 step: 612, loss is 0.00039441906847059727\n",
      "epoch: 22 step: 613, loss is 0.00013153850159142166\n",
      "epoch: 22 step: 614, loss is 0.03710144758224487\n",
      "epoch: 22 step: 615, loss is 0.010901560075581074\n",
      "epoch: 22 step: 616, loss is 0.04339291527867317\n",
      "epoch: 22 step: 617, loss is 0.0003170165582560003\n",
      "epoch: 22 step: 618, loss is 0.0036075173411518335\n",
      "epoch: 22 step: 619, loss is 0.001642757561057806\n",
      "epoch: 22 step: 620, loss is 0.013919842429459095\n",
      "epoch: 22 step: 621, loss is 0.0026627974584698677\n",
      "epoch: 22 step: 622, loss is 0.01341796014457941\n",
      "epoch: 22 step: 623, loss is 0.01446142140775919\n",
      "epoch: 22 step: 624, loss is 0.011534261517226696\n",
      "epoch: 22 step: 625, loss is 0.002214358886703849\n",
      "epoch: 22 step: 626, loss is 0.0001207086315844208\n",
      "epoch: 22 step: 627, loss is 0.0862574577331543\n",
      "epoch: 22 step: 628, loss is 0.012238629162311554\n",
      "epoch: 22 step: 629, loss is 0.0018768716836348176\n",
      "epoch: 22 step: 630, loss is 0.011420744471251965\n",
      "epoch: 22 step: 631, loss is 0.011280983686447144\n",
      "epoch: 22 step: 632, loss is 0.03776968643069267\n",
      "epoch: 22 step: 633, loss is 0.00298066227696836\n",
      "epoch: 22 step: 634, loss is 0.011333711445331573\n",
      "epoch: 22 step: 635, loss is 0.006167465355247259\n",
      "epoch: 22 step: 636, loss is 0.0019839394371956587\n",
      "epoch: 22 step: 637, loss is 0.022882046177983284\n",
      "epoch: 22 step: 638, loss is 0.0005161894951015711\n",
      "epoch: 22 step: 639, loss is 0.0020184172317385674\n",
      "epoch: 22 step: 640, loss is 0.002899907063692808\n",
      "epoch: 22 step: 641, loss is 0.04455338418483734\n",
      "epoch: 22 step: 642, loss is 0.013588601723313332\n",
      "epoch: 22 step: 643, loss is 0.0006313111516647041\n",
      "epoch: 22 step: 644, loss is 0.005353451240807772\n",
      "epoch: 22 step: 645, loss is 0.0002897618105635047\n",
      "epoch: 22 step: 646, loss is 0.0022459665779024363\n",
      "epoch: 22 step: 647, loss is 0.0017211692174896598\n",
      "epoch: 22 step: 648, loss is 0.0015306990826502442\n",
      "epoch: 22 step: 649, loss is 0.006814857944846153\n",
      "epoch: 22 step: 650, loss is 0.00029104368877597153\n",
      "epoch: 22 step: 651, loss is 0.11002463102340698\n",
      "epoch: 22 step: 652, loss is 0.00035728205693885684\n",
      "epoch: 22 step: 653, loss is 0.04947579652070999\n",
      "epoch: 22 step: 654, loss is 0.013597209006547928\n",
      "epoch: 22 step: 655, loss is 0.004960624035447836\n",
      "epoch: 22 step: 656, loss is 0.004133997019380331\n",
      "epoch: 22 step: 657, loss is 0.07528550177812576\n",
      "epoch: 22 step: 658, loss is 0.002263979986310005\n",
      "epoch: 22 step: 659, loss is 0.011145466938614845\n",
      "epoch: 22 step: 660, loss is 0.007072955369949341\n",
      "epoch: 22 step: 661, loss is 0.003953434526920319\n",
      "epoch: 22 step: 662, loss is 0.015761516988277435\n",
      "epoch: 22 step: 663, loss is 0.0019124654354527593\n",
      "epoch: 22 step: 664, loss is 0.00355276046320796\n",
      "epoch: 22 step: 665, loss is 0.03559402748942375\n",
      "epoch: 22 step: 666, loss is 0.007229839451611042\n",
      "epoch: 22 step: 667, loss is 0.00045776867773383856\n",
      "epoch: 22 step: 668, loss is 0.0003301573160570115\n",
      "epoch: 22 step: 669, loss is 0.010286364704370499\n",
      "epoch: 22 step: 670, loss is 0.02102779410779476\n",
      "epoch: 22 step: 671, loss is 0.0011302531929686666\n",
      "epoch: 22 step: 672, loss is 0.0003018855641130358\n",
      "epoch: 22 step: 673, loss is 5.898459130548872e-05\n",
      "epoch: 22 step: 674, loss is 0.0417763851583004\n",
      "epoch: 22 step: 675, loss is 0.023793423548340797\n",
      "epoch: 22 step: 676, loss is 0.002576842438429594\n",
      "epoch: 22 step: 677, loss is 0.00038802879862487316\n",
      "epoch: 22 step: 678, loss is 0.0034654047340154648\n",
      "epoch: 22 step: 679, loss is 0.030452720820903778\n",
      "epoch: 22 step: 680, loss is 0.00013611483154818416\n",
      "epoch: 22 step: 681, loss is 0.002900172723457217\n",
      "epoch: 22 step: 682, loss is 0.010654091835021973\n",
      "epoch: 22 step: 683, loss is 0.0016816620482131839\n",
      "epoch: 22 step: 684, loss is 0.001991822849959135\n",
      "epoch: 22 step: 685, loss is 0.013599036261439323\n",
      "epoch: 22 step: 686, loss is 0.035524651408195496\n",
      "epoch: 22 step: 687, loss is 0.0530250146985054\n",
      "epoch: 22 step: 688, loss is 0.0031238351948559284\n",
      "epoch: 22 step: 689, loss is 0.01669309474527836\n",
      "epoch: 22 step: 690, loss is 0.0011209838557988405\n",
      "epoch: 22 step: 691, loss is 0.017609788104891777\n",
      "epoch: 22 step: 692, loss is 0.007422150112688541\n",
      "epoch: 22 step: 693, loss is 0.0010086604161188006\n",
      "epoch: 22 step: 694, loss is 0.0008938520331867039\n",
      "epoch: 22 step: 695, loss is 0.0035470807924866676\n",
      "epoch: 22 step: 696, loss is 0.05969243124127388\n",
      "epoch: 22 step: 697, loss is 0.0012193224392831326\n",
      "epoch: 22 step: 698, loss is 0.03836668282747269\n",
      "epoch: 22 step: 699, loss is 0.030669834464788437\n",
      "epoch: 22 step: 700, loss is 0.0004667684552259743\n",
      "epoch: 22 step: 701, loss is 0.009420800022780895\n",
      "epoch: 22 step: 702, loss is 0.005636890884488821\n",
      "epoch: 22 step: 703, loss is 0.033190157264471054\n",
      "epoch: 22 step: 704, loss is 0.10866647213697433\n",
      "epoch: 22 step: 705, loss is 0.001789171015843749\n",
      "epoch: 22 step: 706, loss is 0.005202420987188816\n",
      "epoch: 22 step: 707, loss is 0.002433306537568569\n",
      "epoch: 22 step: 708, loss is 0.0065194531343877316\n",
      "epoch: 22 step: 709, loss is 0.005251488648355007\n",
      "epoch: 22 step: 710, loss is 0.0075178141705691814\n",
      "epoch: 22 step: 711, loss is 0.01556001603603363\n",
      "epoch: 22 step: 712, loss is 0.020279107615351677\n",
      "epoch: 22 step: 713, loss is 0.000849136442411691\n",
      "epoch: 22 step: 714, loss is 0.040813982486724854\n",
      "epoch: 22 step: 715, loss is 0.022767119109630585\n",
      "epoch: 22 step: 716, loss is 0.002762395888566971\n",
      "epoch: 22 step: 717, loss is 0.006354626268148422\n",
      "epoch: 22 step: 718, loss is 0.023713620379567146\n",
      "epoch: 22 step: 719, loss is 0.07252371311187744\n",
      "epoch: 22 step: 720, loss is 0.0007898664916865528\n",
      "epoch: 22 step: 721, loss is 0.039416391402482986\n",
      "epoch: 22 step: 722, loss is 0.0011406214907765388\n",
      "epoch: 22 step: 723, loss is 0.07747210562229156\n",
      "epoch: 22 step: 724, loss is 0.013495673425495625\n",
      "epoch: 22 step: 725, loss is 0.020460626110434532\n",
      "epoch: 22 step: 726, loss is 0.0006625324604101479\n",
      "epoch: 22 step: 727, loss is 0.010659554973244667\n",
      "epoch: 22 step: 728, loss is 0.02972782589495182\n",
      "epoch: 22 step: 729, loss is 0.0036851451732218266\n",
      "epoch: 22 step: 730, loss is 0.0014060423709452152\n",
      "epoch: 22 step: 731, loss is 0.0009649816201999784\n",
      "epoch: 22 step: 732, loss is 0.0022718908730894327\n",
      "epoch: 22 step: 733, loss is 0.017410654574632645\n",
      "epoch: 22 step: 734, loss is 0.09914060682058334\n",
      "epoch: 22 step: 735, loss is 0.08414722979068756\n",
      "epoch: 22 step: 736, loss is 0.09389419853687286\n",
      "epoch: 22 step: 737, loss is 0.003263338701799512\n",
      "epoch: 22 step: 738, loss is 0.009035030379891396\n",
      "epoch: 22 step: 739, loss is 0.0007917435141280293\n",
      "epoch: 22 step: 740, loss is 0.0015051196096464992\n",
      "epoch: 22 step: 741, loss is 0.014713447540998459\n",
      "epoch: 22 step: 742, loss is 0.07537820935249329\n",
      "epoch: 22 step: 743, loss is 0.006814182735979557\n",
      "epoch: 22 step: 744, loss is 0.055585335940122604\n",
      "epoch: 22 step: 745, loss is 0.003640511306002736\n",
      "epoch: 22 step: 746, loss is 0.0015453315572813153\n",
      "epoch: 22 step: 747, loss is 0.006165903061628342\n",
      "epoch: 22 step: 748, loss is 0.0045463270507752895\n",
      "epoch: 22 step: 749, loss is 0.030107174068689346\n",
      "epoch: 22 step: 750, loss is 0.011248902417719364\n",
      "epoch: 22 step: 751, loss is 0.027434438467025757\n",
      "epoch: 22 step: 752, loss is 0.0010891390265896916\n",
      "epoch: 22 step: 753, loss is 0.004265682306140661\n",
      "epoch: 22 step: 754, loss is 0.005833991337567568\n",
      "epoch: 22 step: 755, loss is 0.001543738879263401\n",
      "epoch: 22 step: 756, loss is 0.0055323257111012936\n",
      "epoch: 22 step: 757, loss is 0.002850760705769062\n",
      "epoch: 22 step: 758, loss is 0.005528206471353769\n",
      "epoch: 22 step: 759, loss is 0.0006542956689372659\n",
      "epoch: 22 step: 760, loss is 0.031156163662672043\n",
      "epoch: 22 step: 761, loss is 0.043766144663095474\n",
      "epoch: 22 step: 762, loss is 0.00047014773008413613\n",
      "epoch: 22 step: 763, loss is 0.0008647199720144272\n",
      "epoch: 22 step: 764, loss is 0.0519891157746315\n",
      "epoch: 22 step: 765, loss is 0.0009021701407618821\n",
      "epoch: 22 step: 766, loss is 0.016104720532894135\n",
      "epoch: 22 step: 767, loss is 0.004353743512183428\n",
      "epoch: 22 step: 768, loss is 0.0002063376159640029\n",
      "epoch: 22 step: 769, loss is 0.010250935330986977\n",
      "epoch: 22 step: 770, loss is 0.01445097103714943\n",
      "epoch: 22 step: 771, loss is 0.002862080466002226\n",
      "epoch: 22 step: 772, loss is 0.10380889475345612\n",
      "epoch: 22 step: 773, loss is 0.007008939515799284\n",
      "epoch: 22 step: 774, loss is 0.07936705648899078\n",
      "epoch: 22 step: 775, loss is 0.013589724898338318\n",
      "epoch: 22 step: 776, loss is 0.015569867566227913\n",
      "epoch: 22 step: 777, loss is 0.007284141145646572\n",
      "epoch: 22 step: 778, loss is 0.008457621559500694\n",
      "epoch: 22 step: 779, loss is 0.03810759261250496\n",
      "epoch: 22 step: 780, loss is 0.006202148273587227\n",
      "epoch: 22 step: 781, loss is 9.544634667690843e-05\n",
      "epoch: 22 step: 782, loss is 0.0015339076053351164\n",
      "epoch: 22 step: 783, loss is 0.010367996990680695\n",
      "epoch: 22 step: 784, loss is 0.019539950415492058\n",
      "epoch: 22 step: 785, loss is 0.0038927088025957346\n",
      "epoch: 22 step: 786, loss is 0.0039428835734725\n",
      "epoch: 22 step: 787, loss is 0.001049495767802\n",
      "epoch: 22 step: 788, loss is 0.0005243893247097731\n",
      "epoch: 22 step: 789, loss is 0.0004207149613648653\n",
      "epoch: 22 step: 790, loss is 0.01628333330154419\n",
      "epoch: 22 step: 791, loss is 0.014050034806132317\n",
      "epoch: 22 step: 792, loss is 0.06701303273439407\n",
      "epoch: 22 step: 793, loss is 0.0006662991363555193\n",
      "epoch: 22 step: 794, loss is 0.003042215248569846\n",
      "epoch: 22 step: 795, loss is 0.06243475154042244\n",
      "epoch: 22 step: 796, loss is 0.008217494934797287\n",
      "epoch: 22 step: 797, loss is 0.19442397356033325\n",
      "epoch: 22 step: 798, loss is 0.010300571098923683\n",
      "epoch: 22 step: 799, loss is 0.001375062856823206\n",
      "epoch: 22 step: 800, loss is 0.0005012792535126209\n",
      "epoch: 22 step: 801, loss is 0.010084914043545723\n",
      "epoch: 22 step: 802, loss is 0.004844290670007467\n",
      "epoch: 22 step: 803, loss is 0.007133243605494499\n",
      "epoch: 22 step: 804, loss is 0.009220647625625134\n",
      "epoch: 22 step: 805, loss is 0.03444896638393402\n",
      "epoch: 22 step: 806, loss is 0.0011229402152821422\n",
      "epoch: 22 step: 807, loss is 0.001738527207635343\n",
      "epoch: 22 step: 808, loss is 0.22642463445663452\n",
      "epoch: 22 step: 809, loss is 0.042565423995256424\n",
      "epoch: 22 step: 810, loss is 0.023376265540719032\n",
      "epoch: 22 step: 811, loss is 0.07663096487522125\n",
      "epoch: 22 step: 812, loss is 0.05159536004066467\n",
      "epoch: 22 step: 813, loss is 0.0463021919131279\n",
      "epoch: 22 step: 814, loss is 0.0007042031502351165\n",
      "epoch: 22 step: 815, loss is 0.006902586668729782\n",
      "epoch: 22 step: 816, loss is 0.0006388794863596559\n",
      "epoch: 22 step: 817, loss is 0.0007993101608008146\n",
      "epoch: 22 step: 818, loss is 0.005872158333659172\n",
      "epoch: 22 step: 819, loss is 0.0169113427400589\n",
      "epoch: 22 step: 820, loss is 0.0027955418918281794\n",
      "epoch: 22 step: 821, loss is 0.11750239878892899\n",
      "epoch: 22 step: 822, loss is 0.004902765620499849\n",
      "epoch: 22 step: 823, loss is 0.001028439262881875\n",
      "epoch: 22 step: 824, loss is 0.012043198570609093\n",
      "epoch: 22 step: 825, loss is 0.06286804378032684\n",
      "epoch: 22 step: 826, loss is 0.006008961703628302\n",
      "epoch: 22 step: 827, loss is 0.023264069110155106\n",
      "epoch: 22 step: 828, loss is 0.03109324350953102\n",
      "epoch: 22 step: 829, loss is 0.006520378403365612\n",
      "epoch: 22 step: 830, loss is 0.0007766754133626819\n",
      "epoch: 22 step: 831, loss is 0.03723917528986931\n",
      "epoch: 22 step: 832, loss is 0.0036048362962901592\n",
      "epoch: 22 step: 833, loss is 0.0475785955786705\n",
      "epoch: 22 step: 834, loss is 0.017287425696849823\n",
      "epoch: 22 step: 835, loss is 0.0023267464712262154\n",
      "epoch: 22 step: 836, loss is 0.0055365110747516155\n",
      "epoch: 22 step: 837, loss is 0.028350990265607834\n",
      "epoch: 22 step: 838, loss is 0.036465901881456375\n",
      "epoch: 22 step: 839, loss is 0.024111120030283928\n",
      "epoch: 22 step: 840, loss is 0.08947191387414932\n",
      "epoch: 22 step: 841, loss is 0.023464243859052658\n",
      "epoch: 22 step: 842, loss is 0.007901148870587349\n",
      "epoch: 22 step: 843, loss is 0.07667578756809235\n",
      "epoch: 22 step: 844, loss is 0.022168932482600212\n",
      "epoch: 22 step: 845, loss is 0.034236885607242584\n",
      "epoch: 22 step: 846, loss is 0.03217912092804909\n",
      "epoch: 22 step: 847, loss is 0.0635545551776886\n",
      "epoch: 22 step: 848, loss is 0.014795269817113876\n",
      "epoch: 22 step: 849, loss is 0.012956942431628704\n",
      "epoch: 22 step: 850, loss is 0.0013351676752790809\n",
      "epoch: 22 step: 851, loss is 0.002690436551347375\n",
      "epoch: 22 step: 852, loss is 0.08488376438617706\n",
      "epoch: 22 step: 853, loss is 0.005080870818346739\n",
      "epoch: 22 step: 854, loss is 0.015535477548837662\n",
      "epoch: 22 step: 855, loss is 0.025382937863469124\n",
      "epoch: 22 step: 856, loss is 0.0020547497551888227\n",
      "epoch: 22 step: 857, loss is 0.09245982766151428\n",
      "epoch: 22 step: 858, loss is 0.05062625929713249\n",
      "epoch: 22 step: 859, loss is 0.14180316030979156\n",
      "epoch: 22 step: 860, loss is 0.008926666341722012\n",
      "epoch: 22 step: 861, loss is 0.09425868839025497\n",
      "epoch: 22 step: 862, loss is 0.007114163599908352\n",
      "epoch: 22 step: 863, loss is 0.012683317065238953\n",
      "epoch: 22 step: 864, loss is 0.056647270917892456\n",
      "epoch: 22 step: 865, loss is 0.0063474769704043865\n",
      "epoch: 22 step: 866, loss is 0.0005651578539982438\n",
      "epoch: 22 step: 867, loss is 0.004166931379586458\n",
      "epoch: 22 step: 868, loss is 0.11720702052116394\n",
      "epoch: 22 step: 869, loss is 0.00038828104152344167\n",
      "epoch: 22 step: 870, loss is 0.0031990630086511374\n",
      "epoch: 22 step: 871, loss is 0.003936439752578735\n",
      "epoch: 22 step: 872, loss is 0.002450964879244566\n",
      "epoch: 22 step: 873, loss is 0.05157439410686493\n",
      "epoch: 22 step: 874, loss is 5.898714152863249e-05\n",
      "epoch: 22 step: 875, loss is 0.04197484254837036\n",
      "epoch: 22 step: 876, loss is 0.07094192504882812\n",
      "epoch: 22 step: 877, loss is 0.0010473944712430239\n",
      "epoch: 22 step: 878, loss is 0.02172190509736538\n",
      "epoch: 22 step: 879, loss is 0.03768133372068405\n",
      "epoch: 22 step: 880, loss is 0.0025866820942610502\n",
      "epoch: 22 step: 881, loss is 0.014294249936938286\n",
      "epoch: 22 step: 882, loss is 0.003953886684030294\n",
      "epoch: 22 step: 883, loss is 0.07525119185447693\n",
      "epoch: 22 step: 884, loss is 0.007347785867750645\n",
      "epoch: 22 step: 885, loss is 0.016962770372629166\n",
      "epoch: 22 step: 886, loss is 0.005065622739493847\n",
      "epoch: 22 step: 887, loss is 0.012023929506540298\n",
      "epoch: 22 step: 888, loss is 7.958586502354592e-05\n",
      "epoch: 22 step: 889, loss is 0.0678417831659317\n",
      "epoch: 22 step: 890, loss is 0.07412540167570114\n",
      "epoch: 22 step: 891, loss is 0.11722594499588013\n",
      "epoch: 22 step: 892, loss is 0.07970607280731201\n",
      "epoch: 22 step: 893, loss is 0.03645409643650055\n",
      "epoch: 22 step: 894, loss is 0.0015454150270670652\n",
      "epoch: 22 step: 895, loss is 6.940364255569875e-05\n",
      "epoch: 22 step: 896, loss is 0.04760565236210823\n",
      "epoch: 22 step: 897, loss is 0.0007057087495923042\n",
      "epoch: 22 step: 898, loss is 0.005797629244625568\n",
      "epoch: 22 step: 899, loss is 0.0008905911236070096\n",
      "epoch: 22 step: 900, loss is 0.04128972068428993\n",
      "epoch: 22 step: 901, loss is 0.021548641845583916\n",
      "epoch: 22 step: 902, loss is 0.0032410561107099056\n",
      "epoch: 22 step: 903, loss is 0.0040234909392893314\n",
      "epoch: 22 step: 904, loss is 0.11643770337104797\n",
      "epoch: 22 step: 905, loss is 0.0006551887490786612\n",
      "epoch: 22 step: 906, loss is 0.010737715288996696\n",
      "epoch: 22 step: 907, loss is 0.0076169115491211414\n",
      "epoch: 22 step: 908, loss is 0.07786080241203308\n",
      "epoch: 22 step: 909, loss is 0.0016149203293025494\n",
      "epoch: 22 step: 910, loss is 0.00012244471872691065\n",
      "epoch: 22 step: 911, loss is 0.013762091286480427\n",
      "epoch: 22 step: 912, loss is 0.0010532966116443276\n",
      "epoch: 22 step: 913, loss is 0.024501968175172806\n",
      "epoch: 22 step: 914, loss is 0.017851561307907104\n",
      "epoch: 22 step: 915, loss is 0.0407460555434227\n",
      "epoch: 22 step: 916, loss is 0.0052835335955023766\n",
      "epoch: 22 step: 917, loss is 0.0005433119367808104\n",
      "epoch: 22 step: 918, loss is 0.010693026706576347\n",
      "epoch: 22 step: 919, loss is 0.03410722315311432\n",
      "epoch: 22 step: 920, loss is 0.014570682309567928\n",
      "epoch: 22 step: 921, loss is 0.004560121800750494\n",
      "epoch: 22 step: 922, loss is 0.028721118345856667\n",
      "epoch: 22 step: 923, loss is 0.012495772913098335\n",
      "epoch: 22 step: 924, loss is 0.003977252636104822\n",
      "epoch: 22 step: 925, loss is 0.001466494519263506\n",
      "epoch: 22 step: 926, loss is 0.0063338144682347775\n",
      "epoch: 22 step: 927, loss is 0.02944803610444069\n",
      "epoch: 22 step: 928, loss is 0.007115590386092663\n",
      "epoch: 22 step: 929, loss is 0.12800070643424988\n",
      "epoch: 22 step: 930, loss is 0.0003889550280291587\n",
      "epoch: 22 step: 931, loss is 0.02880714274942875\n",
      "epoch: 22 step: 932, loss is 0.0003146688686683774\n",
      "epoch: 22 step: 933, loss is 0.0009588228422217071\n",
      "epoch: 22 step: 934, loss is 0.03462958335876465\n",
      "epoch: 22 step: 935, loss is 0.0020577143877744675\n",
      "epoch: 22 step: 936, loss is 0.003189672948792577\n",
      "epoch: 22 step: 937, loss is 0.03372328728437424\n",
      "epoch: 23 step: 1, loss is 0.01560404896736145\n",
      "epoch: 23 step: 2, loss is 0.0017066884320229292\n",
      "epoch: 23 step: 3, loss is 0.0028439126908779144\n",
      "epoch: 23 step: 4, loss is 7.850686233723536e-05\n",
      "epoch: 23 step: 5, loss is 0.050520725548267365\n",
      "epoch: 23 step: 6, loss is 0.02231575921177864\n",
      "epoch: 23 step: 7, loss is 0.005696080159395933\n",
      "epoch: 23 step: 8, loss is 0.0005982734146527946\n",
      "epoch: 23 step: 9, loss is 0.07088838517665863\n",
      "epoch: 23 step: 10, loss is 0.008162479847669601\n",
      "epoch: 23 step: 11, loss is 0.0011293444549664855\n",
      "epoch: 23 step: 12, loss is 0.03357808664441109\n",
      "epoch: 23 step: 13, loss is 0.027532488107681274\n",
      "epoch: 23 step: 14, loss is 0.03098604641854763\n",
      "epoch: 23 step: 15, loss is 0.03881682828068733\n",
      "epoch: 23 step: 16, loss is 0.005652802996337414\n",
      "epoch: 23 step: 17, loss is 0.004674904048442841\n",
      "epoch: 23 step: 18, loss is 0.011223014444112778\n",
      "epoch: 23 step: 19, loss is 0.0009579163161106408\n",
      "epoch: 23 step: 20, loss is 0.010021616704761982\n",
      "epoch: 23 step: 21, loss is 0.00025517234462313354\n",
      "epoch: 23 step: 22, loss is 0.03651643544435501\n",
      "epoch: 23 step: 23, loss is 0.050550226122140884\n",
      "epoch: 23 step: 24, loss is 0.0008281741756945848\n",
      "epoch: 23 step: 25, loss is 0.0339781828224659\n",
      "epoch: 23 step: 26, loss is 0.018438197672367096\n",
      "epoch: 23 step: 27, loss is 0.005252317059785128\n",
      "epoch: 23 step: 28, loss is 0.019641732797026634\n",
      "epoch: 23 step: 29, loss is 0.0016354830004274845\n",
      "epoch: 23 step: 30, loss is 0.00013154161570128053\n",
      "epoch: 23 step: 31, loss is 0.00825955718755722\n",
      "epoch: 23 step: 32, loss is 0.010388745926320553\n",
      "epoch: 23 step: 33, loss is 0.004138187505304813\n",
      "epoch: 23 step: 34, loss is 0.00025250378530472517\n",
      "epoch: 23 step: 35, loss is 0.02089018188416958\n",
      "epoch: 23 step: 36, loss is 5.853569746250287e-05\n",
      "epoch: 23 step: 37, loss is 0.0010369251249358058\n",
      "epoch: 23 step: 38, loss is 0.004454156383872032\n",
      "epoch: 23 step: 39, loss is 0.0008134180679917336\n",
      "epoch: 23 step: 40, loss is 0.0017236100975424051\n",
      "epoch: 23 step: 41, loss is 0.010546941310167313\n",
      "epoch: 23 step: 42, loss is 0.010013853199779987\n",
      "epoch: 23 step: 43, loss is 0.0014855606714263558\n",
      "epoch: 23 step: 44, loss is 0.00017606104665901512\n",
      "epoch: 23 step: 45, loss is 0.0020494635682553053\n",
      "epoch: 23 step: 46, loss is 0.00939033180475235\n",
      "epoch: 23 step: 47, loss is 0.0014980447012931108\n",
      "epoch: 23 step: 48, loss is 0.031141085550189018\n",
      "epoch: 23 step: 49, loss is 0.01599794067442417\n",
      "epoch: 23 step: 50, loss is 0.02710229903459549\n",
      "epoch: 23 step: 51, loss is 0.04567780718207359\n",
      "epoch: 23 step: 52, loss is 0.008131295442581177\n",
      "epoch: 23 step: 53, loss is 0.008264784701168537\n",
      "epoch: 23 step: 54, loss is 0.0027789853047579527\n",
      "epoch: 23 step: 55, loss is 0.003821759019047022\n",
      "epoch: 23 step: 56, loss is 0.008125144056975842\n",
      "epoch: 23 step: 57, loss is 0.0035292860120534897\n",
      "epoch: 23 step: 58, loss is 0.001567505532875657\n",
      "epoch: 23 step: 59, loss is 0.0317157506942749\n",
      "epoch: 23 step: 60, loss is 0.04316231980919838\n",
      "epoch: 23 step: 61, loss is 0.01275099627673626\n",
      "epoch: 23 step: 62, loss is 0.018224531784653664\n",
      "epoch: 23 step: 63, loss is 0.002592924050986767\n",
      "epoch: 23 step: 64, loss is 0.0031468262895941734\n",
      "epoch: 23 step: 65, loss is 0.0009678677888587117\n",
      "epoch: 23 step: 66, loss is 0.0005030628526583314\n",
      "epoch: 23 step: 67, loss is 0.008952140808105469\n",
      "epoch: 23 step: 68, loss is 0.0029311862308532\n",
      "epoch: 23 step: 69, loss is 0.001721505424939096\n",
      "epoch: 23 step: 70, loss is 0.01882365718483925\n",
      "epoch: 23 step: 71, loss is 0.0011176869738847017\n",
      "epoch: 23 step: 72, loss is 0.08556127548217773\n",
      "epoch: 23 step: 73, loss is 0.005826229695230722\n",
      "epoch: 23 step: 74, loss is 0.0006386204622685909\n",
      "epoch: 23 step: 75, loss is 0.00033254275331273675\n",
      "epoch: 23 step: 76, loss is 0.00033343772520311177\n",
      "epoch: 23 step: 77, loss is 0.0028082341887056828\n",
      "epoch: 23 step: 78, loss is 0.006050552241504192\n",
      "epoch: 23 step: 79, loss is 0.004786680918186903\n",
      "epoch: 23 step: 80, loss is 0.019313203170895576\n",
      "epoch: 23 step: 81, loss is 0.0008574716630391777\n",
      "epoch: 23 step: 82, loss is 0.002349968533962965\n",
      "epoch: 23 step: 83, loss is 0.012222405523061752\n",
      "epoch: 23 step: 84, loss is 0.0017935537034645677\n",
      "epoch: 23 step: 85, loss is 0.00020780961494892836\n",
      "epoch: 23 step: 86, loss is 0.01080386247485876\n",
      "epoch: 23 step: 87, loss is 7.274661038536578e-05\n",
      "epoch: 23 step: 88, loss is 0.0005483348504640162\n",
      "epoch: 23 step: 89, loss is 0.002169164130464196\n",
      "epoch: 23 step: 90, loss is 0.0016814508708193898\n",
      "epoch: 23 step: 91, loss is 0.00031761263380758464\n",
      "epoch: 23 step: 92, loss is 0.02997523359954357\n",
      "epoch: 23 step: 93, loss is 0.0014845718396827579\n",
      "epoch: 23 step: 94, loss is 0.002006577793508768\n",
      "epoch: 23 step: 95, loss is 0.0007146603893488646\n",
      "epoch: 23 step: 96, loss is 0.004018205683678389\n",
      "epoch: 23 step: 97, loss is 0.0003881656448356807\n",
      "epoch: 23 step: 98, loss is 0.001995508326217532\n",
      "epoch: 23 step: 99, loss is 0.0004352245887275785\n",
      "epoch: 23 step: 100, loss is 2.9310469471965916e-05\n",
      "epoch: 23 step: 101, loss is 0.0544530488550663\n",
      "epoch: 23 step: 102, loss is 0.000705631566233933\n",
      "epoch: 23 step: 103, loss is 0.0001785241038305685\n",
      "epoch: 23 step: 104, loss is 0.003763602115213871\n",
      "epoch: 23 step: 105, loss is 0.030216442421078682\n",
      "epoch: 23 step: 106, loss is 0.0005240728496573865\n",
      "epoch: 23 step: 107, loss is 0.0003313953639008105\n",
      "epoch: 23 step: 108, loss is 0.01613105833530426\n",
      "epoch: 23 step: 109, loss is 0.00022724152950104326\n",
      "epoch: 23 step: 110, loss is 0.004364168271422386\n",
      "epoch: 23 step: 111, loss is 0.001160582061856985\n",
      "epoch: 23 step: 112, loss is 0.07316062599420547\n",
      "epoch: 23 step: 113, loss is 0.02843291126191616\n",
      "epoch: 23 step: 114, loss is 0.0006364416913129389\n",
      "epoch: 23 step: 115, loss is 0.00011251767864450812\n",
      "epoch: 23 step: 116, loss is 0.0005515824304893613\n",
      "epoch: 23 step: 117, loss is 0.00416767131537199\n",
      "epoch: 23 step: 118, loss is 0.0015167316887527704\n",
      "epoch: 23 step: 119, loss is 0.00219616643153131\n",
      "epoch: 23 step: 120, loss is 0.00570368068292737\n",
      "epoch: 23 step: 121, loss is 0.00010787863720906898\n",
      "epoch: 23 step: 122, loss is 0.0054654949344694614\n",
      "epoch: 23 step: 123, loss is 0.00023378792684525251\n",
      "epoch: 23 step: 124, loss is 0.00010050390847027302\n",
      "epoch: 23 step: 125, loss is 0.0033254483714699745\n",
      "epoch: 23 step: 126, loss is 0.0020066858269274235\n",
      "epoch: 23 step: 127, loss is 0.0006603093934245408\n",
      "epoch: 23 step: 128, loss is 0.0006092446274124086\n",
      "epoch: 23 step: 129, loss is 0.006759594194591045\n",
      "epoch: 23 step: 130, loss is 0.0012042563175782561\n",
      "epoch: 23 step: 131, loss is 0.03896265849471092\n",
      "epoch: 23 step: 132, loss is 0.010942764580249786\n",
      "epoch: 23 step: 133, loss is 0.01199150737375021\n",
      "epoch: 23 step: 134, loss is 0.0011631312081590295\n",
      "epoch: 23 step: 135, loss is 0.0285145565867424\n",
      "epoch: 23 step: 136, loss is 0.0007494702585972846\n",
      "epoch: 23 step: 137, loss is 0.0028520056512206793\n",
      "epoch: 23 step: 138, loss is 0.00230921502225101\n",
      "epoch: 23 step: 139, loss is 0.0019675036892294884\n",
      "epoch: 23 step: 140, loss is 0.01281588152050972\n",
      "epoch: 23 step: 141, loss is 0.0060905939899384975\n",
      "epoch: 23 step: 142, loss is 0.0049518016166985035\n",
      "epoch: 23 step: 143, loss is 0.00024653493892401457\n",
      "epoch: 23 step: 144, loss is 0.000805194373242557\n",
      "epoch: 23 step: 145, loss is 0.0016658237436786294\n",
      "epoch: 23 step: 146, loss is 0.06793410331010818\n",
      "epoch: 23 step: 147, loss is 0.0002907808229792863\n",
      "epoch: 23 step: 148, loss is 0.0009966398356482387\n",
      "epoch: 23 step: 149, loss is 7.936771726235747e-05\n",
      "epoch: 23 step: 150, loss is 0.00040302646812051535\n",
      "epoch: 23 step: 151, loss is 0.04565148428082466\n",
      "epoch: 23 step: 152, loss is 0.0014224195620045066\n",
      "epoch: 23 step: 153, loss is 0.004000951070338488\n",
      "epoch: 23 step: 154, loss is 0.00014496705261990428\n",
      "epoch: 23 step: 155, loss is 0.0014888269361108541\n",
      "epoch: 23 step: 156, loss is 0.0017924101557582617\n",
      "epoch: 23 step: 157, loss is 0.09919027239084244\n",
      "epoch: 23 step: 158, loss is 0.0024853835348039865\n",
      "epoch: 23 step: 159, loss is 0.00014097982784733176\n",
      "epoch: 23 step: 160, loss is 0.07157311588525772\n",
      "epoch: 23 step: 161, loss is 0.003992944490164518\n",
      "epoch: 23 step: 162, loss is 0.005898575764149427\n",
      "epoch: 23 step: 163, loss is 0.0410819873213768\n",
      "epoch: 23 step: 164, loss is 0.0010863180505111814\n",
      "epoch: 23 step: 165, loss is 0.0030898908153176308\n",
      "epoch: 23 step: 166, loss is 0.0013369948137551546\n",
      "epoch: 23 step: 167, loss is 0.00104256730992347\n",
      "epoch: 23 step: 168, loss is 3.79838929802645e-05\n",
      "epoch: 23 step: 169, loss is 0.0007535295444540679\n",
      "epoch: 23 step: 170, loss is 0.002529730089008808\n",
      "epoch: 23 step: 171, loss is 0.01040972862392664\n",
      "epoch: 23 step: 172, loss is 0.0018463119631633162\n",
      "epoch: 23 step: 173, loss is 0.0100251454859972\n",
      "epoch: 23 step: 174, loss is 0.0008943388820625842\n",
      "epoch: 23 step: 175, loss is 0.01134879793971777\n",
      "epoch: 23 step: 176, loss is 0.023339156061410904\n",
      "epoch: 23 step: 177, loss is 0.0005612231907434762\n",
      "epoch: 23 step: 178, loss is 0.025072382763028145\n",
      "epoch: 23 step: 179, loss is 0.024080049246549606\n",
      "epoch: 23 step: 180, loss is 0.0003491374081932008\n",
      "epoch: 23 step: 181, loss is 0.0005270334659144282\n",
      "epoch: 23 step: 182, loss is 0.10188453644514084\n",
      "epoch: 23 step: 183, loss is 0.05574484542012215\n",
      "epoch: 23 step: 184, loss is 0.003920324146747589\n",
      "epoch: 23 step: 185, loss is 0.0008741748169995844\n",
      "epoch: 23 step: 186, loss is 0.05770128220319748\n",
      "epoch: 23 step: 187, loss is 0.0017864735564216971\n",
      "epoch: 23 step: 188, loss is 0.0029095467180013657\n",
      "epoch: 23 step: 189, loss is 0.03167475759983063\n",
      "epoch: 23 step: 190, loss is 0.001587266568094492\n",
      "epoch: 23 step: 191, loss is 0.004574804566800594\n",
      "epoch: 23 step: 192, loss is 0.007004884071648121\n",
      "epoch: 23 step: 193, loss is 0.011160053312778473\n",
      "epoch: 23 step: 194, loss is 0.048954859375953674\n",
      "epoch: 23 step: 195, loss is 0.0008369204588234425\n",
      "epoch: 23 step: 196, loss is 0.002645164728164673\n",
      "epoch: 23 step: 197, loss is 0.004384292755275965\n",
      "epoch: 23 step: 198, loss is 0.00017812545411288738\n",
      "epoch: 23 step: 199, loss is 0.014735158532857895\n",
      "epoch: 23 step: 200, loss is 0.027689140290021896\n",
      "epoch: 23 step: 201, loss is 0.001547540072351694\n",
      "epoch: 23 step: 202, loss is 0.0015949372900649905\n",
      "epoch: 23 step: 203, loss is 0.004097465891391039\n",
      "epoch: 23 step: 204, loss is 0.04850301519036293\n",
      "epoch: 23 step: 205, loss is 0.013647536747157574\n",
      "epoch: 23 step: 206, loss is 0.031664587557315826\n",
      "epoch: 23 step: 207, loss is 0.03018656000494957\n",
      "epoch: 23 step: 208, loss is 0.001246826839633286\n",
      "epoch: 23 step: 209, loss is 0.004034326411783695\n",
      "epoch: 23 step: 210, loss is 0.07951746135950089\n",
      "epoch: 23 step: 211, loss is 0.001966932788491249\n",
      "epoch: 23 step: 212, loss is 0.029245387762784958\n",
      "epoch: 23 step: 213, loss is 0.0035588976461440325\n",
      "epoch: 23 step: 214, loss is 0.0014248037477955222\n",
      "epoch: 23 step: 215, loss is 0.0021175621077418327\n",
      "epoch: 23 step: 216, loss is 0.01592956855893135\n",
      "epoch: 23 step: 217, loss is 0.0330725833773613\n",
      "epoch: 23 step: 218, loss is 0.0013919164193794131\n",
      "epoch: 23 step: 219, loss is 0.0005414626793935895\n",
      "epoch: 23 step: 220, loss is 0.0010604336857795715\n",
      "epoch: 23 step: 221, loss is 0.01698465645313263\n",
      "epoch: 23 step: 222, loss is 0.0009447555057704449\n",
      "epoch: 23 step: 223, loss is 0.07309897243976593\n",
      "epoch: 23 step: 224, loss is 0.14304600656032562\n",
      "epoch: 23 step: 225, loss is 0.012296730652451515\n",
      "epoch: 23 step: 226, loss is 0.0010317109990864992\n",
      "epoch: 23 step: 227, loss is 0.00033472588984295726\n",
      "epoch: 23 step: 228, loss is 0.0038789515383541584\n",
      "epoch: 23 step: 229, loss is 0.005315642338246107\n",
      "epoch: 23 step: 230, loss is 0.005565238185226917\n",
      "epoch: 23 step: 231, loss is 0.0007012909045442939\n",
      "epoch: 23 step: 232, loss is 0.00033423479180783033\n",
      "epoch: 23 step: 233, loss is 1.0701612154662143e-05\n",
      "epoch: 23 step: 234, loss is 5.914470966672525e-05\n",
      "epoch: 23 step: 235, loss is 0.003778688609600067\n",
      "epoch: 23 step: 236, loss is 0.0036135665141046047\n",
      "epoch: 23 step: 237, loss is 0.0010667720343917608\n",
      "epoch: 23 step: 238, loss is 0.0008640424930490553\n",
      "epoch: 23 step: 239, loss is 0.02390703745186329\n",
      "epoch: 23 step: 240, loss is 0.008879766799509525\n",
      "epoch: 23 step: 241, loss is 0.019323715940117836\n",
      "epoch: 23 step: 242, loss is 0.0020572601351886988\n",
      "epoch: 23 step: 243, loss is 0.008598728105425835\n",
      "epoch: 23 step: 244, loss is 0.00022722652647644281\n",
      "epoch: 23 step: 245, loss is 0.0024736267514526844\n",
      "epoch: 23 step: 246, loss is 0.0039081876166164875\n",
      "epoch: 23 step: 247, loss is 0.06082313880324364\n",
      "epoch: 23 step: 248, loss is 0.004043485503643751\n",
      "epoch: 23 step: 249, loss is 0.0036721767392009497\n",
      "epoch: 23 step: 250, loss is 0.01095510832965374\n",
      "epoch: 23 step: 251, loss is 0.0007190065807662904\n",
      "epoch: 23 step: 252, loss is 0.004155176226049662\n",
      "epoch: 23 step: 253, loss is 0.0009061676100827754\n",
      "epoch: 23 step: 254, loss is 0.06520646810531616\n",
      "epoch: 23 step: 255, loss is 0.006506173871457577\n",
      "epoch: 23 step: 256, loss is 0.0026181042194366455\n",
      "epoch: 23 step: 257, loss is 0.027375876903533936\n",
      "epoch: 23 step: 258, loss is 0.0008428284199908376\n",
      "epoch: 23 step: 259, loss is 0.028254356235265732\n",
      "epoch: 23 step: 260, loss is 0.0010819934541359544\n",
      "epoch: 23 step: 261, loss is 0.0014349721604958177\n",
      "epoch: 23 step: 262, loss is 0.10792498290538788\n",
      "epoch: 23 step: 263, loss is 0.0002943451108876616\n",
      "epoch: 23 step: 264, loss is 0.01979285106062889\n",
      "epoch: 23 step: 265, loss is 0.02526354417204857\n",
      "epoch: 23 step: 266, loss is 0.0035169816110283136\n",
      "epoch: 23 step: 267, loss is 0.00014014939370099455\n",
      "epoch: 23 step: 268, loss is 0.0003942823095712811\n",
      "epoch: 23 step: 269, loss is 8.381954103242606e-05\n",
      "epoch: 23 step: 270, loss is 0.007462680339813232\n",
      "epoch: 23 step: 271, loss is 0.0064410073682665825\n",
      "epoch: 23 step: 272, loss is 0.001620842725969851\n",
      "epoch: 23 step: 273, loss is 0.0004674905794672668\n",
      "epoch: 23 step: 274, loss is 0.012950528413057327\n",
      "epoch: 23 step: 275, loss is 0.004747099708765745\n",
      "epoch: 23 step: 276, loss is 0.006963704247027636\n",
      "epoch: 23 step: 277, loss is 0.005001435987651348\n",
      "epoch: 23 step: 278, loss is 0.004016752354800701\n",
      "epoch: 23 step: 279, loss is 0.0006943351472727954\n",
      "epoch: 23 step: 280, loss is 0.05487614870071411\n",
      "epoch: 23 step: 281, loss is 0.0013246110174804926\n",
      "epoch: 23 step: 282, loss is 0.004830402322113514\n",
      "epoch: 23 step: 283, loss is 0.00025980526697821915\n",
      "epoch: 23 step: 284, loss is 0.0016153547912836075\n",
      "epoch: 23 step: 285, loss is 0.0005050237523391843\n",
      "epoch: 23 step: 286, loss is 0.004868771880865097\n",
      "epoch: 23 step: 287, loss is 0.002380174584686756\n",
      "epoch: 23 step: 288, loss is 0.01830439455807209\n",
      "epoch: 23 step: 289, loss is 0.0017158908303827047\n",
      "epoch: 23 step: 290, loss is 0.033004336059093475\n",
      "epoch: 23 step: 291, loss is 0.007338215131312609\n",
      "epoch: 23 step: 292, loss is 0.021715588867664337\n",
      "epoch: 23 step: 293, loss is 0.005481051281094551\n",
      "epoch: 23 step: 294, loss is 0.0009281232487410307\n",
      "epoch: 23 step: 295, loss is 0.004250073805451393\n",
      "epoch: 23 step: 296, loss is 0.0029163421131670475\n",
      "epoch: 23 step: 297, loss is 0.0003040148294530809\n",
      "epoch: 23 step: 298, loss is 0.0073950085788965225\n",
      "epoch: 23 step: 299, loss is 0.0005786266410723329\n",
      "epoch: 23 step: 300, loss is 0.03309648111462593\n",
      "epoch: 23 step: 301, loss is 0.010568056255578995\n",
      "epoch: 23 step: 302, loss is 0.0009195439633913338\n",
      "epoch: 23 step: 303, loss is 0.0072493418119847775\n",
      "epoch: 23 step: 304, loss is 0.0006462595774792135\n",
      "epoch: 23 step: 305, loss is 0.0052669458091259\n",
      "epoch: 23 step: 306, loss is 0.002807454438880086\n",
      "epoch: 23 step: 307, loss is 0.01351702120155096\n",
      "epoch: 23 step: 308, loss is 4.097974397154758e-06\n",
      "epoch: 23 step: 309, loss is 0.02612796612083912\n",
      "epoch: 23 step: 310, loss is 0.00013528762792702764\n",
      "epoch: 23 step: 311, loss is 0.0043221027590334415\n",
      "epoch: 23 step: 312, loss is 0.00640144944190979\n",
      "epoch: 23 step: 313, loss is 0.0009332164190709591\n",
      "epoch: 23 step: 314, loss is 0.0007585975108668208\n",
      "epoch: 23 step: 315, loss is 0.000312951160594821\n",
      "epoch: 23 step: 316, loss is 0.0013922764919698238\n",
      "epoch: 23 step: 317, loss is 0.030839674174785614\n",
      "epoch: 23 step: 318, loss is 0.0011056577786803246\n",
      "epoch: 23 step: 319, loss is 0.017031311988830566\n",
      "epoch: 23 step: 320, loss is 0.0007897866889834404\n",
      "epoch: 23 step: 321, loss is 0.002715447684749961\n",
      "epoch: 23 step: 322, loss is 0.0010603968985378742\n",
      "epoch: 23 step: 323, loss is 0.0006205305689945817\n",
      "epoch: 23 step: 324, loss is 0.0013056376483291388\n",
      "epoch: 23 step: 325, loss is 0.002556323539465666\n",
      "epoch: 23 step: 326, loss is 0.0010917861945927143\n",
      "epoch: 23 step: 327, loss is 0.0015871210489422083\n",
      "epoch: 23 step: 328, loss is 0.003939301241189241\n",
      "epoch: 23 step: 329, loss is 0.0003029162180610001\n",
      "epoch: 23 step: 330, loss is 0.0002980723511427641\n",
      "epoch: 23 step: 331, loss is 0.00042636290891095996\n",
      "epoch: 23 step: 332, loss is 0.0009887870401144028\n",
      "epoch: 23 step: 333, loss is 0.002001565182581544\n",
      "epoch: 23 step: 334, loss is 0.006876902189105749\n",
      "epoch: 23 step: 335, loss is 0.008282872848212719\n",
      "epoch: 23 step: 336, loss is 0.05399826169013977\n",
      "epoch: 23 step: 337, loss is 6.33835734333843e-05\n",
      "epoch: 23 step: 338, loss is 0.0006736749783158302\n",
      "epoch: 23 step: 339, loss is 0.0005508327740244567\n",
      "epoch: 23 step: 340, loss is 0.0001823563943617046\n",
      "epoch: 23 step: 341, loss is 0.0008291585254482925\n",
      "epoch: 23 step: 342, loss is 0.0006053936667740345\n",
      "epoch: 23 step: 343, loss is 0.04771967977285385\n",
      "epoch: 23 step: 344, loss is 0.004273162689059973\n",
      "epoch: 23 step: 345, loss is 0.0003526936925482005\n",
      "epoch: 23 step: 346, loss is 0.01686089299619198\n",
      "epoch: 23 step: 347, loss is 0.0026118236128240824\n",
      "epoch: 23 step: 348, loss is 0.0012616874882951379\n",
      "epoch: 23 step: 349, loss is 0.0001719575811875984\n",
      "epoch: 23 step: 350, loss is 0.00039097730768844485\n",
      "epoch: 23 step: 351, loss is 0.007686659228056669\n",
      "epoch: 23 step: 352, loss is 0.0011137292021885514\n",
      "epoch: 23 step: 353, loss is 0.009321777150034904\n",
      "epoch: 23 step: 354, loss is 0.0011114960070699453\n",
      "epoch: 23 step: 355, loss is 0.021140528842806816\n",
      "epoch: 23 step: 356, loss is 0.0003859667049255222\n",
      "epoch: 23 step: 357, loss is 0.0004003203066531569\n",
      "epoch: 23 step: 358, loss is 0.0010778028517961502\n",
      "epoch: 23 step: 359, loss is 0.0025512147694826126\n",
      "epoch: 23 step: 360, loss is 0.00030823764973320067\n",
      "epoch: 23 step: 361, loss is 0.001195959048345685\n",
      "epoch: 23 step: 362, loss is 0.00033863395219668746\n",
      "epoch: 23 step: 363, loss is 0.0006100702448748052\n",
      "epoch: 23 step: 364, loss is 0.004250997211784124\n",
      "epoch: 23 step: 365, loss is 0.0002448313171043992\n",
      "epoch: 23 step: 366, loss is 0.0007227041642181575\n",
      "epoch: 23 step: 367, loss is 0.015528063289821148\n",
      "epoch: 23 step: 368, loss is 0.003203095169737935\n",
      "epoch: 23 step: 369, loss is 0.00033125453046523035\n",
      "epoch: 23 step: 370, loss is 0.011590709909796715\n",
      "epoch: 23 step: 371, loss is 0.0017611734801903367\n",
      "epoch: 23 step: 372, loss is 0.00048641866305842996\n",
      "epoch: 23 step: 373, loss is 0.06133229658007622\n",
      "epoch: 23 step: 374, loss is 0.001521099591627717\n",
      "epoch: 23 step: 375, loss is 0.006926060654222965\n",
      "epoch: 23 step: 376, loss is 0.0006299910019151866\n",
      "epoch: 23 step: 377, loss is 0.00023675165721215308\n",
      "epoch: 23 step: 378, loss is 0.0023222132585942745\n",
      "epoch: 23 step: 379, loss is 0.00018808530876412988\n",
      "epoch: 23 step: 380, loss is 0.001567958970554173\n",
      "epoch: 23 step: 381, loss is 0.00014166200708132237\n",
      "epoch: 23 step: 382, loss is 0.005661769304424524\n",
      "epoch: 23 step: 383, loss is 0.06771481037139893\n",
      "epoch: 23 step: 384, loss is 0.04372822120785713\n",
      "epoch: 23 step: 385, loss is 0.001255615963600576\n",
      "epoch: 23 step: 386, loss is 0.0004194697830826044\n",
      "epoch: 23 step: 387, loss is 0.013205282390117645\n",
      "epoch: 23 step: 388, loss is 0.009806685149669647\n",
      "epoch: 23 step: 389, loss is 0.0007654744549654424\n",
      "epoch: 23 step: 390, loss is 0.02129010297358036\n",
      "epoch: 23 step: 391, loss is 0.000431090738857165\n",
      "epoch: 23 step: 392, loss is 0.004227612633258104\n",
      "epoch: 23 step: 393, loss is 0.003559364704415202\n",
      "epoch: 23 step: 394, loss is 4.547374919638969e-05\n",
      "epoch: 23 step: 395, loss is 0.01907355524599552\n",
      "epoch: 23 step: 396, loss is 0.01887655444443226\n",
      "epoch: 23 step: 397, loss is 0.00025980378268286586\n",
      "epoch: 23 step: 398, loss is 0.0011688630329445004\n",
      "epoch: 23 step: 399, loss is 0.00014253701374400407\n",
      "epoch: 23 step: 400, loss is 0.006622451823204756\n",
      "epoch: 23 step: 401, loss is 0.016240103170275688\n",
      "epoch: 23 step: 402, loss is 0.005758718587458134\n",
      "epoch: 23 step: 403, loss is 0.04127737134695053\n",
      "epoch: 23 step: 404, loss is 0.0013963357778266072\n",
      "epoch: 23 step: 405, loss is 0.007547352463006973\n",
      "epoch: 23 step: 406, loss is 0.030461184680461884\n",
      "epoch: 23 step: 407, loss is 0.000763932301197201\n",
      "epoch: 23 step: 408, loss is 0.0003377951215952635\n",
      "epoch: 23 step: 409, loss is 6.0837821365566924e-05\n",
      "epoch: 23 step: 410, loss is 0.004973132163286209\n",
      "epoch: 23 step: 411, loss is 0.014579498209059238\n",
      "epoch: 23 step: 412, loss is 0.0005812317249365151\n",
      "epoch: 23 step: 413, loss is 1.1821658517874312e-05\n",
      "epoch: 23 step: 414, loss is 0.0031765983439981937\n",
      "epoch: 23 step: 415, loss is 0.00035496862255968153\n",
      "epoch: 23 step: 416, loss is 6.901739834574983e-05\n",
      "epoch: 23 step: 417, loss is 0.036641981452703476\n",
      "epoch: 23 step: 418, loss is 0.009638712741434574\n",
      "epoch: 23 step: 419, loss is 0.02669726312160492\n",
      "epoch: 23 step: 420, loss is 0.00044661754509434104\n",
      "epoch: 23 step: 421, loss is 0.0061274198815226555\n",
      "epoch: 23 step: 422, loss is 0.006211315281689167\n",
      "epoch: 23 step: 423, loss is 0.003023190889507532\n",
      "epoch: 23 step: 424, loss is 0.05439551919698715\n",
      "epoch: 23 step: 425, loss is 0.0005276937736198306\n",
      "epoch: 23 step: 426, loss is 0.00041962647810578346\n",
      "epoch: 23 step: 427, loss is 0.00013180772657506168\n",
      "epoch: 23 step: 428, loss is 0.00021489911887329072\n",
      "epoch: 23 step: 429, loss is 0.0006836401298642159\n",
      "epoch: 23 step: 430, loss is 0.019129808992147446\n",
      "epoch: 23 step: 431, loss is 2.977952863147948e-05\n",
      "epoch: 23 step: 432, loss is 0.0012470487272366881\n",
      "epoch: 23 step: 433, loss is 0.001613800646737218\n",
      "epoch: 23 step: 434, loss is 0.02145056053996086\n",
      "epoch: 23 step: 435, loss is 0.0011855982011184096\n",
      "epoch: 23 step: 436, loss is 0.007151532452553511\n",
      "epoch: 23 step: 437, loss is 0.0010107092093676329\n",
      "epoch: 23 step: 438, loss is 0.00031273995409719646\n",
      "epoch: 23 step: 439, loss is 0.00029628639458678663\n",
      "epoch: 23 step: 440, loss is 0.0017682468751445413\n",
      "epoch: 23 step: 441, loss is 0.004128649830818176\n",
      "epoch: 23 step: 442, loss is 0.009225575253367424\n",
      "epoch: 23 step: 443, loss is 0.0018387272721156478\n",
      "epoch: 23 step: 444, loss is 0.0001134486956289038\n",
      "epoch: 23 step: 445, loss is 0.014359299093484879\n",
      "epoch: 23 step: 446, loss is 0.1328195035457611\n",
      "epoch: 23 step: 447, loss is 0.0026242004241794348\n",
      "epoch: 23 step: 448, loss is 0.0007880281773395836\n",
      "epoch: 23 step: 449, loss is 5.253062045085244e-05\n",
      "epoch: 23 step: 450, loss is 0.0003012636152561754\n",
      "epoch: 23 step: 451, loss is 0.0038845366798341274\n",
      "epoch: 23 step: 452, loss is 0.008122353814542294\n",
      "epoch: 23 step: 453, loss is 0.0010046736570075154\n",
      "epoch: 23 step: 454, loss is 0.010290336795151234\n",
      "epoch: 23 step: 455, loss is 9.612276335246861e-05\n",
      "epoch: 23 step: 456, loss is 0.04630560427904129\n",
      "epoch: 23 step: 457, loss is 0.001268186722882092\n",
      "epoch: 23 step: 458, loss is 0.033515144139528275\n",
      "epoch: 23 step: 459, loss is 0.0007519006030634046\n",
      "epoch: 23 step: 460, loss is 0.003423539688810706\n",
      "epoch: 23 step: 461, loss is 0.0024926546029746532\n",
      "epoch: 23 step: 462, loss is 0.009939313866198063\n",
      "epoch: 23 step: 463, loss is 0.0028715706430375576\n",
      "epoch: 23 step: 464, loss is 0.0007239595288410783\n",
      "epoch: 23 step: 465, loss is 0.009159371256828308\n",
      "epoch: 23 step: 466, loss is 0.0003630185092333704\n",
      "epoch: 23 step: 467, loss is 0.013937684707343578\n",
      "epoch: 23 step: 468, loss is 0.019223647192120552\n",
      "epoch: 23 step: 469, loss is 0.0031990197021514177\n",
      "epoch: 23 step: 470, loss is 0.0011124634183943272\n",
      "epoch: 23 step: 471, loss is 0.0027418837416917086\n",
      "epoch: 23 step: 472, loss is 0.004710743669420481\n",
      "epoch: 23 step: 473, loss is 0.019587451592087746\n",
      "epoch: 23 step: 474, loss is 0.002176499692723155\n",
      "epoch: 23 step: 475, loss is 0.01384396106004715\n",
      "epoch: 23 step: 476, loss is 0.01898234151303768\n",
      "epoch: 23 step: 477, loss is 0.041291385889053345\n",
      "epoch: 23 step: 478, loss is 0.04806225374341011\n",
      "epoch: 23 step: 479, loss is 0.00038505488191731274\n",
      "epoch: 23 step: 480, loss is 0.019665027037262917\n",
      "epoch: 23 step: 481, loss is 0.01579705812036991\n",
      "epoch: 23 step: 482, loss is 0.006540115922689438\n",
      "epoch: 23 step: 483, loss is 0.0005613763933070004\n",
      "epoch: 23 step: 484, loss is 3.075474523939192e-05\n",
      "epoch: 23 step: 485, loss is 0.0003505766508169472\n",
      "epoch: 23 step: 486, loss is 0.0013863721396774054\n",
      "epoch: 23 step: 487, loss is 0.0001040055212797597\n",
      "epoch: 23 step: 488, loss is 0.0033025681041181087\n",
      "epoch: 23 step: 489, loss is 0.004213797859847546\n",
      "epoch: 23 step: 490, loss is 0.001184714725241065\n",
      "epoch: 23 step: 491, loss is 0.0006061463500373065\n",
      "epoch: 23 step: 492, loss is 0.011340831406414509\n",
      "epoch: 23 step: 493, loss is 0.001417635940015316\n",
      "epoch: 23 step: 494, loss is 0.00013832675176672637\n",
      "epoch: 23 step: 495, loss is 0.003922400064766407\n",
      "epoch: 23 step: 496, loss is 0.00032414798624813557\n",
      "epoch: 23 step: 497, loss is 0.052607107907533646\n",
      "epoch: 23 step: 498, loss is 0.0017792031867429614\n",
      "epoch: 23 step: 499, loss is 0.006551412865519524\n",
      "epoch: 23 step: 500, loss is 0.09151268750429153\n",
      "epoch: 23 step: 501, loss is 0.0012281951494514942\n",
      "epoch: 23 step: 502, loss is 0.0013055653544142842\n",
      "epoch: 23 step: 503, loss is 0.0008116994285956025\n",
      "epoch: 23 step: 504, loss is 0.00018120152526535094\n",
      "epoch: 23 step: 505, loss is 0.00031841627787798643\n",
      "epoch: 23 step: 506, loss is 0.024252090603113174\n",
      "epoch: 23 step: 507, loss is 0.003228194313123822\n",
      "epoch: 23 step: 508, loss is 0.004146564286202192\n",
      "epoch: 23 step: 509, loss is 0.005965264514088631\n",
      "epoch: 23 step: 510, loss is 0.0025045324582606554\n",
      "epoch: 23 step: 511, loss is 0.06551506370306015\n",
      "epoch: 23 step: 512, loss is 0.03451358899474144\n",
      "epoch: 23 step: 513, loss is 0.0006417777622118592\n",
      "epoch: 23 step: 514, loss is 0.0005287443636916578\n",
      "epoch: 23 step: 515, loss is 0.033144354820251465\n",
      "epoch: 23 step: 516, loss is 0.00011829003051389009\n",
      "epoch: 23 step: 517, loss is 0.0014369841665029526\n",
      "epoch: 23 step: 518, loss is 0.002958853030577302\n",
      "epoch: 23 step: 519, loss is 5.3321731684263796e-05\n",
      "epoch: 23 step: 520, loss is 0.004454764537513256\n",
      "epoch: 23 step: 521, loss is 0.0004268309858161956\n",
      "epoch: 23 step: 522, loss is 0.0009059379226528108\n",
      "epoch: 23 step: 523, loss is 0.008651175536215305\n",
      "epoch: 23 step: 524, loss is 0.0028696071822196245\n",
      "epoch: 23 step: 525, loss is 0.01462516374886036\n",
      "epoch: 23 step: 526, loss is 0.007259618956595659\n",
      "epoch: 23 step: 527, loss is 0.02153451181948185\n",
      "epoch: 23 step: 528, loss is 0.0019944296218454838\n",
      "epoch: 23 step: 529, loss is 0.011677251197397709\n",
      "epoch: 23 step: 530, loss is 0.0003755033540073782\n",
      "epoch: 23 step: 531, loss is 0.0013361083110794425\n",
      "epoch: 23 step: 532, loss is 0.000716778973583132\n",
      "epoch: 23 step: 533, loss is 0.028290001675486565\n",
      "epoch: 23 step: 534, loss is 0.0030972736421972513\n",
      "epoch: 23 step: 535, loss is 0.011212222278118134\n",
      "epoch: 23 step: 536, loss is 0.007166190072894096\n",
      "epoch: 23 step: 537, loss is 0.005148349329829216\n",
      "epoch: 23 step: 538, loss is 0.0003051497624255717\n",
      "epoch: 23 step: 539, loss is 0.0001740304142003879\n",
      "epoch: 23 step: 540, loss is 0.02091321162879467\n",
      "epoch: 23 step: 541, loss is 0.0016039400361478329\n",
      "epoch: 23 step: 542, loss is 0.00717098917812109\n",
      "epoch: 23 step: 543, loss is 0.04834415763616562\n",
      "epoch: 23 step: 544, loss is 0.009547432884573936\n",
      "epoch: 23 step: 545, loss is 0.016829470172524452\n",
      "epoch: 23 step: 546, loss is 0.0034427123609930277\n",
      "epoch: 23 step: 547, loss is 0.0023952540941536427\n",
      "epoch: 23 step: 548, loss is 0.005962679162621498\n",
      "epoch: 23 step: 549, loss is 0.00010444165673106909\n",
      "epoch: 23 step: 550, loss is 0.00019861185865011066\n",
      "epoch: 23 step: 551, loss is 0.022606000304222107\n",
      "epoch: 23 step: 552, loss is 0.013742745853960514\n",
      "epoch: 23 step: 553, loss is 0.006650971248745918\n",
      "epoch: 23 step: 554, loss is 0.007650886662304401\n",
      "epoch: 23 step: 555, loss is 0.002793039195239544\n",
      "epoch: 23 step: 556, loss is 0.0018323055701330304\n",
      "epoch: 23 step: 557, loss is 0.0004791034443769604\n",
      "epoch: 23 step: 558, loss is 0.0030642703641206026\n",
      "epoch: 23 step: 559, loss is 0.0007165015558712184\n",
      "epoch: 23 step: 560, loss is 0.0017461521783843637\n",
      "epoch: 23 step: 561, loss is 0.0005823460523970425\n",
      "epoch: 23 step: 562, loss is 0.009748297743499279\n",
      "epoch: 23 step: 563, loss is 0.00019225578580517322\n",
      "epoch: 23 step: 564, loss is 0.029435886070132256\n",
      "epoch: 23 step: 565, loss is 0.0007463869405910373\n",
      "epoch: 23 step: 566, loss is 0.002471993677318096\n",
      "epoch: 23 step: 567, loss is 0.0018978777807205915\n",
      "epoch: 23 step: 568, loss is 0.0463266596198082\n",
      "epoch: 23 step: 569, loss is 0.008288303390145302\n",
      "epoch: 23 step: 570, loss is 0.0007965561817400157\n",
      "epoch: 23 step: 571, loss is 0.0022940956987440586\n",
      "epoch: 23 step: 572, loss is 0.001467120717279613\n",
      "epoch: 23 step: 573, loss is 9.574666182743385e-05\n",
      "epoch: 23 step: 574, loss is 0.004811558406800032\n",
      "epoch: 23 step: 575, loss is 0.00044723140308633447\n",
      "epoch: 23 step: 576, loss is 0.001893966575153172\n",
      "epoch: 23 step: 577, loss is 0.001134198042564094\n",
      "epoch: 23 step: 578, loss is 0.003024912439286709\n",
      "epoch: 23 step: 579, loss is 0.05276796594262123\n",
      "epoch: 23 step: 580, loss is 0.0019902947824448347\n",
      "epoch: 23 step: 581, loss is 0.003697388805449009\n",
      "epoch: 23 step: 582, loss is 0.0007685336167924106\n",
      "epoch: 23 step: 583, loss is 0.0006017671548761427\n",
      "epoch: 23 step: 584, loss is 0.08396600186824799\n",
      "epoch: 23 step: 585, loss is 0.004700302146375179\n",
      "epoch: 23 step: 586, loss is 0.0005783059750683606\n",
      "epoch: 23 step: 587, loss is 0.04720083996653557\n",
      "epoch: 23 step: 588, loss is 0.056266676634550095\n",
      "epoch: 23 step: 589, loss is 0.0006353787612169981\n",
      "epoch: 23 step: 590, loss is 0.00031971358112059534\n",
      "epoch: 23 step: 591, loss is 0.005429962184280157\n",
      "epoch: 23 step: 592, loss is 0.03497060015797615\n",
      "epoch: 23 step: 593, loss is 0.0007694137166254222\n",
      "epoch: 23 step: 594, loss is 0.027591899037361145\n",
      "epoch: 23 step: 595, loss is 0.010471531189978123\n",
      "epoch: 23 step: 596, loss is 0.0015310450689867139\n",
      "epoch: 23 step: 597, loss is 0.005735878832638264\n",
      "epoch: 23 step: 598, loss is 0.0010924023808911443\n",
      "epoch: 23 step: 599, loss is 0.00013773847604170442\n",
      "epoch: 23 step: 600, loss is 0.10204043239355087\n",
      "epoch: 23 step: 601, loss is 0.07141140103340149\n",
      "epoch: 23 step: 602, loss is 0.009386511519551277\n",
      "epoch: 23 step: 603, loss is 0.0027596699073910713\n",
      "epoch: 23 step: 604, loss is 0.0023675078991800547\n",
      "epoch: 23 step: 605, loss is 0.014390009455382824\n",
      "epoch: 23 step: 606, loss is 0.022801542654633522\n",
      "epoch: 23 step: 607, loss is 0.00889448169618845\n",
      "epoch: 23 step: 608, loss is 0.01750905066728592\n",
      "epoch: 23 step: 609, loss is 0.003727866103872657\n",
      "epoch: 23 step: 610, loss is 0.0021933403331786394\n",
      "epoch: 23 step: 611, loss is 0.005810724571347237\n",
      "epoch: 23 step: 612, loss is 0.006720952223986387\n",
      "epoch: 23 step: 613, loss is 0.004491099156439304\n",
      "epoch: 23 step: 614, loss is 0.004562320187687874\n",
      "epoch: 23 step: 615, loss is 0.014133671298623085\n",
      "epoch: 23 step: 616, loss is 0.012138932012021542\n",
      "epoch: 23 step: 617, loss is 0.013139227405190468\n",
      "epoch: 23 step: 618, loss is 0.03512472286820412\n",
      "epoch: 23 step: 619, loss is 4.659860860556364e-05\n",
      "epoch: 23 step: 620, loss is 0.050713952630758286\n",
      "epoch: 23 step: 621, loss is 0.002087595872581005\n",
      "epoch: 23 step: 622, loss is 0.0058429064229130745\n",
      "epoch: 23 step: 623, loss is 0.0007211124175228179\n",
      "epoch: 23 step: 624, loss is 0.0008003396796993911\n",
      "epoch: 23 step: 625, loss is 0.0012795432703569531\n",
      "epoch: 23 step: 626, loss is 0.001766358152963221\n",
      "epoch: 23 step: 627, loss is 0.011471972800791264\n",
      "epoch: 23 step: 628, loss is 0.008765523321926594\n",
      "epoch: 23 step: 629, loss is 0.002054047305136919\n",
      "epoch: 23 step: 630, loss is 0.11113110184669495\n",
      "epoch: 23 step: 631, loss is 0.00970674678683281\n",
      "epoch: 23 step: 632, loss is 0.007024324964731932\n",
      "epoch: 23 step: 633, loss is 0.002056959317997098\n",
      "epoch: 23 step: 634, loss is 0.004470893181860447\n",
      "epoch: 23 step: 635, loss is 0.027941692620515823\n",
      "epoch: 23 step: 636, loss is 0.0036757229827344418\n",
      "epoch: 23 step: 637, loss is 0.005270510911941528\n",
      "epoch: 23 step: 638, loss is 0.0008827393758110702\n",
      "epoch: 23 step: 639, loss is 0.008906537666916847\n",
      "epoch: 23 step: 640, loss is 0.0023330049589276314\n",
      "epoch: 23 step: 641, loss is 0.0004211054474581033\n",
      "epoch: 23 step: 642, loss is 0.014005551114678383\n",
      "epoch: 23 step: 643, loss is 0.0016749699134379625\n",
      "epoch: 23 step: 644, loss is 0.01878141425549984\n",
      "epoch: 23 step: 645, loss is 0.006050744093954563\n",
      "epoch: 23 step: 646, loss is 0.003764472668990493\n",
      "epoch: 23 step: 647, loss is 0.0011023749830201268\n",
      "epoch: 23 step: 648, loss is 0.019467487931251526\n",
      "epoch: 23 step: 649, loss is 0.022044286131858826\n",
      "epoch: 23 step: 650, loss is 0.005503964144736528\n",
      "epoch: 23 step: 651, loss is 0.006846755277365446\n",
      "epoch: 23 step: 652, loss is 0.0005511160125024617\n",
      "epoch: 23 step: 653, loss is 0.0007260976126417518\n",
      "epoch: 23 step: 654, loss is 0.007479475345462561\n",
      "epoch: 23 step: 655, loss is 0.005015483591705561\n",
      "epoch: 23 step: 656, loss is 0.00399954617023468\n",
      "epoch: 23 step: 657, loss is 0.00059152627363801\n",
      "epoch: 23 step: 658, loss is 0.003544627223163843\n",
      "epoch: 23 step: 659, loss is 0.012737350538372993\n",
      "epoch: 23 step: 660, loss is 0.014813605695962906\n",
      "epoch: 23 step: 661, loss is 0.003543965285643935\n",
      "epoch: 23 step: 662, loss is 0.019154004752635956\n",
      "epoch: 23 step: 663, loss is 0.005551335401833057\n",
      "epoch: 23 step: 664, loss is 0.0003850530774798244\n",
      "epoch: 23 step: 665, loss is 0.006605734582990408\n",
      "epoch: 23 step: 666, loss is 0.01003009732812643\n",
      "epoch: 23 step: 667, loss is 0.0001890138373710215\n",
      "epoch: 23 step: 668, loss is 0.0014464460546150804\n",
      "epoch: 23 step: 669, loss is 0.0013597391080111265\n",
      "epoch: 23 step: 670, loss is 0.005234432406723499\n",
      "epoch: 23 step: 671, loss is 0.07136717438697815\n",
      "epoch: 23 step: 672, loss is 0.013477122411131859\n",
      "epoch: 23 step: 673, loss is 0.003400407498702407\n",
      "epoch: 23 step: 674, loss is 0.005475055892020464\n",
      "epoch: 23 step: 675, loss is 0.0005846929852850735\n",
      "epoch: 23 step: 676, loss is 0.0064774733036756516\n",
      "epoch: 23 step: 677, loss is 0.02628357522189617\n",
      "epoch: 23 step: 678, loss is 0.002022716449573636\n",
      "epoch: 23 step: 679, loss is 0.00019519725174177438\n",
      "epoch: 23 step: 680, loss is 0.0025403164327144623\n",
      "epoch: 23 step: 681, loss is 0.1033526211977005\n",
      "epoch: 23 step: 682, loss is 0.0016241871053352952\n",
      "epoch: 23 step: 683, loss is 0.003791677998378873\n",
      "epoch: 23 step: 684, loss is 0.0011212158715352416\n",
      "epoch: 23 step: 685, loss is 0.02730223536491394\n",
      "epoch: 23 step: 686, loss is 0.0010045032249763608\n",
      "epoch: 23 step: 687, loss is 0.004296382889151573\n",
      "epoch: 23 step: 688, loss is 0.054448146373033524\n",
      "epoch: 23 step: 689, loss is 0.05847568437457085\n",
      "epoch: 23 step: 690, loss is 0.004975350573658943\n",
      "epoch: 23 step: 691, loss is 0.002599504077807069\n",
      "epoch: 23 step: 692, loss is 0.03327507898211479\n",
      "epoch: 23 step: 693, loss is 0.0016531437868252397\n",
      "epoch: 23 step: 694, loss is 0.0007681749411858618\n",
      "epoch: 23 step: 695, loss is 0.00034594914177432656\n",
      "epoch: 23 step: 696, loss is 0.0006393372896127403\n",
      "epoch: 23 step: 697, loss is 0.010362372733652592\n",
      "epoch: 23 step: 698, loss is 0.018491879105567932\n",
      "epoch: 23 step: 699, loss is 0.0026690985541790724\n",
      "epoch: 23 step: 700, loss is 0.002238321118056774\n",
      "epoch: 23 step: 701, loss is 0.029188714921474457\n",
      "epoch: 23 step: 702, loss is 0.00976167619228363\n",
      "epoch: 23 step: 703, loss is 0.011471792124211788\n",
      "epoch: 23 step: 704, loss is 0.07656104862689972\n",
      "epoch: 23 step: 705, loss is 0.002374578732997179\n",
      "epoch: 23 step: 706, loss is 0.009748928248882294\n",
      "epoch: 23 step: 707, loss is 0.0008041081018745899\n",
      "epoch: 23 step: 708, loss is 0.009109247475862503\n",
      "epoch: 23 step: 709, loss is 0.019711103290319443\n",
      "epoch: 23 step: 710, loss is 0.0013962056254968047\n",
      "epoch: 23 step: 711, loss is 0.00010846194345504045\n",
      "epoch: 23 step: 712, loss is 0.0022541664075106382\n",
      "epoch: 23 step: 713, loss is 0.032828398048877716\n",
      "epoch: 23 step: 714, loss is 0.0068763187155127525\n",
      "epoch: 23 step: 715, loss is 0.10791133344173431\n",
      "epoch: 23 step: 716, loss is 0.0007035922608338296\n",
      "epoch: 23 step: 717, loss is 0.0015080006560310721\n",
      "epoch: 23 step: 718, loss is 0.0032058926299214363\n",
      "epoch: 23 step: 719, loss is 0.0038759782910346985\n",
      "epoch: 23 step: 720, loss is 0.00024075934197753668\n",
      "epoch: 23 step: 721, loss is 6.958076119190082e-05\n",
      "epoch: 23 step: 722, loss is 0.06645900756120682\n",
      "epoch: 23 step: 723, loss is 0.0129323061555624\n",
      "epoch: 23 step: 724, loss is 0.04927828535437584\n",
      "epoch: 23 step: 725, loss is 0.010757293552160263\n",
      "epoch: 23 step: 726, loss is 0.0010949752759188414\n",
      "epoch: 23 step: 727, loss is 0.0018312004394829273\n",
      "epoch: 23 step: 728, loss is 0.0005984012386761606\n",
      "epoch: 23 step: 729, loss is 0.004293560050427914\n",
      "epoch: 23 step: 730, loss is 0.00664183683693409\n",
      "epoch: 23 step: 731, loss is 0.00026960542891174555\n",
      "epoch: 23 step: 732, loss is 0.31666019558906555\n",
      "epoch: 23 step: 733, loss is 0.017894970253109932\n",
      "epoch: 23 step: 734, loss is 0.003137188730761409\n",
      "epoch: 23 step: 735, loss is 0.16662068665027618\n",
      "epoch: 23 step: 736, loss is 0.006986507214605808\n",
      "epoch: 23 step: 737, loss is 0.00570258405059576\n",
      "epoch: 23 step: 738, loss is 0.0026503095868974924\n",
      "epoch: 23 step: 739, loss is 6.020100772730075e-05\n",
      "epoch: 23 step: 740, loss is 0.13225050270557404\n",
      "epoch: 23 step: 741, loss is 0.046438295394182205\n",
      "epoch: 23 step: 742, loss is 0.031439587473869324\n",
      "epoch: 23 step: 743, loss is 0.0007864566287025809\n",
      "epoch: 23 step: 744, loss is 0.0331396609544754\n",
      "epoch: 23 step: 745, loss is 0.004459809977561235\n",
      "epoch: 23 step: 746, loss is 0.00013912052963860333\n",
      "epoch: 23 step: 747, loss is 0.009181767702102661\n",
      "epoch: 23 step: 748, loss is 0.012918522581458092\n",
      "epoch: 23 step: 749, loss is 0.00013551789743360132\n",
      "epoch: 23 step: 750, loss is 0.0014623238239437342\n",
      "epoch: 23 step: 751, loss is 0.0022970526479184628\n",
      "epoch: 23 step: 752, loss is 0.0009396602981723845\n",
      "epoch: 23 step: 753, loss is 0.00022442698536906391\n",
      "epoch: 23 step: 754, loss is 0.0012233284069225192\n",
      "epoch: 23 step: 755, loss is 0.0015654430026188493\n",
      "epoch: 23 step: 756, loss is 0.0038047502748668194\n",
      "epoch: 23 step: 757, loss is 0.015692496672272682\n",
      "epoch: 23 step: 758, loss is 0.02518584579229355\n",
      "epoch: 23 step: 759, loss is 0.09825358539819717\n",
      "epoch: 23 step: 760, loss is 0.013460129499435425\n",
      "epoch: 23 step: 761, loss is 0.0003479422884993255\n",
      "epoch: 23 step: 762, loss is 0.015685878694057465\n",
      "epoch: 23 step: 763, loss is 0.0012744541745632887\n",
      "epoch: 23 step: 764, loss is 0.004659918136894703\n",
      "epoch: 23 step: 765, loss is 0.00026043495745398104\n",
      "epoch: 23 step: 766, loss is 0.03500211983919144\n",
      "epoch: 23 step: 767, loss is 0.03247718885540962\n",
      "epoch: 23 step: 768, loss is 0.0017835674807429314\n",
      "epoch: 23 step: 769, loss is 0.05457727611064911\n",
      "epoch: 23 step: 770, loss is 0.02338666282594204\n",
      "epoch: 23 step: 771, loss is 0.021681878715753555\n",
      "epoch: 23 step: 772, loss is 0.12174999713897705\n",
      "epoch: 23 step: 773, loss is 0.0048680086620152\n",
      "epoch: 23 step: 774, loss is 0.042530424892902374\n",
      "epoch: 23 step: 775, loss is 0.007124174851924181\n",
      "epoch: 23 step: 776, loss is 0.0010826654033735394\n",
      "epoch: 23 step: 777, loss is 0.0007212572963908315\n",
      "epoch: 23 step: 778, loss is 0.0008161516161635518\n",
      "epoch: 23 step: 779, loss is 0.0007254649535752833\n",
      "epoch: 23 step: 780, loss is 0.009727892465889454\n",
      "epoch: 23 step: 781, loss is 0.048560574650764465\n",
      "epoch: 23 step: 782, loss is 0.003938278183341026\n",
      "epoch: 23 step: 783, loss is 0.0023649444337934256\n",
      "epoch: 23 step: 784, loss is 0.00266842357814312\n",
      "epoch: 23 step: 785, loss is 0.01314660906791687\n",
      "epoch: 23 step: 786, loss is 0.025001054629683495\n",
      "epoch: 23 step: 787, loss is 0.003544342704117298\n",
      "epoch: 23 step: 788, loss is 0.0011130472412332892\n",
      "epoch: 23 step: 789, loss is 0.08551745116710663\n",
      "epoch: 23 step: 790, loss is 0.01110256277024746\n",
      "epoch: 23 step: 791, loss is 0.017014482989907265\n",
      "epoch: 23 step: 792, loss is 0.000300416664686054\n",
      "epoch: 23 step: 793, loss is 0.014758510515093803\n",
      "epoch: 23 step: 794, loss is 0.012984558008611202\n",
      "epoch: 23 step: 795, loss is 0.011032762937247753\n",
      "epoch: 23 step: 796, loss is 0.052412062883377075\n",
      "epoch: 23 step: 797, loss is 0.011446663178503513\n",
      "epoch: 23 step: 798, loss is 0.0016196933574974537\n",
      "epoch: 23 step: 799, loss is 0.008833220228552818\n",
      "epoch: 23 step: 800, loss is 0.006755120120942593\n",
      "epoch: 23 step: 801, loss is 0.005099811591207981\n",
      "epoch: 23 step: 802, loss is 0.011406667530536652\n",
      "epoch: 23 step: 803, loss is 0.0012828200124204159\n",
      "epoch: 23 step: 804, loss is 0.00029779007309116423\n",
      "epoch: 23 step: 805, loss is 0.0019344881875440478\n",
      "epoch: 23 step: 806, loss is 0.001975464168936014\n",
      "epoch: 23 step: 807, loss is 0.0005822967505082488\n",
      "epoch: 23 step: 808, loss is 0.007565215229988098\n",
      "epoch: 23 step: 809, loss is 5.8365763834444806e-05\n",
      "epoch: 23 step: 810, loss is 0.004234800580888987\n",
      "epoch: 23 step: 811, loss is 0.006618340499699116\n",
      "epoch: 23 step: 812, loss is 0.008291170932352543\n",
      "epoch: 23 step: 813, loss is 0.08143164217472076\n",
      "epoch: 23 step: 814, loss is 0.022970760241150856\n",
      "epoch: 23 step: 815, loss is 0.045649707317352295\n",
      "epoch: 23 step: 816, loss is 0.0002525601885281503\n",
      "epoch: 23 step: 817, loss is 0.0013322720769792795\n",
      "epoch: 23 step: 818, loss is 0.08198892325162888\n",
      "epoch: 23 step: 819, loss is 0.012416334822773933\n",
      "epoch: 23 step: 820, loss is 0.006228368263691664\n",
      "epoch: 23 step: 821, loss is 0.0022781253792345524\n",
      "epoch: 23 step: 822, loss is 0.016352035105228424\n",
      "epoch: 23 step: 823, loss is 0.0031656825449317694\n",
      "epoch: 23 step: 824, loss is 0.007191473618149757\n",
      "epoch: 23 step: 825, loss is 0.0052552116103470325\n",
      "epoch: 23 step: 826, loss is 0.0012811298947781324\n",
      "epoch: 23 step: 827, loss is 0.05353362113237381\n",
      "epoch: 23 step: 828, loss is 0.00036486677709035575\n",
      "epoch: 23 step: 829, loss is 0.003349076025187969\n",
      "epoch: 23 step: 830, loss is 0.0016810940578579903\n",
      "epoch: 23 step: 831, loss is 0.0007815994904376566\n",
      "epoch: 23 step: 832, loss is 0.004667198285460472\n",
      "epoch: 23 step: 833, loss is 0.018643582239747047\n",
      "epoch: 23 step: 834, loss is 0.021541370078921318\n",
      "epoch: 23 step: 835, loss is 0.0015003811568021774\n",
      "epoch: 23 step: 836, loss is 0.0014690801035612822\n",
      "epoch: 23 step: 837, loss is 0.02077968791127205\n",
      "epoch: 23 step: 838, loss is 0.02467096596956253\n",
      "epoch: 23 step: 839, loss is 0.006093284580856562\n",
      "epoch: 23 step: 840, loss is 0.005712875630706549\n",
      "epoch: 23 step: 841, loss is 0.01981578953564167\n",
      "epoch: 23 step: 842, loss is 0.0065477401949465275\n",
      "epoch: 23 step: 843, loss is 0.001394755207002163\n",
      "epoch: 23 step: 844, loss is 0.03765677288174629\n",
      "epoch: 23 step: 845, loss is 0.015521019697189331\n",
      "epoch: 23 step: 846, loss is 0.03065616264939308\n",
      "epoch: 23 step: 847, loss is 0.008381614461541176\n",
      "epoch: 23 step: 848, loss is 0.005343569442629814\n",
      "epoch: 23 step: 849, loss is 0.0001956924534169957\n",
      "epoch: 23 step: 850, loss is 0.01245613768696785\n",
      "epoch: 23 step: 851, loss is 0.047510821372270584\n",
      "epoch: 23 step: 852, loss is 0.009994548745453358\n",
      "epoch: 23 step: 853, loss is 0.14635075628757477\n",
      "epoch: 23 step: 854, loss is 0.00021287024719640613\n",
      "epoch: 23 step: 855, loss is 0.040548257529735565\n",
      "epoch: 23 step: 856, loss is 0.0015779859386384487\n",
      "epoch: 23 step: 857, loss is 0.006791745312511921\n",
      "epoch: 23 step: 858, loss is 0.005487474612891674\n",
      "epoch: 23 step: 859, loss is 0.0010369998635724187\n",
      "epoch: 23 step: 860, loss is 0.001918994472362101\n",
      "epoch: 23 step: 861, loss is 0.007851138710975647\n",
      "epoch: 23 step: 862, loss is 6.0311514971544966e-05\n",
      "epoch: 23 step: 863, loss is 0.002694277325645089\n",
      "epoch: 23 step: 864, loss is 0.0038826516829431057\n",
      "epoch: 23 step: 865, loss is 0.011847893707454205\n",
      "epoch: 23 step: 866, loss is 0.006583489011973143\n",
      "epoch: 23 step: 867, loss is 0.004842663649469614\n",
      "epoch: 23 step: 868, loss is 0.008007630705833435\n",
      "epoch: 23 step: 869, loss is 0.006892696022987366\n",
      "epoch: 23 step: 870, loss is 0.05443696305155754\n",
      "epoch: 23 step: 871, loss is 0.0016384604386985302\n",
      "epoch: 23 step: 872, loss is 0.000470190541818738\n",
      "epoch: 23 step: 873, loss is 0.014581664465367794\n",
      "epoch: 23 step: 874, loss is 0.04687399044632912\n",
      "epoch: 23 step: 875, loss is 0.0031882955227047205\n",
      "epoch: 23 step: 876, loss is 0.008223411627113819\n",
      "epoch: 23 step: 877, loss is 0.0003179606283083558\n",
      "epoch: 23 step: 878, loss is 0.0026509761810302734\n",
      "epoch: 23 step: 879, loss is 0.009256390854716301\n",
      "epoch: 23 step: 880, loss is 0.0002470065956003964\n",
      "epoch: 23 step: 881, loss is 0.06877034157514572\n",
      "epoch: 23 step: 882, loss is 0.0003583599755074829\n",
      "epoch: 23 step: 883, loss is 0.002798341680318117\n",
      "epoch: 23 step: 884, loss is 0.03138398379087448\n",
      "epoch: 23 step: 885, loss is 0.002234691521152854\n",
      "epoch: 23 step: 886, loss is 0.0010298732668161392\n",
      "epoch: 23 step: 887, loss is 0.0020401114597916603\n",
      "epoch: 23 step: 888, loss is 0.0013346527703106403\n",
      "epoch: 23 step: 889, loss is 0.002662317594513297\n",
      "epoch: 23 step: 890, loss is 0.005376923363655806\n",
      "epoch: 23 step: 891, loss is 0.00045940926065668464\n",
      "epoch: 23 step: 892, loss is 3.5947145079262555e-05\n",
      "epoch: 23 step: 893, loss is 0.001088928198441863\n",
      "epoch: 23 step: 894, loss is 0.002264948794618249\n",
      "epoch: 23 step: 895, loss is 0.007866833359003067\n",
      "epoch: 23 step: 896, loss is 0.010202989913523197\n",
      "epoch: 23 step: 897, loss is 0.018150905147194862\n",
      "epoch: 23 step: 898, loss is 0.009015909396111965\n",
      "epoch: 23 step: 899, loss is 0.058790259063243866\n",
      "epoch: 23 step: 900, loss is 0.0187087245285511\n",
      "epoch: 23 step: 901, loss is 9.35373900574632e-05\n",
      "epoch: 23 step: 902, loss is 0.0024003423750400543\n",
      "epoch: 23 step: 903, loss is 0.014954878017306328\n",
      "epoch: 23 step: 904, loss is 0.00045515396050177515\n",
      "epoch: 23 step: 905, loss is 0.004953206516802311\n",
      "epoch: 23 step: 906, loss is 6.980012403801084e-05\n",
      "epoch: 23 step: 907, loss is 0.004249530844390392\n",
      "epoch: 23 step: 908, loss is 0.0027184749487787485\n",
      "epoch: 23 step: 909, loss is 0.03687748685479164\n",
      "epoch: 23 step: 910, loss is 0.0192682147026062\n",
      "epoch: 23 step: 911, loss is 0.00017859683430287987\n",
      "epoch: 23 step: 912, loss is 0.0022854201961308718\n",
      "epoch: 23 step: 913, loss is 0.056271202862262726\n",
      "epoch: 23 step: 914, loss is 0.02018439583480358\n",
      "epoch: 23 step: 915, loss is 0.002978481352329254\n",
      "epoch: 23 step: 916, loss is 0.004315223544836044\n",
      "epoch: 23 step: 917, loss is 0.004895628429949284\n",
      "epoch: 23 step: 918, loss is 0.000270921882474795\n",
      "epoch: 23 step: 919, loss is 0.0016645481809973717\n",
      "epoch: 23 step: 920, loss is 0.0017409221036359668\n",
      "epoch: 23 step: 921, loss is 0.055992353707551956\n",
      "epoch: 23 step: 922, loss is 0.04307299107313156\n",
      "epoch: 23 step: 923, loss is 0.015541610307991505\n",
      "epoch: 23 step: 924, loss is 0.027294736355543137\n",
      "epoch: 23 step: 925, loss is 0.10766837745904922\n",
      "epoch: 23 step: 926, loss is 0.0026669311337172985\n",
      "epoch: 23 step: 927, loss is 0.010188786312937737\n",
      "epoch: 23 step: 928, loss is 0.01583639718592167\n",
      "epoch: 23 step: 929, loss is 0.0015076675917953253\n",
      "epoch: 23 step: 930, loss is 0.01788881979882717\n",
      "epoch: 23 step: 931, loss is 0.009997582994401455\n",
      "epoch: 23 step: 932, loss is 0.015011298470199108\n",
      "epoch: 23 step: 933, loss is 0.007638091687113047\n",
      "epoch: 23 step: 934, loss is 0.0026638172566890717\n",
      "epoch: 23 step: 935, loss is 0.09442426264286041\n",
      "epoch: 23 step: 936, loss is 0.01647176407277584\n",
      "epoch: 23 step: 937, loss is 0.0005036369548179209\n",
      "epoch: 24 step: 1, loss is 0.036864273250103\n",
      "epoch: 24 step: 2, loss is 0.0002738667244557291\n",
      "epoch: 24 step: 3, loss is 0.005433196201920509\n",
      "epoch: 24 step: 4, loss is 0.00514933792874217\n",
      "epoch: 24 step: 5, loss is 0.025507116690278053\n",
      "epoch: 24 step: 6, loss is 0.007035769522190094\n",
      "epoch: 24 step: 7, loss is 0.02693246863782406\n",
      "epoch: 24 step: 8, loss is 0.005575173068791628\n",
      "epoch: 24 step: 9, loss is 0.05165870487689972\n",
      "epoch: 24 step: 10, loss is 0.01537599042057991\n",
      "epoch: 24 step: 11, loss is 0.01580316573381424\n",
      "epoch: 24 step: 12, loss is 0.0023265755735337734\n",
      "epoch: 24 step: 13, loss is 0.005085144657641649\n",
      "epoch: 24 step: 14, loss is 0.004269114695489407\n",
      "epoch: 24 step: 15, loss is 0.001756887650117278\n",
      "epoch: 24 step: 16, loss is 0.020899934694170952\n",
      "epoch: 24 step: 17, loss is 0.04110930860042572\n",
      "epoch: 24 step: 18, loss is 0.004274984821677208\n",
      "epoch: 24 step: 19, loss is 0.019364237785339355\n",
      "epoch: 24 step: 20, loss is 0.005896632559597492\n",
      "epoch: 24 step: 21, loss is 0.00042915460653603077\n",
      "epoch: 24 step: 22, loss is 0.017179924994707108\n",
      "epoch: 24 step: 23, loss is 0.0012810166226699948\n",
      "epoch: 24 step: 24, loss is 0.004455715883523226\n",
      "epoch: 24 step: 25, loss is 0.005806231405586004\n",
      "epoch: 24 step: 26, loss is 0.009678881615400314\n",
      "epoch: 24 step: 27, loss is 0.009268569760024548\n",
      "epoch: 24 step: 28, loss is 0.0657079741358757\n",
      "epoch: 24 step: 29, loss is 0.010737394914031029\n",
      "epoch: 24 step: 30, loss is 0.006843368988484144\n",
      "epoch: 24 step: 31, loss is 0.0007465534727089107\n",
      "epoch: 24 step: 32, loss is 0.004363108426332474\n",
      "epoch: 24 step: 33, loss is 0.001677948865108192\n",
      "epoch: 24 step: 34, loss is 8.916571823647246e-05\n",
      "epoch: 24 step: 35, loss is 0.059825100004673004\n",
      "epoch: 24 step: 36, loss is 0.0012853232910856605\n",
      "epoch: 24 step: 37, loss is 0.010184130631387234\n",
      "epoch: 24 step: 38, loss is 0.002181394724175334\n",
      "epoch: 24 step: 39, loss is 0.00021898694103583694\n",
      "epoch: 24 step: 40, loss is 0.018724342808127403\n",
      "epoch: 24 step: 41, loss is 0.010384362190961838\n",
      "epoch: 24 step: 42, loss is 0.0032628285698592663\n",
      "epoch: 24 step: 43, loss is 0.009680942632257938\n",
      "epoch: 24 step: 44, loss is 0.05607559159398079\n",
      "epoch: 24 step: 45, loss is 0.003764319932088256\n",
      "epoch: 24 step: 46, loss is 0.0004686199245043099\n",
      "epoch: 24 step: 47, loss is 0.0004583138506859541\n",
      "epoch: 24 step: 48, loss is 0.0003571264969650656\n",
      "epoch: 24 step: 49, loss is 0.01636272296309471\n",
      "epoch: 24 step: 50, loss is 0.0008861560490913689\n",
      "epoch: 24 step: 51, loss is 0.0168791264295578\n",
      "epoch: 24 step: 52, loss is 0.0014753959840163589\n",
      "epoch: 24 step: 53, loss is 0.0007294625975191593\n",
      "epoch: 24 step: 54, loss is 0.00043900328455492854\n",
      "epoch: 24 step: 55, loss is 0.001206579152494669\n",
      "epoch: 24 step: 56, loss is 0.0006163211073726416\n",
      "epoch: 24 step: 57, loss is 0.0005906095029786229\n",
      "epoch: 24 step: 58, loss is 0.00043884594924747944\n",
      "epoch: 24 step: 59, loss is 7.416966400342062e-05\n",
      "epoch: 24 step: 60, loss is 0.00835406593978405\n",
      "epoch: 24 step: 61, loss is 0.028289923444390297\n",
      "epoch: 24 step: 62, loss is 0.0059618572704494\n",
      "epoch: 24 step: 63, loss is 0.014314515516161919\n",
      "epoch: 24 step: 64, loss is 9.82094497885555e-05\n",
      "epoch: 24 step: 65, loss is 0.0013779144501313567\n",
      "epoch: 24 step: 66, loss is 0.020914876833558083\n",
      "epoch: 24 step: 67, loss is 0.053543075919151306\n",
      "epoch: 24 step: 68, loss is 0.0038402576465159655\n",
      "epoch: 24 step: 69, loss is 0.001928926445543766\n",
      "epoch: 24 step: 70, loss is 0.007148859556764364\n",
      "epoch: 24 step: 71, loss is 0.03738120198249817\n",
      "epoch: 24 step: 72, loss is 0.0008690841495990753\n",
      "epoch: 24 step: 73, loss is 0.0010739252902567387\n",
      "epoch: 24 step: 74, loss is 0.0024152761325240135\n",
      "epoch: 24 step: 75, loss is 0.00017918359662871808\n",
      "epoch: 24 step: 76, loss is 0.00019286960014142096\n",
      "epoch: 24 step: 77, loss is 0.00044775742571800947\n",
      "epoch: 24 step: 78, loss is 0.04419655352830887\n",
      "epoch: 24 step: 79, loss is 0.03714967891573906\n",
      "epoch: 24 step: 80, loss is 0.0020207681227475405\n",
      "epoch: 24 step: 81, loss is 0.00643443688750267\n",
      "epoch: 24 step: 82, loss is 0.000938432349357754\n",
      "epoch: 24 step: 83, loss is 0.0032188622280955315\n",
      "epoch: 24 step: 84, loss is 0.005121973808854818\n",
      "epoch: 24 step: 85, loss is 0.030199507251381874\n",
      "epoch: 24 step: 86, loss is 0.0008331310236826539\n",
      "epoch: 24 step: 87, loss is 0.00010419033787911758\n",
      "epoch: 24 step: 88, loss is 0.0010141694219782948\n",
      "epoch: 24 step: 89, loss is 0.0005703734932467341\n",
      "epoch: 24 step: 90, loss is 0.004718616604804993\n",
      "epoch: 24 step: 91, loss is 0.003363934811204672\n",
      "epoch: 24 step: 92, loss is 0.006898425053805113\n",
      "epoch: 24 step: 93, loss is 0.006264000199735165\n",
      "epoch: 24 step: 94, loss is 0.0004772197571583092\n",
      "epoch: 24 step: 95, loss is 0.00351709988899529\n",
      "epoch: 24 step: 96, loss is 0.03264310583472252\n",
      "epoch: 24 step: 97, loss is 0.00014990183990448713\n",
      "epoch: 24 step: 98, loss is 0.010624166578054428\n",
      "epoch: 24 step: 99, loss is 0.0002860035456251353\n",
      "epoch: 24 step: 100, loss is 0.0007352873217314482\n",
      "epoch: 24 step: 101, loss is 0.00901437271386385\n",
      "epoch: 24 step: 102, loss is 0.0049994951114058495\n",
      "epoch: 24 step: 103, loss is 0.0009638334158807993\n",
      "epoch: 24 step: 104, loss is 0.0685327798128128\n",
      "epoch: 24 step: 105, loss is 0.09733842313289642\n",
      "epoch: 24 step: 106, loss is 0.0038616028614342213\n",
      "epoch: 24 step: 107, loss is 0.016532745212316513\n",
      "epoch: 24 step: 108, loss is 0.0009766134899109602\n",
      "epoch: 24 step: 109, loss is 0.0009360312251374125\n",
      "epoch: 24 step: 110, loss is 0.04699284955859184\n",
      "epoch: 24 step: 111, loss is 0.00037923615309409797\n",
      "epoch: 24 step: 112, loss is 0.010299747809767723\n",
      "epoch: 24 step: 113, loss is 0.0006796931847929955\n",
      "epoch: 24 step: 114, loss is 0.017584916204214096\n",
      "epoch: 24 step: 115, loss is 0.07349713146686554\n",
      "epoch: 24 step: 116, loss is 0.0437723807990551\n",
      "epoch: 24 step: 117, loss is 0.0035521569661796093\n",
      "epoch: 24 step: 118, loss is 0.005433922167867422\n",
      "epoch: 24 step: 119, loss is 0.007374539505690336\n",
      "epoch: 24 step: 120, loss is 0.009285004809498787\n",
      "epoch: 24 step: 121, loss is 0.0007003729697316885\n",
      "epoch: 24 step: 122, loss is 0.015533958561718464\n",
      "epoch: 24 step: 123, loss is 0.008529290556907654\n",
      "epoch: 24 step: 124, loss is 0.009431494399905205\n",
      "epoch: 24 step: 125, loss is 0.036830779165029526\n",
      "epoch: 24 step: 126, loss is 0.026199819520115852\n",
      "epoch: 24 step: 127, loss is 0.0007400905014947057\n",
      "epoch: 24 step: 128, loss is 0.0006213680608198047\n",
      "epoch: 24 step: 129, loss is 0.0005325020174495876\n",
      "epoch: 24 step: 130, loss is 0.0001659902191022411\n",
      "epoch: 24 step: 131, loss is 0.006109142210334539\n",
      "epoch: 24 step: 132, loss is 0.001339317299425602\n",
      "epoch: 24 step: 133, loss is 0.008493954315781593\n",
      "epoch: 24 step: 134, loss is 0.040534380823373795\n",
      "epoch: 24 step: 135, loss is 0.006419583689421415\n",
      "epoch: 24 step: 136, loss is 0.0010944610694423318\n",
      "epoch: 24 step: 137, loss is 0.046512652188539505\n",
      "epoch: 24 step: 138, loss is 0.08439086377620697\n",
      "epoch: 24 step: 139, loss is 0.0008728672983124852\n",
      "epoch: 24 step: 140, loss is 0.014868924394249916\n",
      "epoch: 24 step: 141, loss is 0.007278009317815304\n",
      "epoch: 24 step: 142, loss is 0.0036263326182961464\n",
      "epoch: 24 step: 143, loss is 0.00021003039728384465\n",
      "epoch: 24 step: 144, loss is 0.00034456796129234135\n",
      "epoch: 24 step: 145, loss is 0.0016923480434343219\n",
      "epoch: 24 step: 146, loss is 0.001049935701303184\n",
      "epoch: 24 step: 147, loss is 0.00078829366248101\n",
      "epoch: 24 step: 148, loss is 0.0005740977940149605\n",
      "epoch: 24 step: 149, loss is 0.022458557039499283\n",
      "epoch: 24 step: 150, loss is 0.0200638547539711\n",
      "epoch: 24 step: 151, loss is 0.026200011372566223\n",
      "epoch: 24 step: 152, loss is 0.006405167747288942\n",
      "epoch: 24 step: 153, loss is 0.007915145717561245\n",
      "epoch: 24 step: 154, loss is 0.001392686739563942\n",
      "epoch: 24 step: 155, loss is 0.0009249910362996161\n",
      "epoch: 24 step: 156, loss is 0.06512091308832169\n",
      "epoch: 24 step: 157, loss is 0.0007807181682437658\n",
      "epoch: 24 step: 158, loss is 0.0017160875722765923\n",
      "epoch: 24 step: 159, loss is 0.1039823591709137\n",
      "epoch: 24 step: 160, loss is 0.040625087916851044\n",
      "epoch: 24 step: 161, loss is 0.0003078065055888146\n",
      "epoch: 24 step: 162, loss is 0.0034275741782039404\n",
      "epoch: 24 step: 163, loss is 0.0024406039156019688\n",
      "epoch: 24 step: 164, loss is 0.0022713213693350554\n",
      "epoch: 24 step: 165, loss is 0.06773258745670319\n",
      "epoch: 24 step: 166, loss is 0.01162044983357191\n",
      "epoch: 24 step: 167, loss is 0.0074677676893770695\n",
      "epoch: 24 step: 168, loss is 0.0004222982097417116\n",
      "epoch: 24 step: 169, loss is 0.03466423600912094\n",
      "epoch: 24 step: 170, loss is 0.002701537450775504\n",
      "epoch: 24 step: 171, loss is 0.014237793162465096\n",
      "epoch: 24 step: 172, loss is 0.0012669151183217764\n",
      "epoch: 24 step: 173, loss is 0.0065931836143136024\n",
      "epoch: 24 step: 174, loss is 0.018732598051428795\n",
      "epoch: 24 step: 175, loss is 0.020937951281666756\n",
      "epoch: 24 step: 176, loss is 0.070280522108078\n",
      "epoch: 24 step: 177, loss is 0.07881522178649902\n",
      "epoch: 24 step: 178, loss is 0.00665666488930583\n",
      "epoch: 24 step: 179, loss is 0.031968846917152405\n",
      "epoch: 24 step: 180, loss is 0.02189548872411251\n",
      "epoch: 24 step: 181, loss is 0.003169320523738861\n",
      "epoch: 24 step: 182, loss is 0.0010183954145759344\n",
      "epoch: 24 step: 183, loss is 0.014745447784662247\n",
      "epoch: 24 step: 184, loss is 0.0006626463145948946\n",
      "epoch: 24 step: 185, loss is 0.012291310355067253\n",
      "epoch: 24 step: 186, loss is 0.05782954394817352\n",
      "epoch: 24 step: 187, loss is 0.09456784278154373\n",
      "epoch: 24 step: 188, loss is 0.008807649835944176\n",
      "epoch: 24 step: 189, loss is 0.0003514853015076369\n",
      "epoch: 24 step: 190, loss is 6.152006244519725e-05\n",
      "epoch: 24 step: 191, loss is 0.001140071195550263\n",
      "epoch: 24 step: 192, loss is 0.026696208864450455\n",
      "epoch: 24 step: 193, loss is 0.025284508243203163\n",
      "epoch: 24 step: 194, loss is 0.0020286855287849903\n",
      "epoch: 24 step: 195, loss is 0.007197363302111626\n",
      "epoch: 24 step: 196, loss is 0.017672915011644363\n",
      "epoch: 24 step: 197, loss is 0.014484448358416557\n",
      "epoch: 24 step: 198, loss is 0.00024122218019329011\n",
      "epoch: 24 step: 199, loss is 0.021108075976371765\n",
      "epoch: 24 step: 200, loss is 0.003424202324822545\n",
      "epoch: 24 step: 201, loss is 0.0011376369511708617\n",
      "epoch: 24 step: 202, loss is 0.007517389953136444\n",
      "epoch: 24 step: 203, loss is 0.03824684023857117\n",
      "epoch: 24 step: 204, loss is 0.02719796821475029\n",
      "epoch: 24 step: 205, loss is 0.04802212491631508\n",
      "epoch: 24 step: 206, loss is 0.0008400801452808082\n",
      "epoch: 24 step: 207, loss is 0.00968931708484888\n",
      "epoch: 24 step: 208, loss is 0.00039109436329454184\n",
      "epoch: 24 step: 209, loss is 0.05036268010735512\n",
      "epoch: 24 step: 210, loss is 0.0014260240131989121\n",
      "epoch: 24 step: 211, loss is 0.0020479438826441765\n",
      "epoch: 24 step: 212, loss is 0.053253866732120514\n",
      "epoch: 24 step: 213, loss is 0.012451750226318836\n",
      "epoch: 24 step: 214, loss is 0.00039983034366741776\n",
      "epoch: 24 step: 215, loss is 0.0005876314826309681\n",
      "epoch: 24 step: 216, loss is 0.003083624877035618\n",
      "epoch: 24 step: 217, loss is 0.010618547908961773\n",
      "epoch: 24 step: 218, loss is 0.024422531947493553\n",
      "epoch: 24 step: 219, loss is 0.000448485225206241\n",
      "epoch: 24 step: 220, loss is 0.0021207702811807394\n",
      "epoch: 24 step: 221, loss is 0.003799246624112129\n",
      "epoch: 24 step: 222, loss is 0.0033851806074380875\n",
      "epoch: 24 step: 223, loss is 0.00035034341271966696\n",
      "epoch: 24 step: 224, loss is 0.061221130192279816\n",
      "epoch: 24 step: 225, loss is 0.008489839732646942\n",
      "epoch: 24 step: 226, loss is 0.0034349008928984404\n",
      "epoch: 24 step: 227, loss is 0.02844448946416378\n",
      "epoch: 24 step: 228, loss is 0.07233157753944397\n",
      "epoch: 24 step: 229, loss is 0.0013378763105720282\n",
      "epoch: 24 step: 230, loss is 0.04607146605849266\n",
      "epoch: 24 step: 231, loss is 0.00018138671293854713\n",
      "epoch: 24 step: 232, loss is 0.024468133226037025\n",
      "epoch: 24 step: 233, loss is 0.02461141347885132\n",
      "epoch: 24 step: 234, loss is 0.02033914066851139\n",
      "epoch: 24 step: 235, loss is 0.0034177801571786404\n",
      "epoch: 24 step: 236, loss is 0.05159272626042366\n",
      "epoch: 24 step: 237, loss is 0.05861672759056091\n",
      "epoch: 24 step: 238, loss is 0.010635214857757092\n",
      "epoch: 24 step: 239, loss is 0.0002652574039530009\n",
      "epoch: 24 step: 240, loss is 0.020172791555523872\n",
      "epoch: 24 step: 241, loss is 0.002523476257920265\n",
      "epoch: 24 step: 242, loss is 0.0005016232607886195\n",
      "epoch: 24 step: 243, loss is 0.00011662163160508499\n",
      "epoch: 24 step: 244, loss is 0.015495828352868557\n",
      "epoch: 24 step: 245, loss is 0.007611567620187998\n",
      "epoch: 24 step: 246, loss is 0.08393759280443192\n",
      "epoch: 24 step: 247, loss is 0.002987097017467022\n",
      "epoch: 24 step: 248, loss is 0.0002080641279462725\n",
      "epoch: 24 step: 249, loss is 0.06693307310342789\n",
      "epoch: 24 step: 250, loss is 0.001603676239028573\n",
      "epoch: 24 step: 251, loss is 0.002154576824977994\n",
      "epoch: 24 step: 252, loss is 0.03092782199382782\n",
      "epoch: 24 step: 253, loss is 0.03576083853840828\n",
      "epoch: 24 step: 254, loss is 0.01885131374001503\n",
      "epoch: 24 step: 255, loss is 0.032243505120277405\n",
      "epoch: 24 step: 256, loss is 0.006537734996527433\n",
      "epoch: 24 step: 257, loss is 0.07888973504304886\n",
      "epoch: 24 step: 258, loss is 0.027566051110625267\n",
      "epoch: 24 step: 259, loss is 0.0200294628739357\n",
      "epoch: 24 step: 260, loss is 0.12397117912769318\n",
      "epoch: 24 step: 261, loss is 0.00020938774105161428\n",
      "epoch: 24 step: 262, loss is 0.04381132870912552\n",
      "epoch: 24 step: 263, loss is 0.016283396631479263\n",
      "epoch: 24 step: 264, loss is 0.00016712825163267553\n",
      "epoch: 24 step: 265, loss is 0.08271966129541397\n",
      "epoch: 24 step: 266, loss is 0.010557896457612514\n",
      "epoch: 24 step: 267, loss is 0.013311559334397316\n",
      "epoch: 24 step: 268, loss is 0.012900960631668568\n",
      "epoch: 24 step: 269, loss is 0.01753804460167885\n",
      "epoch: 24 step: 270, loss is 0.06820322573184967\n",
      "epoch: 24 step: 271, loss is 0.002193189924582839\n",
      "epoch: 24 step: 272, loss is 0.053576353937387466\n",
      "epoch: 24 step: 273, loss is 0.009797779843211174\n",
      "epoch: 24 step: 274, loss is 0.08032465726137161\n",
      "epoch: 24 step: 275, loss is 0.0010367778595536947\n",
      "epoch: 24 step: 276, loss is 0.002910148585215211\n",
      "epoch: 24 step: 277, loss is 0.0012097794096916914\n",
      "epoch: 24 step: 278, loss is 0.005710684694349766\n",
      "epoch: 24 step: 279, loss is 0.00926274061203003\n",
      "epoch: 24 step: 280, loss is 0.08028551936149597\n",
      "epoch: 24 step: 281, loss is 0.03221740946173668\n",
      "epoch: 24 step: 282, loss is 0.054680075496435165\n",
      "epoch: 24 step: 283, loss is 0.015597919933497906\n",
      "epoch: 24 step: 284, loss is 0.03759155422449112\n",
      "epoch: 24 step: 285, loss is 0.001399952918291092\n",
      "epoch: 24 step: 286, loss is 0.001068915007635951\n",
      "epoch: 24 step: 287, loss is 0.00022472564887721092\n",
      "epoch: 24 step: 288, loss is 0.0037524874787777662\n",
      "epoch: 24 step: 289, loss is 0.014903029426932335\n",
      "epoch: 24 step: 290, loss is 0.0025333703961223364\n",
      "epoch: 24 step: 291, loss is 0.001056432374753058\n",
      "epoch: 24 step: 292, loss is 0.001405998133122921\n",
      "epoch: 24 step: 293, loss is 0.07485676556825638\n",
      "epoch: 24 step: 294, loss is 0.0019697356037795544\n",
      "epoch: 24 step: 295, loss is 0.0006898660794831812\n",
      "epoch: 24 step: 296, loss is 0.01708352379500866\n",
      "epoch: 24 step: 297, loss is 0.07022646814584732\n",
      "epoch: 24 step: 298, loss is 7.418362656608224e-05\n",
      "epoch: 24 step: 299, loss is 0.0010161856189370155\n",
      "epoch: 24 step: 300, loss is 0.07691168785095215\n",
      "epoch: 24 step: 301, loss is 0.018717825412750244\n",
      "epoch: 24 step: 302, loss is 0.008720992133021355\n",
      "epoch: 24 step: 303, loss is 0.001656044041737914\n",
      "epoch: 24 step: 304, loss is 0.03519885241985321\n",
      "epoch: 24 step: 305, loss is 0.0003621922805905342\n",
      "epoch: 24 step: 306, loss is 0.001193082774989307\n",
      "epoch: 24 step: 307, loss is 0.019458778202533722\n",
      "epoch: 24 step: 308, loss is 0.004245578311383724\n",
      "epoch: 24 step: 309, loss is 0.00015749553858768195\n",
      "epoch: 24 step: 310, loss is 0.1640157699584961\n",
      "epoch: 24 step: 311, loss is 0.0009852612856775522\n",
      "epoch: 24 step: 312, loss is 0.015388437546789646\n",
      "epoch: 24 step: 313, loss is 0.002704154932871461\n",
      "epoch: 24 step: 314, loss is 0.0011877628276124597\n",
      "epoch: 24 step: 315, loss is 0.007906381040811539\n",
      "epoch: 24 step: 316, loss is 0.0006945968489162624\n",
      "epoch: 24 step: 317, loss is 0.00857850443571806\n",
      "epoch: 24 step: 318, loss is 0.032725196331739426\n",
      "epoch: 24 step: 319, loss is 0.00015890930080786347\n",
      "epoch: 24 step: 320, loss is 0.00190166721586138\n",
      "epoch: 24 step: 321, loss is 0.004586784169077873\n",
      "epoch: 24 step: 322, loss is 0.003737964667379856\n",
      "epoch: 24 step: 323, loss is 0.0024101233575493097\n",
      "epoch: 24 step: 324, loss is 0.002308902330696583\n",
      "epoch: 24 step: 325, loss is 0.015829579904675484\n",
      "epoch: 24 step: 326, loss is 0.0033417048398405313\n",
      "epoch: 24 step: 327, loss is 0.006872992496937513\n",
      "epoch: 24 step: 328, loss is 0.003162052948027849\n",
      "epoch: 24 step: 329, loss is 0.0019207061268389225\n",
      "epoch: 24 step: 330, loss is 0.05573633313179016\n",
      "epoch: 24 step: 331, loss is 0.0009580758051015437\n",
      "epoch: 24 step: 332, loss is 0.01135092880576849\n",
      "epoch: 24 step: 333, loss is 0.0005693502025678754\n",
      "epoch: 24 step: 334, loss is 0.03164022043347359\n",
      "epoch: 24 step: 335, loss is 0.0006625534151680768\n",
      "epoch: 24 step: 336, loss is 0.001237240619957447\n",
      "epoch: 24 step: 337, loss is 0.0032681317534297705\n",
      "epoch: 24 step: 338, loss is 0.0015757880173623562\n",
      "epoch: 24 step: 339, loss is 0.002048600697889924\n",
      "epoch: 24 step: 340, loss is 3.4978409530594945e-05\n",
      "epoch: 24 step: 341, loss is 0.025472186505794525\n",
      "epoch: 24 step: 342, loss is 0.03206733614206314\n",
      "epoch: 24 step: 343, loss is 0.018973099067807198\n",
      "epoch: 24 step: 344, loss is 0.002860626671463251\n",
      "epoch: 24 step: 345, loss is 0.06924913078546524\n",
      "epoch: 24 step: 346, loss is 0.023518359288573265\n",
      "epoch: 24 step: 347, loss is 0.004618978593498468\n",
      "epoch: 24 step: 348, loss is 0.002181043615564704\n",
      "epoch: 24 step: 349, loss is 0.0013137529604136944\n",
      "epoch: 24 step: 350, loss is 0.002637457335367799\n",
      "epoch: 24 step: 351, loss is 0.002613112796097994\n",
      "epoch: 24 step: 352, loss is 0.0020544982980936766\n",
      "epoch: 24 step: 353, loss is 0.011025423184037209\n",
      "epoch: 24 step: 354, loss is 7.65192016842775e-05\n",
      "epoch: 24 step: 355, loss is 0.038378991186618805\n",
      "epoch: 24 step: 356, loss is 0.000315963028697297\n",
      "epoch: 24 step: 357, loss is 0.011444318108260632\n",
      "epoch: 24 step: 358, loss is 0.0007578659569844604\n",
      "epoch: 24 step: 359, loss is 2.4369772290810943e-05\n",
      "epoch: 24 step: 360, loss is 0.06042877212166786\n",
      "epoch: 24 step: 361, loss is 0.0003580409102141857\n",
      "epoch: 24 step: 362, loss is 0.0019423370249569416\n",
      "epoch: 24 step: 363, loss is 0.0016223870916292071\n",
      "epoch: 24 step: 364, loss is 0.005233601201325655\n",
      "epoch: 24 step: 365, loss is 0.0017281166510656476\n",
      "epoch: 24 step: 366, loss is 0.00014044663112144917\n",
      "epoch: 24 step: 367, loss is 0.014092105440795422\n",
      "epoch: 24 step: 368, loss is 0.004429022781550884\n",
      "epoch: 24 step: 369, loss is 0.0058505660854279995\n",
      "epoch: 24 step: 370, loss is 0.04988165199756622\n",
      "epoch: 24 step: 371, loss is 0.0019412138499319553\n",
      "epoch: 24 step: 372, loss is 0.12773731350898743\n",
      "epoch: 24 step: 373, loss is 0.033429328352212906\n",
      "epoch: 24 step: 374, loss is 0.0005536536336876452\n",
      "epoch: 24 step: 375, loss is 0.0017246016068384051\n",
      "epoch: 24 step: 376, loss is 0.07972919195890427\n",
      "epoch: 24 step: 377, loss is 0.0023543064016848803\n",
      "epoch: 24 step: 378, loss is 0.028168432414531708\n",
      "epoch: 24 step: 379, loss is 0.000304106913972646\n",
      "epoch: 24 step: 380, loss is 0.012922496534883976\n",
      "epoch: 24 step: 381, loss is 0.0019915946759283543\n",
      "epoch: 24 step: 382, loss is 0.010305343195796013\n",
      "epoch: 24 step: 383, loss is 0.0014694716082885861\n",
      "epoch: 24 step: 384, loss is 0.18469342589378357\n",
      "epoch: 24 step: 385, loss is 0.06294883042573929\n",
      "epoch: 24 step: 386, loss is 0.015951456502079964\n",
      "epoch: 24 step: 387, loss is 0.0066380733624100685\n",
      "epoch: 24 step: 388, loss is 0.003373275510966778\n",
      "epoch: 24 step: 389, loss is 0.03446046635508537\n",
      "epoch: 24 step: 390, loss is 0.0004855450533796102\n",
      "epoch: 24 step: 391, loss is 0.00978098250925541\n",
      "epoch: 24 step: 392, loss is 0.001688737072981894\n",
      "epoch: 24 step: 393, loss is 0.0032802510540932417\n",
      "epoch: 24 step: 394, loss is 0.014461526647210121\n",
      "epoch: 24 step: 395, loss is 0.049132030457258224\n",
      "epoch: 24 step: 396, loss is 0.00509636802598834\n",
      "epoch: 24 step: 397, loss is 0.04983896389603615\n",
      "epoch: 24 step: 398, loss is 0.01909305341541767\n",
      "epoch: 24 step: 399, loss is 0.022889913991093636\n",
      "epoch: 24 step: 400, loss is 0.0014263199409469962\n",
      "epoch: 24 step: 401, loss is 0.026403501629829407\n",
      "epoch: 24 step: 402, loss is 0.07945936173200607\n",
      "epoch: 24 step: 403, loss is 0.015904292464256287\n",
      "epoch: 24 step: 404, loss is 0.0017399939242750406\n",
      "epoch: 24 step: 405, loss is 0.013910305686295033\n",
      "epoch: 24 step: 406, loss is 0.03587609902024269\n",
      "epoch: 24 step: 407, loss is 0.17523574829101562\n",
      "epoch: 24 step: 408, loss is 0.06127200275659561\n",
      "epoch: 24 step: 409, loss is 0.017986111342906952\n",
      "epoch: 24 step: 410, loss is 0.0053784046322107315\n",
      "epoch: 24 step: 411, loss is 0.0021686539985239506\n",
      "epoch: 24 step: 412, loss is 0.019068069756031036\n",
      "epoch: 24 step: 413, loss is 0.0021389939356595278\n",
      "epoch: 24 step: 414, loss is 0.0014239242300391197\n",
      "epoch: 24 step: 415, loss is 0.016218066215515137\n",
      "epoch: 24 step: 416, loss is 0.022563472390174866\n",
      "epoch: 24 step: 417, loss is 0.006313263438642025\n",
      "epoch: 24 step: 418, loss is 0.008067777380347252\n",
      "epoch: 24 step: 419, loss is 0.0051624649204313755\n",
      "epoch: 24 step: 420, loss is 0.0033932749647647142\n",
      "epoch: 24 step: 421, loss is 0.0288203414529562\n",
      "epoch: 24 step: 422, loss is 0.10495207458734512\n",
      "epoch: 24 step: 423, loss is 0.002493207808583975\n",
      "epoch: 24 step: 424, loss is 0.012098638340830803\n",
      "epoch: 24 step: 425, loss is 0.0024072814267128706\n",
      "epoch: 24 step: 426, loss is 0.12827494740486145\n",
      "epoch: 24 step: 427, loss is 0.007856924086809158\n",
      "epoch: 24 step: 428, loss is 0.0161447674036026\n",
      "epoch: 24 step: 429, loss is 0.0042354632169008255\n",
      "epoch: 24 step: 430, loss is 0.008485558442771435\n",
      "epoch: 24 step: 431, loss is 0.08772196620702744\n",
      "epoch: 24 step: 432, loss is 0.0007397600566036999\n",
      "epoch: 24 step: 433, loss is 0.0007297690026462078\n",
      "epoch: 24 step: 434, loss is 0.0006049633375369012\n",
      "epoch: 24 step: 435, loss is 0.004608245100826025\n",
      "epoch: 24 step: 436, loss is 0.005468017887324095\n",
      "epoch: 24 step: 437, loss is 0.033129990100860596\n",
      "epoch: 24 step: 438, loss is 0.0055516427382826805\n",
      "epoch: 24 step: 439, loss is 0.032543737441301346\n",
      "epoch: 24 step: 440, loss is 0.020036881789565086\n",
      "epoch: 24 step: 441, loss is 0.0028572934679687023\n",
      "epoch: 24 step: 442, loss is 0.08472474664449692\n",
      "epoch: 24 step: 443, loss is 0.035947173833847046\n",
      "epoch: 24 step: 444, loss is 0.0017637189012020826\n",
      "epoch: 24 step: 445, loss is 0.012525458820164204\n",
      "epoch: 24 step: 446, loss is 0.00016923891962505877\n",
      "epoch: 24 step: 447, loss is 0.002111617010086775\n",
      "epoch: 24 step: 448, loss is 0.0002802082453854382\n",
      "epoch: 24 step: 449, loss is 0.012765634804964066\n",
      "epoch: 24 step: 450, loss is 0.012821814976632595\n",
      "epoch: 24 step: 451, loss is 0.00010714508971432224\n",
      "epoch: 24 step: 452, loss is 0.031244033947587013\n",
      "epoch: 24 step: 453, loss is 0.0005264278152026236\n",
      "epoch: 24 step: 454, loss is 0.001256993506103754\n",
      "epoch: 24 step: 455, loss is 0.021532434970140457\n",
      "epoch: 24 step: 456, loss is 0.039202816784381866\n",
      "epoch: 24 step: 457, loss is 0.025837944820523262\n",
      "epoch: 24 step: 458, loss is 0.004765228368341923\n",
      "epoch: 24 step: 459, loss is 0.030983421951532364\n",
      "epoch: 24 step: 460, loss is 0.0010581599781289697\n",
      "epoch: 24 step: 461, loss is 0.0003737166989594698\n",
      "epoch: 24 step: 462, loss is 0.026433339342474937\n",
      "epoch: 24 step: 463, loss is 0.00042965507600456476\n",
      "epoch: 24 step: 464, loss is 0.010413696989417076\n",
      "epoch: 24 step: 465, loss is 0.009438781067728996\n",
      "epoch: 24 step: 466, loss is 0.005939983297139406\n",
      "epoch: 24 step: 467, loss is 0.0013701830757781863\n",
      "epoch: 24 step: 468, loss is 0.005941562354564667\n",
      "epoch: 24 step: 469, loss is 0.00024179818865377456\n",
      "epoch: 24 step: 470, loss is 0.05330819636583328\n",
      "epoch: 24 step: 471, loss is 0.013828400522470474\n",
      "epoch: 24 step: 472, loss is 0.004569028038531542\n",
      "epoch: 24 step: 473, loss is 0.0016650580801069736\n",
      "epoch: 24 step: 474, loss is 0.017138557508587837\n",
      "epoch: 24 step: 475, loss is 0.011506726033985615\n",
      "epoch: 24 step: 476, loss is 0.0018543641781434417\n",
      "epoch: 24 step: 477, loss is 0.02695137821137905\n",
      "epoch: 24 step: 478, loss is 0.010857813991606236\n",
      "epoch: 24 step: 479, loss is 0.03134623169898987\n",
      "epoch: 24 step: 480, loss is 0.00067976611899212\n",
      "epoch: 24 step: 481, loss is 0.027658330276608467\n",
      "epoch: 24 step: 482, loss is 0.00617772713303566\n",
      "epoch: 24 step: 483, loss is 0.0005120811401866376\n",
      "epoch: 24 step: 484, loss is 0.0032829884439706802\n",
      "epoch: 24 step: 485, loss is 0.0005814430769532919\n",
      "epoch: 24 step: 486, loss is 8.996099495561793e-05\n",
      "epoch: 24 step: 487, loss is 0.020590296015143394\n",
      "epoch: 24 step: 488, loss is 0.0002120935096172616\n",
      "epoch: 24 step: 489, loss is 0.000102521589724347\n",
      "epoch: 24 step: 490, loss is 0.055435795336961746\n",
      "epoch: 24 step: 491, loss is 0.015731217339634895\n",
      "epoch: 24 step: 492, loss is 0.0030048778280615807\n",
      "epoch: 24 step: 493, loss is 0.008469236083328724\n",
      "epoch: 24 step: 494, loss is 0.030671387910842896\n",
      "epoch: 24 step: 495, loss is 0.0025106987450271845\n",
      "epoch: 24 step: 496, loss is 0.05753360316157341\n",
      "epoch: 24 step: 497, loss is 0.006442196201533079\n",
      "epoch: 24 step: 498, loss is 0.003307799808681011\n",
      "epoch: 24 step: 499, loss is 0.017439229413866997\n",
      "epoch: 24 step: 500, loss is 0.04876675084233284\n",
      "epoch: 24 step: 501, loss is 0.0015549357049167156\n",
      "epoch: 24 step: 502, loss is 0.0004852119309362024\n",
      "epoch: 24 step: 503, loss is 0.002351681236177683\n",
      "epoch: 24 step: 504, loss is 0.0002423205296508968\n",
      "epoch: 24 step: 505, loss is 0.005120954010635614\n",
      "epoch: 24 step: 506, loss is 0.03848837688565254\n",
      "epoch: 24 step: 507, loss is 0.0007952922605909407\n",
      "epoch: 24 step: 508, loss is 0.08353018760681152\n",
      "epoch: 24 step: 509, loss is 0.0077590313740074635\n",
      "epoch: 24 step: 510, loss is 0.0015589569229632616\n",
      "epoch: 24 step: 511, loss is 0.04913488030433655\n",
      "epoch: 24 step: 512, loss is 0.00035156140802428126\n",
      "epoch: 24 step: 513, loss is 0.05673234164714813\n",
      "epoch: 24 step: 514, loss is 0.026559198275208473\n",
      "epoch: 24 step: 515, loss is 0.0004739876021631062\n",
      "epoch: 24 step: 516, loss is 0.0006630721618421376\n",
      "epoch: 24 step: 517, loss is 0.006593007594347\n",
      "epoch: 24 step: 518, loss is 0.0029421942308545113\n",
      "epoch: 24 step: 519, loss is 0.004785037133842707\n",
      "epoch: 24 step: 520, loss is 0.014566034078598022\n",
      "epoch: 24 step: 521, loss is 0.07213625311851501\n",
      "epoch: 24 step: 522, loss is 0.020046591758728027\n",
      "epoch: 24 step: 523, loss is 0.0032441760413348675\n",
      "epoch: 24 step: 524, loss is 0.04579298943281174\n",
      "epoch: 24 step: 525, loss is 0.013043822720646858\n",
      "epoch: 24 step: 526, loss is 0.06458915024995804\n",
      "epoch: 24 step: 527, loss is 0.010760289616882801\n",
      "epoch: 24 step: 528, loss is 0.004048761446028948\n",
      "epoch: 24 step: 529, loss is 0.023047948256134987\n",
      "epoch: 24 step: 530, loss is 0.00011523567809490487\n",
      "epoch: 24 step: 531, loss is 0.01499448623508215\n",
      "epoch: 24 step: 532, loss is 0.052880462259054184\n",
      "epoch: 24 step: 533, loss is 0.04042743146419525\n",
      "epoch: 24 step: 534, loss is 0.03090101107954979\n",
      "epoch: 24 step: 535, loss is 0.0007764598703943193\n",
      "epoch: 24 step: 536, loss is 0.0049176933243870735\n",
      "epoch: 24 step: 537, loss is 0.05164734646677971\n",
      "epoch: 24 step: 538, loss is 0.041571661829948425\n",
      "epoch: 24 step: 539, loss is 0.00397652480751276\n",
      "epoch: 24 step: 540, loss is 0.08225890249013901\n",
      "epoch: 24 step: 541, loss is 0.03344307839870453\n",
      "epoch: 24 step: 542, loss is 0.042357493191957474\n",
      "epoch: 24 step: 543, loss is 0.01109024416655302\n",
      "epoch: 24 step: 544, loss is 0.00023063259141054004\n",
      "epoch: 24 step: 545, loss is 0.04305211454629898\n",
      "epoch: 24 step: 546, loss is 0.0021925424225628376\n",
      "epoch: 24 step: 547, loss is 0.0031276463996618986\n",
      "epoch: 24 step: 548, loss is 0.15563882887363434\n",
      "epoch: 24 step: 549, loss is 0.16224808990955353\n",
      "epoch: 24 step: 550, loss is 0.03169795870780945\n",
      "epoch: 24 step: 551, loss is 0.005656845401972532\n",
      "epoch: 24 step: 552, loss is 0.0037598717026412487\n",
      "epoch: 24 step: 553, loss is 0.10739067196846008\n",
      "epoch: 24 step: 554, loss is 0.00012713461183011532\n",
      "epoch: 24 step: 555, loss is 0.08772391825914383\n",
      "epoch: 24 step: 556, loss is 0.029598625376820564\n",
      "epoch: 24 step: 557, loss is 0.027402088046073914\n",
      "epoch: 24 step: 558, loss is 0.011046198196709156\n",
      "epoch: 24 step: 559, loss is 0.002361972350627184\n",
      "epoch: 24 step: 560, loss is 0.04670129343867302\n",
      "epoch: 24 step: 561, loss is 0.0016536565963178873\n",
      "epoch: 24 step: 562, loss is 0.011036480776965618\n",
      "epoch: 24 step: 563, loss is 0.010227150283753872\n",
      "epoch: 24 step: 564, loss is 0.09618211537599564\n",
      "epoch: 24 step: 565, loss is 0.006103207357227802\n",
      "epoch: 24 step: 566, loss is 0.004096917808055878\n",
      "epoch: 24 step: 567, loss is 0.023252811282873154\n",
      "epoch: 24 step: 568, loss is 0.004568777047097683\n",
      "epoch: 24 step: 569, loss is 0.00970297958701849\n",
      "epoch: 24 step: 570, loss is 0.009198940359055996\n",
      "epoch: 24 step: 571, loss is 0.12442483007907867\n",
      "epoch: 24 step: 572, loss is 0.13098613917827606\n",
      "epoch: 24 step: 573, loss is 0.009616343304514885\n",
      "epoch: 24 step: 574, loss is 0.03895629942417145\n",
      "epoch: 24 step: 575, loss is 0.03632689267396927\n",
      "epoch: 24 step: 576, loss is 0.030646659433841705\n",
      "epoch: 24 step: 577, loss is 0.047245580703020096\n",
      "epoch: 24 step: 578, loss is 0.13693106174468994\n",
      "epoch: 24 step: 579, loss is 0.0012092255055904388\n",
      "epoch: 24 step: 580, loss is 0.06866520643234253\n",
      "epoch: 24 step: 581, loss is 0.053144969046115875\n",
      "epoch: 24 step: 582, loss is 0.0020337740425020456\n",
      "epoch: 24 step: 583, loss is 0.010526142083108425\n",
      "epoch: 24 step: 584, loss is 0.01815011352300644\n",
      "epoch: 24 step: 585, loss is 0.007502933498471975\n",
      "epoch: 24 step: 586, loss is 0.01007523387670517\n",
      "epoch: 24 step: 587, loss is 0.018101727589964867\n",
      "epoch: 24 step: 588, loss is 0.0620986744761467\n",
      "epoch: 24 step: 589, loss is 0.057406798005104065\n",
      "epoch: 24 step: 590, loss is 0.02478303760290146\n",
      "epoch: 24 step: 591, loss is 0.09460757672786713\n",
      "epoch: 24 step: 592, loss is 0.03480326011776924\n",
      "epoch: 24 step: 593, loss is 0.013592640869319439\n",
      "epoch: 24 step: 594, loss is 0.02145582064986229\n",
      "epoch: 24 step: 595, loss is 0.02377208136022091\n",
      "epoch: 24 step: 596, loss is 0.0009232766460627317\n",
      "epoch: 24 step: 597, loss is 0.001108529744669795\n",
      "epoch: 24 step: 598, loss is 0.007954531349241734\n",
      "epoch: 24 step: 599, loss is 0.002416844479739666\n",
      "epoch: 24 step: 600, loss is 0.012321222573518753\n",
      "epoch: 24 step: 601, loss is 0.0650029256939888\n",
      "epoch: 24 step: 602, loss is 0.00551651744171977\n",
      "epoch: 24 step: 603, loss is 0.02169603668153286\n",
      "epoch: 24 step: 604, loss is 0.004102833569049835\n",
      "epoch: 24 step: 605, loss is 0.0020154232624918222\n",
      "epoch: 24 step: 606, loss is 0.0021216953173279762\n",
      "epoch: 24 step: 607, loss is 0.048621710389852524\n",
      "epoch: 24 step: 608, loss is 0.007580106146633625\n",
      "epoch: 24 step: 609, loss is 0.002562520792707801\n",
      "epoch: 24 step: 610, loss is 0.01596038043498993\n",
      "epoch: 24 step: 611, loss is 0.005725040566176176\n",
      "epoch: 24 step: 612, loss is 0.09166417270898819\n",
      "epoch: 24 step: 613, loss is 0.012150445021688938\n",
      "epoch: 24 step: 614, loss is 0.01500686351209879\n",
      "epoch: 24 step: 615, loss is 0.010208280757069588\n",
      "epoch: 24 step: 616, loss is 0.008396390825510025\n",
      "epoch: 24 step: 617, loss is 0.009195524267852306\n",
      "epoch: 24 step: 618, loss is 0.007109384983778\n",
      "epoch: 24 step: 619, loss is 0.0363294780254364\n",
      "epoch: 24 step: 620, loss is 0.06782948970794678\n",
      "epoch: 24 step: 621, loss is 0.0013285824097692966\n",
      "epoch: 24 step: 622, loss is 0.021660421043634415\n",
      "epoch: 24 step: 623, loss is 0.0039259688928723335\n",
      "epoch: 24 step: 624, loss is 0.005075258668512106\n",
      "epoch: 24 step: 625, loss is 0.09390967339277267\n",
      "epoch: 24 step: 626, loss is 0.0055670952424407005\n",
      "epoch: 24 step: 627, loss is 0.012214262038469315\n",
      "epoch: 24 step: 628, loss is 0.0016315561952069402\n",
      "epoch: 24 step: 629, loss is 0.0170438215136528\n",
      "epoch: 24 step: 630, loss is 0.03662861883640289\n",
      "epoch: 24 step: 631, loss is 0.05107967182993889\n",
      "epoch: 24 step: 632, loss is 0.003402512287721038\n",
      "epoch: 24 step: 633, loss is 0.0019310201751068234\n",
      "epoch: 24 step: 634, loss is 0.002883725566789508\n",
      "epoch: 24 step: 635, loss is 0.0970694050192833\n",
      "epoch: 24 step: 636, loss is 0.010761484503746033\n",
      "epoch: 24 step: 637, loss is 0.02016741968691349\n",
      "epoch: 24 step: 638, loss is 0.0518094003200531\n",
      "epoch: 24 step: 639, loss is 0.021370146423578262\n",
      "epoch: 24 step: 640, loss is 0.0050675817765295506\n",
      "epoch: 24 step: 641, loss is 0.013976502232253551\n",
      "epoch: 24 step: 642, loss is 0.026298508048057556\n",
      "epoch: 24 step: 643, loss is 0.0011152013903483748\n",
      "epoch: 24 step: 644, loss is 0.021094953641295433\n",
      "epoch: 24 step: 645, loss is 0.0012046522460877895\n",
      "epoch: 24 step: 646, loss is 0.0019721658900380135\n",
      "epoch: 24 step: 647, loss is 0.015931619331240654\n",
      "epoch: 24 step: 648, loss is 0.005653075408190489\n",
      "epoch: 24 step: 649, loss is 0.03250245749950409\n",
      "epoch: 24 step: 650, loss is 0.002797868335619569\n",
      "epoch: 24 step: 651, loss is 0.00242230505682528\n",
      "epoch: 24 step: 652, loss is 0.015173567458987236\n",
      "epoch: 24 step: 653, loss is 0.06783011555671692\n",
      "epoch: 24 step: 654, loss is 0.003979479894042015\n",
      "epoch: 24 step: 655, loss is 0.014513880014419556\n",
      "epoch: 24 step: 656, loss is 0.00597324687987566\n",
      "epoch: 24 step: 657, loss is 0.026633335277438164\n",
      "epoch: 24 step: 658, loss is 0.004644442815333605\n",
      "epoch: 24 step: 659, loss is 0.018831860274076462\n",
      "epoch: 24 step: 660, loss is 0.032856058329343796\n",
      "epoch: 24 step: 661, loss is 0.015713350847363472\n",
      "epoch: 24 step: 662, loss is 0.0022516159806400537\n",
      "epoch: 24 step: 663, loss is 0.0035910496953874826\n",
      "epoch: 24 step: 664, loss is 0.011535024270415306\n",
      "epoch: 24 step: 665, loss is 0.00043813700904138386\n",
      "epoch: 24 step: 666, loss is 0.010813946835696697\n",
      "epoch: 24 step: 667, loss is 0.0280317235738039\n",
      "epoch: 24 step: 668, loss is 0.03987182676792145\n",
      "epoch: 24 step: 669, loss is 9.801034320844337e-05\n",
      "epoch: 24 step: 670, loss is 0.010472921654582024\n",
      "epoch: 24 step: 671, loss is 0.01102941669523716\n",
      "epoch: 24 step: 672, loss is 0.0011780300410464406\n",
      "epoch: 24 step: 673, loss is 0.003610922023653984\n",
      "epoch: 24 step: 674, loss is 0.010406257584691048\n",
      "epoch: 24 step: 675, loss is 0.058641817420721054\n",
      "epoch: 24 step: 676, loss is 0.002471123356372118\n",
      "epoch: 24 step: 677, loss is 0.02505926974117756\n",
      "epoch: 24 step: 678, loss is 0.0023204388562589884\n",
      "epoch: 24 step: 679, loss is 0.08705615997314453\n",
      "epoch: 24 step: 680, loss is 0.13411648571491241\n",
      "epoch: 24 step: 681, loss is 0.0028066937811672688\n",
      "epoch: 24 step: 682, loss is 0.006182011682540178\n",
      "epoch: 24 step: 683, loss is 0.0440574586391449\n",
      "epoch: 24 step: 684, loss is 0.03340184688568115\n",
      "epoch: 24 step: 685, loss is 0.0006129543180577457\n",
      "epoch: 24 step: 686, loss is 0.004651966039091349\n",
      "epoch: 24 step: 687, loss is 0.002133907051756978\n",
      "epoch: 24 step: 688, loss is 0.0074266898445785046\n",
      "epoch: 24 step: 689, loss is 0.0027325947303324938\n",
      "epoch: 24 step: 690, loss is 0.00215961248613894\n",
      "epoch: 24 step: 691, loss is 0.0011962641729041934\n",
      "epoch: 24 step: 692, loss is 0.060982219874858856\n",
      "epoch: 24 step: 693, loss is 0.07974912971258163\n",
      "epoch: 24 step: 694, loss is 0.0029103290289640427\n",
      "epoch: 24 step: 695, loss is 0.08222176134586334\n",
      "epoch: 24 step: 696, loss is 0.01660940796136856\n",
      "epoch: 24 step: 697, loss is 0.0066326637752354145\n",
      "epoch: 24 step: 698, loss is 0.003015305148437619\n",
      "epoch: 24 step: 699, loss is 0.023616844788193703\n",
      "epoch: 24 step: 700, loss is 0.02710544690489769\n",
      "epoch: 24 step: 701, loss is 0.020080508664250374\n",
      "epoch: 24 step: 702, loss is 0.0014956668019294739\n",
      "epoch: 24 step: 703, loss is 0.0021313256584107876\n",
      "epoch: 24 step: 704, loss is 0.0028158389031887054\n",
      "epoch: 24 step: 705, loss is 0.0007701574941165745\n",
      "epoch: 24 step: 706, loss is 0.0026901504024863243\n",
      "epoch: 24 step: 707, loss is 0.015165434218943119\n",
      "epoch: 24 step: 708, loss is 0.0008315873565152287\n",
      "epoch: 24 step: 709, loss is 0.007910385727882385\n",
      "epoch: 24 step: 710, loss is 0.04959064722061157\n",
      "epoch: 24 step: 711, loss is 0.08533827215433121\n",
      "epoch: 24 step: 712, loss is 0.0012680755462497473\n",
      "epoch: 24 step: 713, loss is 0.004896451719105244\n",
      "epoch: 24 step: 714, loss is 0.016812382265925407\n",
      "epoch: 24 step: 715, loss is 0.013328654691576958\n",
      "epoch: 24 step: 716, loss is 0.000875637459103018\n",
      "epoch: 24 step: 717, loss is 0.06901693344116211\n",
      "epoch: 24 step: 718, loss is 0.029440533369779587\n",
      "epoch: 24 step: 719, loss is 0.017806241288781166\n",
      "epoch: 24 step: 720, loss is 0.04321223497390747\n",
      "epoch: 24 step: 721, loss is 0.05022658407688141\n",
      "epoch: 24 step: 722, loss is 0.0020639735739678144\n",
      "epoch: 24 step: 723, loss is 0.029383527114987373\n",
      "epoch: 24 step: 724, loss is 8.889280434232205e-05\n",
      "epoch: 24 step: 725, loss is 0.06826750934123993\n",
      "epoch: 24 step: 726, loss is 0.0321771502494812\n",
      "epoch: 24 step: 727, loss is 0.03623276203870773\n",
      "epoch: 24 step: 728, loss is 0.017073528841137886\n",
      "epoch: 24 step: 729, loss is 0.022641953080892563\n",
      "epoch: 24 step: 730, loss is 0.012578052468597889\n",
      "epoch: 24 step: 731, loss is 0.015116897411644459\n",
      "epoch: 24 step: 732, loss is 0.016111135482788086\n",
      "epoch: 24 step: 733, loss is 0.003619099734351039\n",
      "epoch: 24 step: 734, loss is 0.03509869426488876\n",
      "epoch: 24 step: 735, loss is 0.02207290567457676\n",
      "epoch: 24 step: 736, loss is 0.0023462402168661356\n",
      "epoch: 24 step: 737, loss is 0.002323069144040346\n",
      "epoch: 24 step: 738, loss is 0.019475487992167473\n",
      "epoch: 24 step: 739, loss is 0.0005131730576977134\n",
      "epoch: 24 step: 740, loss is 0.004445737227797508\n",
      "epoch: 24 step: 741, loss is 0.11015007644891739\n",
      "epoch: 24 step: 742, loss is 0.001826011692173779\n",
      "epoch: 24 step: 743, loss is 0.002650403417646885\n",
      "epoch: 24 step: 744, loss is 0.013051219284534454\n",
      "epoch: 24 step: 745, loss is 0.0036694821901619434\n",
      "epoch: 24 step: 746, loss is 0.0031498041935265064\n",
      "epoch: 24 step: 747, loss is 0.013118070550262928\n",
      "epoch: 24 step: 748, loss is 0.07061553001403809\n",
      "epoch: 24 step: 749, loss is 0.006904831621795893\n",
      "epoch: 24 step: 750, loss is 0.03613460063934326\n",
      "epoch: 24 step: 751, loss is 0.006567421834915876\n",
      "epoch: 24 step: 752, loss is 0.0013219149550423026\n",
      "epoch: 24 step: 753, loss is 0.0009895437397062778\n",
      "epoch: 24 step: 754, loss is 0.05282983183860779\n",
      "epoch: 24 step: 755, loss is 0.07841354608535767\n",
      "epoch: 24 step: 756, loss is 0.013246605172753334\n",
      "epoch: 24 step: 757, loss is 0.0007376624271273613\n",
      "epoch: 24 step: 758, loss is 0.0485071986913681\n",
      "epoch: 24 step: 759, loss is 0.012093630619347095\n",
      "epoch: 24 step: 760, loss is 0.06028742715716362\n",
      "epoch: 24 step: 761, loss is 0.004678091499954462\n",
      "epoch: 24 step: 762, loss is 0.0367688424885273\n",
      "epoch: 24 step: 763, loss is 0.0059337737038731575\n",
      "epoch: 24 step: 764, loss is 0.002383654937148094\n",
      "epoch: 24 step: 765, loss is 0.09970367699861526\n",
      "epoch: 24 step: 766, loss is 0.025880109518766403\n",
      "epoch: 24 step: 767, loss is 0.018569830805063248\n",
      "epoch: 24 step: 768, loss is 0.006659762933850288\n",
      "epoch: 24 step: 769, loss is 0.035754285752773285\n",
      "epoch: 24 step: 770, loss is 0.014386797323822975\n",
      "epoch: 24 step: 771, loss is 0.0010877640452235937\n",
      "epoch: 24 step: 772, loss is 0.013316896744072437\n",
      "epoch: 24 step: 773, loss is 0.037449587136507034\n",
      "epoch: 24 step: 774, loss is 0.005486777983605862\n",
      "epoch: 24 step: 775, loss is 0.010044324211776257\n",
      "epoch: 24 step: 776, loss is 0.0013933039736002684\n",
      "epoch: 24 step: 777, loss is 0.00035743630724027753\n",
      "epoch: 24 step: 778, loss is 0.010119213722646236\n",
      "epoch: 24 step: 779, loss is 0.0008326050592586398\n",
      "epoch: 24 step: 780, loss is 0.04517769813537598\n",
      "epoch: 24 step: 781, loss is 0.058299075812101364\n",
      "epoch: 24 step: 782, loss is 0.01689778082072735\n",
      "epoch: 24 step: 783, loss is 0.0059177759103477\n",
      "epoch: 24 step: 784, loss is 0.051815200597047806\n",
      "epoch: 24 step: 785, loss is 0.07441528141498566\n",
      "epoch: 24 step: 786, loss is 0.00825640931725502\n",
      "epoch: 24 step: 787, loss is 0.09468908607959747\n",
      "epoch: 24 step: 788, loss is 0.000323647225741297\n",
      "epoch: 24 step: 789, loss is 0.00015865695604588836\n",
      "epoch: 24 step: 790, loss is 0.025090215727686882\n",
      "epoch: 24 step: 791, loss is 0.009697397239506245\n",
      "epoch: 24 step: 792, loss is 0.08052154630422592\n",
      "epoch: 24 step: 793, loss is 0.030893484130501747\n",
      "epoch: 24 step: 794, loss is 0.06042284145951271\n",
      "epoch: 24 step: 795, loss is 0.0008193584508262575\n",
      "epoch: 24 step: 796, loss is 0.055690351873636246\n",
      "epoch: 24 step: 797, loss is 0.0035956825595349073\n",
      "epoch: 24 step: 798, loss is 0.0015257718041539192\n",
      "epoch: 24 step: 799, loss is 0.0028824086766690016\n",
      "epoch: 24 step: 800, loss is 0.009770482778549194\n",
      "epoch: 24 step: 801, loss is 0.02432115375995636\n",
      "epoch: 24 step: 802, loss is 0.016688179224729538\n",
      "epoch: 24 step: 803, loss is 0.016820015385746956\n",
      "epoch: 24 step: 804, loss is 0.0007097448105923831\n",
      "epoch: 24 step: 805, loss is 0.061121731996536255\n",
      "epoch: 24 step: 806, loss is 0.04623587802052498\n",
      "epoch: 24 step: 807, loss is 0.016477182507514954\n",
      "epoch: 24 step: 808, loss is 0.03835839405655861\n",
      "epoch: 24 step: 809, loss is 0.012191149406135082\n",
      "epoch: 24 step: 810, loss is 0.0791628286242485\n",
      "epoch: 24 step: 811, loss is 0.10423360019922256\n",
      "epoch: 24 step: 812, loss is 0.011212182231247425\n",
      "epoch: 24 step: 813, loss is 0.00017012175521813333\n",
      "epoch: 24 step: 814, loss is 0.004241510294377804\n",
      "epoch: 24 step: 815, loss is 0.005674262065440416\n",
      "epoch: 24 step: 816, loss is 0.0023363784421235323\n",
      "epoch: 24 step: 817, loss is 1.6252701243502088e-05\n",
      "epoch: 24 step: 818, loss is 0.006222207099199295\n",
      "epoch: 24 step: 819, loss is 0.10217154026031494\n",
      "epoch: 24 step: 820, loss is 0.0027154688723385334\n",
      "epoch: 24 step: 821, loss is 0.0045959968119859695\n",
      "epoch: 24 step: 822, loss is 0.022293994203209877\n",
      "epoch: 24 step: 823, loss is 0.011716400273144245\n",
      "epoch: 24 step: 824, loss is 0.0424172505736351\n",
      "epoch: 24 step: 825, loss is 0.0066886297427117825\n",
      "epoch: 24 step: 826, loss is 0.0044794147834181786\n",
      "epoch: 24 step: 827, loss is 0.0008241096511483192\n",
      "epoch: 24 step: 828, loss is 0.01469787023961544\n",
      "epoch: 24 step: 829, loss is 0.00680941017344594\n",
      "epoch: 24 step: 830, loss is 0.0015938098076730967\n",
      "epoch: 24 step: 831, loss is 0.0002159870055038482\n",
      "epoch: 24 step: 832, loss is 0.05233713984489441\n",
      "epoch: 24 step: 833, loss is 0.006506048142910004\n",
      "epoch: 24 step: 834, loss is 0.014879978261888027\n",
      "epoch: 24 step: 835, loss is 0.052440740168094635\n",
      "epoch: 24 step: 836, loss is 0.0015683722449466586\n",
      "epoch: 24 step: 837, loss is 0.0005668594967573881\n",
      "epoch: 24 step: 838, loss is 0.009074023924767971\n",
      "epoch: 24 step: 839, loss is 0.0004352178075350821\n",
      "epoch: 24 step: 840, loss is 0.0012916030827909708\n",
      "epoch: 24 step: 841, loss is 0.002209086436778307\n",
      "epoch: 24 step: 842, loss is 0.002572113648056984\n",
      "epoch: 24 step: 843, loss is 0.08208859711885452\n",
      "epoch: 24 step: 844, loss is 0.001900760573334992\n",
      "epoch: 24 step: 845, loss is 0.056129977107048035\n",
      "epoch: 24 step: 846, loss is 0.1517529934644699\n",
      "epoch: 24 step: 847, loss is 0.004156073555350304\n",
      "epoch: 24 step: 848, loss is 0.11826957017183304\n",
      "epoch: 24 step: 849, loss is 0.014122203923761845\n",
      "epoch: 24 step: 850, loss is 0.007261646445840597\n",
      "epoch: 24 step: 851, loss is 0.0007015113369561732\n",
      "epoch: 24 step: 852, loss is 0.012650938704609871\n",
      "epoch: 24 step: 853, loss is 0.008723827078938484\n",
      "epoch: 24 step: 854, loss is 0.0021221358329057693\n",
      "epoch: 24 step: 855, loss is 0.01253692340105772\n",
      "epoch: 24 step: 856, loss is 0.04646352678537369\n",
      "epoch: 24 step: 857, loss is 0.02525043860077858\n",
      "epoch: 24 step: 858, loss is 0.017631663009524345\n",
      "epoch: 24 step: 859, loss is 0.03729819506406784\n",
      "epoch: 24 step: 860, loss is 0.0016135871410369873\n",
      "epoch: 24 step: 861, loss is 0.0022667013108730316\n",
      "epoch: 24 step: 862, loss is 0.02616840787231922\n",
      "epoch: 24 step: 863, loss is 0.005580511875450611\n",
      "epoch: 24 step: 864, loss is 0.1239001676440239\n",
      "epoch: 24 step: 865, loss is 0.015046151354908943\n",
      "epoch: 24 step: 866, loss is 0.01142482552677393\n",
      "epoch: 24 step: 867, loss is 0.06359133124351501\n",
      "epoch: 24 step: 868, loss is 0.020282143726944923\n",
      "epoch: 24 step: 869, loss is 0.007786184083670378\n",
      "epoch: 24 step: 870, loss is 0.0033590977545827627\n",
      "epoch: 24 step: 871, loss is 0.0043725017458200455\n",
      "epoch: 24 step: 872, loss is 0.0013825896894559264\n",
      "epoch: 24 step: 873, loss is 0.011686752550303936\n",
      "epoch: 24 step: 874, loss is 0.012514792382717133\n",
      "epoch: 24 step: 875, loss is 0.015460630878806114\n",
      "epoch: 24 step: 876, loss is 0.004511243663728237\n",
      "epoch: 24 step: 877, loss is 0.021846378222107887\n",
      "epoch: 24 step: 878, loss is 0.0005027111619710922\n",
      "epoch: 24 step: 879, loss is 0.014234058558940887\n",
      "epoch: 24 step: 880, loss is 0.013747450895607471\n",
      "epoch: 24 step: 881, loss is 0.007805859204381704\n",
      "epoch: 24 step: 882, loss is 0.004201217088848352\n",
      "epoch: 24 step: 883, loss is 0.0009424350573681295\n",
      "epoch: 24 step: 884, loss is 0.03311534598469734\n",
      "epoch: 24 step: 885, loss is 0.01618856005370617\n",
      "epoch: 24 step: 886, loss is 0.003118464257568121\n",
      "epoch: 24 step: 887, loss is 0.01497762743383646\n",
      "epoch: 24 step: 888, loss is 0.04665830731391907\n",
      "epoch: 24 step: 889, loss is 6.6689848608803e-05\n",
      "epoch: 24 step: 890, loss is 0.007011030800640583\n",
      "epoch: 24 step: 891, loss is 0.0005920804687775671\n",
      "epoch: 24 step: 892, loss is 0.01420594286173582\n",
      "epoch: 24 step: 893, loss is 0.010215832851827145\n",
      "epoch: 24 step: 894, loss is 0.0007025804952718318\n",
      "epoch: 24 step: 895, loss is 0.0016994676552712917\n",
      "epoch: 24 step: 896, loss is 0.004298337269574404\n",
      "epoch: 24 step: 897, loss is 0.1413475126028061\n",
      "epoch: 24 step: 898, loss is 0.04972585290670395\n",
      "epoch: 24 step: 899, loss is 0.008749905042350292\n",
      "epoch: 24 step: 900, loss is 0.015634827315807343\n",
      "epoch: 24 step: 901, loss is 0.0004887197283096611\n",
      "epoch: 24 step: 902, loss is 0.0034002375323325396\n",
      "epoch: 24 step: 903, loss is 0.00043850569636560977\n",
      "epoch: 24 step: 904, loss is 0.00011724701471393928\n",
      "epoch: 24 step: 905, loss is 0.03086243011057377\n",
      "epoch: 24 step: 906, loss is 0.00783943198621273\n",
      "epoch: 24 step: 907, loss is 0.06191117689013481\n",
      "epoch: 24 step: 908, loss is 0.0012174536241218448\n",
      "epoch: 24 step: 909, loss is 0.0007698583067394793\n",
      "epoch: 24 step: 910, loss is 0.0019287047907710075\n",
      "epoch: 24 step: 911, loss is 0.003701188135892153\n",
      "epoch: 24 step: 912, loss is 0.0020903011318296194\n",
      "epoch: 24 step: 913, loss is 0.0003043979231733829\n",
      "epoch: 24 step: 914, loss is 0.026063114404678345\n",
      "epoch: 24 step: 915, loss is 0.005678907502442598\n",
      "epoch: 24 step: 916, loss is 0.0020059726666659117\n",
      "epoch: 24 step: 917, loss is 0.008656646125018597\n",
      "epoch: 24 step: 918, loss is 0.0018572357948869467\n",
      "epoch: 24 step: 919, loss is 0.0035590140614658594\n",
      "epoch: 24 step: 920, loss is 0.009944061748683453\n",
      "epoch: 24 step: 921, loss is 0.020500237122178078\n",
      "epoch: 24 step: 922, loss is 0.00034962588688358665\n",
      "epoch: 24 step: 923, loss is 0.0017121699638664722\n",
      "epoch: 24 step: 924, loss is 0.09865569323301315\n",
      "epoch: 24 step: 925, loss is 0.00030223323847167194\n",
      "epoch: 24 step: 926, loss is 0.027870437130331993\n",
      "epoch: 24 step: 927, loss is 0.0026827885303646326\n",
      "epoch: 24 step: 928, loss is 0.036836761981248856\n",
      "epoch: 24 step: 929, loss is 0.033552832901477814\n",
      "epoch: 24 step: 930, loss is 0.00011015753261744976\n",
      "epoch: 24 step: 931, loss is 0.0019539224449545145\n",
      "epoch: 24 step: 932, loss is 0.0018020069692283869\n",
      "epoch: 24 step: 933, loss is 0.01432617288082838\n",
      "epoch: 24 step: 934, loss is 0.0201598908752203\n",
      "epoch: 24 step: 935, loss is 0.005123227369040251\n",
      "epoch: 24 step: 936, loss is 0.00024743861285969615\n",
      "epoch: 24 step: 937, loss is 0.00018679133791010827\n",
      "epoch: 25 step: 1, loss is 0.0021768989972770214\n",
      "epoch: 25 step: 2, loss is 0.054629478603601456\n",
      "epoch: 25 step: 3, loss is 0.0002839118242263794\n",
      "epoch: 25 step: 4, loss is 0.0031182579696178436\n",
      "epoch: 25 step: 5, loss is 0.0003436892875470221\n",
      "epoch: 25 step: 6, loss is 0.0010858811438083649\n",
      "epoch: 25 step: 7, loss is 0.0064566126093268394\n",
      "epoch: 25 step: 8, loss is 0.0028733660001307726\n",
      "epoch: 25 step: 9, loss is 0.0007432182319462299\n",
      "epoch: 25 step: 10, loss is 0.0011800231877714396\n",
      "epoch: 25 step: 11, loss is 0.00028906844090670347\n",
      "epoch: 25 step: 12, loss is 0.00120967673137784\n",
      "epoch: 25 step: 13, loss is 0.008132153190672398\n",
      "epoch: 25 step: 14, loss is 0.008943017572164536\n",
      "epoch: 25 step: 15, loss is 0.00893242284655571\n",
      "epoch: 25 step: 16, loss is 0.005973215214908123\n",
      "epoch: 25 step: 17, loss is 0.003707422874867916\n",
      "epoch: 25 step: 18, loss is 0.0003907119098585099\n",
      "epoch: 25 step: 19, loss is 0.013145482167601585\n",
      "epoch: 25 step: 20, loss is 0.002369342837482691\n",
      "epoch: 25 step: 21, loss is 0.0003793089999817312\n",
      "epoch: 25 step: 22, loss is 0.035817958414554596\n",
      "epoch: 25 step: 23, loss is 0.002735051792114973\n",
      "epoch: 25 step: 24, loss is 0.0035775317810475826\n",
      "epoch: 25 step: 25, loss is 0.00021032622316852212\n",
      "epoch: 25 step: 26, loss is 7.338668365264311e-05\n",
      "epoch: 25 step: 27, loss is 0.014502006582915783\n",
      "epoch: 25 step: 28, loss is 0.0012345927534624934\n",
      "epoch: 25 step: 29, loss is 0.007320428267121315\n",
      "epoch: 25 step: 30, loss is 0.0028002732433378696\n",
      "epoch: 25 step: 31, loss is 0.01830553077161312\n",
      "epoch: 25 step: 32, loss is 0.0012697819620370865\n",
      "epoch: 25 step: 33, loss is 0.0015650365967303514\n",
      "epoch: 25 step: 34, loss is 0.05466507375240326\n",
      "epoch: 25 step: 35, loss is 0.00018758671649266034\n",
      "epoch: 25 step: 36, loss is 0.0032034285832196474\n",
      "epoch: 25 step: 37, loss is 0.0020831692963838577\n",
      "epoch: 25 step: 38, loss is 0.0006333089550025761\n",
      "epoch: 25 step: 39, loss is 0.0001123462279792875\n",
      "epoch: 25 step: 40, loss is 0.0033156624995172024\n",
      "epoch: 25 step: 41, loss is 0.0018777749501168728\n",
      "epoch: 25 step: 42, loss is 0.0010310219367966056\n",
      "epoch: 25 step: 43, loss is 0.005700303241610527\n",
      "epoch: 25 step: 44, loss is 0.0003541230980772525\n",
      "epoch: 25 step: 45, loss is 0.00015956278366502374\n",
      "epoch: 25 step: 46, loss is 0.009290640242397785\n",
      "epoch: 25 step: 47, loss is 0.03014710359275341\n",
      "epoch: 25 step: 48, loss is 0.012163423001766205\n",
      "epoch: 25 step: 49, loss is 0.0020420157816261053\n",
      "epoch: 25 step: 50, loss is 0.0022332915104925632\n",
      "epoch: 25 step: 51, loss is 0.0010605996940284967\n",
      "epoch: 25 step: 52, loss is 0.016587940976023674\n",
      "epoch: 25 step: 53, loss is 0.001445017522200942\n",
      "epoch: 25 step: 54, loss is 0.0025673455093055964\n",
      "epoch: 25 step: 55, loss is 0.040066108107566833\n",
      "epoch: 25 step: 56, loss is 0.000995765090920031\n",
      "epoch: 25 step: 57, loss is 0.0002766649704426527\n",
      "epoch: 25 step: 58, loss is 0.013088246807456017\n",
      "epoch: 25 step: 59, loss is 0.04029654338955879\n",
      "epoch: 25 step: 60, loss is 0.0034439440350979567\n",
      "epoch: 25 step: 61, loss is 0.07278730720281601\n",
      "epoch: 25 step: 62, loss is 0.0016017454909160733\n",
      "epoch: 25 step: 63, loss is 0.003963169176131487\n",
      "epoch: 25 step: 64, loss is 0.0050778514705598354\n",
      "epoch: 25 step: 65, loss is 0.004030260723084211\n",
      "epoch: 25 step: 66, loss is 0.00021632187417708337\n",
      "epoch: 25 step: 67, loss is 0.0018412823555991054\n",
      "epoch: 25 step: 68, loss is 0.005636705085635185\n",
      "epoch: 25 step: 69, loss is 0.001852546352893114\n",
      "epoch: 25 step: 70, loss is 0.10118679702281952\n",
      "epoch: 25 step: 71, loss is 0.0011285938089713454\n",
      "epoch: 25 step: 72, loss is 0.0006510532693937421\n",
      "epoch: 25 step: 73, loss is 0.004813859239220619\n",
      "epoch: 25 step: 74, loss is 0.014336256310343742\n",
      "epoch: 25 step: 75, loss is 0.00846406165510416\n",
      "epoch: 25 step: 76, loss is 0.003000010270625353\n",
      "epoch: 25 step: 77, loss is 0.02527763694524765\n",
      "epoch: 25 step: 78, loss is 0.0036016914527863264\n",
      "epoch: 25 step: 79, loss is 0.001096144551411271\n",
      "epoch: 25 step: 80, loss is 0.00096407305682078\n",
      "epoch: 25 step: 81, loss is 0.010102310217916965\n",
      "epoch: 25 step: 82, loss is 0.00590138603001833\n",
      "epoch: 25 step: 83, loss is 0.0500577837228775\n",
      "epoch: 25 step: 84, loss is 0.001556583447381854\n",
      "epoch: 25 step: 85, loss is 0.026395292952656746\n",
      "epoch: 25 step: 86, loss is 0.0013248049654066563\n",
      "epoch: 25 step: 87, loss is 0.004036983009427786\n",
      "epoch: 25 step: 88, loss is 5.9471025451784953e-05\n",
      "epoch: 25 step: 89, loss is 0.0053711156360805035\n",
      "epoch: 25 step: 90, loss is 0.00013272298383526504\n",
      "epoch: 25 step: 91, loss is 0.0007607848383486271\n",
      "epoch: 25 step: 92, loss is 0.03134210780262947\n",
      "epoch: 25 step: 93, loss is 0.001970491372048855\n",
      "epoch: 25 step: 94, loss is 0.0013531516306102276\n",
      "epoch: 25 step: 95, loss is 0.015911903232336044\n",
      "epoch: 25 step: 96, loss is 0.006302666384726763\n",
      "epoch: 25 step: 97, loss is 0.002788658021017909\n",
      "epoch: 25 step: 98, loss is 0.002140544820576906\n",
      "epoch: 25 step: 99, loss is 0.0018160154577344656\n",
      "epoch: 25 step: 100, loss is 0.008250316604971886\n",
      "epoch: 25 step: 101, loss is 0.002941936021670699\n",
      "epoch: 25 step: 102, loss is 0.0010814836714416742\n",
      "epoch: 25 step: 103, loss is 0.00022559137141797692\n",
      "epoch: 25 step: 104, loss is 0.020948316901922226\n",
      "epoch: 25 step: 105, loss is 0.008562578819692135\n",
      "epoch: 25 step: 106, loss is 0.0038046010304242373\n",
      "epoch: 25 step: 107, loss is 0.028103530406951904\n",
      "epoch: 25 step: 108, loss is 0.008894202299416065\n",
      "epoch: 25 step: 109, loss is 0.0018384044524282217\n",
      "epoch: 25 step: 110, loss is 0.0002667405642569065\n",
      "epoch: 25 step: 111, loss is 0.030488483607769012\n",
      "epoch: 25 step: 112, loss is 0.0016294252127408981\n",
      "epoch: 25 step: 113, loss is 0.001174539909698069\n",
      "epoch: 25 step: 114, loss is 0.006131647154688835\n",
      "epoch: 25 step: 115, loss is 0.0026371364947408438\n",
      "epoch: 25 step: 116, loss is 0.009192223660647869\n",
      "epoch: 25 step: 117, loss is 0.000715401372872293\n",
      "epoch: 25 step: 118, loss is 0.0021260608918964863\n",
      "epoch: 25 step: 119, loss is 0.00033012358471751213\n",
      "epoch: 25 step: 120, loss is 0.008852547965943813\n",
      "epoch: 25 step: 121, loss is 0.0005349597777239978\n",
      "epoch: 25 step: 122, loss is 0.0070772934705019\n",
      "epoch: 25 step: 123, loss is 0.00029491892200894654\n",
      "epoch: 25 step: 124, loss is 0.0018191513372585177\n",
      "epoch: 25 step: 125, loss is 0.008751471526920795\n",
      "epoch: 25 step: 126, loss is 0.00017116811068262905\n",
      "epoch: 25 step: 127, loss is 0.00010062066576210782\n",
      "epoch: 25 step: 128, loss is 0.0008991993963718414\n",
      "epoch: 25 step: 129, loss is 0.10030131787061691\n",
      "epoch: 25 step: 130, loss is 0.019791249185800552\n",
      "epoch: 25 step: 131, loss is 0.008531111292541027\n",
      "epoch: 25 step: 132, loss is 0.047725576907396317\n",
      "epoch: 25 step: 133, loss is 0.032892610877752304\n",
      "epoch: 25 step: 134, loss is 0.0018886488396674395\n",
      "epoch: 25 step: 135, loss is 4.019883999717422e-05\n",
      "epoch: 25 step: 136, loss is 0.003464499721303582\n",
      "epoch: 25 step: 137, loss is 3.558715616236441e-05\n",
      "epoch: 25 step: 138, loss is 6.46967237116769e-05\n",
      "epoch: 25 step: 139, loss is 0.05205051600933075\n",
      "epoch: 25 step: 140, loss is 0.001921525807119906\n",
      "epoch: 25 step: 141, loss is 0.00014544652367476374\n",
      "epoch: 25 step: 142, loss is 0.001400459324941039\n",
      "epoch: 25 step: 143, loss is 0.0002325525420019403\n",
      "epoch: 25 step: 144, loss is 0.003910650033503771\n",
      "epoch: 25 step: 145, loss is 0.001734095043502748\n",
      "epoch: 25 step: 146, loss is 0.07012270390987396\n",
      "epoch: 25 step: 147, loss is 0.002006793161854148\n",
      "epoch: 25 step: 148, loss is 0.037498824298381805\n",
      "epoch: 25 step: 149, loss is 0.06529906392097473\n",
      "epoch: 25 step: 150, loss is 0.07820195704698563\n",
      "epoch: 25 step: 151, loss is 0.0006960909231565893\n",
      "epoch: 25 step: 152, loss is 0.0006850717472843826\n",
      "epoch: 25 step: 153, loss is 0.0016162142856046557\n",
      "epoch: 25 step: 154, loss is 0.028477802872657776\n",
      "epoch: 25 step: 155, loss is 0.0037472874391824007\n",
      "epoch: 25 step: 156, loss is 0.0024544273037463427\n",
      "epoch: 25 step: 157, loss is 0.01519683375954628\n",
      "epoch: 25 step: 158, loss is 0.045007623732089996\n",
      "epoch: 25 step: 159, loss is 0.0040163276717066765\n",
      "epoch: 25 step: 160, loss is 0.04386407881975174\n",
      "epoch: 25 step: 161, loss is 0.011648951098322868\n",
      "epoch: 25 step: 162, loss is 0.06241035833954811\n",
      "epoch: 25 step: 163, loss is 0.04453134536743164\n",
      "epoch: 25 step: 164, loss is 0.002562905428931117\n",
      "epoch: 25 step: 165, loss is 0.0007168863667175174\n",
      "epoch: 25 step: 166, loss is 0.0008577138651162386\n",
      "epoch: 25 step: 167, loss is 0.00013787746138405055\n",
      "epoch: 25 step: 168, loss is 0.001015913556329906\n",
      "epoch: 25 step: 169, loss is 0.005231468938291073\n",
      "epoch: 25 step: 170, loss is 0.00036505592288449407\n",
      "epoch: 25 step: 171, loss is 0.000580908905249089\n",
      "epoch: 25 step: 172, loss is 0.0017336151795461774\n",
      "epoch: 25 step: 173, loss is 0.00461882958188653\n",
      "epoch: 25 step: 174, loss is 0.017911389470100403\n",
      "epoch: 25 step: 175, loss is 0.01583418808877468\n",
      "epoch: 25 step: 176, loss is 0.004422862082719803\n",
      "epoch: 25 step: 177, loss is 0.0024010983761399984\n",
      "epoch: 25 step: 178, loss is 0.002873079851269722\n",
      "epoch: 25 step: 179, loss is 0.00039996288251131773\n",
      "epoch: 25 step: 180, loss is 0.019860006868839264\n",
      "epoch: 25 step: 181, loss is 0.0036530497018247843\n",
      "epoch: 25 step: 182, loss is 0.014658586122095585\n",
      "epoch: 25 step: 183, loss is 0.02638045698404312\n",
      "epoch: 25 step: 184, loss is 0.0201683659106493\n",
      "epoch: 25 step: 185, loss is 0.05392081290483475\n",
      "epoch: 25 step: 186, loss is 0.015455433167517185\n",
      "epoch: 25 step: 187, loss is 0.00020055603818036616\n",
      "epoch: 25 step: 188, loss is 0.0029700491577386856\n",
      "epoch: 25 step: 189, loss is 0.0031852079555392265\n",
      "epoch: 25 step: 190, loss is 0.00941464863717556\n",
      "epoch: 25 step: 191, loss is 0.0061738090589642525\n",
      "epoch: 25 step: 192, loss is 0.0037422124296426773\n",
      "epoch: 25 step: 193, loss is 0.0005106762982904911\n",
      "epoch: 25 step: 194, loss is 0.01642041839659214\n",
      "epoch: 25 step: 195, loss is 0.0029715734999626875\n",
      "epoch: 25 step: 196, loss is 0.0009637834155000746\n",
      "epoch: 25 step: 197, loss is 0.011948809958994389\n",
      "epoch: 25 step: 198, loss is 0.02328222617506981\n",
      "epoch: 25 step: 199, loss is 0.018653731793165207\n",
      "epoch: 25 step: 200, loss is 0.004386995919048786\n",
      "epoch: 25 step: 201, loss is 0.010552040301263332\n",
      "epoch: 25 step: 202, loss is 0.001444240566343069\n",
      "epoch: 25 step: 203, loss is 4.392404298414476e-05\n",
      "epoch: 25 step: 204, loss is 0.00033786380663514137\n",
      "epoch: 25 step: 205, loss is 0.004245624411851168\n",
      "epoch: 25 step: 206, loss is 0.011463374830782413\n",
      "epoch: 25 step: 207, loss is 0.013804580084979534\n",
      "epoch: 25 step: 208, loss is 0.004240355454385281\n",
      "epoch: 25 step: 209, loss is 0.013403249904513359\n",
      "epoch: 25 step: 210, loss is 0.014262406155467033\n",
      "epoch: 25 step: 211, loss is 0.0011088073952123523\n",
      "epoch: 25 step: 212, loss is 0.00925878994166851\n",
      "epoch: 25 step: 213, loss is 0.06207434460520744\n",
      "epoch: 25 step: 214, loss is 0.001426597824320197\n",
      "epoch: 25 step: 215, loss is 0.02961043454706669\n",
      "epoch: 25 step: 216, loss is 0.038628797978162766\n",
      "epoch: 25 step: 217, loss is 0.0005493107601068914\n",
      "epoch: 25 step: 218, loss is 0.0016678903484717011\n",
      "epoch: 25 step: 219, loss is 0.0011534668738022447\n",
      "epoch: 25 step: 220, loss is 0.0701981633901596\n",
      "epoch: 25 step: 221, loss is 0.0034572535660117865\n",
      "epoch: 25 step: 222, loss is 0.010354316793382168\n",
      "epoch: 25 step: 223, loss is 0.007086966652423143\n",
      "epoch: 25 step: 224, loss is 0.0031790058128535748\n",
      "epoch: 25 step: 225, loss is 0.0032261903397738934\n",
      "epoch: 25 step: 226, loss is 0.004525819327682257\n",
      "epoch: 25 step: 227, loss is 0.004342453554272652\n",
      "epoch: 25 step: 228, loss is 0.018476003780961037\n",
      "epoch: 25 step: 229, loss is 0.0014095372753217816\n",
      "epoch: 25 step: 230, loss is 0.011970752850174904\n",
      "epoch: 25 step: 231, loss is 0.002951750997453928\n",
      "epoch: 25 step: 232, loss is 0.04110657796263695\n",
      "epoch: 25 step: 233, loss is 0.005411235149949789\n",
      "epoch: 25 step: 234, loss is 0.009364567697048187\n",
      "epoch: 25 step: 235, loss is 0.007645711302757263\n",
      "epoch: 25 step: 236, loss is 0.0015749478479847312\n",
      "epoch: 25 step: 237, loss is 0.013311030343174934\n",
      "epoch: 25 step: 238, loss is 0.0009951057145372033\n",
      "epoch: 25 step: 239, loss is 0.03285997360944748\n",
      "epoch: 25 step: 240, loss is 0.0037403597962111235\n",
      "epoch: 25 step: 241, loss is 0.002488394035026431\n",
      "epoch: 25 step: 242, loss is 0.000578358129132539\n",
      "epoch: 25 step: 243, loss is 0.0030948789790272713\n",
      "epoch: 25 step: 244, loss is 0.05165305733680725\n",
      "epoch: 25 step: 245, loss is 0.005036647897213697\n",
      "epoch: 25 step: 246, loss is 0.01630997098982334\n",
      "epoch: 25 step: 247, loss is 0.0008055482758209109\n",
      "epoch: 25 step: 248, loss is 0.0001619393879082054\n",
      "epoch: 25 step: 249, loss is 0.004861314315348864\n",
      "epoch: 25 step: 250, loss is 0.01208307035267353\n",
      "epoch: 25 step: 251, loss is 0.0010454774601384997\n",
      "epoch: 25 step: 252, loss is 0.0003933364059776068\n",
      "epoch: 25 step: 253, loss is 0.0005014525377191603\n",
      "epoch: 25 step: 254, loss is 0.0011733098654076457\n",
      "epoch: 25 step: 255, loss is 0.09080416709184647\n",
      "epoch: 25 step: 256, loss is 0.04130805283784866\n",
      "epoch: 25 step: 257, loss is 0.000662031292449683\n",
      "epoch: 25 step: 258, loss is 0.050321586430072784\n",
      "epoch: 25 step: 259, loss is 0.00010148977162316442\n",
      "epoch: 25 step: 260, loss is 0.0032149699982255697\n",
      "epoch: 25 step: 261, loss is 6.475989357568324e-05\n",
      "epoch: 25 step: 262, loss is 0.0037859315052628517\n",
      "epoch: 25 step: 263, loss is 0.00013416512229014188\n",
      "epoch: 25 step: 264, loss is 0.018580598756670952\n",
      "epoch: 25 step: 265, loss is 0.0005698184249922633\n",
      "epoch: 25 step: 266, loss is 0.00560558307915926\n",
      "epoch: 25 step: 267, loss is 0.002171983476728201\n",
      "epoch: 25 step: 268, loss is 0.0019370901864022017\n",
      "epoch: 25 step: 269, loss is 0.008234281092882156\n",
      "epoch: 25 step: 270, loss is 0.004275249782949686\n",
      "epoch: 25 step: 271, loss is 0.00041478779166936874\n",
      "epoch: 25 step: 272, loss is 0.002594206016510725\n",
      "epoch: 25 step: 273, loss is 0.028276415541768074\n",
      "epoch: 25 step: 274, loss is 0.08479893952608109\n",
      "epoch: 25 step: 275, loss is 0.01439371332526207\n",
      "epoch: 25 step: 276, loss is 0.006734851282089949\n",
      "epoch: 25 step: 277, loss is 0.004854985512793064\n",
      "epoch: 25 step: 278, loss is 0.0014227188657969236\n",
      "epoch: 25 step: 279, loss is 0.0018648384138941765\n",
      "epoch: 25 step: 280, loss is 0.017862489446997643\n",
      "epoch: 25 step: 281, loss is 0.005962375085800886\n",
      "epoch: 25 step: 282, loss is 0.00047429537517018616\n",
      "epoch: 25 step: 283, loss is 0.02520732954144478\n",
      "epoch: 25 step: 284, loss is 0.0020150726195424795\n",
      "epoch: 25 step: 285, loss is 0.0005562177975662053\n",
      "epoch: 25 step: 286, loss is 0.00030428843456320465\n",
      "epoch: 25 step: 287, loss is 0.00042495151865296066\n",
      "epoch: 25 step: 288, loss is 0.0041302284225821495\n",
      "epoch: 25 step: 289, loss is 0.0007542790262959898\n",
      "epoch: 25 step: 290, loss is 0.0010925967944785953\n",
      "epoch: 25 step: 291, loss is 0.011298504658043385\n",
      "epoch: 25 step: 292, loss is 0.01545203197747469\n",
      "epoch: 25 step: 293, loss is 0.016061827540397644\n",
      "epoch: 25 step: 294, loss is 0.0007227290188893676\n",
      "epoch: 25 step: 295, loss is 0.024734502658247948\n",
      "epoch: 25 step: 296, loss is 0.0009669236023910344\n",
      "epoch: 25 step: 297, loss is 0.001700148917734623\n",
      "epoch: 25 step: 298, loss is 0.007786998525261879\n",
      "epoch: 25 step: 299, loss is 0.006215390283614397\n",
      "epoch: 25 step: 300, loss is 3.814087904174812e-05\n",
      "epoch: 25 step: 301, loss is 0.003080270253121853\n",
      "epoch: 25 step: 302, loss is 0.03159484267234802\n",
      "epoch: 25 step: 303, loss is 0.006352692376822233\n",
      "epoch: 25 step: 304, loss is 0.007043845020234585\n",
      "epoch: 25 step: 305, loss is 0.009153523482382298\n",
      "epoch: 25 step: 306, loss is 0.0007351275999099016\n",
      "epoch: 25 step: 307, loss is 0.000318692356813699\n",
      "epoch: 25 step: 308, loss is 0.002016690094023943\n",
      "epoch: 25 step: 309, loss is 0.008002838119864464\n",
      "epoch: 25 step: 310, loss is 0.0007346353959292173\n",
      "epoch: 25 step: 311, loss is 0.016903717070817947\n",
      "epoch: 25 step: 312, loss is 0.001118094427511096\n",
      "epoch: 25 step: 313, loss is 0.009520560503005981\n",
      "epoch: 25 step: 314, loss is 0.02548416703939438\n",
      "epoch: 25 step: 315, loss is 0.040317319333553314\n",
      "epoch: 25 step: 316, loss is 0.000723119592294097\n",
      "epoch: 25 step: 317, loss is 0.0033295629546046257\n",
      "epoch: 25 step: 318, loss is 0.02124635875225067\n",
      "epoch: 25 step: 319, loss is 0.004125040490180254\n",
      "epoch: 25 step: 320, loss is 0.0002569249481894076\n",
      "epoch: 25 step: 321, loss is 0.024799872189760208\n",
      "epoch: 25 step: 322, loss is 0.04790373891592026\n",
      "epoch: 25 step: 323, loss is 0.0022080913186073303\n",
      "epoch: 25 step: 324, loss is 0.001378035405650735\n",
      "epoch: 25 step: 325, loss is 0.04892164096236229\n",
      "epoch: 25 step: 326, loss is 0.0002695206494536251\n",
      "epoch: 25 step: 327, loss is 0.019727321341633797\n",
      "epoch: 25 step: 328, loss is 0.00039411082980223\n",
      "epoch: 25 step: 329, loss is 0.005156460218131542\n",
      "epoch: 25 step: 330, loss is 2.0260065866750665e-05\n",
      "epoch: 25 step: 331, loss is 0.08126092702150345\n",
      "epoch: 25 step: 332, loss is 0.00011785217066062614\n",
      "epoch: 25 step: 333, loss is 0.0010736131807789207\n",
      "epoch: 25 step: 334, loss is 0.00043474623817019165\n",
      "epoch: 25 step: 335, loss is 0.0010110217845067382\n",
      "epoch: 25 step: 336, loss is 0.05876212194561958\n",
      "epoch: 25 step: 337, loss is 6.375515658874065e-05\n",
      "epoch: 25 step: 338, loss is 0.021474895998835564\n",
      "epoch: 25 step: 339, loss is 0.043557651340961456\n",
      "epoch: 25 step: 340, loss is 0.011017685756087303\n",
      "epoch: 25 step: 341, loss is 0.0005404633702710271\n",
      "epoch: 25 step: 342, loss is 0.0007171954493969679\n",
      "epoch: 25 step: 343, loss is 0.06097230315208435\n",
      "epoch: 25 step: 344, loss is 0.003130801487714052\n",
      "epoch: 25 step: 345, loss is 0.07582444697618484\n",
      "epoch: 25 step: 346, loss is 0.0013610734604299068\n",
      "epoch: 25 step: 347, loss is 0.003623603843152523\n",
      "epoch: 25 step: 348, loss is 0.010303358547389507\n",
      "epoch: 25 step: 349, loss is 0.08120207488536835\n",
      "epoch: 25 step: 350, loss is 0.011187153868377209\n",
      "epoch: 25 step: 351, loss is 0.0005678882589563727\n",
      "epoch: 25 step: 352, loss is 0.008531197905540466\n",
      "epoch: 25 step: 353, loss is 0.010096699930727482\n",
      "epoch: 25 step: 354, loss is 0.0247439406812191\n",
      "epoch: 25 step: 355, loss is 0.02178141474723816\n",
      "epoch: 25 step: 356, loss is 0.02347201108932495\n",
      "epoch: 25 step: 357, loss is 2.0274048438295722e-05\n",
      "epoch: 25 step: 358, loss is 0.0044576129876077175\n",
      "epoch: 25 step: 359, loss is 0.0017244034679606557\n",
      "epoch: 25 step: 360, loss is 0.08479964733123779\n",
      "epoch: 25 step: 361, loss is 0.017371060326695442\n",
      "epoch: 25 step: 362, loss is 0.002035361248999834\n",
      "epoch: 25 step: 363, loss is 0.08258122950792313\n",
      "epoch: 25 step: 364, loss is 0.0041364142671227455\n",
      "epoch: 25 step: 365, loss is 0.0009286831482313573\n",
      "epoch: 25 step: 366, loss is 0.0012639896012842655\n",
      "epoch: 25 step: 367, loss is 0.001654955092817545\n",
      "epoch: 25 step: 368, loss is 0.0012710907030850649\n",
      "epoch: 25 step: 369, loss is 0.012109874747693539\n",
      "epoch: 25 step: 370, loss is 0.004233593121170998\n",
      "epoch: 25 step: 371, loss is 0.0031511965207755566\n",
      "epoch: 25 step: 372, loss is 8.48277923068963e-05\n",
      "epoch: 25 step: 373, loss is 0.010908090509474277\n",
      "epoch: 25 step: 374, loss is 0.003903210861608386\n",
      "epoch: 25 step: 375, loss is 0.00048216787399724126\n",
      "epoch: 25 step: 376, loss is 0.00010460051271365955\n",
      "epoch: 25 step: 377, loss is 0.03306734561920166\n",
      "epoch: 25 step: 378, loss is 0.00032544363057240844\n",
      "epoch: 25 step: 379, loss is 0.0016359440051019192\n",
      "epoch: 25 step: 380, loss is 5.0984512199647725e-05\n",
      "epoch: 25 step: 381, loss is 0.0005637455615215003\n",
      "epoch: 25 step: 382, loss is 0.009053079411387444\n",
      "epoch: 25 step: 383, loss is 0.00034133894951082766\n",
      "epoch: 25 step: 384, loss is 0.010800613090395927\n",
      "epoch: 25 step: 385, loss is 0.010433055460453033\n",
      "epoch: 25 step: 386, loss is 0.0035833586007356644\n",
      "epoch: 25 step: 387, loss is 0.000506410317029804\n",
      "epoch: 25 step: 388, loss is 0.0010528702987357974\n",
      "epoch: 25 step: 389, loss is 0.0031457061413675547\n",
      "epoch: 25 step: 390, loss is 0.0007634648936800659\n",
      "epoch: 25 step: 391, loss is 0.00020978055545128882\n",
      "epoch: 25 step: 392, loss is 0.00603462615981698\n",
      "epoch: 25 step: 393, loss is 0.0015467859338968992\n",
      "epoch: 25 step: 394, loss is 0.0028188517317175865\n",
      "epoch: 25 step: 395, loss is 0.00023072745534591377\n",
      "epoch: 25 step: 396, loss is 0.0011033860500901937\n",
      "epoch: 25 step: 397, loss is 0.027192648500204086\n",
      "epoch: 25 step: 398, loss is 0.020383674651384354\n",
      "epoch: 25 step: 399, loss is 0.0016064319061115384\n",
      "epoch: 25 step: 400, loss is 0.010741387493908405\n",
      "epoch: 25 step: 401, loss is 0.029049310833215714\n",
      "epoch: 25 step: 402, loss is 0.009249120950698853\n",
      "epoch: 25 step: 403, loss is 0.0015478945570066571\n",
      "epoch: 25 step: 404, loss is 0.0028627170249819756\n",
      "epoch: 25 step: 405, loss is 0.0037534513976424932\n",
      "epoch: 25 step: 406, loss is 0.0035488782450556755\n",
      "epoch: 25 step: 407, loss is 0.019928524270653725\n",
      "epoch: 25 step: 408, loss is 0.0001713821111479774\n",
      "epoch: 25 step: 409, loss is 0.043870147317647934\n",
      "epoch: 25 step: 410, loss is 2.7684427550411783e-05\n",
      "epoch: 25 step: 411, loss is 0.0012647754047065973\n",
      "epoch: 25 step: 412, loss is 3.9722239307593554e-05\n",
      "epoch: 25 step: 413, loss is 0.0003961340116802603\n",
      "epoch: 25 step: 414, loss is 0.0002679873432498425\n",
      "epoch: 25 step: 415, loss is 0.015862995758652687\n",
      "epoch: 25 step: 416, loss is 0.006486678030341864\n",
      "epoch: 25 step: 417, loss is 0.004527927841991186\n",
      "epoch: 25 step: 418, loss is 0.0007864733343012631\n",
      "epoch: 25 step: 419, loss is 0.05722178518772125\n",
      "epoch: 25 step: 420, loss is 0.014436288736760616\n",
      "epoch: 25 step: 421, loss is 0.004891282878816128\n",
      "epoch: 25 step: 422, loss is 0.0011888836743310094\n",
      "epoch: 25 step: 423, loss is 0.004460285417735577\n",
      "epoch: 25 step: 424, loss is 0.0026243054307997227\n",
      "epoch: 25 step: 425, loss is 0.022022847086191177\n",
      "epoch: 25 step: 426, loss is 0.08666417002677917\n",
      "epoch: 25 step: 427, loss is 0.06014908850193024\n",
      "epoch: 25 step: 428, loss is 0.027358677238225937\n",
      "epoch: 25 step: 429, loss is 0.03219200670719147\n",
      "epoch: 25 step: 430, loss is 0.01435765065252781\n",
      "epoch: 25 step: 431, loss is 0.0021901477593928576\n",
      "epoch: 25 step: 432, loss is 0.0011199607979506254\n",
      "epoch: 25 step: 433, loss is 0.013442394323647022\n",
      "epoch: 25 step: 434, loss is 0.02793819271028042\n",
      "epoch: 25 step: 435, loss is 0.011314313858747482\n",
      "epoch: 25 step: 436, loss is 0.0005206315545365214\n",
      "epoch: 25 step: 437, loss is 0.013755571097135544\n",
      "epoch: 25 step: 438, loss is 0.0006708498694933951\n",
      "epoch: 25 step: 439, loss is 0.003165357280522585\n",
      "epoch: 25 step: 440, loss is 0.0002459472161717713\n",
      "epoch: 25 step: 441, loss is 0.10380816459655762\n",
      "epoch: 25 step: 442, loss is 0.00038733347901143134\n",
      "epoch: 25 step: 443, loss is 0.001830357825383544\n",
      "epoch: 25 step: 444, loss is 0.10334480553865433\n",
      "epoch: 25 step: 445, loss is 0.00915050134062767\n",
      "epoch: 25 step: 446, loss is 0.11743458360433578\n",
      "epoch: 25 step: 447, loss is 0.0016600418603047729\n",
      "epoch: 25 step: 448, loss is 0.001679239678196609\n",
      "epoch: 25 step: 449, loss is 0.04541017487645149\n",
      "epoch: 25 step: 450, loss is 0.005649800878018141\n",
      "epoch: 25 step: 451, loss is 0.0152866430580616\n",
      "epoch: 25 step: 452, loss is 0.007018460892140865\n",
      "epoch: 25 step: 453, loss is 0.0012113575357943773\n",
      "epoch: 25 step: 454, loss is 0.00011651867680484429\n",
      "epoch: 25 step: 455, loss is 0.0007834893767721951\n",
      "epoch: 25 step: 456, loss is 0.007559165358543396\n",
      "epoch: 25 step: 457, loss is 0.01797463372349739\n",
      "epoch: 25 step: 458, loss is 0.004012371879070997\n",
      "epoch: 25 step: 459, loss is 0.02042493037879467\n",
      "epoch: 25 step: 460, loss is 0.017748689278960228\n",
      "epoch: 25 step: 461, loss is 0.038933947682380676\n",
      "epoch: 25 step: 462, loss is 0.0009615502203814685\n",
      "epoch: 25 step: 463, loss is 0.041211649775505066\n",
      "epoch: 25 step: 464, loss is 0.1607440561056137\n",
      "epoch: 25 step: 465, loss is 0.005013623274862766\n",
      "epoch: 25 step: 466, loss is 0.0034271241165697575\n",
      "epoch: 25 step: 467, loss is 0.02652515098452568\n",
      "epoch: 25 step: 468, loss is 0.0025661587715148926\n",
      "epoch: 25 step: 469, loss is 0.021806500852108\n",
      "epoch: 25 step: 470, loss is 0.001078960602171719\n",
      "epoch: 25 step: 471, loss is 0.00485778134316206\n",
      "epoch: 25 step: 472, loss is 0.0003315617504995316\n",
      "epoch: 25 step: 473, loss is 0.04041058570146561\n",
      "epoch: 25 step: 474, loss is 0.0016997701022773981\n",
      "epoch: 25 step: 475, loss is 0.07513086497783661\n",
      "epoch: 25 step: 476, loss is 9.114149725064635e-05\n",
      "epoch: 25 step: 477, loss is 0.0013842033222317696\n",
      "epoch: 25 step: 478, loss is 0.005265078507363796\n",
      "epoch: 25 step: 479, loss is 0.04504195228219032\n",
      "epoch: 25 step: 480, loss is 0.015785163268446922\n",
      "epoch: 25 step: 481, loss is 0.009201357141137123\n",
      "epoch: 25 step: 482, loss is 6.561676855199039e-05\n",
      "epoch: 25 step: 483, loss is 0.004645196255296469\n",
      "epoch: 25 step: 484, loss is 0.04337457939982414\n",
      "epoch: 25 step: 485, loss is 0.013480296358466148\n",
      "epoch: 25 step: 486, loss is 0.034517236053943634\n",
      "epoch: 25 step: 487, loss is 0.003635867964476347\n",
      "epoch: 25 step: 488, loss is 0.04758617281913757\n",
      "epoch: 25 step: 489, loss is 0.06280312687158585\n",
      "epoch: 25 step: 490, loss is 0.0005495088407769799\n",
      "epoch: 25 step: 491, loss is 0.012493519112467766\n",
      "epoch: 25 step: 492, loss is 0.006161741446703672\n",
      "epoch: 25 step: 493, loss is 0.008737210184335709\n",
      "epoch: 25 step: 494, loss is 0.0005194784607738256\n",
      "epoch: 25 step: 495, loss is 0.00041759497253224254\n",
      "epoch: 25 step: 496, loss is 0.010170768946409225\n",
      "epoch: 25 step: 497, loss is 0.0025936015881597996\n",
      "epoch: 25 step: 498, loss is 0.002811664715409279\n",
      "epoch: 25 step: 499, loss is 0.00712869968265295\n",
      "epoch: 25 step: 500, loss is 0.000428449478931725\n",
      "epoch: 25 step: 501, loss is 0.0023983882274478674\n",
      "epoch: 25 step: 502, loss is 0.0427502803504467\n",
      "epoch: 25 step: 503, loss is 0.0030043795704841614\n",
      "epoch: 25 step: 504, loss is 0.007220258936285973\n",
      "epoch: 25 step: 505, loss is 0.005114413797855377\n",
      "epoch: 25 step: 506, loss is 0.006327812559902668\n",
      "epoch: 25 step: 507, loss is 0.0005074609071016312\n",
      "epoch: 25 step: 508, loss is 0.009969337843358517\n",
      "epoch: 25 step: 509, loss is 0.0060147373005747795\n",
      "epoch: 25 step: 510, loss is 0.007768306881189346\n",
      "epoch: 25 step: 511, loss is 0.0020883996039628983\n",
      "epoch: 25 step: 512, loss is 0.006594110745936632\n",
      "epoch: 25 step: 513, loss is 0.0005096262320876122\n",
      "epoch: 25 step: 514, loss is 1.5324614651035517e-05\n",
      "epoch: 25 step: 515, loss is 0.0030139239970594645\n",
      "epoch: 25 step: 516, loss is 0.0018313033506274223\n",
      "epoch: 25 step: 517, loss is 0.0024248005356639624\n",
      "epoch: 25 step: 518, loss is 0.019099166616797447\n",
      "epoch: 25 step: 519, loss is 0.005756799131631851\n",
      "epoch: 25 step: 520, loss is 0.0028205406852066517\n",
      "epoch: 25 step: 521, loss is 0.01788206771016121\n",
      "epoch: 25 step: 522, loss is 0.001644715084694326\n",
      "epoch: 25 step: 523, loss is 0.00859828945249319\n",
      "epoch: 25 step: 524, loss is 0.005556793883442879\n",
      "epoch: 25 step: 525, loss is 0.005945793353021145\n",
      "epoch: 25 step: 526, loss is 0.0001412117708241567\n",
      "epoch: 25 step: 527, loss is 0.013432634063065052\n",
      "epoch: 25 step: 528, loss is 0.009363812394440174\n",
      "epoch: 25 step: 529, loss is 0.0031823378521949053\n",
      "epoch: 25 step: 530, loss is 0.00035552112967707217\n",
      "epoch: 25 step: 531, loss is 0.001710873213596642\n",
      "epoch: 25 step: 532, loss is 0.0013516962062567472\n",
      "epoch: 25 step: 533, loss is 0.007259704638272524\n",
      "epoch: 25 step: 534, loss is 0.031562644988298416\n",
      "epoch: 25 step: 535, loss is 0.031309932470321655\n",
      "epoch: 25 step: 536, loss is 0.08514686673879623\n",
      "epoch: 25 step: 537, loss is 0.00018325717246625572\n",
      "epoch: 25 step: 538, loss is 0.02902543731033802\n",
      "epoch: 25 step: 539, loss is 0.0017070429166778922\n",
      "epoch: 25 step: 540, loss is 0.001165685709565878\n",
      "epoch: 25 step: 541, loss is 0.015980888158082962\n",
      "epoch: 25 step: 542, loss is 0.0008110097842290998\n",
      "epoch: 25 step: 543, loss is 0.00036037329118698835\n",
      "epoch: 25 step: 544, loss is 0.0030276828911155462\n",
      "epoch: 25 step: 545, loss is 0.00845374260097742\n",
      "epoch: 25 step: 546, loss is 0.015376782044768333\n",
      "epoch: 25 step: 547, loss is 0.07233083993196487\n",
      "epoch: 25 step: 548, loss is 0.0014470661990344524\n",
      "epoch: 25 step: 549, loss is 0.020639121532440186\n",
      "epoch: 25 step: 550, loss is 0.01977105438709259\n",
      "epoch: 25 step: 551, loss is 0.00022676237858831882\n",
      "epoch: 25 step: 552, loss is 0.0016785133630037308\n",
      "epoch: 25 step: 553, loss is 0.0010415237629786134\n",
      "epoch: 25 step: 554, loss is 0.004745433572679758\n",
      "epoch: 25 step: 555, loss is 0.001082678441889584\n",
      "epoch: 25 step: 556, loss is 0.006603925488889217\n",
      "epoch: 25 step: 557, loss is 0.0013308050110936165\n",
      "epoch: 25 step: 558, loss is 0.006386792287230492\n",
      "epoch: 25 step: 559, loss is 0.0006571297999471426\n",
      "epoch: 25 step: 560, loss is 0.0016817405121400952\n",
      "epoch: 25 step: 561, loss is 0.0012595119187608361\n",
      "epoch: 25 step: 562, loss is 0.020880231633782387\n",
      "epoch: 25 step: 563, loss is 0.026215896010398865\n",
      "epoch: 25 step: 564, loss is 0.009628336876630783\n",
      "epoch: 25 step: 565, loss is 0.009559599682688713\n",
      "epoch: 25 step: 566, loss is 0.0023593187797814608\n",
      "epoch: 25 step: 567, loss is 0.029182255268096924\n",
      "epoch: 25 step: 568, loss is 0.000142560267704539\n",
      "epoch: 25 step: 569, loss is 0.0002467429731041193\n",
      "epoch: 25 step: 570, loss is 0.043757591396570206\n",
      "epoch: 25 step: 571, loss is 0.0061807213351130486\n",
      "epoch: 25 step: 572, loss is 0.0004758358409162611\n",
      "epoch: 25 step: 573, loss is 0.0014038441004231572\n",
      "epoch: 25 step: 574, loss is 0.01235664077103138\n",
      "epoch: 25 step: 575, loss is 0.005980717483907938\n",
      "epoch: 25 step: 576, loss is 0.019722651690244675\n",
      "epoch: 25 step: 577, loss is 0.012174703180789948\n",
      "epoch: 25 step: 578, loss is 0.0005610303487628698\n",
      "epoch: 25 step: 579, loss is 0.02537466213107109\n",
      "epoch: 25 step: 580, loss is 0.00017835346807260066\n",
      "epoch: 25 step: 581, loss is 0.00044715480180457234\n",
      "epoch: 25 step: 582, loss is 0.00045619250158779323\n",
      "epoch: 25 step: 583, loss is 0.00037015462294220924\n",
      "epoch: 25 step: 584, loss is 0.017506657168269157\n",
      "epoch: 25 step: 585, loss is 0.0025835775304585695\n",
      "epoch: 25 step: 586, loss is 0.0031876324210315943\n",
      "epoch: 25 step: 587, loss is 0.0024185574147850275\n",
      "epoch: 25 step: 588, loss is 0.017477527260780334\n",
      "epoch: 25 step: 589, loss is 0.06840251386165619\n",
      "epoch: 25 step: 590, loss is 0.009465457871556282\n",
      "epoch: 25 step: 591, loss is 0.07650545239448547\n",
      "epoch: 25 step: 592, loss is 0.007532419636845589\n",
      "epoch: 25 step: 593, loss is 0.0010581136448308825\n",
      "epoch: 25 step: 594, loss is 0.006147630047053099\n",
      "epoch: 25 step: 595, loss is 0.0007477061590179801\n",
      "epoch: 25 step: 596, loss is 0.003660866990685463\n",
      "epoch: 25 step: 597, loss is 0.0036689997650682926\n",
      "epoch: 25 step: 598, loss is 0.0007583642145618796\n",
      "epoch: 25 step: 599, loss is 0.0011193215614184737\n",
      "epoch: 25 step: 600, loss is 0.000399451149860397\n",
      "epoch: 25 step: 601, loss is 0.0002760903735179454\n",
      "epoch: 25 step: 602, loss is 0.0004508740094024688\n",
      "epoch: 25 step: 603, loss is 0.01997126080095768\n",
      "epoch: 25 step: 604, loss is 0.0005004639970138669\n",
      "epoch: 25 step: 605, loss is 0.025891616940498352\n",
      "epoch: 25 step: 606, loss is 0.014155618846416473\n",
      "epoch: 25 step: 607, loss is 2.864277303160634e-05\n",
      "epoch: 25 step: 608, loss is 0.018416522070765495\n",
      "epoch: 25 step: 609, loss is 0.002944013336673379\n",
      "epoch: 25 step: 610, loss is 0.11447852104902267\n",
      "epoch: 25 step: 611, loss is 0.003973064012825489\n",
      "epoch: 25 step: 612, loss is 0.005188643001019955\n",
      "epoch: 25 step: 613, loss is 0.032682228833436966\n",
      "epoch: 25 step: 614, loss is 0.025816457346081734\n",
      "epoch: 25 step: 615, loss is 0.0017626445041969419\n",
      "epoch: 25 step: 616, loss is 0.00043771773925982416\n",
      "epoch: 25 step: 617, loss is 0.004907580558210611\n",
      "epoch: 25 step: 618, loss is 0.0027538721915334463\n",
      "epoch: 25 step: 619, loss is 0.0001298405695706606\n",
      "epoch: 25 step: 620, loss is 0.0013061122735962272\n",
      "epoch: 25 step: 621, loss is 0.0014768318505957723\n",
      "epoch: 25 step: 622, loss is 0.0012030629441142082\n",
      "epoch: 25 step: 623, loss is 0.008863487280905247\n",
      "epoch: 25 step: 624, loss is 0.00032519662636332214\n",
      "epoch: 25 step: 625, loss is 0.01137938629835844\n",
      "epoch: 25 step: 626, loss is 0.0035397233441472054\n",
      "epoch: 25 step: 627, loss is 0.0005485950387082994\n",
      "epoch: 25 step: 628, loss is 0.014508773572742939\n",
      "epoch: 25 step: 629, loss is 0.0015352218179032207\n",
      "epoch: 25 step: 630, loss is 0.0010884538060054183\n",
      "epoch: 25 step: 631, loss is 0.04185356944799423\n",
      "epoch: 25 step: 632, loss is 0.000123561781947501\n",
      "epoch: 25 step: 633, loss is 0.0005147625925019383\n",
      "epoch: 25 step: 634, loss is 0.003946464508771896\n",
      "epoch: 25 step: 635, loss is 0.0003668085264507681\n",
      "epoch: 25 step: 636, loss is 0.06849776953458786\n",
      "epoch: 25 step: 637, loss is 0.00613063620403409\n",
      "epoch: 25 step: 638, loss is 0.01929589733481407\n",
      "epoch: 25 step: 639, loss is 0.005920985247939825\n",
      "epoch: 25 step: 640, loss is 0.006914958357810974\n",
      "epoch: 25 step: 641, loss is 0.026712363585829735\n",
      "epoch: 25 step: 642, loss is 0.0028335186652839184\n",
      "epoch: 25 step: 643, loss is 0.0035450158175081015\n",
      "epoch: 25 step: 644, loss is 0.05892586335539818\n",
      "epoch: 25 step: 645, loss is 0.0037673525512218475\n",
      "epoch: 25 step: 646, loss is 0.011593764647841454\n",
      "epoch: 25 step: 647, loss is 0.0020140851847827435\n",
      "epoch: 25 step: 648, loss is 0.007745449896901846\n",
      "epoch: 25 step: 649, loss is 0.0006255068583413959\n",
      "epoch: 25 step: 650, loss is 0.08201158791780472\n",
      "epoch: 25 step: 651, loss is 0.01878221705555916\n",
      "epoch: 25 step: 652, loss is 0.03485969454050064\n",
      "epoch: 25 step: 653, loss is 0.01681758649647236\n",
      "epoch: 25 step: 654, loss is 0.05636928230524063\n",
      "epoch: 25 step: 655, loss is 0.09582804888486862\n",
      "epoch: 25 step: 656, loss is 0.0009354432695545256\n",
      "epoch: 25 step: 657, loss is 0.0009245822438970208\n",
      "epoch: 25 step: 658, loss is 0.02664942294359207\n",
      "epoch: 25 step: 659, loss is 0.007732980884611607\n",
      "epoch: 25 step: 660, loss is 0.0002134412934537977\n",
      "epoch: 25 step: 661, loss is 0.0032760482281446457\n",
      "epoch: 25 step: 662, loss is 0.0003647066478151828\n",
      "epoch: 25 step: 663, loss is 0.002650015288963914\n",
      "epoch: 25 step: 664, loss is 0.027900706976652145\n",
      "epoch: 25 step: 665, loss is 0.00016396564024034888\n",
      "epoch: 25 step: 666, loss is 0.0018648386467248201\n",
      "epoch: 25 step: 667, loss is 0.018287604674696922\n",
      "epoch: 25 step: 668, loss is 0.05830869451165199\n",
      "epoch: 25 step: 669, loss is 1.2795773727702908e-05\n",
      "epoch: 25 step: 670, loss is 0.006589542143046856\n",
      "epoch: 25 step: 671, loss is 0.0036899414844810963\n",
      "epoch: 25 step: 672, loss is 0.0014324432704597712\n",
      "epoch: 25 step: 673, loss is 0.03245825693011284\n",
      "epoch: 25 step: 674, loss is 0.0024864408187568188\n",
      "epoch: 25 step: 675, loss is 0.0034898670855909586\n",
      "epoch: 25 step: 676, loss is 0.008219100534915924\n",
      "epoch: 25 step: 677, loss is 0.004503784701228142\n",
      "epoch: 25 step: 678, loss is 0.013029529713094234\n",
      "epoch: 25 step: 679, loss is 0.0037901317700743675\n",
      "epoch: 25 step: 680, loss is 3.776532685151324e-05\n",
      "epoch: 25 step: 681, loss is 0.011123981326818466\n",
      "epoch: 25 step: 682, loss is 0.01805778034031391\n",
      "epoch: 25 step: 683, loss is 0.0036358044017106295\n",
      "epoch: 25 step: 684, loss is 0.00013990387378726155\n",
      "epoch: 25 step: 685, loss is 0.013873662799596786\n",
      "epoch: 25 step: 686, loss is 0.0008143313461914659\n",
      "epoch: 25 step: 687, loss is 0.011524511501193047\n",
      "epoch: 25 step: 688, loss is 0.0012040375731885433\n",
      "epoch: 25 step: 689, loss is 0.01933254674077034\n",
      "epoch: 25 step: 690, loss is 0.05128897726535797\n",
      "epoch: 25 step: 691, loss is 0.001356648514047265\n",
      "epoch: 25 step: 692, loss is 0.0007891725981608033\n",
      "epoch: 25 step: 693, loss is 0.0135832903906703\n",
      "epoch: 25 step: 694, loss is 0.004651396535336971\n",
      "epoch: 25 step: 695, loss is 0.0006240279180929065\n",
      "epoch: 25 step: 696, loss is 0.0015254206955432892\n",
      "epoch: 25 step: 697, loss is 0.00013343403406906873\n",
      "epoch: 25 step: 698, loss is 0.002198358764871955\n",
      "epoch: 25 step: 699, loss is 0.0008362940861843526\n",
      "epoch: 25 step: 700, loss is 0.003748531686142087\n",
      "epoch: 25 step: 701, loss is 0.0011694597778841853\n",
      "epoch: 25 step: 702, loss is 0.012154285795986652\n",
      "epoch: 25 step: 703, loss is 0.059790998697280884\n",
      "epoch: 25 step: 704, loss is 0.00441390136256814\n",
      "epoch: 25 step: 705, loss is 0.008190534077584743\n",
      "epoch: 25 step: 706, loss is 0.00018282936071045697\n",
      "epoch: 25 step: 707, loss is 0.007152313832193613\n",
      "epoch: 25 step: 708, loss is 0.008450713939964771\n",
      "epoch: 25 step: 709, loss is 0.007039080373942852\n",
      "epoch: 25 step: 710, loss is 0.04605717957019806\n",
      "epoch: 25 step: 711, loss is 0.009724424220621586\n",
      "epoch: 25 step: 712, loss is 0.00033695256570354104\n",
      "epoch: 25 step: 713, loss is 0.013351243920624256\n",
      "epoch: 25 step: 714, loss is 0.0098103703930974\n",
      "epoch: 25 step: 715, loss is 0.0026352652348577976\n",
      "epoch: 25 step: 716, loss is 0.007425602991133928\n",
      "epoch: 25 step: 717, loss is 0.017352543771266937\n",
      "epoch: 25 step: 718, loss is 0.0015541960019618273\n",
      "epoch: 25 step: 719, loss is 0.0010314683895558119\n",
      "epoch: 25 step: 720, loss is 0.0020311756525188684\n",
      "epoch: 25 step: 721, loss is 0.0008378766942769289\n",
      "epoch: 25 step: 722, loss is 0.005953748244792223\n",
      "epoch: 25 step: 723, loss is 9.812256030272692e-05\n",
      "epoch: 25 step: 724, loss is 0.0007151991594582796\n",
      "epoch: 25 step: 725, loss is 0.0012675983598455787\n",
      "epoch: 25 step: 726, loss is 0.0024966439232230186\n",
      "epoch: 25 step: 727, loss is 0.0013085661921650171\n",
      "epoch: 25 step: 728, loss is 0.020748840644955635\n",
      "epoch: 25 step: 729, loss is 0.0002648507943376899\n",
      "epoch: 25 step: 730, loss is 0.0026756208389997482\n",
      "epoch: 25 step: 731, loss is 0.0017381127690896392\n",
      "epoch: 25 step: 732, loss is 0.005672483704984188\n",
      "epoch: 25 step: 733, loss is 0.000353784067556262\n",
      "epoch: 25 step: 734, loss is 0.0006808906327933073\n",
      "epoch: 25 step: 735, loss is 0.0005123228183947504\n",
      "epoch: 25 step: 736, loss is 0.010146728716790676\n",
      "epoch: 25 step: 737, loss is 0.0004968722350895405\n",
      "epoch: 25 step: 738, loss is 0.0002984448801726103\n",
      "epoch: 25 step: 739, loss is 0.001070123864337802\n",
      "epoch: 25 step: 740, loss is 0.0005803723470307887\n",
      "epoch: 25 step: 741, loss is 0.0028897710144519806\n",
      "epoch: 25 step: 742, loss is 0.002152112079784274\n",
      "epoch: 25 step: 743, loss is 0.04789220914244652\n",
      "epoch: 25 step: 744, loss is 0.0003660255460999906\n",
      "epoch: 25 step: 745, loss is 0.0017726392252370715\n",
      "epoch: 25 step: 746, loss is 0.041982587426900864\n",
      "epoch: 25 step: 747, loss is 0.0016414034180343151\n",
      "epoch: 25 step: 748, loss is 0.0006168860709294677\n",
      "epoch: 25 step: 749, loss is 0.03440024331212044\n",
      "epoch: 25 step: 750, loss is 0.0012956673745065928\n",
      "epoch: 25 step: 751, loss is 0.03506358712911606\n",
      "epoch: 25 step: 752, loss is 0.07343728095293045\n",
      "epoch: 25 step: 753, loss is 0.00021577705047093332\n",
      "epoch: 25 step: 754, loss is 0.003220922779291868\n",
      "epoch: 25 step: 755, loss is 0.0002997738483827561\n",
      "epoch: 25 step: 756, loss is 0.029908910393714905\n",
      "epoch: 25 step: 757, loss is 0.0008129766210913658\n",
      "epoch: 25 step: 758, loss is 0.005001337267458439\n",
      "epoch: 25 step: 759, loss is 0.0014366514515131712\n",
      "epoch: 25 step: 760, loss is 0.0035192680079489946\n",
      "epoch: 25 step: 761, loss is 0.005768953822553158\n",
      "epoch: 25 step: 762, loss is 0.003563511651009321\n",
      "epoch: 25 step: 763, loss is 0.002057212172076106\n",
      "epoch: 25 step: 764, loss is 0.0018503714818507433\n",
      "epoch: 25 step: 765, loss is 0.0007099427166394889\n",
      "epoch: 25 step: 766, loss is 0.004295565653592348\n",
      "epoch: 25 step: 767, loss is 0.0041998145170509815\n",
      "epoch: 25 step: 768, loss is 0.001739961444400251\n",
      "epoch: 25 step: 769, loss is 0.05844471603631973\n",
      "epoch: 25 step: 770, loss is 0.021402694284915924\n",
      "epoch: 25 step: 771, loss is 0.027454888448119164\n",
      "epoch: 25 step: 772, loss is 0.00783085823059082\n",
      "epoch: 25 step: 773, loss is 0.005156236235052347\n",
      "epoch: 25 step: 774, loss is 0.0028059440664947033\n",
      "epoch: 25 step: 775, loss is 0.02497352845966816\n",
      "epoch: 25 step: 776, loss is 0.005953394342213869\n",
      "epoch: 25 step: 777, loss is 0.05915161967277527\n",
      "epoch: 25 step: 778, loss is 0.005557691212743521\n",
      "epoch: 25 step: 779, loss is 0.0023796609602868557\n",
      "epoch: 25 step: 780, loss is 0.03273067623376846\n",
      "epoch: 25 step: 781, loss is 0.011926297098398209\n",
      "epoch: 25 step: 782, loss is 0.012708485126495361\n",
      "epoch: 25 step: 783, loss is 0.0008769563864916563\n",
      "epoch: 25 step: 784, loss is 0.04906143620610237\n",
      "epoch: 25 step: 785, loss is 0.010090121999382973\n",
      "epoch: 25 step: 786, loss is 0.07563178241252899\n",
      "epoch: 25 step: 787, loss is 0.0034327395260334015\n",
      "epoch: 25 step: 788, loss is 0.0012485907645896077\n",
      "epoch: 25 step: 789, loss is 0.004622561391443014\n",
      "epoch: 25 step: 790, loss is 0.0009723546099849045\n",
      "epoch: 25 step: 791, loss is 0.017585035413503647\n",
      "epoch: 25 step: 792, loss is 0.003840927965939045\n",
      "epoch: 25 step: 793, loss is 0.002199037466198206\n",
      "epoch: 25 step: 794, loss is 0.06809259206056595\n",
      "epoch: 25 step: 795, loss is 0.00025409861700609326\n",
      "epoch: 25 step: 796, loss is 0.01323339156806469\n",
      "epoch: 25 step: 797, loss is 0.005749380216002464\n",
      "epoch: 25 step: 798, loss is 0.03130442649126053\n",
      "epoch: 25 step: 799, loss is 0.0075911348685622215\n",
      "epoch: 25 step: 800, loss is 0.0020523362327367067\n",
      "epoch: 25 step: 801, loss is 0.004592192359268665\n",
      "epoch: 25 step: 802, loss is 0.0008818870992399752\n",
      "epoch: 25 step: 803, loss is 0.07473284006118774\n",
      "epoch: 25 step: 804, loss is 0.00017611974908504635\n",
      "epoch: 25 step: 805, loss is 0.0015584036009386182\n",
      "epoch: 25 step: 806, loss is 0.018752427771687508\n",
      "epoch: 25 step: 807, loss is 0.00852520763874054\n",
      "epoch: 25 step: 808, loss is 0.02931896224617958\n",
      "epoch: 25 step: 809, loss is 0.0052499715238809586\n",
      "epoch: 25 step: 810, loss is 0.0008289840770885348\n",
      "epoch: 25 step: 811, loss is 0.0016219905810430646\n",
      "epoch: 25 step: 812, loss is 0.0021504804026335478\n",
      "epoch: 25 step: 813, loss is 0.003909515216946602\n",
      "epoch: 25 step: 814, loss is 0.002214426174759865\n",
      "epoch: 25 step: 815, loss is 0.00019010981486644596\n",
      "epoch: 25 step: 816, loss is 0.00014773970178794116\n",
      "epoch: 25 step: 817, loss is 0.019956020638346672\n",
      "epoch: 25 step: 818, loss is 0.010862567462027073\n",
      "epoch: 25 step: 819, loss is 8.275097570731305e-06\n",
      "epoch: 25 step: 820, loss is 0.002662815386429429\n",
      "epoch: 25 step: 821, loss is 0.04310814291238785\n",
      "epoch: 25 step: 822, loss is 0.013128709979355335\n",
      "epoch: 25 step: 823, loss is 0.004956680350005627\n",
      "epoch: 25 step: 824, loss is 0.0006063153268769383\n",
      "epoch: 25 step: 825, loss is 0.00034233415499329567\n",
      "epoch: 25 step: 826, loss is 0.005422497168183327\n",
      "epoch: 25 step: 827, loss is 0.0001361245522275567\n",
      "epoch: 25 step: 828, loss is 0.0016001120675355196\n",
      "epoch: 25 step: 829, loss is 0.002278238069266081\n",
      "epoch: 25 step: 830, loss is 0.00019631319446489215\n",
      "epoch: 25 step: 831, loss is 0.03168264776468277\n",
      "epoch: 25 step: 832, loss is 0.00020724092610180378\n",
      "epoch: 25 step: 833, loss is 6.259732617763802e-05\n",
      "epoch: 25 step: 834, loss is 0.0033927499316632748\n",
      "epoch: 25 step: 835, loss is 0.02651471272110939\n",
      "epoch: 25 step: 836, loss is 0.0051409094594419\n",
      "epoch: 25 step: 837, loss is 6.437933188863099e-05\n",
      "epoch: 25 step: 838, loss is 0.015534044243395329\n",
      "epoch: 25 step: 839, loss is 0.0005778672639280558\n",
      "epoch: 25 step: 840, loss is 0.0006388586480170488\n",
      "epoch: 25 step: 841, loss is 0.0317264087498188\n",
      "epoch: 25 step: 842, loss is 0.00815308652818203\n",
      "epoch: 25 step: 843, loss is 0.0009873880771920085\n",
      "epoch: 25 step: 844, loss is 0.19621779024600983\n",
      "epoch: 25 step: 845, loss is 0.00031018350273370743\n",
      "epoch: 25 step: 846, loss is 0.0015636923490092158\n",
      "epoch: 25 step: 847, loss is 0.025314949452877045\n",
      "epoch: 25 step: 848, loss is 0.0013111631851643324\n",
      "epoch: 25 step: 849, loss is 0.04070233553647995\n",
      "epoch: 25 step: 850, loss is 0.0020700518507510424\n",
      "epoch: 25 step: 851, loss is 0.0013791740639135242\n",
      "epoch: 25 step: 852, loss is 0.018455492332577705\n",
      "epoch: 25 step: 853, loss is 0.00639319745823741\n",
      "epoch: 25 step: 854, loss is 0.030476227402687073\n",
      "epoch: 25 step: 855, loss is 0.025839652866125107\n",
      "epoch: 25 step: 856, loss is 0.12490101158618927\n",
      "epoch: 25 step: 857, loss is 0.00041064206743612885\n",
      "epoch: 25 step: 858, loss is 0.003016561269760132\n",
      "epoch: 25 step: 859, loss is 0.0036137523129582405\n",
      "epoch: 25 step: 860, loss is 0.005693349055945873\n",
      "epoch: 25 step: 861, loss is 0.07957810908555984\n",
      "epoch: 25 step: 862, loss is 0.0007545794942416251\n",
      "epoch: 25 step: 863, loss is 0.0048201605677604675\n",
      "epoch: 25 step: 864, loss is 0.00027397830854170024\n",
      "epoch: 25 step: 865, loss is 0.003403809852898121\n",
      "epoch: 25 step: 866, loss is 0.0014959430554881692\n",
      "epoch: 25 step: 867, loss is 0.0017005016561597586\n",
      "epoch: 25 step: 868, loss is 0.0019818865694105625\n",
      "epoch: 25 step: 869, loss is 0.0018310172017663717\n",
      "epoch: 25 step: 870, loss is 0.09337916970252991\n",
      "epoch: 25 step: 871, loss is 0.0012492210371419787\n",
      "epoch: 25 step: 872, loss is 0.002064906992018223\n",
      "epoch: 25 step: 873, loss is 0.014529337175190449\n",
      "epoch: 25 step: 874, loss is 0.0014800162753090262\n",
      "epoch: 25 step: 875, loss is 0.002519482048228383\n",
      "epoch: 25 step: 876, loss is 0.029187364503741264\n",
      "epoch: 25 step: 877, loss is 0.00045925070298835635\n",
      "epoch: 25 step: 878, loss is 0.0017223170725628734\n",
      "epoch: 25 step: 879, loss is 0.003819885663688183\n",
      "epoch: 25 step: 880, loss is 0.0026555275544524193\n",
      "epoch: 25 step: 881, loss is 0.010686851106584072\n",
      "epoch: 25 step: 882, loss is 0.01272090245038271\n",
      "epoch: 25 step: 883, loss is 0.0017032638425007463\n",
      "epoch: 25 step: 884, loss is 0.011430645361542702\n",
      "epoch: 25 step: 885, loss is 0.000966883497312665\n",
      "epoch: 25 step: 886, loss is 0.012983860448002815\n",
      "epoch: 25 step: 887, loss is 0.002972864545881748\n",
      "epoch: 25 step: 888, loss is 0.04140128195285797\n",
      "epoch: 25 step: 889, loss is 0.005242967512458563\n",
      "epoch: 25 step: 890, loss is 0.024913562461733818\n",
      "epoch: 25 step: 891, loss is 0.02492617443203926\n",
      "epoch: 25 step: 892, loss is 0.04870585724711418\n",
      "epoch: 25 step: 893, loss is 0.01039071287959814\n",
      "epoch: 25 step: 894, loss is 0.001167236128821969\n",
      "epoch: 25 step: 895, loss is 0.04651638865470886\n",
      "epoch: 25 step: 896, loss is 0.010122464038431644\n",
      "epoch: 25 step: 897, loss is 0.004048037342727184\n",
      "epoch: 25 step: 898, loss is 0.0009610457927919924\n",
      "epoch: 25 step: 899, loss is 0.0001469132985221222\n",
      "epoch: 25 step: 900, loss is 0.006912400014698505\n",
      "epoch: 25 step: 901, loss is 0.006938613019883633\n",
      "epoch: 25 step: 902, loss is 0.032571300864219666\n",
      "epoch: 25 step: 903, loss is 0.004013163037598133\n",
      "epoch: 25 step: 904, loss is 0.0010649969335645437\n",
      "epoch: 25 step: 905, loss is 0.005582426209002733\n",
      "epoch: 25 step: 906, loss is 0.0011222665198147297\n",
      "epoch: 25 step: 907, loss is 5.067626261734404e-05\n",
      "epoch: 25 step: 908, loss is 0.0015755838248878717\n",
      "epoch: 25 step: 909, loss is 0.008054714649915695\n",
      "epoch: 25 step: 910, loss is 0.0033716631587594748\n",
      "epoch: 25 step: 911, loss is 0.011107771657407284\n",
      "epoch: 25 step: 912, loss is 0.0012992416741326451\n",
      "epoch: 25 step: 913, loss is 0.23546022176742554\n",
      "epoch: 25 step: 914, loss is 0.00015566687216050923\n",
      "epoch: 25 step: 915, loss is 0.00048304267693310976\n",
      "epoch: 25 step: 916, loss is 0.038334477692842484\n",
      "epoch: 25 step: 917, loss is 0.0009218328050337732\n",
      "epoch: 25 step: 918, loss is 0.027096090838313103\n",
      "epoch: 25 step: 919, loss is 0.0005607749917544425\n",
      "epoch: 25 step: 920, loss is 0.005353350192308426\n",
      "epoch: 25 step: 921, loss is 0.009252930991351604\n",
      "epoch: 25 step: 922, loss is 0.0004003436479251832\n",
      "epoch: 25 step: 923, loss is 0.019720235839486122\n",
      "epoch: 25 step: 924, loss is 0.01744929701089859\n",
      "epoch: 25 step: 925, loss is 0.0002949816989712417\n",
      "epoch: 25 step: 926, loss is 0.10016094893217087\n",
      "epoch: 25 step: 927, loss is 0.0002179988077841699\n",
      "epoch: 25 step: 928, loss is 0.14922691881656647\n",
      "epoch: 25 step: 929, loss is 0.004556907806545496\n",
      "epoch: 25 step: 930, loss is 0.03270568698644638\n",
      "epoch: 25 step: 931, loss is 0.0021278990898281336\n",
      "epoch: 25 step: 932, loss is 0.0031861141324043274\n",
      "epoch: 25 step: 933, loss is 0.11931116133928299\n",
      "epoch: 25 step: 934, loss is 0.00034655252238735557\n",
      "epoch: 25 step: 935, loss is 0.008358575403690338\n",
      "epoch: 25 step: 936, loss is 0.006068715360015631\n",
      "epoch: 25 step: 937, loss is 0.0034478134475648403\n",
      "epoch: 26 step: 1, loss is 0.0016427411464974284\n",
      "epoch: 26 step: 2, loss is 0.00035040281363762915\n",
      "epoch: 26 step: 3, loss is 0.013584006577730179\n",
      "epoch: 26 step: 4, loss is 0.08071582019329071\n",
      "epoch: 26 step: 5, loss is 0.017202137038111687\n",
      "epoch: 26 step: 6, loss is 0.004157067742198706\n",
      "epoch: 26 step: 7, loss is 0.04205530881881714\n",
      "epoch: 26 step: 8, loss is 0.0097797317430377\n",
      "epoch: 26 step: 9, loss is 0.0004693166119977832\n",
      "epoch: 26 step: 10, loss is 0.03611431643366814\n",
      "epoch: 26 step: 11, loss is 0.022921467199921608\n",
      "epoch: 26 step: 12, loss is 0.03282194212079048\n",
      "epoch: 26 step: 13, loss is 0.05308252200484276\n",
      "epoch: 26 step: 14, loss is 0.04604184255003929\n",
      "epoch: 26 step: 15, loss is 0.0190266165882349\n",
      "epoch: 26 step: 16, loss is 0.03493904322385788\n",
      "epoch: 26 step: 17, loss is 0.004991441965103149\n",
      "epoch: 26 step: 18, loss is 0.0004222805437166244\n",
      "epoch: 26 step: 19, loss is 0.003693348029628396\n",
      "epoch: 26 step: 20, loss is 0.0005203991895541549\n",
      "epoch: 26 step: 21, loss is 0.0017472541658207774\n",
      "epoch: 26 step: 22, loss is 0.004556098487228155\n",
      "epoch: 26 step: 23, loss is 0.0003713149926625192\n",
      "epoch: 26 step: 24, loss is 0.02205478399991989\n",
      "epoch: 26 step: 25, loss is 0.01021722424775362\n",
      "epoch: 26 step: 26, loss is 0.0012049268698319793\n",
      "epoch: 26 step: 27, loss is 0.003973102197051048\n",
      "epoch: 26 step: 28, loss is 0.0007354646222665906\n",
      "epoch: 26 step: 29, loss is 0.00020045757992193103\n",
      "epoch: 26 step: 30, loss is 0.0058537437580525875\n",
      "epoch: 26 step: 31, loss is 0.003427058458328247\n",
      "epoch: 26 step: 32, loss is 0.0032439667265862226\n",
      "epoch: 26 step: 33, loss is 0.001475572818890214\n",
      "epoch: 26 step: 34, loss is 0.05396667495369911\n",
      "epoch: 26 step: 35, loss is 0.004569857381284237\n",
      "epoch: 26 step: 36, loss is 0.01980648562312126\n",
      "epoch: 26 step: 37, loss is 0.0002688101085368544\n",
      "epoch: 26 step: 38, loss is 0.0046959505416452885\n",
      "epoch: 26 step: 39, loss is 6.50904985377565e-05\n",
      "epoch: 26 step: 40, loss is 0.0008473946945741773\n",
      "epoch: 26 step: 41, loss is 0.0005946413148194551\n",
      "epoch: 26 step: 42, loss is 0.0008222900214605033\n",
      "epoch: 26 step: 43, loss is 0.0001772290706867352\n",
      "epoch: 26 step: 44, loss is 0.001060880720615387\n",
      "epoch: 26 step: 45, loss is 0.0007546847919002175\n",
      "epoch: 26 step: 46, loss is 0.035080693662166595\n",
      "epoch: 26 step: 47, loss is 0.0037401453591883183\n",
      "epoch: 26 step: 48, loss is 0.008862290531396866\n",
      "epoch: 26 step: 49, loss is 0.0005668108351528645\n",
      "epoch: 26 step: 50, loss is 0.0246221125125885\n",
      "epoch: 26 step: 51, loss is 0.008062615990638733\n",
      "epoch: 26 step: 52, loss is 0.0007776390411891043\n",
      "epoch: 26 step: 53, loss is 0.005598422139883041\n",
      "epoch: 26 step: 54, loss is 0.05324365571141243\n",
      "epoch: 26 step: 55, loss is 0.012766805477440357\n",
      "epoch: 26 step: 56, loss is 0.0240622591227293\n",
      "epoch: 26 step: 57, loss is 0.0022146860137581825\n",
      "epoch: 26 step: 58, loss is 0.004279415588825941\n",
      "epoch: 26 step: 59, loss is 0.018428029492497444\n",
      "epoch: 26 step: 60, loss is 0.025003908202052116\n",
      "epoch: 26 step: 61, loss is 0.004861943889409304\n",
      "epoch: 26 step: 62, loss is 4.0551509300712496e-05\n",
      "epoch: 26 step: 63, loss is 0.028034888207912445\n",
      "epoch: 26 step: 64, loss is 0.0014156134566292167\n",
      "epoch: 26 step: 65, loss is 0.03412746638059616\n",
      "epoch: 26 step: 66, loss is 0.0482926145195961\n",
      "epoch: 26 step: 67, loss is 0.000646628497634083\n",
      "epoch: 26 step: 68, loss is 0.0007384587661363184\n",
      "epoch: 26 step: 69, loss is 0.06997397541999817\n",
      "epoch: 26 step: 70, loss is 0.002481290837749839\n",
      "epoch: 26 step: 71, loss is 0.0036204822827130556\n",
      "epoch: 26 step: 72, loss is 0.0009912579553201795\n",
      "epoch: 26 step: 73, loss is 0.0022514411248266697\n",
      "epoch: 26 step: 74, loss is 0.00011218169674975798\n",
      "epoch: 26 step: 75, loss is 0.0059499372728168964\n",
      "epoch: 26 step: 76, loss is 0.0024418793618679047\n",
      "epoch: 26 step: 77, loss is 0.0003726999566424638\n",
      "epoch: 26 step: 78, loss is 0.007319404743611813\n",
      "epoch: 26 step: 79, loss is 0.015121597796678543\n",
      "epoch: 26 step: 80, loss is 0.001273585483431816\n",
      "epoch: 26 step: 81, loss is 0.0001388207165291533\n",
      "epoch: 26 step: 82, loss is 0.005152334459125996\n",
      "epoch: 26 step: 83, loss is 0.0019393636612221599\n",
      "epoch: 26 step: 84, loss is 0.0009716241038404405\n",
      "epoch: 26 step: 85, loss is 0.005903393961489201\n",
      "epoch: 26 step: 86, loss is 0.0002828061406034976\n",
      "epoch: 26 step: 87, loss is 0.028426561504602432\n",
      "epoch: 26 step: 88, loss is 0.00018135384016204625\n",
      "epoch: 26 step: 89, loss is 0.0010510325664654374\n",
      "epoch: 26 step: 90, loss is 0.00011842045205412433\n",
      "epoch: 26 step: 91, loss is 0.00011495474609546363\n",
      "epoch: 26 step: 92, loss is 0.1161411851644516\n",
      "epoch: 26 step: 93, loss is 0.000345353881129995\n",
      "epoch: 26 step: 94, loss is 0.0011465050047263503\n",
      "epoch: 26 step: 95, loss is 0.009836658835411072\n",
      "epoch: 26 step: 96, loss is 0.009271892718970776\n",
      "epoch: 26 step: 97, loss is 0.0025983217637985945\n",
      "epoch: 26 step: 98, loss is 0.0004154902999289334\n",
      "epoch: 26 step: 99, loss is 0.0013876648154109716\n",
      "epoch: 26 step: 100, loss is 0.0012801897246390581\n",
      "epoch: 26 step: 101, loss is 0.0012575116707012057\n",
      "epoch: 26 step: 102, loss is 0.00012017961125820875\n",
      "epoch: 26 step: 103, loss is 0.06445158272981644\n",
      "epoch: 26 step: 104, loss is 0.08523804694414139\n",
      "epoch: 26 step: 105, loss is 0.010838070884346962\n",
      "epoch: 26 step: 106, loss is 1.3682454664376564e-05\n",
      "epoch: 26 step: 107, loss is 0.02567118965089321\n",
      "epoch: 26 step: 108, loss is 0.00012031758524244651\n",
      "epoch: 26 step: 109, loss is 0.0015067559434100986\n",
      "epoch: 26 step: 110, loss is 0.004762595519423485\n",
      "epoch: 26 step: 111, loss is 0.00013965896505396813\n",
      "epoch: 26 step: 112, loss is 0.020162761211395264\n",
      "epoch: 26 step: 113, loss is 2.68666281044716e-05\n",
      "epoch: 26 step: 114, loss is 0.003563731675967574\n",
      "epoch: 26 step: 115, loss is 0.006558896508067846\n",
      "epoch: 26 step: 116, loss is 0.000936557597015053\n",
      "epoch: 26 step: 117, loss is 0.0011914928909391165\n",
      "epoch: 26 step: 118, loss is 0.0012412393698468804\n",
      "epoch: 26 step: 119, loss is 0.0006954669370315969\n",
      "epoch: 26 step: 120, loss is 0.00457023736089468\n",
      "epoch: 26 step: 121, loss is 0.00016384749324060977\n",
      "epoch: 26 step: 122, loss is 0.008075183257460594\n",
      "epoch: 26 step: 123, loss is 0.00015832392091397196\n",
      "epoch: 26 step: 124, loss is 0.0017009424045681953\n",
      "epoch: 26 step: 125, loss is 0.00037133131991140544\n",
      "epoch: 26 step: 126, loss is 0.002719711046665907\n",
      "epoch: 26 step: 127, loss is 0.0009975344873964787\n",
      "epoch: 26 step: 128, loss is 0.0004088164132554084\n",
      "epoch: 26 step: 129, loss is 0.00635564373806119\n",
      "epoch: 26 step: 130, loss is 0.00021936598932370543\n",
      "epoch: 26 step: 131, loss is 0.03992772474884987\n",
      "epoch: 26 step: 132, loss is 0.022545212879776955\n",
      "epoch: 26 step: 133, loss is 0.018834169954061508\n",
      "epoch: 26 step: 134, loss is 0.0006085992208682001\n",
      "epoch: 26 step: 135, loss is 0.0010934986639767885\n",
      "epoch: 26 step: 136, loss is 0.00319886626675725\n",
      "epoch: 26 step: 137, loss is 0.10814615339040756\n",
      "epoch: 26 step: 138, loss is 0.00029498981893993914\n",
      "epoch: 26 step: 139, loss is 0.0004921029321849346\n",
      "epoch: 26 step: 140, loss is 0.007193076424300671\n",
      "epoch: 26 step: 141, loss is 5.1986520702484995e-05\n",
      "epoch: 26 step: 142, loss is 0.0006219950737431645\n",
      "epoch: 26 step: 143, loss is 0.009284598752856255\n",
      "epoch: 26 step: 144, loss is 0.00012358369713183492\n",
      "epoch: 26 step: 145, loss is 0.044182587414979935\n",
      "epoch: 26 step: 146, loss is 0.004264175891876221\n",
      "epoch: 26 step: 147, loss is 0.004256893880665302\n",
      "epoch: 26 step: 148, loss is 0.03489828109741211\n",
      "epoch: 26 step: 149, loss is 0.08834046125411987\n",
      "epoch: 26 step: 150, loss is 0.0002139784483006224\n",
      "epoch: 26 step: 151, loss is 0.007485326379537582\n",
      "epoch: 26 step: 152, loss is 0.006007872987538576\n",
      "epoch: 26 step: 153, loss is 0.0004340041778050363\n",
      "epoch: 26 step: 154, loss is 0.00515360664576292\n",
      "epoch: 26 step: 155, loss is 0.06492409110069275\n",
      "epoch: 26 step: 156, loss is 0.005421990994364023\n",
      "epoch: 26 step: 157, loss is 0.011521014384925365\n",
      "epoch: 26 step: 158, loss is 0.0020042764954268932\n",
      "epoch: 26 step: 159, loss is 0.06940919160842896\n",
      "epoch: 26 step: 160, loss is 0.0005109396879561245\n",
      "epoch: 26 step: 161, loss is 0.00017563880828674883\n",
      "epoch: 26 step: 162, loss is 0.030844569206237793\n",
      "epoch: 26 step: 163, loss is 0.08268502354621887\n",
      "epoch: 26 step: 164, loss is 0.028338562697172165\n",
      "epoch: 26 step: 165, loss is 0.05976402014493942\n",
      "epoch: 26 step: 166, loss is 0.0012047139462083578\n",
      "epoch: 26 step: 167, loss is 0.008187063038349152\n",
      "epoch: 26 step: 168, loss is 0.0034504409413784742\n",
      "epoch: 26 step: 169, loss is 0.0029590679332613945\n",
      "epoch: 26 step: 170, loss is 0.024516630917787552\n",
      "epoch: 26 step: 171, loss is 0.06606219708919525\n",
      "epoch: 26 step: 172, loss is 0.05916885286569595\n",
      "epoch: 26 step: 173, loss is 0.004954895004630089\n",
      "epoch: 26 step: 174, loss is 0.0008874945342540741\n",
      "epoch: 26 step: 175, loss is 0.06591207534074783\n",
      "epoch: 26 step: 176, loss is 0.000812006474006921\n",
      "epoch: 26 step: 177, loss is 0.003829771187156439\n",
      "epoch: 26 step: 178, loss is 0.031082523986697197\n",
      "epoch: 26 step: 179, loss is 0.0013854828430339694\n",
      "epoch: 26 step: 180, loss is 0.09626907855272293\n",
      "epoch: 26 step: 181, loss is 0.025753971189260483\n",
      "epoch: 26 step: 182, loss is 0.002797268331050873\n",
      "epoch: 26 step: 183, loss is 0.12199235707521439\n",
      "epoch: 26 step: 184, loss is 0.0038837860338389874\n",
      "epoch: 26 step: 185, loss is 0.008557739667594433\n",
      "epoch: 26 step: 186, loss is 0.0012202397920191288\n",
      "epoch: 26 step: 187, loss is 0.0015409351326525211\n",
      "epoch: 26 step: 188, loss is 0.016084685921669006\n",
      "epoch: 26 step: 189, loss is 0.005872046574950218\n",
      "epoch: 26 step: 190, loss is 0.019093526527285576\n",
      "epoch: 26 step: 191, loss is 0.004389909561723471\n",
      "epoch: 26 step: 192, loss is 0.006852556951344013\n",
      "epoch: 26 step: 193, loss is 0.0012278725625947118\n",
      "epoch: 26 step: 194, loss is 0.0012814856600016356\n",
      "epoch: 26 step: 195, loss is 0.005873973481357098\n",
      "epoch: 26 step: 196, loss is 0.0076537649147212505\n",
      "epoch: 26 step: 197, loss is 0.06170865148305893\n",
      "epoch: 26 step: 198, loss is 0.014615693129599094\n",
      "epoch: 26 step: 199, loss is 0.0008105639717541635\n",
      "epoch: 26 step: 200, loss is 0.03109937720000744\n",
      "epoch: 26 step: 201, loss is 0.11404281854629517\n",
      "epoch: 26 step: 202, loss is 0.015273239463567734\n",
      "epoch: 26 step: 203, loss is 0.0006233158055692911\n",
      "epoch: 26 step: 204, loss is 0.010742364451289177\n",
      "epoch: 26 step: 205, loss is 0.013622802682220936\n",
      "epoch: 26 step: 206, loss is 0.0015292189782485366\n",
      "epoch: 26 step: 207, loss is 0.0007303798338398337\n",
      "epoch: 26 step: 208, loss is 0.0008582356967963278\n",
      "epoch: 26 step: 209, loss is 0.0396176278591156\n",
      "epoch: 26 step: 210, loss is 0.0701601505279541\n",
      "epoch: 26 step: 211, loss is 0.006323126144707203\n",
      "epoch: 26 step: 212, loss is 0.0013721727300435305\n",
      "epoch: 26 step: 213, loss is 0.000669748755171895\n",
      "epoch: 26 step: 214, loss is 0.043103210628032684\n",
      "epoch: 26 step: 215, loss is 0.004242537543177605\n",
      "epoch: 26 step: 216, loss is 0.019480612128973007\n",
      "epoch: 26 step: 217, loss is 0.010711918585002422\n",
      "epoch: 26 step: 218, loss is 0.0003819312551058829\n",
      "epoch: 26 step: 219, loss is 0.010090133175253868\n",
      "epoch: 26 step: 220, loss is 0.0018971923273056746\n",
      "epoch: 26 step: 221, loss is 0.06904724985361099\n",
      "epoch: 26 step: 222, loss is 0.009901752695441246\n",
      "epoch: 26 step: 223, loss is 0.05756978690624237\n",
      "epoch: 26 step: 224, loss is 0.013802208006381989\n",
      "epoch: 26 step: 225, loss is 0.004124216735363007\n",
      "epoch: 26 step: 226, loss is 0.002769033657386899\n",
      "epoch: 26 step: 227, loss is 0.0018157798331230879\n",
      "epoch: 26 step: 228, loss is 0.0038267485797405243\n",
      "epoch: 26 step: 229, loss is 0.0037492173723876476\n",
      "epoch: 26 step: 230, loss is 0.08413490653038025\n",
      "epoch: 26 step: 231, loss is 0.023659681901335716\n",
      "epoch: 26 step: 232, loss is 0.0672554150223732\n",
      "epoch: 26 step: 233, loss is 0.0405845046043396\n",
      "epoch: 26 step: 234, loss is 0.001013107248581946\n",
      "epoch: 26 step: 235, loss is 0.003395983949303627\n",
      "epoch: 26 step: 236, loss is 0.002862252527847886\n",
      "epoch: 26 step: 237, loss is 0.10456593334674835\n",
      "epoch: 26 step: 238, loss is 0.07484528422355652\n",
      "epoch: 26 step: 239, loss is 0.00581083120778203\n",
      "epoch: 26 step: 240, loss is 0.0011435893829911947\n",
      "epoch: 26 step: 241, loss is 0.006673792842775583\n",
      "epoch: 26 step: 242, loss is 0.0015339378733187914\n",
      "epoch: 26 step: 243, loss is 0.000613829935900867\n",
      "epoch: 26 step: 244, loss is 0.0003445743350312114\n",
      "epoch: 26 step: 245, loss is 0.004325643181800842\n",
      "epoch: 26 step: 246, loss is 0.02477835677564144\n",
      "epoch: 26 step: 247, loss is 0.036681100726127625\n",
      "epoch: 26 step: 248, loss is 0.009032905101776123\n",
      "epoch: 26 step: 249, loss is 0.02059982903301716\n",
      "epoch: 26 step: 250, loss is 0.012247600592672825\n",
      "epoch: 26 step: 251, loss is 8.693749259691685e-05\n",
      "epoch: 26 step: 252, loss is 0.0003365255251992494\n",
      "epoch: 26 step: 253, loss is 0.028141789138317108\n",
      "epoch: 26 step: 254, loss is 0.05154351517558098\n",
      "epoch: 26 step: 255, loss is 0.0020351207349449396\n",
      "epoch: 26 step: 256, loss is 0.05736764520406723\n",
      "epoch: 26 step: 257, loss is 0.011834627017378807\n",
      "epoch: 26 step: 258, loss is 0.024872232228517532\n",
      "epoch: 26 step: 259, loss is 0.005054315086454153\n",
      "epoch: 26 step: 260, loss is 0.0008916701190173626\n",
      "epoch: 26 step: 261, loss is 0.014104150235652924\n",
      "epoch: 26 step: 262, loss is 0.0006707889260724187\n",
      "epoch: 26 step: 263, loss is 0.0002621617750264704\n",
      "epoch: 26 step: 264, loss is 0.023872442543506622\n",
      "epoch: 26 step: 265, loss is 0.027881348505616188\n",
      "epoch: 26 step: 266, loss is 0.01614026539027691\n",
      "epoch: 26 step: 267, loss is 0.01540242787450552\n",
      "epoch: 26 step: 268, loss is 0.11625455319881439\n",
      "epoch: 26 step: 269, loss is 0.0002666245272848755\n",
      "epoch: 26 step: 270, loss is 0.0102301687002182\n",
      "epoch: 26 step: 271, loss is 0.011133654043078423\n",
      "epoch: 26 step: 272, loss is 0.010902520269155502\n",
      "epoch: 26 step: 273, loss is 0.03885025903582573\n",
      "epoch: 26 step: 274, loss is 0.0043808529153466225\n",
      "epoch: 26 step: 275, loss is 0.00020312084234319627\n",
      "epoch: 26 step: 276, loss is 0.007189934607595205\n",
      "epoch: 26 step: 277, loss is 0.00029539308161474764\n",
      "epoch: 26 step: 278, loss is 0.004154453054070473\n",
      "epoch: 26 step: 279, loss is 0.008382736705243587\n",
      "epoch: 26 step: 280, loss is 0.008950415998697281\n",
      "epoch: 26 step: 281, loss is 0.0014290701365098357\n",
      "epoch: 26 step: 282, loss is 0.0075829531997442245\n",
      "epoch: 26 step: 283, loss is 0.015729768201708794\n",
      "epoch: 26 step: 284, loss is 0.019028212875127792\n",
      "epoch: 26 step: 285, loss is 0.00010309889330528677\n",
      "epoch: 26 step: 286, loss is 0.013445907272398472\n",
      "epoch: 26 step: 287, loss is 0.0012808034662157297\n",
      "epoch: 26 step: 288, loss is 0.0008435634081251919\n",
      "epoch: 26 step: 289, loss is 0.003512355964630842\n",
      "epoch: 26 step: 290, loss is 0.03642741218209267\n",
      "epoch: 26 step: 291, loss is 0.0002430597523925826\n",
      "epoch: 26 step: 292, loss is 0.00042000311077572405\n",
      "epoch: 26 step: 293, loss is 0.000731342239305377\n",
      "epoch: 26 step: 294, loss is 0.0017138468101620674\n",
      "epoch: 26 step: 295, loss is 0.002627874258905649\n",
      "epoch: 26 step: 296, loss is 0.0008016654755920172\n",
      "epoch: 26 step: 297, loss is 0.0038698080461472273\n",
      "epoch: 26 step: 298, loss is 0.06269687414169312\n",
      "epoch: 26 step: 299, loss is 0.02950209192931652\n",
      "epoch: 26 step: 300, loss is 0.001156085985712707\n",
      "epoch: 26 step: 301, loss is 0.000489101919811219\n",
      "epoch: 26 step: 302, loss is 4.485955651034601e-05\n",
      "epoch: 26 step: 303, loss is 0.00045917704119347036\n",
      "epoch: 26 step: 304, loss is 0.0004423466161824763\n",
      "epoch: 26 step: 305, loss is 0.004483124706894159\n",
      "epoch: 26 step: 306, loss is 0.057161036878824234\n",
      "epoch: 26 step: 307, loss is 0.0020222209859639406\n",
      "epoch: 26 step: 308, loss is 0.009377111680805683\n",
      "epoch: 26 step: 309, loss is 0.0009774910286068916\n",
      "epoch: 26 step: 310, loss is 0.0009856288088485599\n",
      "epoch: 26 step: 311, loss is 0.0006659992504864931\n",
      "epoch: 26 step: 312, loss is 0.0010264546144753695\n",
      "epoch: 26 step: 313, loss is 0.012870981357991695\n",
      "epoch: 26 step: 314, loss is 0.005532754585146904\n",
      "epoch: 26 step: 315, loss is 0.006937921047210693\n",
      "epoch: 26 step: 316, loss is 0.015342723578214645\n",
      "epoch: 26 step: 317, loss is 0.0032586720772087574\n",
      "epoch: 26 step: 318, loss is 0.0032585759181529284\n",
      "epoch: 26 step: 319, loss is 0.0030723351519554853\n",
      "epoch: 26 step: 320, loss is 0.0014614228857681155\n",
      "epoch: 26 step: 321, loss is 5.7547913456801325e-05\n",
      "epoch: 26 step: 322, loss is 0.007179081905633211\n",
      "epoch: 26 step: 323, loss is 0.00023619407147634774\n",
      "epoch: 26 step: 324, loss is 0.016831697896122932\n",
      "epoch: 26 step: 325, loss is 0.00047722188173793256\n",
      "epoch: 26 step: 326, loss is 8.448413427686319e-05\n",
      "epoch: 26 step: 327, loss is 0.01367294229567051\n",
      "epoch: 26 step: 328, loss is 0.001152226934209466\n",
      "epoch: 26 step: 329, loss is 0.00042937626130878925\n",
      "epoch: 26 step: 330, loss is 0.003162704175338149\n",
      "epoch: 26 step: 331, loss is 0.0026660722214728594\n",
      "epoch: 26 step: 332, loss is 0.0006469227373600006\n",
      "epoch: 26 step: 333, loss is 0.00017599639249965549\n",
      "epoch: 26 step: 334, loss is 0.06404556334018707\n",
      "epoch: 26 step: 335, loss is 0.02053758129477501\n",
      "epoch: 26 step: 336, loss is 0.0030468201730400324\n",
      "epoch: 26 step: 337, loss is 0.0014641903107985854\n",
      "epoch: 26 step: 338, loss is 0.00032991665648296475\n",
      "epoch: 26 step: 339, loss is 0.0022929301485419273\n",
      "epoch: 26 step: 340, loss is 0.0002630662638694048\n",
      "epoch: 26 step: 341, loss is 0.002477674512192607\n",
      "epoch: 26 step: 342, loss is 0.000347501045325771\n",
      "epoch: 26 step: 343, loss is 0.02149766869843006\n",
      "epoch: 26 step: 344, loss is 0.012858922593295574\n",
      "epoch: 26 step: 345, loss is 0.03249930217862129\n",
      "epoch: 26 step: 346, loss is 0.004043710418045521\n",
      "epoch: 26 step: 347, loss is 0.012015831656754017\n",
      "epoch: 26 step: 348, loss is 0.0020431282464414835\n",
      "epoch: 26 step: 349, loss is 0.006447319872677326\n",
      "epoch: 26 step: 350, loss is 0.07804197072982788\n",
      "epoch: 26 step: 351, loss is 0.00045111984945833683\n",
      "epoch: 26 step: 352, loss is 0.06261328607797623\n",
      "epoch: 26 step: 353, loss is 0.023972924798727036\n",
      "epoch: 26 step: 354, loss is 0.014427831396460533\n",
      "epoch: 26 step: 355, loss is 0.015020180493593216\n",
      "epoch: 26 step: 356, loss is 0.002747158519923687\n",
      "epoch: 26 step: 357, loss is 0.0002479891409166157\n",
      "epoch: 26 step: 358, loss is 0.00048557925038039684\n",
      "epoch: 26 step: 359, loss is 0.012164048850536346\n",
      "epoch: 26 step: 360, loss is 0.007680907845497131\n",
      "epoch: 26 step: 361, loss is 0.0018510243389755487\n",
      "epoch: 26 step: 362, loss is 0.0031226451974362135\n",
      "epoch: 26 step: 363, loss is 0.0003735533682629466\n",
      "epoch: 26 step: 364, loss is 0.007939320057630539\n",
      "epoch: 26 step: 365, loss is 0.0028233323246240616\n",
      "epoch: 26 step: 366, loss is 0.04696593061089516\n",
      "epoch: 26 step: 367, loss is 6.134319846751168e-05\n",
      "epoch: 26 step: 368, loss is 0.0003834199160337448\n",
      "epoch: 26 step: 369, loss is 0.0023403100203722715\n",
      "epoch: 26 step: 370, loss is 0.019563140347599983\n",
      "epoch: 26 step: 371, loss is 0.0028318397235125303\n",
      "epoch: 26 step: 372, loss is 0.03833039849996567\n",
      "epoch: 26 step: 373, loss is 0.0011293505085632205\n",
      "epoch: 26 step: 374, loss is 0.017669955268502235\n",
      "epoch: 26 step: 375, loss is 0.0178473349660635\n",
      "epoch: 26 step: 376, loss is 0.009369752369821072\n",
      "epoch: 26 step: 377, loss is 0.0006801951676607132\n",
      "epoch: 26 step: 378, loss is 0.00010273006773786619\n",
      "epoch: 26 step: 379, loss is 0.00031247216975316405\n",
      "epoch: 26 step: 380, loss is 0.04117994382977486\n",
      "epoch: 26 step: 381, loss is 0.002351264236494899\n",
      "epoch: 26 step: 382, loss is 0.0017529011238366365\n",
      "epoch: 26 step: 383, loss is 0.0135078439489007\n",
      "epoch: 26 step: 384, loss is 2.27589607675327e-05\n",
      "epoch: 26 step: 385, loss is 0.00020707081421278417\n",
      "epoch: 26 step: 386, loss is 0.07104378193616867\n",
      "epoch: 26 step: 387, loss is 0.02858385071158409\n",
      "epoch: 26 step: 388, loss is 0.01779874600470066\n",
      "epoch: 26 step: 389, loss is 0.004806903190910816\n",
      "epoch: 26 step: 390, loss is 0.00018986525537911803\n",
      "epoch: 26 step: 391, loss is 0.0004524991672951728\n",
      "epoch: 26 step: 392, loss is 0.03482772037386894\n",
      "epoch: 26 step: 393, loss is 0.023099195212125778\n",
      "epoch: 26 step: 394, loss is 0.0028862410690635443\n",
      "epoch: 26 step: 395, loss is 0.00019880862964782864\n",
      "epoch: 26 step: 396, loss is 0.012279129587113857\n",
      "epoch: 26 step: 397, loss is 0.0005107850884087384\n",
      "epoch: 26 step: 398, loss is 0.0010222833370789886\n",
      "epoch: 26 step: 399, loss is 0.0019588114228099585\n",
      "epoch: 26 step: 400, loss is 0.0010913587175309658\n",
      "epoch: 26 step: 401, loss is 0.0002300167689099908\n",
      "epoch: 26 step: 402, loss is 0.011406314559280872\n",
      "epoch: 26 step: 403, loss is 0.004645512904971838\n",
      "epoch: 26 step: 404, loss is 0.00122373818885535\n",
      "epoch: 26 step: 405, loss is 0.0015619852347299457\n",
      "epoch: 26 step: 406, loss is 0.004687334410846233\n",
      "epoch: 26 step: 407, loss is 0.003206132212653756\n",
      "epoch: 26 step: 408, loss is 0.00015740063099656254\n",
      "epoch: 26 step: 409, loss is 0.09824393689632416\n",
      "epoch: 26 step: 410, loss is 0.002163487486541271\n",
      "epoch: 26 step: 411, loss is 0.006442215759307146\n",
      "epoch: 26 step: 412, loss is 0.0012496449053287506\n",
      "epoch: 26 step: 413, loss is 0.007055769208818674\n",
      "epoch: 26 step: 414, loss is 0.00035024905810132623\n",
      "epoch: 26 step: 415, loss is 0.002329762326553464\n",
      "epoch: 26 step: 416, loss is 0.0031496479641646147\n",
      "epoch: 26 step: 417, loss is 0.02484712563455105\n",
      "epoch: 26 step: 418, loss is 0.0018143898341804743\n",
      "epoch: 26 step: 419, loss is 0.0009855919051915407\n",
      "epoch: 26 step: 420, loss is 2.0876845155726187e-05\n",
      "epoch: 26 step: 421, loss is 0.004355232231318951\n",
      "epoch: 26 step: 422, loss is 0.003380006877705455\n",
      "epoch: 26 step: 423, loss is 0.001122362562455237\n",
      "epoch: 26 step: 424, loss is 0.001441494096070528\n",
      "epoch: 26 step: 425, loss is 0.0004842996713705361\n",
      "epoch: 26 step: 426, loss is 0.05049707740545273\n",
      "epoch: 26 step: 427, loss is 0.0031105452217161655\n",
      "epoch: 26 step: 428, loss is 0.000774229527451098\n",
      "epoch: 26 step: 429, loss is 0.011896026320755482\n",
      "epoch: 26 step: 430, loss is 0.001995011232793331\n",
      "epoch: 26 step: 431, loss is 0.0066795069724321365\n",
      "epoch: 26 step: 432, loss is 0.006574698258191347\n",
      "epoch: 26 step: 433, loss is 0.00016648601740598679\n",
      "epoch: 26 step: 434, loss is 0.0004113062168471515\n",
      "epoch: 26 step: 435, loss is 0.0006221008370630443\n",
      "epoch: 26 step: 436, loss is 0.0072445436380803585\n",
      "epoch: 26 step: 437, loss is 0.0001495798642281443\n",
      "epoch: 26 step: 438, loss is 0.006812062580138445\n",
      "epoch: 26 step: 439, loss is 0.0008493148488923907\n",
      "epoch: 26 step: 440, loss is 0.006706922315061092\n",
      "epoch: 26 step: 441, loss is 0.003462725318968296\n",
      "epoch: 26 step: 442, loss is 0.0012086834758520126\n",
      "epoch: 26 step: 443, loss is 0.001453446689993143\n",
      "epoch: 26 step: 444, loss is 0.00017682203906588256\n",
      "epoch: 26 step: 445, loss is 0.00726253492757678\n",
      "epoch: 26 step: 446, loss is 0.008986195549368858\n",
      "epoch: 26 step: 447, loss is 0.00046835182001814246\n",
      "epoch: 26 step: 448, loss is 0.007808659225702286\n",
      "epoch: 26 step: 449, loss is 0.02501094341278076\n",
      "epoch: 26 step: 450, loss is 0.0015480867587029934\n",
      "epoch: 26 step: 451, loss is 0.018570315092802048\n",
      "epoch: 26 step: 452, loss is 0.058355674147605896\n",
      "epoch: 26 step: 453, loss is 0.0028894315473735332\n",
      "epoch: 26 step: 454, loss is 0.0011418916983529925\n",
      "epoch: 26 step: 455, loss is 0.008004541508853436\n",
      "epoch: 26 step: 456, loss is 0.026610055938363075\n",
      "epoch: 26 step: 457, loss is 0.0015084397746250033\n",
      "epoch: 26 step: 458, loss is 0.002970488043501973\n",
      "epoch: 26 step: 459, loss is 0.04820866882801056\n",
      "epoch: 26 step: 460, loss is 0.000750442617572844\n",
      "epoch: 26 step: 461, loss is 0.00034813612001016736\n",
      "epoch: 26 step: 462, loss is 0.002583986148238182\n",
      "epoch: 26 step: 463, loss is 0.004032278433442116\n",
      "epoch: 26 step: 464, loss is 0.01914616860449314\n",
      "epoch: 26 step: 465, loss is 0.0005840322701260448\n",
      "epoch: 26 step: 466, loss is 0.0007797644357196987\n",
      "epoch: 26 step: 467, loss is 0.00018029112834483385\n",
      "epoch: 26 step: 468, loss is 0.012350684963166714\n",
      "epoch: 26 step: 469, loss is 0.001806285697966814\n",
      "epoch: 26 step: 470, loss is 0.0006211755098775029\n",
      "epoch: 26 step: 471, loss is 4.713720500149066e-06\n",
      "epoch: 26 step: 472, loss is 0.002194040222093463\n",
      "epoch: 26 step: 473, loss is 0.0029252376407384872\n",
      "epoch: 26 step: 474, loss is 0.00019112964218948036\n",
      "epoch: 26 step: 475, loss is 0.0018224784871563315\n",
      "epoch: 26 step: 476, loss is 0.0060149249620735645\n",
      "epoch: 26 step: 477, loss is 0.05306653678417206\n",
      "epoch: 26 step: 478, loss is 5.147321644471958e-05\n",
      "epoch: 26 step: 479, loss is 0.07218950241804123\n",
      "epoch: 26 step: 480, loss is 0.0030804676935076714\n",
      "epoch: 26 step: 481, loss is 0.0019948489498347044\n",
      "epoch: 26 step: 482, loss is 0.01882099360227585\n",
      "epoch: 26 step: 483, loss is 0.10556039214134216\n",
      "epoch: 26 step: 484, loss is 0.0007186406292021275\n",
      "epoch: 26 step: 485, loss is 0.028015701100230217\n",
      "epoch: 26 step: 486, loss is 0.002394481562077999\n",
      "epoch: 26 step: 487, loss is 0.014621123671531677\n",
      "epoch: 26 step: 488, loss is 0.067221499979496\n",
      "epoch: 26 step: 489, loss is 0.004373095463961363\n",
      "epoch: 26 step: 490, loss is 0.013441032730042934\n",
      "epoch: 26 step: 491, loss is 0.0016286008758470416\n",
      "epoch: 26 step: 492, loss is 0.030287709087133408\n",
      "epoch: 26 step: 493, loss is 0.029912112280726433\n",
      "epoch: 26 step: 494, loss is 0.001816329313442111\n",
      "epoch: 26 step: 495, loss is 0.0044210925698280334\n",
      "epoch: 26 step: 496, loss is 0.0010023531503975391\n",
      "epoch: 26 step: 497, loss is 0.0008653926779516041\n",
      "epoch: 26 step: 498, loss is 9.962703188648447e-05\n",
      "epoch: 26 step: 499, loss is 0.0006942382897250354\n",
      "epoch: 26 step: 500, loss is 0.0005097909597679973\n",
      "epoch: 26 step: 501, loss is 0.0007127245771698654\n",
      "epoch: 26 step: 502, loss is 4.6645549446111545e-05\n",
      "epoch: 26 step: 503, loss is 0.005517636425793171\n",
      "epoch: 26 step: 504, loss is 0.03337806835770607\n",
      "epoch: 26 step: 505, loss is 0.0016843017656356096\n",
      "epoch: 26 step: 506, loss is 0.0008591735386289656\n",
      "epoch: 26 step: 507, loss is 0.0006444238824769855\n",
      "epoch: 26 step: 508, loss is 0.00012647241237573326\n",
      "epoch: 26 step: 509, loss is 0.03623966872692108\n",
      "epoch: 26 step: 510, loss is 0.010966832749545574\n",
      "epoch: 26 step: 511, loss is 0.008314697071909904\n",
      "epoch: 26 step: 512, loss is 6.652058800682425e-05\n",
      "epoch: 26 step: 513, loss is 0.04944468289613724\n",
      "epoch: 26 step: 514, loss is 0.001282361103221774\n",
      "epoch: 26 step: 515, loss is 0.05672462657094002\n",
      "epoch: 26 step: 516, loss is 0.00034225446870550513\n",
      "epoch: 26 step: 517, loss is 0.0008357870392501354\n",
      "epoch: 26 step: 518, loss is 0.04826713725924492\n",
      "epoch: 26 step: 519, loss is 0.0041454811580479145\n",
      "epoch: 26 step: 520, loss is 0.02200392261147499\n",
      "epoch: 26 step: 521, loss is 0.0039555649273097515\n",
      "epoch: 26 step: 522, loss is 0.03940534591674805\n",
      "epoch: 26 step: 523, loss is 0.010767243802547455\n",
      "epoch: 26 step: 524, loss is 0.0012340067187324166\n",
      "epoch: 26 step: 525, loss is 0.01960163377225399\n",
      "epoch: 26 step: 526, loss is 0.0735379159450531\n",
      "epoch: 26 step: 527, loss is 0.0004614439094439149\n",
      "epoch: 26 step: 528, loss is 0.002611791482195258\n",
      "epoch: 26 step: 529, loss is 0.011759279295802116\n",
      "epoch: 26 step: 530, loss is 0.0003317143418826163\n",
      "epoch: 26 step: 531, loss is 0.104148730635643\n",
      "epoch: 26 step: 532, loss is 0.002975562121719122\n",
      "epoch: 26 step: 533, loss is 0.02451329305768013\n",
      "epoch: 26 step: 534, loss is 0.0022383127361536026\n",
      "epoch: 26 step: 535, loss is 0.03720330074429512\n",
      "epoch: 26 step: 536, loss is 0.003503635060042143\n",
      "epoch: 26 step: 537, loss is 0.00040169828571379185\n",
      "epoch: 26 step: 538, loss is 0.00923206377774477\n",
      "epoch: 26 step: 539, loss is 0.08354974538087845\n",
      "epoch: 26 step: 540, loss is 0.0029524872079491615\n",
      "epoch: 26 step: 541, loss is 0.0687033012509346\n",
      "epoch: 26 step: 542, loss is 0.09502511471509933\n",
      "epoch: 26 step: 543, loss is 0.0002395246847299859\n",
      "epoch: 26 step: 544, loss is 0.005109828431159258\n",
      "epoch: 26 step: 545, loss is 0.001171030686236918\n",
      "epoch: 26 step: 546, loss is 0.0005068785976618528\n",
      "epoch: 26 step: 547, loss is 0.017709920182824135\n",
      "epoch: 26 step: 548, loss is 0.0038991745095700026\n",
      "epoch: 26 step: 549, loss is 0.03114878572523594\n",
      "epoch: 26 step: 550, loss is 0.00014059923705644906\n",
      "epoch: 26 step: 551, loss is 0.017835648730397224\n",
      "epoch: 26 step: 552, loss is 0.005405116826295853\n",
      "epoch: 26 step: 553, loss is 0.00048755822353996336\n",
      "epoch: 26 step: 554, loss is 0.00762708717957139\n",
      "epoch: 26 step: 555, loss is 0.00570819852873683\n",
      "epoch: 26 step: 556, loss is 0.20195768773555756\n",
      "epoch: 26 step: 557, loss is 0.0025866737123578787\n",
      "epoch: 26 step: 558, loss is 0.005245801527053118\n",
      "epoch: 26 step: 559, loss is 0.0015801646513864398\n",
      "epoch: 26 step: 560, loss is 0.007201887667179108\n",
      "epoch: 26 step: 561, loss is 0.013458391651511192\n",
      "epoch: 26 step: 562, loss is 0.00015777285443618894\n",
      "epoch: 26 step: 563, loss is 0.003269077278673649\n",
      "epoch: 26 step: 564, loss is 0.0013947171391919255\n",
      "epoch: 26 step: 565, loss is 0.002158951247110963\n",
      "epoch: 26 step: 566, loss is 9.598824544809759e-05\n",
      "epoch: 26 step: 567, loss is 0.002410234184935689\n",
      "epoch: 26 step: 568, loss is 0.03144709765911102\n",
      "epoch: 26 step: 569, loss is 0.0020776791498064995\n",
      "epoch: 26 step: 570, loss is 1.4153481060930062e-05\n",
      "epoch: 26 step: 571, loss is 0.0018457293044775724\n",
      "epoch: 26 step: 572, loss is 0.0003581292403396219\n",
      "epoch: 26 step: 573, loss is 0.012221124954521656\n",
      "epoch: 26 step: 574, loss is 0.0012517739087343216\n",
      "epoch: 26 step: 575, loss is 0.004963984247297049\n",
      "epoch: 26 step: 576, loss is 0.006348491180688143\n",
      "epoch: 26 step: 577, loss is 0.0006874370737932622\n",
      "epoch: 26 step: 578, loss is 0.0016947643598541617\n",
      "epoch: 26 step: 579, loss is 0.012940478511154652\n",
      "epoch: 26 step: 580, loss is 0.049730297178030014\n",
      "epoch: 26 step: 581, loss is 0.0800306424498558\n",
      "epoch: 26 step: 582, loss is 0.011921280063688755\n",
      "epoch: 26 step: 583, loss is 0.00040214115870185196\n",
      "epoch: 26 step: 584, loss is 0.0005762336077168584\n",
      "epoch: 26 step: 585, loss is 0.0012169394176453352\n",
      "epoch: 26 step: 586, loss is 0.005313195288181305\n",
      "epoch: 26 step: 587, loss is 0.014259287156164646\n",
      "epoch: 26 step: 588, loss is 0.01437603123486042\n",
      "epoch: 26 step: 589, loss is 0.06531428545713425\n",
      "epoch: 26 step: 590, loss is 0.010470293462276459\n",
      "epoch: 26 step: 591, loss is 0.0024683258961886168\n",
      "epoch: 26 step: 592, loss is 0.0004746996273752302\n",
      "epoch: 26 step: 593, loss is 0.001041049719788134\n",
      "epoch: 26 step: 594, loss is 0.0011833413736894727\n",
      "epoch: 26 step: 595, loss is 0.003118160180747509\n",
      "epoch: 26 step: 596, loss is 0.0028337566182017326\n",
      "epoch: 26 step: 597, loss is 0.008154835551977158\n",
      "epoch: 26 step: 598, loss is 0.04386213794350624\n",
      "epoch: 26 step: 599, loss is 0.0006860443390905857\n",
      "epoch: 26 step: 600, loss is 0.0007774802506901324\n",
      "epoch: 26 step: 601, loss is 0.08705370128154755\n",
      "epoch: 26 step: 602, loss is 0.00017256467253901064\n",
      "epoch: 26 step: 603, loss is 0.0015390249900519848\n",
      "epoch: 26 step: 604, loss is 0.01259683258831501\n",
      "epoch: 26 step: 605, loss is 0.003143149660900235\n",
      "epoch: 26 step: 606, loss is 0.002051286632195115\n",
      "epoch: 26 step: 607, loss is 0.022277645766735077\n",
      "epoch: 26 step: 608, loss is 0.0028952383436262608\n",
      "epoch: 26 step: 609, loss is 0.0440187081694603\n",
      "epoch: 26 step: 610, loss is 0.00036020579864270985\n",
      "epoch: 26 step: 611, loss is 0.016675522550940514\n",
      "epoch: 26 step: 612, loss is 0.0027616205625236034\n",
      "epoch: 26 step: 613, loss is 0.00024307651619892567\n",
      "epoch: 26 step: 614, loss is 0.10286257416009903\n",
      "epoch: 26 step: 615, loss is 0.0026541713159531355\n",
      "epoch: 26 step: 616, loss is 0.009350988082587719\n",
      "epoch: 26 step: 617, loss is 0.008485764265060425\n",
      "epoch: 26 step: 618, loss is 9.718596993479878e-05\n",
      "epoch: 26 step: 619, loss is 0.06214625760912895\n",
      "epoch: 26 step: 620, loss is 0.0005645007477141917\n",
      "epoch: 26 step: 621, loss is 0.0013312820810824633\n",
      "epoch: 26 step: 622, loss is 0.0009732989128679037\n",
      "epoch: 26 step: 623, loss is 0.0012783249840140343\n",
      "epoch: 26 step: 624, loss is 0.0065019456669688225\n",
      "epoch: 26 step: 625, loss is 9.43579216254875e-05\n",
      "epoch: 26 step: 626, loss is 0.0002787202247418463\n",
      "epoch: 26 step: 627, loss is 0.00010826394282048568\n",
      "epoch: 26 step: 628, loss is 0.017391832545399666\n",
      "epoch: 26 step: 629, loss is 0.038757145404815674\n",
      "epoch: 26 step: 630, loss is 5.8700781664811075e-05\n",
      "epoch: 26 step: 631, loss is 0.0030619949102401733\n",
      "epoch: 26 step: 632, loss is 0.024365367367863655\n",
      "epoch: 26 step: 633, loss is 0.0007091031293384731\n",
      "epoch: 26 step: 634, loss is 0.011788037605583668\n",
      "epoch: 26 step: 635, loss is 0.002633552299812436\n",
      "epoch: 26 step: 636, loss is 0.0005372704472392797\n",
      "epoch: 26 step: 637, loss is 0.001163496170192957\n",
      "epoch: 26 step: 638, loss is 0.0006795665249228477\n",
      "epoch: 26 step: 639, loss is 0.012453143484890461\n",
      "epoch: 26 step: 640, loss is 0.010267745703458786\n",
      "epoch: 26 step: 641, loss is 0.0005402033566497266\n",
      "epoch: 26 step: 642, loss is 0.015941541641950607\n",
      "epoch: 26 step: 643, loss is 0.00655642943456769\n",
      "epoch: 26 step: 644, loss is 0.04854057729244232\n",
      "epoch: 26 step: 645, loss is 0.03254702687263489\n",
      "epoch: 26 step: 646, loss is 0.0015046440530568361\n",
      "epoch: 26 step: 647, loss is 0.011655417270958424\n",
      "epoch: 26 step: 648, loss is 0.0031470537651330233\n",
      "epoch: 26 step: 649, loss is 0.0002828050637617707\n",
      "epoch: 26 step: 650, loss is 0.004925596062093973\n",
      "epoch: 26 step: 651, loss is 9.658880298957229e-05\n",
      "epoch: 26 step: 652, loss is 0.0004491735599003732\n",
      "epoch: 26 step: 653, loss is 0.016482722014188766\n",
      "epoch: 26 step: 654, loss is 0.00024199792824219912\n",
      "epoch: 26 step: 655, loss is 0.00018048733181785792\n",
      "epoch: 26 step: 656, loss is 0.08830063045024872\n",
      "epoch: 26 step: 657, loss is 1.1077871931775007e-05\n",
      "epoch: 26 step: 658, loss is 0.03956199809908867\n",
      "epoch: 26 step: 659, loss is 0.040544915944337845\n",
      "epoch: 26 step: 660, loss is 0.025744853541254997\n",
      "epoch: 26 step: 661, loss is 0.007070885971188545\n",
      "epoch: 26 step: 662, loss is 0.0009724534465931356\n",
      "epoch: 26 step: 663, loss is 0.00014943898713681847\n",
      "epoch: 26 step: 664, loss is 0.06411765515804291\n",
      "epoch: 26 step: 665, loss is 0.00024617809685878456\n",
      "epoch: 26 step: 666, loss is 0.06081987917423248\n",
      "epoch: 26 step: 667, loss is 0.0010243606520816684\n",
      "epoch: 26 step: 668, loss is 0.039012983441352844\n",
      "epoch: 26 step: 669, loss is 9.795307414606214e-05\n",
      "epoch: 26 step: 670, loss is 0.0008410217124037445\n",
      "epoch: 26 step: 671, loss is 0.06145836412906647\n",
      "epoch: 26 step: 672, loss is 0.011038566939532757\n",
      "epoch: 26 step: 673, loss is 0.00021370765171013772\n",
      "epoch: 26 step: 674, loss is 0.0008545385207980871\n",
      "epoch: 26 step: 675, loss is 3.13407726935111e-05\n",
      "epoch: 26 step: 676, loss is 0.07836514711380005\n",
      "epoch: 26 step: 677, loss is 0.02280755713582039\n",
      "epoch: 26 step: 678, loss is 0.008919456042349339\n",
      "epoch: 26 step: 679, loss is 0.007833252660930157\n",
      "epoch: 26 step: 680, loss is 0.00036904349690303206\n",
      "epoch: 26 step: 681, loss is 0.007380191702395678\n",
      "epoch: 26 step: 682, loss is 0.0005657959845848382\n",
      "epoch: 26 step: 683, loss is 0.0016938721528276801\n",
      "epoch: 26 step: 684, loss is 0.0028039426542818546\n",
      "epoch: 26 step: 685, loss is 0.029251260682940483\n",
      "epoch: 26 step: 686, loss is 0.0335078239440918\n",
      "epoch: 26 step: 687, loss is 0.00014953871141187847\n",
      "epoch: 26 step: 688, loss is 0.0019805999472737312\n",
      "epoch: 26 step: 689, loss is 0.003737672232091427\n",
      "epoch: 26 step: 690, loss is 0.00037838841672055423\n",
      "epoch: 26 step: 691, loss is 0.0006875037215650082\n",
      "epoch: 26 step: 692, loss is 0.020629333332180977\n",
      "epoch: 26 step: 693, loss is 0.001749307499267161\n",
      "epoch: 26 step: 694, loss is 0.0002801408409141004\n",
      "epoch: 26 step: 695, loss is 0.020478587597608566\n",
      "epoch: 26 step: 696, loss is 0.029730772599577904\n",
      "epoch: 26 step: 697, loss is 0.033092644065618515\n",
      "epoch: 26 step: 698, loss is 0.005695634055882692\n",
      "epoch: 26 step: 699, loss is 0.0008759624324738979\n",
      "epoch: 26 step: 700, loss is 0.0008683728519827127\n",
      "epoch: 26 step: 701, loss is 0.007363310549408197\n",
      "epoch: 26 step: 702, loss is 0.000585984846111387\n",
      "epoch: 26 step: 703, loss is 0.001613402389921248\n",
      "epoch: 26 step: 704, loss is 0.09088741987943649\n",
      "epoch: 26 step: 705, loss is 0.013265705667436123\n",
      "epoch: 26 step: 706, loss is 0.0007384335622191429\n",
      "epoch: 26 step: 707, loss is 0.04680000618100166\n",
      "epoch: 26 step: 708, loss is 0.012810424901545048\n",
      "epoch: 26 step: 709, loss is 0.010618215426802635\n",
      "epoch: 26 step: 710, loss is 0.005769362673163414\n",
      "epoch: 26 step: 711, loss is 0.0028996437322348356\n",
      "epoch: 26 step: 712, loss is 0.0005744614754803479\n",
      "epoch: 26 step: 713, loss is 0.0034045821521431208\n",
      "epoch: 26 step: 714, loss is 0.003980704117566347\n",
      "epoch: 26 step: 715, loss is 0.006548888515681028\n",
      "epoch: 26 step: 716, loss is 0.024091597646474838\n",
      "epoch: 26 step: 717, loss is 0.004301973152905703\n",
      "epoch: 26 step: 718, loss is 0.021900609135627747\n",
      "epoch: 26 step: 719, loss is 0.010965077206492424\n",
      "epoch: 26 step: 720, loss is 0.004143872763961554\n",
      "epoch: 26 step: 721, loss is 0.0006116281147114933\n",
      "epoch: 26 step: 722, loss is 0.07800403237342834\n",
      "epoch: 26 step: 723, loss is 0.09532495588064194\n",
      "epoch: 26 step: 724, loss is 0.03036114200949669\n",
      "epoch: 26 step: 725, loss is 0.018436428159475327\n",
      "epoch: 26 step: 726, loss is 0.00166138238273561\n",
      "epoch: 26 step: 727, loss is 0.004128879867494106\n",
      "epoch: 26 step: 728, loss is 0.04152270406484604\n",
      "epoch: 26 step: 729, loss is 0.0034215981140732765\n",
      "epoch: 26 step: 730, loss is 0.002396022668108344\n",
      "epoch: 26 step: 731, loss is 0.0021576285362243652\n",
      "epoch: 26 step: 732, loss is 0.00023604696616530418\n",
      "epoch: 26 step: 733, loss is 0.029139703139662743\n",
      "epoch: 26 step: 734, loss is 0.0003400287823751569\n",
      "epoch: 26 step: 735, loss is 0.001342148520052433\n",
      "epoch: 26 step: 736, loss is 0.013473023660480976\n",
      "epoch: 26 step: 737, loss is 0.021022828295826912\n",
      "epoch: 26 step: 738, loss is 0.007669993210583925\n",
      "epoch: 26 step: 739, loss is 0.0037832283414900303\n",
      "epoch: 26 step: 740, loss is 0.04109926149249077\n",
      "epoch: 26 step: 741, loss is 0.061032671481370926\n",
      "epoch: 26 step: 742, loss is 0.0002831091114785522\n",
      "epoch: 26 step: 743, loss is 0.006518070586025715\n",
      "epoch: 26 step: 744, loss is 0.0465647429227829\n",
      "epoch: 26 step: 745, loss is 0.02826830744743347\n",
      "epoch: 26 step: 746, loss is 0.0020052201580256224\n",
      "epoch: 26 step: 747, loss is 0.00031643142574466765\n",
      "epoch: 26 step: 748, loss is 0.03314386308193207\n",
      "epoch: 26 step: 749, loss is 0.02554374188184738\n",
      "epoch: 26 step: 750, loss is 0.06567308306694031\n",
      "epoch: 26 step: 751, loss is 0.02865435928106308\n",
      "epoch: 26 step: 752, loss is 1.0561843737377785e-05\n",
      "epoch: 26 step: 753, loss is 0.0033680235501378775\n",
      "epoch: 26 step: 754, loss is 0.0002049824397545308\n",
      "epoch: 26 step: 755, loss is 0.0005149778444319963\n",
      "epoch: 26 step: 756, loss is 0.032539889216423035\n",
      "epoch: 26 step: 757, loss is 0.03211328014731407\n",
      "epoch: 26 step: 758, loss is 0.028102416545152664\n",
      "epoch: 26 step: 759, loss is 0.029544617980718613\n",
      "epoch: 26 step: 760, loss is 0.07176901400089264\n",
      "epoch: 26 step: 761, loss is 0.0055937692523002625\n",
      "epoch: 26 step: 762, loss is 0.011019594967365265\n",
      "epoch: 26 step: 763, loss is 0.0003532532136887312\n",
      "epoch: 26 step: 764, loss is 0.047311101108789444\n",
      "epoch: 26 step: 765, loss is 0.00026393949519842863\n",
      "epoch: 26 step: 766, loss is 0.0019611488096415997\n",
      "epoch: 26 step: 767, loss is 0.0033617650624364614\n",
      "epoch: 26 step: 768, loss is 0.01308220811188221\n",
      "epoch: 26 step: 769, loss is 0.018232740461826324\n",
      "epoch: 26 step: 770, loss is 0.004640776198357344\n",
      "epoch: 26 step: 771, loss is 0.00025596044724807143\n",
      "epoch: 26 step: 772, loss is 0.0002731860731728375\n",
      "epoch: 26 step: 773, loss is 0.0006122232298366725\n",
      "epoch: 26 step: 774, loss is 0.007370627485215664\n",
      "epoch: 26 step: 775, loss is 0.006486754398792982\n",
      "epoch: 26 step: 776, loss is 0.01551727019250393\n",
      "epoch: 26 step: 777, loss is 0.05133828520774841\n",
      "epoch: 26 step: 778, loss is 0.013158281333744526\n",
      "epoch: 26 step: 779, loss is 0.0792468711733818\n",
      "epoch: 26 step: 780, loss is 0.006634929217398167\n",
      "epoch: 26 step: 781, loss is 0.0029283203184604645\n",
      "epoch: 26 step: 782, loss is 0.0041036284528672695\n",
      "epoch: 26 step: 783, loss is 0.018247179687023163\n",
      "epoch: 26 step: 784, loss is 7.045276288408786e-05\n",
      "epoch: 26 step: 785, loss is 0.03225714713335037\n",
      "epoch: 26 step: 786, loss is 0.0038718937430530787\n",
      "epoch: 26 step: 787, loss is 0.002451930893585086\n",
      "epoch: 26 step: 788, loss is 0.0038315074052661657\n",
      "epoch: 26 step: 789, loss is 0.00025003193877637386\n",
      "epoch: 26 step: 790, loss is 0.004592383746057749\n",
      "epoch: 26 step: 791, loss is 0.00016250874614343047\n",
      "epoch: 26 step: 792, loss is 0.004492654465138912\n",
      "epoch: 26 step: 793, loss is 0.03193820267915726\n",
      "epoch: 26 step: 794, loss is 0.008158458396792412\n",
      "epoch: 26 step: 795, loss is 0.07019709795713425\n",
      "epoch: 26 step: 796, loss is 0.0017181019065901637\n",
      "epoch: 26 step: 797, loss is 0.00014468685549218208\n",
      "epoch: 26 step: 798, loss is 0.004191022366285324\n",
      "epoch: 26 step: 799, loss is 0.07010737806558609\n",
      "epoch: 26 step: 800, loss is 0.0010296485852450132\n",
      "epoch: 26 step: 801, loss is 0.01036922913044691\n",
      "epoch: 26 step: 802, loss is 0.01206940971314907\n",
      "epoch: 26 step: 803, loss is 0.019183501601219177\n",
      "epoch: 26 step: 804, loss is 0.0037016726564615965\n",
      "epoch: 26 step: 805, loss is 0.023648405447602272\n",
      "epoch: 26 step: 806, loss is 0.004583784379065037\n",
      "epoch: 26 step: 807, loss is 0.028714897111058235\n",
      "epoch: 26 step: 808, loss is 0.00554659916087985\n",
      "epoch: 26 step: 809, loss is 0.00032314661075361073\n",
      "epoch: 26 step: 810, loss is 0.006681807339191437\n",
      "epoch: 26 step: 811, loss is 0.01758706010878086\n",
      "epoch: 26 step: 812, loss is 0.05833433195948601\n",
      "epoch: 26 step: 813, loss is 0.0017262762412428856\n",
      "epoch: 26 step: 814, loss is 0.0005541973514482379\n",
      "epoch: 26 step: 815, loss is 0.00030520535074174404\n",
      "epoch: 26 step: 816, loss is 0.11342228949069977\n",
      "epoch: 26 step: 817, loss is 1.2361797416815534e-05\n",
      "epoch: 26 step: 818, loss is 0.001677107298746705\n",
      "epoch: 26 step: 819, loss is 0.011755963787436485\n",
      "epoch: 26 step: 820, loss is 0.02537594921886921\n",
      "epoch: 26 step: 821, loss is 0.08289623260498047\n",
      "epoch: 26 step: 822, loss is 0.07516400516033173\n",
      "epoch: 26 step: 823, loss is 0.017283707857131958\n",
      "epoch: 26 step: 824, loss is 0.0006015630788169801\n",
      "epoch: 26 step: 825, loss is 0.061424367129802704\n",
      "epoch: 26 step: 826, loss is 0.05843217298388481\n",
      "epoch: 26 step: 827, loss is 0.03568125143647194\n",
      "epoch: 26 step: 828, loss is 0.06890931725502014\n",
      "epoch: 26 step: 829, loss is 0.008230066858232021\n",
      "epoch: 26 step: 830, loss is 0.0005079013062641025\n",
      "epoch: 26 step: 831, loss is 0.02572130784392357\n",
      "epoch: 26 step: 832, loss is 0.00031799913267605007\n",
      "epoch: 26 step: 833, loss is 0.05896247923374176\n",
      "epoch: 26 step: 834, loss is 0.00027544720796868205\n",
      "epoch: 26 step: 835, loss is 0.012750018388032913\n",
      "epoch: 26 step: 836, loss is 0.006119164172559977\n",
      "epoch: 26 step: 837, loss is 0.0404769629240036\n",
      "epoch: 26 step: 838, loss is 0.011888430453836918\n",
      "epoch: 26 step: 839, loss is 0.006728033535182476\n",
      "epoch: 26 step: 840, loss is 0.03199005126953125\n",
      "epoch: 26 step: 841, loss is 0.010514450259506702\n",
      "epoch: 26 step: 842, loss is 0.005507191177457571\n",
      "epoch: 26 step: 843, loss is 0.002684767358005047\n",
      "epoch: 26 step: 844, loss is 0.004987668711692095\n",
      "epoch: 26 step: 845, loss is 0.013319885358214378\n",
      "epoch: 26 step: 846, loss is 0.06348687410354614\n",
      "epoch: 26 step: 847, loss is 0.00013748739729635417\n",
      "epoch: 26 step: 848, loss is 0.014375829137861729\n",
      "epoch: 26 step: 849, loss is 0.0060693928971886635\n",
      "epoch: 26 step: 850, loss is 0.1696603149175644\n",
      "epoch: 26 step: 851, loss is 0.008833721280097961\n",
      "epoch: 26 step: 852, loss is 0.0011760869529098272\n",
      "epoch: 26 step: 853, loss is 0.00014734217256773263\n",
      "epoch: 26 step: 854, loss is 0.02547542378306389\n",
      "epoch: 26 step: 855, loss is 0.002567507326602936\n",
      "epoch: 26 step: 856, loss is 0.0233844593167305\n",
      "epoch: 26 step: 857, loss is 0.0009408183395862579\n",
      "epoch: 26 step: 858, loss is 0.0015652287984266877\n",
      "epoch: 26 step: 859, loss is 0.0036803872790187597\n",
      "epoch: 26 step: 860, loss is 0.0007659510592930019\n",
      "epoch: 26 step: 861, loss is 0.02305995672941208\n",
      "epoch: 26 step: 862, loss is 0.017964519560337067\n",
      "epoch: 26 step: 863, loss is 0.003460252657532692\n",
      "epoch: 26 step: 864, loss is 0.00503068882972002\n",
      "epoch: 26 step: 865, loss is 0.03612256422638893\n",
      "epoch: 26 step: 866, loss is 0.06317655742168427\n",
      "epoch: 26 step: 867, loss is 0.0035431014839559793\n",
      "epoch: 26 step: 868, loss is 0.012978562153875828\n",
      "epoch: 26 step: 869, loss is 0.0004759814473800361\n",
      "epoch: 26 step: 870, loss is 0.007539559155702591\n",
      "epoch: 26 step: 871, loss is 0.0015163089847192168\n",
      "epoch: 26 step: 872, loss is 0.002039584331214428\n",
      "epoch: 26 step: 873, loss is 0.040100473910570145\n",
      "epoch: 26 step: 874, loss is 0.0013036722084507346\n",
      "epoch: 26 step: 875, loss is 0.03657587617635727\n",
      "epoch: 26 step: 876, loss is 0.0033621785696595907\n",
      "epoch: 26 step: 877, loss is 0.11511751264333725\n",
      "epoch: 26 step: 878, loss is 0.003435910679399967\n",
      "epoch: 26 step: 879, loss is 0.025832409039139748\n",
      "epoch: 26 step: 880, loss is 0.05509325489401817\n",
      "epoch: 26 step: 881, loss is 0.012955816462635994\n",
      "epoch: 26 step: 882, loss is 0.016770580783486366\n",
      "epoch: 26 step: 883, loss is 0.0007162155816331506\n",
      "epoch: 26 step: 884, loss is 0.045741140842437744\n",
      "epoch: 26 step: 885, loss is 0.04609256982803345\n",
      "epoch: 26 step: 886, loss is 0.0012974789133295417\n",
      "epoch: 26 step: 887, loss is 0.08120463788509369\n",
      "epoch: 26 step: 888, loss is 0.014355278573930264\n",
      "epoch: 26 step: 889, loss is 0.005342746619135141\n",
      "epoch: 26 step: 890, loss is 0.0015160279581323266\n",
      "epoch: 26 step: 891, loss is 0.0001135765778599307\n",
      "epoch: 26 step: 892, loss is 0.021045038476586342\n",
      "epoch: 26 step: 893, loss is 0.0006636659381911159\n",
      "epoch: 26 step: 894, loss is 0.0294467955827713\n",
      "epoch: 26 step: 895, loss is 0.01501539722084999\n",
      "epoch: 26 step: 896, loss is 0.06906062364578247\n",
      "epoch: 26 step: 897, loss is 0.003330076113343239\n",
      "epoch: 26 step: 898, loss is 0.02533780038356781\n",
      "epoch: 26 step: 899, loss is 0.002030743984505534\n",
      "epoch: 26 step: 900, loss is 0.0014727426460012794\n",
      "epoch: 26 step: 901, loss is 0.004151517990976572\n",
      "epoch: 26 step: 902, loss is 0.050891127437353134\n",
      "epoch: 26 step: 903, loss is 0.0018346309661865234\n",
      "epoch: 26 step: 904, loss is 0.00018246949184685946\n",
      "epoch: 26 step: 905, loss is 0.035806700587272644\n",
      "epoch: 26 step: 906, loss is 0.00042861312977038324\n",
      "epoch: 26 step: 907, loss is 0.0585944689810276\n",
      "epoch: 26 step: 908, loss is 0.01050455030053854\n",
      "epoch: 26 step: 909, loss is 0.07873398810625076\n",
      "epoch: 26 step: 910, loss is 0.008973469957709312\n",
      "epoch: 26 step: 911, loss is 0.008142692036926746\n",
      "epoch: 26 step: 912, loss is 0.0007985757547430694\n",
      "epoch: 26 step: 913, loss is 0.013542743399739265\n",
      "epoch: 26 step: 914, loss is 0.00941910594701767\n",
      "epoch: 26 step: 915, loss is 0.0018896694527938962\n",
      "epoch: 26 step: 916, loss is 0.07088810205459595\n",
      "epoch: 26 step: 917, loss is 0.021292896941304207\n",
      "epoch: 26 step: 918, loss is 0.0019865494687110186\n",
      "epoch: 26 step: 919, loss is 0.03839083015918732\n",
      "epoch: 26 step: 920, loss is 0.10600121319293976\n",
      "epoch: 26 step: 921, loss is 0.008064468391239643\n",
      "epoch: 26 step: 922, loss is 0.0005788257694803178\n",
      "epoch: 26 step: 923, loss is 0.0005256120348349214\n",
      "epoch: 26 step: 924, loss is 0.031129896640777588\n",
      "epoch: 26 step: 925, loss is 0.025903277099132538\n",
      "epoch: 26 step: 926, loss is 0.00029523478588089347\n",
      "epoch: 26 step: 927, loss is 0.0005169892683625221\n",
      "epoch: 26 step: 928, loss is 0.007655606139451265\n",
      "epoch: 26 step: 929, loss is 0.0033561212476342916\n",
      "epoch: 26 step: 930, loss is 0.002026668982580304\n",
      "epoch: 26 step: 931, loss is 0.0017195326508954167\n",
      "epoch: 26 step: 932, loss is 0.014509941451251507\n",
      "epoch: 26 step: 933, loss is 0.007103866431862116\n",
      "epoch: 26 step: 934, loss is 0.0007397335721179843\n",
      "epoch: 26 step: 935, loss is 0.0037413386162370443\n",
      "epoch: 26 step: 936, loss is 0.025801820680499077\n",
      "epoch: 26 step: 937, loss is 0.0014164395397529006\n",
      "epoch: 27 step: 1, loss is 0.0005187790375202894\n",
      "epoch: 27 step: 2, loss is 0.0362330824136734\n",
      "epoch: 27 step: 3, loss is 0.00026239457656629384\n",
      "epoch: 27 step: 4, loss is 0.0011397505877539515\n",
      "epoch: 27 step: 5, loss is 0.004616967868059874\n",
      "epoch: 27 step: 6, loss is 0.002624069806188345\n",
      "epoch: 27 step: 7, loss is 0.015288185328245163\n",
      "epoch: 27 step: 8, loss is 0.0017743755597621202\n",
      "epoch: 27 step: 9, loss is 0.00019299572159070522\n",
      "epoch: 27 step: 10, loss is 0.00019178474030923098\n",
      "epoch: 27 step: 11, loss is 0.002214600797742605\n",
      "epoch: 27 step: 12, loss is 0.006432597991079092\n",
      "epoch: 27 step: 13, loss is 0.05444854870438576\n",
      "epoch: 27 step: 14, loss is 0.000602187414187938\n",
      "epoch: 27 step: 15, loss is 0.02890978753566742\n",
      "epoch: 27 step: 16, loss is 0.0005831117159686983\n",
      "epoch: 27 step: 17, loss is 0.0019739139825105667\n",
      "epoch: 27 step: 18, loss is 0.008952902629971504\n",
      "epoch: 27 step: 19, loss is 0.002153808483853936\n",
      "epoch: 27 step: 20, loss is 6.379433034453541e-05\n",
      "epoch: 27 step: 21, loss is 0.0035100942477583885\n",
      "epoch: 27 step: 22, loss is 0.004824152681976557\n",
      "epoch: 27 step: 23, loss is 0.005567292217165232\n",
      "epoch: 27 step: 24, loss is 9.62205831456231e-06\n",
      "epoch: 27 step: 25, loss is 0.03079334832727909\n",
      "epoch: 27 step: 26, loss is 0.011066220700740814\n",
      "epoch: 27 step: 27, loss is 0.007341273594647646\n",
      "epoch: 27 step: 28, loss is 0.00047467637341469526\n",
      "epoch: 27 step: 29, loss is 0.0011813320452347398\n",
      "epoch: 27 step: 30, loss is 0.004471927415579557\n",
      "epoch: 27 step: 31, loss is 0.01296829991042614\n",
      "epoch: 27 step: 32, loss is 0.0013993128668516874\n",
      "epoch: 27 step: 33, loss is 0.0009957390138879418\n",
      "epoch: 27 step: 34, loss is 0.0033275079913437366\n",
      "epoch: 27 step: 35, loss is 0.005543842446058989\n",
      "epoch: 27 step: 36, loss is 0.004788333084434271\n",
      "epoch: 27 step: 37, loss is 0.0005246049258857965\n",
      "epoch: 27 step: 38, loss is 0.012047058902680874\n",
      "epoch: 27 step: 39, loss is 0.0021564920898526907\n",
      "epoch: 27 step: 40, loss is 0.0002182981843361631\n",
      "epoch: 27 step: 41, loss is 0.0031671193428337574\n",
      "epoch: 27 step: 42, loss is 0.005674577783793211\n",
      "epoch: 27 step: 43, loss is 0.001458622980862856\n",
      "epoch: 27 step: 44, loss is 0.00017352536087855697\n",
      "epoch: 27 step: 45, loss is 0.0007088517304509878\n",
      "epoch: 27 step: 46, loss is 0.025319868698716164\n",
      "epoch: 27 step: 47, loss is 0.00027370345196686685\n",
      "epoch: 27 step: 48, loss is 0.0034726965241134167\n",
      "epoch: 27 step: 49, loss is 6.192177534103394e-05\n",
      "epoch: 27 step: 50, loss is 0.005094648338854313\n",
      "epoch: 27 step: 51, loss is 0.0019738366827368736\n",
      "epoch: 27 step: 52, loss is 0.03320802003145218\n",
      "epoch: 27 step: 53, loss is 0.0011576316319406033\n",
      "epoch: 27 step: 54, loss is 0.0022995113395154476\n",
      "epoch: 27 step: 55, loss is 0.0011752346763387322\n",
      "epoch: 27 step: 56, loss is 0.08878334611654282\n",
      "epoch: 27 step: 57, loss is 0.042741574347019196\n",
      "epoch: 27 step: 58, loss is 0.0044636898674070835\n",
      "epoch: 27 step: 59, loss is 0.0200211089104414\n",
      "epoch: 27 step: 60, loss is 0.008278148248791695\n",
      "epoch: 27 step: 61, loss is 0.0010067897383123636\n",
      "epoch: 27 step: 62, loss is 0.0012493940303102136\n",
      "epoch: 27 step: 63, loss is 0.003287996631115675\n",
      "epoch: 27 step: 64, loss is 0.0003559165634214878\n",
      "epoch: 27 step: 65, loss is 0.0013936897739768028\n",
      "epoch: 27 step: 66, loss is 0.0025616497732698917\n",
      "epoch: 27 step: 67, loss is 0.0005160284345038235\n",
      "epoch: 27 step: 68, loss is 0.0005994628882035613\n",
      "epoch: 27 step: 69, loss is 0.007793258409947157\n",
      "epoch: 27 step: 70, loss is 0.001566097023896873\n",
      "epoch: 27 step: 71, loss is 0.003404549090191722\n",
      "epoch: 27 step: 72, loss is 0.000823842128738761\n",
      "epoch: 27 step: 73, loss is 6.524171476485208e-05\n",
      "epoch: 27 step: 74, loss is 0.00025714546791277826\n",
      "epoch: 27 step: 75, loss is 0.007130722515285015\n",
      "epoch: 27 step: 76, loss is 0.0002683836792130023\n",
      "epoch: 27 step: 77, loss is 0.04439378157258034\n",
      "epoch: 27 step: 78, loss is 0.012076840735971928\n",
      "epoch: 27 step: 79, loss is 0.0019308888586238027\n",
      "epoch: 27 step: 80, loss is 0.00016490362759213895\n",
      "epoch: 27 step: 81, loss is 0.000555644859559834\n",
      "epoch: 27 step: 82, loss is 0.0010397146688774228\n",
      "epoch: 27 step: 83, loss is 0.0011267043882980943\n",
      "epoch: 27 step: 84, loss is 0.00245619285851717\n",
      "epoch: 27 step: 85, loss is 0.01050267368555069\n",
      "epoch: 27 step: 86, loss is 0.0012684210669249296\n",
      "epoch: 27 step: 87, loss is 0.002338019199669361\n",
      "epoch: 27 step: 88, loss is 0.006783459335565567\n",
      "epoch: 27 step: 89, loss is 0.00025411995011381805\n",
      "epoch: 27 step: 90, loss is 0.0021747946739196777\n",
      "epoch: 27 step: 91, loss is 0.005997941829264164\n",
      "epoch: 27 step: 92, loss is 0.0008188650826923549\n",
      "epoch: 27 step: 93, loss is 0.0010100475046783686\n",
      "epoch: 27 step: 94, loss is 0.0009497799328528345\n",
      "epoch: 27 step: 95, loss is 0.00028553279116749763\n",
      "epoch: 27 step: 96, loss is 0.011426126584410667\n",
      "epoch: 27 step: 97, loss is 0.003186684800311923\n",
      "epoch: 27 step: 98, loss is 0.0011451776372268796\n",
      "epoch: 27 step: 99, loss is 0.0031684485729783773\n",
      "epoch: 27 step: 100, loss is 0.0009183188085444272\n",
      "epoch: 27 step: 101, loss is 0.0016336359549313784\n",
      "epoch: 27 step: 102, loss is 0.00037060744944028556\n",
      "epoch: 27 step: 103, loss is 0.03412511199712753\n",
      "epoch: 27 step: 104, loss is 0.0003483887412585318\n",
      "epoch: 27 step: 105, loss is 0.004066644236445427\n",
      "epoch: 27 step: 106, loss is 0.011722556315362453\n",
      "epoch: 27 step: 107, loss is 0.0002867752918973565\n",
      "epoch: 27 step: 108, loss is 0.001905362936668098\n",
      "epoch: 27 step: 109, loss is 0.0035340238828212023\n",
      "epoch: 27 step: 110, loss is 0.00824976060539484\n",
      "epoch: 27 step: 111, loss is 0.002866446739062667\n",
      "epoch: 27 step: 112, loss is 0.0034280524123460054\n",
      "epoch: 27 step: 113, loss is 5.180522566661239e-05\n",
      "epoch: 27 step: 114, loss is 0.03725855052471161\n",
      "epoch: 27 step: 115, loss is 0.0009890825022011995\n",
      "epoch: 27 step: 116, loss is 0.004335332661867142\n",
      "epoch: 27 step: 117, loss is 3.6257057217881083e-05\n",
      "epoch: 27 step: 118, loss is 0.006515452638268471\n",
      "epoch: 27 step: 119, loss is 0.0014796718023717403\n",
      "epoch: 27 step: 120, loss is 0.001761847292073071\n",
      "epoch: 27 step: 121, loss is 0.002134618815034628\n",
      "epoch: 27 step: 122, loss is 0.004308581817895174\n",
      "epoch: 27 step: 123, loss is 0.00031126191606745124\n",
      "epoch: 27 step: 124, loss is 0.061678845435380936\n",
      "epoch: 27 step: 125, loss is 0.0003519777674227953\n",
      "epoch: 27 step: 126, loss is 0.0002928416652139276\n",
      "epoch: 27 step: 127, loss is 0.0004651685885619372\n",
      "epoch: 27 step: 128, loss is 0.009892658330500126\n",
      "epoch: 27 step: 129, loss is 0.0004016805032733828\n",
      "epoch: 27 step: 130, loss is 0.030674835667014122\n",
      "epoch: 27 step: 131, loss is 0.0708184465765953\n",
      "epoch: 27 step: 132, loss is 0.019107528030872345\n",
      "epoch: 27 step: 133, loss is 0.00014351030404213816\n",
      "epoch: 27 step: 134, loss is 0.00042824383126571774\n",
      "epoch: 27 step: 135, loss is 0.0007752021192573011\n",
      "epoch: 27 step: 136, loss is 0.0006245759432204068\n",
      "epoch: 27 step: 137, loss is 0.0010786079801619053\n",
      "epoch: 27 step: 138, loss is 0.0003749144379980862\n",
      "epoch: 27 step: 139, loss is 0.0017406910192221403\n",
      "epoch: 27 step: 140, loss is 2.2385758711607195e-05\n",
      "epoch: 27 step: 141, loss is 0.000628110661637038\n",
      "epoch: 27 step: 142, loss is 0.00023791099374648184\n",
      "epoch: 27 step: 143, loss is 0.05108794942498207\n",
      "epoch: 27 step: 144, loss is 0.00022588582942262292\n",
      "epoch: 27 step: 145, loss is 0.00023229887301567942\n",
      "epoch: 27 step: 146, loss is 0.0245754923671484\n",
      "epoch: 27 step: 147, loss is 0.006311568897217512\n",
      "epoch: 27 step: 148, loss is 0.0007187576848082244\n",
      "epoch: 27 step: 149, loss is 0.0012729174923151731\n",
      "epoch: 27 step: 150, loss is 0.005684910342097282\n",
      "epoch: 27 step: 151, loss is 0.022158440202474594\n",
      "epoch: 27 step: 152, loss is 0.024117635563015938\n",
      "epoch: 27 step: 153, loss is 0.001918582245707512\n",
      "epoch: 27 step: 154, loss is 0.022738520056009293\n",
      "epoch: 27 step: 155, loss is 0.00036019450635649264\n",
      "epoch: 27 step: 156, loss is 0.03613533824682236\n",
      "epoch: 27 step: 157, loss is 4.019145490019582e-05\n",
      "epoch: 27 step: 158, loss is 0.00048201007302850485\n",
      "epoch: 27 step: 159, loss is 0.011706613935530186\n",
      "epoch: 27 step: 160, loss is 0.0003259411605540663\n",
      "epoch: 27 step: 161, loss is 0.0002467135200276971\n",
      "epoch: 27 step: 162, loss is 0.0001402322668582201\n",
      "epoch: 27 step: 163, loss is 0.0006577749154530466\n",
      "epoch: 27 step: 164, loss is 0.00021561096946243197\n",
      "epoch: 27 step: 165, loss is 4.893217555945739e-05\n",
      "epoch: 27 step: 166, loss is 6.912597018526867e-05\n",
      "epoch: 27 step: 167, loss is 0.0005676101427525282\n",
      "epoch: 27 step: 168, loss is 0.0010539420181885362\n",
      "epoch: 27 step: 169, loss is 0.0006009317585267127\n",
      "epoch: 27 step: 170, loss is 0.00036924929008819163\n",
      "epoch: 27 step: 171, loss is 0.00364007824100554\n",
      "epoch: 27 step: 172, loss is 0.00018872003420256078\n",
      "epoch: 27 step: 173, loss is 0.00011535453086253256\n",
      "epoch: 27 step: 174, loss is 6.217359623406082e-05\n",
      "epoch: 27 step: 175, loss is 0.000810620840638876\n",
      "epoch: 27 step: 176, loss is 0.007497766520828009\n",
      "epoch: 27 step: 177, loss is 0.01821037568151951\n",
      "epoch: 27 step: 178, loss is 0.009071885608136654\n",
      "epoch: 27 step: 179, loss is 0.0031654289923608303\n",
      "epoch: 27 step: 180, loss is 0.002629221184179187\n",
      "epoch: 27 step: 181, loss is 0.0001392239792039618\n",
      "epoch: 27 step: 182, loss is 0.004167409148067236\n",
      "epoch: 27 step: 183, loss is 0.012383051216602325\n",
      "epoch: 27 step: 184, loss is 0.00030414987122640014\n",
      "epoch: 27 step: 185, loss is 0.00037351783248595893\n",
      "epoch: 27 step: 186, loss is 0.00033235453884117305\n",
      "epoch: 27 step: 187, loss is 0.0016037505120038986\n",
      "epoch: 27 step: 188, loss is 3.626854595495388e-05\n",
      "epoch: 27 step: 189, loss is 0.0014627145137637854\n",
      "epoch: 27 step: 190, loss is 0.010731156915426254\n",
      "epoch: 27 step: 191, loss is 0.002473263069987297\n",
      "epoch: 27 step: 192, loss is 0.0024738688953220844\n",
      "epoch: 27 step: 193, loss is 0.0025935708545148373\n",
      "epoch: 27 step: 194, loss is 0.004274825565516949\n",
      "epoch: 27 step: 195, loss is 0.02892274782061577\n",
      "epoch: 27 step: 196, loss is 9.893888636725023e-05\n",
      "epoch: 27 step: 197, loss is 0.0019184128614142537\n",
      "epoch: 27 step: 198, loss is 0.0008198911091312766\n",
      "epoch: 27 step: 199, loss is 0.00015542971959803253\n",
      "epoch: 27 step: 200, loss is 0.001631687511689961\n",
      "epoch: 27 step: 201, loss is 0.001840991317294538\n",
      "epoch: 27 step: 202, loss is 0.0022152315359562635\n",
      "epoch: 27 step: 203, loss is 0.00021966315398458391\n",
      "epoch: 27 step: 204, loss is 9.687418059911579e-05\n",
      "epoch: 27 step: 205, loss is 0.00047313212417066097\n",
      "epoch: 27 step: 206, loss is 0.001926516299135983\n",
      "epoch: 27 step: 207, loss is 0.0007926810649223626\n",
      "epoch: 27 step: 208, loss is 0.008145374245941639\n",
      "epoch: 27 step: 209, loss is 0.002936011180281639\n",
      "epoch: 27 step: 210, loss is 0.09263516962528229\n",
      "epoch: 27 step: 211, loss is 0.0010208075400441885\n",
      "epoch: 27 step: 212, loss is 0.008069411851465702\n",
      "epoch: 27 step: 213, loss is 0.0013143718242645264\n",
      "epoch: 27 step: 214, loss is 0.0005882757250219584\n",
      "epoch: 27 step: 215, loss is 0.00013081944780424237\n",
      "epoch: 27 step: 216, loss is 0.00016064592637121677\n",
      "epoch: 27 step: 217, loss is 0.0020611754152923822\n",
      "epoch: 27 step: 218, loss is 0.0005263187340460718\n",
      "epoch: 27 step: 219, loss is 0.00038364555803127587\n",
      "epoch: 27 step: 220, loss is 0.0005982945440337062\n",
      "epoch: 27 step: 221, loss is 0.004701642319560051\n",
      "epoch: 27 step: 222, loss is 3.600943455239758e-05\n",
      "epoch: 27 step: 223, loss is 0.00018214691954199225\n",
      "epoch: 27 step: 224, loss is 0.003735830308869481\n",
      "epoch: 27 step: 225, loss is 0.0004427870153449476\n",
      "epoch: 27 step: 226, loss is 0.00030216790037229657\n",
      "epoch: 27 step: 227, loss is 0.00011251687828917056\n",
      "epoch: 27 step: 228, loss is 0.00011031594476662576\n",
      "epoch: 27 step: 229, loss is 0.0061086262576282024\n",
      "epoch: 27 step: 230, loss is 0.0364382266998291\n",
      "epoch: 27 step: 231, loss is 0.0007113830652087927\n",
      "epoch: 27 step: 232, loss is 0.07037387043237686\n",
      "epoch: 27 step: 233, loss is 0.015798036009073257\n",
      "epoch: 27 step: 234, loss is 0.0009312901529483497\n",
      "epoch: 27 step: 235, loss is 0.00333408429287374\n",
      "epoch: 27 step: 236, loss is 0.05446496978402138\n",
      "epoch: 27 step: 237, loss is 0.0028875654097646475\n",
      "epoch: 27 step: 238, loss is 0.0005897132796235383\n",
      "epoch: 27 step: 239, loss is 0.00030639723991043866\n",
      "epoch: 27 step: 240, loss is 0.013997645117342472\n",
      "epoch: 27 step: 241, loss is 0.012242152355611324\n",
      "epoch: 27 step: 242, loss is 0.002720961580052972\n",
      "epoch: 27 step: 243, loss is 0.04994042590260506\n",
      "epoch: 27 step: 244, loss is 0.004975255113095045\n",
      "epoch: 27 step: 245, loss is 0.00011632664973149076\n",
      "epoch: 27 step: 246, loss is 0.00043282494880259037\n",
      "epoch: 27 step: 247, loss is 0.0006004790775477886\n",
      "epoch: 27 step: 248, loss is 0.0013756705448031425\n",
      "epoch: 27 step: 249, loss is 0.012245655059814453\n",
      "epoch: 27 step: 250, loss is 0.024908171966671944\n",
      "epoch: 27 step: 251, loss is 4.193442146060988e-05\n",
      "epoch: 27 step: 252, loss is 0.0032387070823460817\n",
      "epoch: 27 step: 253, loss is 0.011733382940292358\n",
      "epoch: 27 step: 254, loss is 0.017233040183782578\n",
      "epoch: 27 step: 255, loss is 0.0005865402636118233\n",
      "epoch: 27 step: 256, loss is 0.0004160533717367798\n",
      "epoch: 27 step: 257, loss is 0.0009546682122163475\n",
      "epoch: 27 step: 258, loss is 6.471670349128544e-05\n",
      "epoch: 27 step: 259, loss is 0.009642478078603745\n",
      "epoch: 27 step: 260, loss is 0.00029713852563872933\n",
      "epoch: 27 step: 261, loss is 0.005874198395758867\n",
      "epoch: 27 step: 262, loss is 0.00028937385650351644\n",
      "epoch: 27 step: 263, loss is 0.00043924106284976006\n",
      "epoch: 27 step: 264, loss is 0.004922587890177965\n",
      "epoch: 27 step: 265, loss is 0.03568538650870323\n",
      "epoch: 27 step: 266, loss is 0.0004523939860519022\n",
      "epoch: 27 step: 267, loss is 4.6969867071311455e-06\n",
      "epoch: 27 step: 268, loss is 0.0006627267575822771\n",
      "epoch: 27 step: 269, loss is 0.0021866271272301674\n",
      "epoch: 27 step: 270, loss is 0.001430433476343751\n",
      "epoch: 27 step: 271, loss is 0.00047810786054469645\n",
      "epoch: 27 step: 272, loss is 0.007395908702164888\n",
      "epoch: 27 step: 273, loss is 0.0006632612203247845\n",
      "epoch: 27 step: 274, loss is 0.00270616402849555\n",
      "epoch: 27 step: 275, loss is 0.001465876353904605\n",
      "epoch: 27 step: 276, loss is 0.0015762181719765067\n",
      "epoch: 27 step: 277, loss is 0.0227515809237957\n",
      "epoch: 27 step: 278, loss is 0.0020236149430274963\n",
      "epoch: 27 step: 279, loss is 0.00143816031049937\n",
      "epoch: 27 step: 280, loss is 0.008973184041678905\n",
      "epoch: 27 step: 281, loss is 0.002602875232696533\n",
      "epoch: 27 step: 282, loss is 0.0015557315200567245\n",
      "epoch: 27 step: 283, loss is 0.000369966437574476\n",
      "epoch: 27 step: 284, loss is 0.03258123993873596\n",
      "epoch: 27 step: 285, loss is 0.010550728999078274\n",
      "epoch: 27 step: 286, loss is 0.00015562177577521652\n",
      "epoch: 27 step: 287, loss is 0.0013351470697671175\n",
      "epoch: 27 step: 288, loss is 0.00488760881125927\n",
      "epoch: 27 step: 289, loss is 0.010793989524245262\n",
      "epoch: 27 step: 290, loss is 0.011270630173385143\n",
      "epoch: 27 step: 291, loss is 0.0008920918917283416\n",
      "epoch: 27 step: 292, loss is 0.0010308936471119523\n",
      "epoch: 27 step: 293, loss is 0.0011621363228186965\n",
      "epoch: 27 step: 294, loss is 0.009284998290240765\n",
      "epoch: 27 step: 295, loss is 0.0010365272173658013\n",
      "epoch: 27 step: 296, loss is 0.0004440202610567212\n",
      "epoch: 27 step: 297, loss is 0.003268024418503046\n",
      "epoch: 27 step: 298, loss is 0.003209518501535058\n",
      "epoch: 27 step: 299, loss is 0.0036955424584448338\n",
      "epoch: 27 step: 300, loss is 1.367201775792637e-06\n",
      "epoch: 27 step: 301, loss is 0.0008022460388019681\n",
      "epoch: 27 step: 302, loss is 8.63613240653649e-05\n",
      "epoch: 27 step: 303, loss is 0.0031431408133357763\n",
      "epoch: 27 step: 304, loss is 0.0011786047834903002\n",
      "epoch: 27 step: 305, loss is 0.0005786372348666191\n",
      "epoch: 27 step: 306, loss is 0.012460136786103249\n",
      "epoch: 27 step: 307, loss is 2.157511335099116e-05\n",
      "epoch: 27 step: 308, loss is 0.00023478531511500478\n",
      "epoch: 27 step: 309, loss is 0.05662792921066284\n",
      "epoch: 27 step: 310, loss is 0.000863943831063807\n",
      "epoch: 27 step: 311, loss is 0.000522275164257735\n",
      "epoch: 27 step: 312, loss is 0.0005584845202974975\n",
      "epoch: 27 step: 313, loss is 5.7238852605223656e-05\n",
      "epoch: 27 step: 314, loss is 0.005268207751214504\n",
      "epoch: 27 step: 315, loss is 0.0037790616042912006\n",
      "epoch: 27 step: 316, loss is 0.08545564115047455\n",
      "epoch: 27 step: 317, loss is 0.0018568106461316347\n",
      "epoch: 27 step: 318, loss is 0.0013930487912148237\n",
      "epoch: 27 step: 319, loss is 9.862447041086853e-05\n",
      "epoch: 27 step: 320, loss is 0.00016987111303023994\n",
      "epoch: 27 step: 321, loss is 0.02213793806731701\n",
      "epoch: 27 step: 322, loss is 0.006121745798736811\n",
      "epoch: 27 step: 323, loss is 0.004358476959168911\n",
      "epoch: 27 step: 324, loss is 0.0009005614556372166\n",
      "epoch: 27 step: 325, loss is 0.0004641964624170214\n",
      "epoch: 27 step: 326, loss is 0.0005609531071968377\n",
      "epoch: 27 step: 327, loss is 0.001105545787140727\n",
      "epoch: 27 step: 328, loss is 1.4705684407090303e-05\n",
      "epoch: 27 step: 329, loss is 0.01988060586154461\n",
      "epoch: 27 step: 330, loss is 0.0023563753347843885\n",
      "epoch: 27 step: 331, loss is 0.0006609380943700671\n",
      "epoch: 27 step: 332, loss is 0.007513590157032013\n",
      "epoch: 27 step: 333, loss is 0.0001315545232500881\n",
      "epoch: 27 step: 334, loss is 0.0015973830595612526\n",
      "epoch: 27 step: 335, loss is 0.004228498321026564\n",
      "epoch: 27 step: 336, loss is 0.001060575246810913\n",
      "epoch: 27 step: 337, loss is 0.00025759704294614494\n",
      "epoch: 27 step: 338, loss is 0.07939492911100388\n",
      "epoch: 27 step: 339, loss is 0.0014538037357851863\n",
      "epoch: 27 step: 340, loss is 0.00011168244964210317\n",
      "epoch: 27 step: 341, loss is 0.00011017204815289006\n",
      "epoch: 27 step: 342, loss is 0.0012000121641904116\n",
      "epoch: 27 step: 343, loss is 0.06733391433954239\n",
      "epoch: 27 step: 344, loss is 0.0025202822871506214\n",
      "epoch: 27 step: 345, loss is 0.0030026184394955635\n",
      "epoch: 27 step: 346, loss is 0.0011857852805405855\n",
      "epoch: 27 step: 347, loss is 0.0026951658073812723\n",
      "epoch: 27 step: 348, loss is 0.003096748376265168\n",
      "epoch: 27 step: 349, loss is 0.01647246815264225\n",
      "epoch: 27 step: 350, loss is 0.002795611508190632\n",
      "epoch: 27 step: 351, loss is 0.011236033402383327\n",
      "epoch: 27 step: 352, loss is 0.00016938864428084344\n",
      "epoch: 27 step: 353, loss is 0.02632286213338375\n",
      "epoch: 27 step: 354, loss is 0.004204416181892157\n",
      "epoch: 27 step: 355, loss is 0.006208688952028751\n",
      "epoch: 27 step: 356, loss is 0.010311964899301529\n",
      "epoch: 27 step: 357, loss is 0.029442664235830307\n",
      "epoch: 27 step: 358, loss is 0.0010100294603034854\n",
      "epoch: 27 step: 359, loss is 0.00036294470191933215\n",
      "epoch: 27 step: 360, loss is 0.003808789188042283\n",
      "epoch: 27 step: 361, loss is 0.000660724297631532\n",
      "epoch: 27 step: 362, loss is 0.0011769444681704044\n",
      "epoch: 27 step: 363, loss is 0.0002023640408879146\n",
      "epoch: 27 step: 364, loss is 0.029968759045004845\n",
      "epoch: 27 step: 365, loss is 7.737776286376175e-06\n",
      "epoch: 27 step: 366, loss is 0.00019957838230766356\n",
      "epoch: 27 step: 367, loss is 0.0017276826547458768\n",
      "epoch: 27 step: 368, loss is 0.0005218599108047783\n",
      "epoch: 27 step: 369, loss is 0.02602139301598072\n",
      "epoch: 27 step: 370, loss is 0.00010955958714475855\n",
      "epoch: 27 step: 371, loss is 0.054861802607774734\n",
      "epoch: 27 step: 372, loss is 0.00030829934985376894\n",
      "epoch: 27 step: 373, loss is 0.0003548382082954049\n",
      "epoch: 27 step: 374, loss is 0.001987718977034092\n",
      "epoch: 27 step: 375, loss is 0.0039691319689154625\n",
      "epoch: 27 step: 376, loss is 0.0004088166751898825\n",
      "epoch: 27 step: 377, loss is 0.02450600452721119\n",
      "epoch: 27 step: 378, loss is 0.007275396026670933\n",
      "epoch: 27 step: 379, loss is 0.0010523980017751455\n",
      "epoch: 27 step: 380, loss is 0.017489898949861526\n",
      "epoch: 27 step: 381, loss is 0.0020766379311680794\n",
      "epoch: 27 step: 382, loss is 0.0036301473155617714\n",
      "epoch: 27 step: 383, loss is 0.034541044384241104\n",
      "epoch: 27 step: 384, loss is 0.0004342416359577328\n",
      "epoch: 27 step: 385, loss is 0.0013564323307946324\n",
      "epoch: 27 step: 386, loss is 0.0022098920308053493\n",
      "epoch: 27 step: 387, loss is 0.0027280375361442566\n",
      "epoch: 27 step: 388, loss is 0.00017457902140449733\n",
      "epoch: 27 step: 389, loss is 0.00017317719175480306\n",
      "epoch: 27 step: 390, loss is 0.00011503400310175493\n",
      "epoch: 27 step: 391, loss is 7.075794565025717e-05\n",
      "epoch: 27 step: 392, loss is 0.002272899728268385\n",
      "epoch: 27 step: 393, loss is 0.0002669786335900426\n",
      "epoch: 27 step: 394, loss is 0.0016971511067822576\n",
      "epoch: 27 step: 395, loss is 0.0002654730633366853\n",
      "epoch: 27 step: 396, loss is 0.00016474274161737412\n",
      "epoch: 27 step: 397, loss is 0.0005230409442447126\n",
      "epoch: 27 step: 398, loss is 0.0001900397037388757\n",
      "epoch: 27 step: 399, loss is 0.026720689609646797\n",
      "epoch: 27 step: 400, loss is 2.6457262720214203e-05\n",
      "epoch: 27 step: 401, loss is 0.0008908602758310735\n",
      "epoch: 27 step: 402, loss is 0.06831540167331696\n",
      "epoch: 27 step: 403, loss is 5.682893970515579e-05\n",
      "epoch: 27 step: 404, loss is 0.012186345644295216\n",
      "epoch: 27 step: 405, loss is 0.023145658895373344\n",
      "epoch: 27 step: 406, loss is 0.01216572243720293\n",
      "epoch: 27 step: 407, loss is 8.139803685480729e-05\n",
      "epoch: 27 step: 408, loss is 7.430206460412592e-05\n",
      "epoch: 27 step: 409, loss is 0.0011939572868868709\n",
      "epoch: 27 step: 410, loss is 0.00039099485729821026\n",
      "epoch: 27 step: 411, loss is 0.002453463850542903\n",
      "epoch: 27 step: 412, loss is 0.044460173696279526\n",
      "epoch: 27 step: 413, loss is 0.04291735589504242\n",
      "epoch: 27 step: 414, loss is 0.002138912444934249\n",
      "epoch: 27 step: 415, loss is 0.012439615093171597\n",
      "epoch: 27 step: 416, loss is 6.386446329997852e-05\n",
      "epoch: 27 step: 417, loss is 0.05052665248513222\n",
      "epoch: 27 step: 418, loss is 0.0168780367821455\n",
      "epoch: 27 step: 419, loss is 0.03295861557126045\n",
      "epoch: 27 step: 420, loss is 0.0005693656275980175\n",
      "epoch: 27 step: 421, loss is 0.015949973836541176\n",
      "epoch: 27 step: 422, loss is 0.029840830713510513\n",
      "epoch: 27 step: 423, loss is 8.376988989766687e-05\n",
      "epoch: 27 step: 424, loss is 0.026316912844777107\n",
      "epoch: 27 step: 425, loss is 0.0020457673817873\n",
      "epoch: 27 step: 426, loss is 0.003330955281853676\n",
      "epoch: 27 step: 427, loss is 0.006633778102695942\n",
      "epoch: 27 step: 428, loss is 0.0017256425926461816\n",
      "epoch: 27 step: 429, loss is 0.010966463014483452\n",
      "epoch: 27 step: 430, loss is 0.008387616835534573\n",
      "epoch: 27 step: 431, loss is 0.0009115251014009118\n",
      "epoch: 27 step: 432, loss is 0.0002881361870095134\n",
      "epoch: 27 step: 433, loss is 0.00025281106354668736\n",
      "epoch: 27 step: 434, loss is 0.0012698274804279208\n",
      "epoch: 27 step: 435, loss is 0.03651542589068413\n",
      "epoch: 27 step: 436, loss is 0.003444073023274541\n",
      "epoch: 27 step: 437, loss is 0.000248521042522043\n",
      "epoch: 27 step: 438, loss is 0.02239139936864376\n",
      "epoch: 27 step: 439, loss is 0.00882731843739748\n",
      "epoch: 27 step: 440, loss is 0.0004077735939063132\n",
      "epoch: 27 step: 441, loss is 0.0007525201654061675\n",
      "epoch: 27 step: 442, loss is 0.006465367041528225\n",
      "epoch: 27 step: 443, loss is 0.001091203186661005\n",
      "epoch: 27 step: 444, loss is 0.004959980025887489\n",
      "epoch: 27 step: 445, loss is 0.0002260945038869977\n",
      "epoch: 27 step: 446, loss is 0.021674776449799538\n",
      "epoch: 27 step: 447, loss is 0.011069518513977528\n",
      "epoch: 27 step: 448, loss is 0.0017477653454989195\n",
      "epoch: 27 step: 449, loss is 0.0005131693324074149\n",
      "epoch: 27 step: 450, loss is 1.1634250768111087e-05\n",
      "epoch: 27 step: 451, loss is 1.475487351854099e-05\n",
      "epoch: 27 step: 452, loss is 0.01735936664044857\n",
      "epoch: 27 step: 453, loss is 0.04315507784485817\n",
      "epoch: 27 step: 454, loss is 0.0026190883945673704\n",
      "epoch: 27 step: 455, loss is 0.000661310157738626\n",
      "epoch: 27 step: 456, loss is 9.537731966702268e-05\n",
      "epoch: 27 step: 457, loss is 0.00030624738428741693\n",
      "epoch: 27 step: 458, loss is 0.00019984340178780258\n",
      "epoch: 27 step: 459, loss is 0.0002613744873087853\n",
      "epoch: 27 step: 460, loss is 0.0003660137881524861\n",
      "epoch: 27 step: 461, loss is 0.0001641652052057907\n",
      "epoch: 27 step: 462, loss is 4.938218626193702e-05\n",
      "epoch: 27 step: 463, loss is 0.0003815263626165688\n",
      "epoch: 27 step: 464, loss is 0.00018707952403929085\n",
      "epoch: 27 step: 465, loss is 0.0019378067227080464\n",
      "epoch: 27 step: 466, loss is 0.00020624151511583477\n",
      "epoch: 27 step: 467, loss is 2.7360007152310573e-05\n",
      "epoch: 27 step: 468, loss is 0.001010058680549264\n",
      "epoch: 27 step: 469, loss is 0.04775569587945938\n",
      "epoch: 27 step: 470, loss is 0.03540140762925148\n",
      "epoch: 27 step: 471, loss is 0.00042697531171143055\n",
      "epoch: 27 step: 472, loss is 0.0006257006316445768\n",
      "epoch: 27 step: 473, loss is 0.03732307255268097\n",
      "epoch: 27 step: 474, loss is 0.05692003667354584\n",
      "epoch: 27 step: 475, loss is 7.29590974515304e-05\n",
      "epoch: 27 step: 476, loss is 0.0011885305866599083\n",
      "epoch: 27 step: 477, loss is 0.00455773388966918\n",
      "epoch: 27 step: 478, loss is 0.006419373210519552\n",
      "epoch: 27 step: 479, loss is 0.03364086151123047\n",
      "epoch: 27 step: 480, loss is 0.0036359725054353476\n",
      "epoch: 27 step: 481, loss is 0.04087295010685921\n",
      "epoch: 27 step: 482, loss is 0.03522949293255806\n",
      "epoch: 27 step: 483, loss is 0.004563400987535715\n",
      "epoch: 27 step: 484, loss is 8.597156556788832e-05\n",
      "epoch: 27 step: 485, loss is 0.0005239942111074924\n",
      "epoch: 27 step: 486, loss is 0.10973917692899704\n",
      "epoch: 27 step: 487, loss is 0.0049928356893360615\n",
      "epoch: 27 step: 488, loss is 0.014829841442406178\n",
      "epoch: 27 step: 489, loss is 0.002113010035827756\n",
      "epoch: 27 step: 490, loss is 0.00016080775822047144\n",
      "epoch: 27 step: 491, loss is 0.00026723663904704154\n",
      "epoch: 27 step: 492, loss is 0.0006104197818785906\n",
      "epoch: 27 step: 493, loss is 0.024747153744101524\n",
      "epoch: 27 step: 494, loss is 0.04749997332692146\n",
      "epoch: 27 step: 495, loss is 2.9080494641675614e-05\n",
      "epoch: 27 step: 496, loss is 0.01023861300200224\n",
      "epoch: 27 step: 497, loss is 0.07702185958623886\n",
      "epoch: 27 step: 498, loss is 0.0013543894747272134\n",
      "epoch: 27 step: 499, loss is 0.005750753916800022\n",
      "epoch: 27 step: 500, loss is 0.12274419516324997\n",
      "epoch: 27 step: 501, loss is 0.0024960502050817013\n",
      "epoch: 27 step: 502, loss is 0.0016222300473600626\n",
      "epoch: 27 step: 503, loss is 0.00036461130366660655\n",
      "epoch: 27 step: 504, loss is 1.2585985132318456e-05\n",
      "epoch: 27 step: 505, loss is 0.07191970944404602\n",
      "epoch: 27 step: 506, loss is 0.011632977984845638\n",
      "epoch: 27 step: 507, loss is 2.474985012668185e-05\n",
      "epoch: 27 step: 508, loss is 0.00025174187612719834\n",
      "epoch: 27 step: 509, loss is 0.008274747058749199\n",
      "epoch: 27 step: 510, loss is 0.0004925178363919258\n",
      "epoch: 27 step: 511, loss is 0.007396118715405464\n",
      "epoch: 27 step: 512, loss is 0.004029098432511091\n",
      "epoch: 27 step: 513, loss is 0.008636770769953728\n",
      "epoch: 27 step: 514, loss is 0.000565250578802079\n",
      "epoch: 27 step: 515, loss is 0.12236997485160828\n",
      "epoch: 27 step: 516, loss is 0.03396479785442352\n",
      "epoch: 27 step: 517, loss is 0.00014627141354139894\n",
      "epoch: 27 step: 518, loss is 0.00041334552224725485\n",
      "epoch: 27 step: 519, loss is 0.0007264309679158032\n",
      "epoch: 27 step: 520, loss is 0.00033656327286735177\n",
      "epoch: 27 step: 521, loss is 0.04933757334947586\n",
      "epoch: 27 step: 522, loss is 0.01634611375629902\n",
      "epoch: 27 step: 523, loss is 0.00011577617260627449\n",
      "epoch: 27 step: 524, loss is 0.01082941610366106\n",
      "epoch: 27 step: 525, loss is 0.0006989530520513654\n",
      "epoch: 27 step: 526, loss is 0.0014084865106269717\n",
      "epoch: 27 step: 527, loss is 0.0024703629314899445\n",
      "epoch: 27 step: 528, loss is 0.018940597772598267\n",
      "epoch: 27 step: 529, loss is 0.00454719690605998\n",
      "epoch: 27 step: 530, loss is 0.10804009437561035\n",
      "epoch: 27 step: 531, loss is 0.04869905859231949\n",
      "epoch: 27 step: 532, loss is 0.018474522978067398\n",
      "epoch: 27 step: 533, loss is 0.1336769312620163\n",
      "epoch: 27 step: 534, loss is 0.005967210046947002\n",
      "epoch: 27 step: 535, loss is 0.022763429209589958\n",
      "epoch: 27 step: 536, loss is 0.03349650278687477\n",
      "epoch: 27 step: 537, loss is 0.022636335343122482\n",
      "epoch: 27 step: 538, loss is 0.02455807849764824\n",
      "epoch: 27 step: 539, loss is 0.08716726303100586\n",
      "epoch: 27 step: 540, loss is 0.04538548365235329\n",
      "epoch: 27 step: 541, loss is 0.002675652038305998\n",
      "epoch: 27 step: 542, loss is 0.007971248589456081\n",
      "epoch: 27 step: 543, loss is 0.00971626304090023\n",
      "epoch: 27 step: 544, loss is 3.2959393138298765e-05\n",
      "epoch: 27 step: 545, loss is 0.024894602596759796\n",
      "epoch: 27 step: 546, loss is 0.0005329515552148223\n",
      "epoch: 27 step: 547, loss is 0.02588479220867157\n",
      "epoch: 27 step: 548, loss is 0.010824163444340229\n",
      "epoch: 27 step: 549, loss is 0.0038925278931856155\n",
      "epoch: 27 step: 550, loss is 0.0019847326911985874\n",
      "epoch: 27 step: 551, loss is 0.0026516946963965893\n",
      "epoch: 27 step: 552, loss is 0.03062627650797367\n",
      "epoch: 27 step: 553, loss is 0.15102514624595642\n",
      "epoch: 27 step: 554, loss is 0.010111802257597446\n",
      "epoch: 27 step: 555, loss is 0.0033954521641135216\n",
      "epoch: 27 step: 556, loss is 0.0017733355052769184\n",
      "epoch: 27 step: 557, loss is 0.0017343598883599043\n",
      "epoch: 27 step: 558, loss is 0.0030182257760316133\n",
      "epoch: 27 step: 559, loss is 0.19658862054347992\n",
      "epoch: 27 step: 560, loss is 0.0008870733436197042\n",
      "epoch: 27 step: 561, loss is 0.002801415976136923\n",
      "epoch: 27 step: 562, loss is 0.008902187459170818\n",
      "epoch: 27 step: 563, loss is 0.023352283984422684\n",
      "epoch: 27 step: 564, loss is 0.0024187080562114716\n",
      "epoch: 27 step: 565, loss is 0.010400879196822643\n",
      "epoch: 27 step: 566, loss is 0.0010631120530888438\n",
      "epoch: 27 step: 567, loss is 0.0058817872777581215\n",
      "epoch: 27 step: 568, loss is 0.0116278026252985\n",
      "epoch: 27 step: 569, loss is 0.0004304280737414956\n",
      "epoch: 27 step: 570, loss is 0.013595768250524998\n",
      "epoch: 27 step: 571, loss is 0.023507559671998024\n",
      "epoch: 27 step: 572, loss is 0.005539661273360252\n",
      "epoch: 27 step: 573, loss is 0.004970696289092302\n",
      "epoch: 27 step: 574, loss is 0.00021451860084198415\n",
      "epoch: 27 step: 575, loss is 0.003069466445595026\n",
      "epoch: 27 step: 576, loss is 0.001090143108740449\n",
      "epoch: 27 step: 577, loss is 0.0023233003448694944\n",
      "epoch: 27 step: 578, loss is 0.0020595774985849857\n",
      "epoch: 27 step: 579, loss is 6.600805500056595e-05\n",
      "epoch: 27 step: 580, loss is 0.001874585635960102\n",
      "epoch: 27 step: 581, loss is 0.0020804163068532944\n",
      "epoch: 27 step: 582, loss is 0.004919688683003187\n",
      "epoch: 27 step: 583, loss is 0.0017745158402249217\n",
      "epoch: 27 step: 584, loss is 0.008289960213005543\n",
      "epoch: 27 step: 585, loss is 0.002876688726246357\n",
      "epoch: 27 step: 586, loss is 0.0015791794285178185\n",
      "epoch: 27 step: 587, loss is 0.06610856205224991\n",
      "epoch: 27 step: 588, loss is 0.001231826958246529\n",
      "epoch: 27 step: 589, loss is 0.004001476336270571\n",
      "epoch: 27 step: 590, loss is 0.02394219860434532\n",
      "epoch: 27 step: 591, loss is 0.011615593917667866\n",
      "epoch: 27 step: 592, loss is 0.027104532346129417\n",
      "epoch: 27 step: 593, loss is 0.0007185765425674617\n",
      "epoch: 27 step: 594, loss is 0.012098269537091255\n",
      "epoch: 27 step: 595, loss is 0.005429568234831095\n",
      "epoch: 27 step: 596, loss is 8.334255107911304e-05\n",
      "epoch: 27 step: 597, loss is 0.004110001027584076\n",
      "epoch: 27 step: 598, loss is 0.0025091629941016436\n",
      "epoch: 27 step: 599, loss is 0.0003463513567112386\n",
      "epoch: 27 step: 600, loss is 0.005902384407818317\n",
      "epoch: 27 step: 601, loss is 0.019779566675424576\n",
      "epoch: 27 step: 602, loss is 0.0002633378899190575\n",
      "epoch: 27 step: 603, loss is 0.008862219750881195\n",
      "epoch: 27 step: 604, loss is 0.00019765795150306076\n",
      "epoch: 27 step: 605, loss is 0.02725457400083542\n",
      "epoch: 27 step: 606, loss is 0.005414054729044437\n",
      "epoch: 27 step: 607, loss is 0.002493669744580984\n",
      "epoch: 27 step: 608, loss is 0.002773870248347521\n",
      "epoch: 27 step: 609, loss is 0.0029745339415967464\n",
      "epoch: 27 step: 610, loss is 0.0004803556948900223\n",
      "epoch: 27 step: 611, loss is 0.0011931848712265491\n",
      "epoch: 27 step: 612, loss is 0.03546271473169327\n",
      "epoch: 27 step: 613, loss is 0.00046734116040170193\n",
      "epoch: 27 step: 614, loss is 0.01737232878804207\n",
      "epoch: 27 step: 615, loss is 0.0003705161507241428\n",
      "epoch: 27 step: 616, loss is 0.03103729337453842\n",
      "epoch: 27 step: 617, loss is 0.05270317941904068\n",
      "epoch: 27 step: 618, loss is 0.04409100487828255\n",
      "epoch: 27 step: 619, loss is 0.007292812690138817\n",
      "epoch: 27 step: 620, loss is 0.04693664610385895\n",
      "epoch: 27 step: 621, loss is 0.00025038901367224753\n",
      "epoch: 27 step: 622, loss is 0.021278182044625282\n",
      "epoch: 27 step: 623, loss is 0.0008274270221590996\n",
      "epoch: 27 step: 624, loss is 0.0006171016721054912\n",
      "epoch: 27 step: 625, loss is 0.014193988405168056\n",
      "epoch: 27 step: 626, loss is 0.002693564398214221\n",
      "epoch: 27 step: 627, loss is 0.000249261996941641\n",
      "epoch: 27 step: 628, loss is 0.0027295441832393408\n",
      "epoch: 27 step: 629, loss is 0.00020570849301293492\n",
      "epoch: 27 step: 630, loss is 0.0038017055485397577\n",
      "epoch: 27 step: 631, loss is 0.010763605125248432\n",
      "epoch: 27 step: 632, loss is 0.014848279766738415\n",
      "epoch: 27 step: 633, loss is 6.752927583875135e-05\n",
      "epoch: 27 step: 634, loss is 0.004938347265124321\n",
      "epoch: 27 step: 635, loss is 0.010617537423968315\n",
      "epoch: 27 step: 636, loss is 0.025638191029429436\n",
      "epoch: 27 step: 637, loss is 0.022832566872239113\n",
      "epoch: 27 step: 638, loss is 0.015858128666877747\n",
      "epoch: 27 step: 639, loss is 0.0007534970645792782\n",
      "epoch: 27 step: 640, loss is 0.006491395644843578\n",
      "epoch: 27 step: 641, loss is 0.0001936954795382917\n",
      "epoch: 27 step: 642, loss is 0.002344427164644003\n",
      "epoch: 27 step: 643, loss is 0.0003675441548693925\n",
      "epoch: 27 step: 644, loss is 0.0016692242352291942\n",
      "epoch: 27 step: 645, loss is 0.04127712547779083\n",
      "epoch: 27 step: 646, loss is 0.1397874504327774\n",
      "epoch: 27 step: 647, loss is 0.01546090841293335\n",
      "epoch: 27 step: 648, loss is 0.0004666554450523108\n",
      "epoch: 27 step: 649, loss is 0.011234563775360584\n",
      "epoch: 27 step: 650, loss is 0.00034920108737424016\n",
      "epoch: 27 step: 651, loss is 0.00042449808097444475\n",
      "epoch: 27 step: 652, loss is 0.022048477083444595\n",
      "epoch: 27 step: 653, loss is 0.0012634973973035812\n",
      "epoch: 27 step: 654, loss is 0.0007985924021340907\n",
      "epoch: 27 step: 655, loss is 0.032060541212558746\n",
      "epoch: 27 step: 656, loss is 0.0005243869381956756\n",
      "epoch: 27 step: 657, loss is 0.0007767096976749599\n",
      "epoch: 27 step: 658, loss is 0.00045002714614383876\n",
      "epoch: 27 step: 659, loss is 0.10829542577266693\n",
      "epoch: 27 step: 660, loss is 0.004942003171890974\n",
      "epoch: 27 step: 661, loss is 0.01748977042734623\n",
      "epoch: 27 step: 662, loss is 0.006320500746369362\n",
      "epoch: 27 step: 663, loss is 0.003984900191426277\n",
      "epoch: 27 step: 664, loss is 0.005049577914178371\n",
      "epoch: 27 step: 665, loss is 0.006635029800236225\n",
      "epoch: 27 step: 666, loss is 0.000156466499902308\n",
      "epoch: 27 step: 667, loss is 0.0022388556972146034\n",
      "epoch: 27 step: 668, loss is 0.003552396083250642\n",
      "epoch: 27 step: 669, loss is 0.08851324766874313\n",
      "epoch: 27 step: 670, loss is 1.521037302154582e-05\n",
      "epoch: 27 step: 671, loss is 0.0006611183634959161\n",
      "epoch: 27 step: 672, loss is 0.004907179158180952\n",
      "epoch: 27 step: 673, loss is 0.10005928575992584\n",
      "epoch: 27 step: 674, loss is 0.001396793988533318\n",
      "epoch: 27 step: 675, loss is 0.001041167532093823\n",
      "epoch: 27 step: 676, loss is 0.0022348735947161913\n",
      "epoch: 27 step: 677, loss is 0.0005854313494637609\n",
      "epoch: 27 step: 678, loss is 0.0018607123056426644\n",
      "epoch: 27 step: 679, loss is 0.002829821314662695\n",
      "epoch: 27 step: 680, loss is 3.8941252569202334e-05\n",
      "epoch: 27 step: 681, loss is 0.007687774486839771\n",
      "epoch: 27 step: 682, loss is 0.007982198148965836\n",
      "epoch: 27 step: 683, loss is 3.470942101557739e-05\n",
      "epoch: 27 step: 684, loss is 0.025510549545288086\n",
      "epoch: 27 step: 685, loss is 0.007352224085479975\n",
      "epoch: 27 step: 686, loss is 0.00185555184725672\n",
      "epoch: 27 step: 687, loss is 0.000986946513876319\n",
      "epoch: 27 step: 688, loss is 0.000629206420853734\n",
      "epoch: 27 step: 689, loss is 0.02152889221906662\n",
      "epoch: 27 step: 690, loss is 0.02343532256782055\n",
      "epoch: 27 step: 691, loss is 0.0057905372232198715\n",
      "epoch: 27 step: 692, loss is 0.01406780257821083\n",
      "epoch: 27 step: 693, loss is 0.007764841895550489\n",
      "epoch: 27 step: 694, loss is 0.002343865344300866\n",
      "epoch: 27 step: 695, loss is 0.0037048228550702333\n",
      "epoch: 27 step: 696, loss is 3.6863534660369623e-06\n",
      "epoch: 27 step: 697, loss is 0.004199698101729155\n",
      "epoch: 27 step: 698, loss is 0.006046068388968706\n",
      "epoch: 27 step: 699, loss is 0.0070115807466208935\n",
      "epoch: 27 step: 700, loss is 0.011652697809040546\n",
      "epoch: 27 step: 701, loss is 0.0029504296835511923\n",
      "epoch: 27 step: 702, loss is 0.0012519042938947678\n",
      "epoch: 27 step: 703, loss is 0.0007088590646162629\n",
      "epoch: 27 step: 704, loss is 0.00488292146474123\n",
      "epoch: 27 step: 705, loss is 0.132655069231987\n",
      "epoch: 27 step: 706, loss is 0.0017092619091272354\n",
      "epoch: 27 step: 707, loss is 0.01835649646818638\n",
      "epoch: 27 step: 708, loss is 0.03425917029380798\n",
      "epoch: 27 step: 709, loss is 0.0012130624381825328\n",
      "epoch: 27 step: 710, loss is 0.009171473793685436\n",
      "epoch: 27 step: 711, loss is 0.0018043845193460584\n",
      "epoch: 27 step: 712, loss is 0.007683017756789923\n",
      "epoch: 27 step: 713, loss is 0.01471151877194643\n",
      "epoch: 27 step: 714, loss is 0.0011102494318038225\n",
      "epoch: 27 step: 715, loss is 0.017355529591441154\n",
      "epoch: 27 step: 716, loss is 0.08214724063873291\n",
      "epoch: 27 step: 717, loss is 0.016975320875644684\n",
      "epoch: 27 step: 718, loss is 0.013649389147758484\n",
      "epoch: 27 step: 719, loss is 0.025252217426896095\n",
      "epoch: 27 step: 720, loss is 0.0011404751567170024\n",
      "epoch: 27 step: 721, loss is 0.0018459297716617584\n",
      "epoch: 27 step: 722, loss is 0.001789816771633923\n",
      "epoch: 27 step: 723, loss is 0.0002604400215204805\n",
      "epoch: 27 step: 724, loss is 0.001397149171680212\n",
      "epoch: 27 step: 725, loss is 0.07392407208681107\n",
      "epoch: 27 step: 726, loss is 0.001818624441511929\n",
      "epoch: 27 step: 727, loss is 0.005023056175559759\n",
      "epoch: 27 step: 728, loss is 0.017719309777021408\n",
      "epoch: 27 step: 729, loss is 0.0031882962211966515\n",
      "epoch: 27 step: 730, loss is 0.001276219729334116\n",
      "epoch: 27 step: 731, loss is 0.053213994950056076\n",
      "epoch: 27 step: 732, loss is 0.02222982980310917\n",
      "epoch: 27 step: 733, loss is 0.002814398379996419\n",
      "epoch: 27 step: 734, loss is 0.027152949944138527\n",
      "epoch: 27 step: 735, loss is 0.014250474981963634\n",
      "epoch: 27 step: 736, loss is 0.004036253318190575\n",
      "epoch: 27 step: 737, loss is 0.011963781900703907\n",
      "epoch: 27 step: 738, loss is 0.000739856855943799\n",
      "epoch: 27 step: 739, loss is 0.010761608369648457\n",
      "epoch: 27 step: 740, loss is 0.0016031274572014809\n",
      "epoch: 27 step: 741, loss is 0.0006842282600700855\n",
      "epoch: 27 step: 742, loss is 0.0018022917211055756\n",
      "epoch: 27 step: 743, loss is 0.006706742569804192\n",
      "epoch: 27 step: 744, loss is 0.0005911306943744421\n",
      "epoch: 27 step: 745, loss is 0.010200527496635914\n",
      "epoch: 27 step: 746, loss is 6.02069849264808e-05\n",
      "epoch: 27 step: 747, loss is 0.030101321637630463\n",
      "epoch: 27 step: 748, loss is 0.00038382463390007615\n",
      "epoch: 27 step: 749, loss is 0.0002574311220087111\n",
      "epoch: 27 step: 750, loss is 0.0009509850642643869\n",
      "epoch: 27 step: 751, loss is 0.0035355552099645138\n",
      "epoch: 27 step: 752, loss is 0.0004680612764786929\n",
      "epoch: 27 step: 753, loss is 0.0012320749228820205\n",
      "epoch: 27 step: 754, loss is 0.00199137139134109\n",
      "epoch: 27 step: 755, loss is 4.3710138015740085e-06\n",
      "epoch: 27 step: 756, loss is 0.0012725988635793328\n",
      "epoch: 27 step: 757, loss is 0.0016487501561641693\n",
      "epoch: 27 step: 758, loss is 0.0008183887694031\n",
      "epoch: 27 step: 759, loss is 0.001431850716471672\n",
      "epoch: 27 step: 760, loss is 4.054995588376187e-05\n",
      "epoch: 27 step: 761, loss is 0.006874576210975647\n",
      "epoch: 27 step: 762, loss is 0.003566844156011939\n",
      "epoch: 27 step: 763, loss is 0.0016886088997125626\n",
      "epoch: 27 step: 764, loss is 0.002294221194460988\n",
      "epoch: 27 step: 765, loss is 0.0074235848151147366\n",
      "epoch: 27 step: 766, loss is 0.09185593575239182\n",
      "epoch: 27 step: 767, loss is 0.019411781802773476\n",
      "epoch: 27 step: 768, loss is 0.11633538454771042\n",
      "epoch: 27 step: 769, loss is 0.006714880000799894\n",
      "epoch: 27 step: 770, loss is 0.031090592965483665\n",
      "epoch: 27 step: 771, loss is 4.500641443883069e-05\n",
      "epoch: 27 step: 772, loss is 0.05261394754052162\n",
      "epoch: 27 step: 773, loss is 0.019278405234217644\n",
      "epoch: 27 step: 774, loss is 0.0018501216545701027\n",
      "epoch: 27 step: 775, loss is 0.003340094583109021\n",
      "epoch: 27 step: 776, loss is 0.00027690493152476847\n",
      "epoch: 27 step: 777, loss is 0.0012553200358524919\n",
      "epoch: 27 step: 778, loss is 0.02408461458981037\n",
      "epoch: 27 step: 779, loss is 0.0035714046098291874\n",
      "epoch: 27 step: 780, loss is 0.00047381961485370994\n",
      "epoch: 27 step: 781, loss is 0.006029642652720213\n",
      "epoch: 27 step: 782, loss is 0.0011817198246717453\n",
      "epoch: 27 step: 783, loss is 0.0027206281665712595\n",
      "epoch: 27 step: 784, loss is 0.003446320304647088\n",
      "epoch: 27 step: 785, loss is 0.010677387937903404\n",
      "epoch: 27 step: 786, loss is 0.00028132510487921536\n",
      "epoch: 27 step: 787, loss is 0.00046840833965688944\n",
      "epoch: 27 step: 788, loss is 0.0010276398388668895\n",
      "epoch: 27 step: 789, loss is 0.002007151022553444\n",
      "epoch: 27 step: 790, loss is 0.00218668463639915\n",
      "epoch: 27 step: 791, loss is 0.003405523719266057\n",
      "epoch: 27 step: 792, loss is 0.008318756707012653\n",
      "epoch: 27 step: 793, loss is 0.0013893223367631435\n",
      "epoch: 27 step: 794, loss is 0.05150196701288223\n",
      "epoch: 27 step: 795, loss is 0.0018755443161353469\n",
      "epoch: 27 step: 796, loss is 0.00945892184972763\n",
      "epoch: 27 step: 797, loss is 0.007081851828843355\n",
      "epoch: 27 step: 798, loss is 0.005580112338066101\n",
      "epoch: 27 step: 799, loss is 0.03292057663202286\n",
      "epoch: 27 step: 800, loss is 0.003997180610895157\n",
      "epoch: 27 step: 801, loss is 8.046541188377887e-05\n",
      "epoch: 27 step: 802, loss is 4.533432002062909e-05\n",
      "epoch: 27 step: 803, loss is 0.004423107486218214\n",
      "epoch: 27 step: 804, loss is 6.918556027812883e-05\n",
      "epoch: 27 step: 805, loss is 0.0016761603765189648\n",
      "epoch: 27 step: 806, loss is 0.007052384316921234\n",
      "epoch: 27 step: 807, loss is 0.1501762717962265\n",
      "epoch: 27 step: 808, loss is 0.08770126849412918\n",
      "epoch: 27 step: 809, loss is 0.06829521059989929\n",
      "epoch: 27 step: 810, loss is 0.005329827778041363\n",
      "epoch: 27 step: 811, loss is 0.04606974497437477\n",
      "epoch: 27 step: 812, loss is 0.04073929786682129\n",
      "epoch: 27 step: 813, loss is 0.0304474588483572\n",
      "epoch: 27 step: 814, loss is 0.0017978313844650984\n",
      "epoch: 27 step: 815, loss is 0.002977101830765605\n",
      "epoch: 27 step: 816, loss is 0.0004920721985399723\n",
      "epoch: 27 step: 817, loss is 0.012102741748094559\n",
      "epoch: 27 step: 818, loss is 0.0005135601386427879\n",
      "epoch: 27 step: 819, loss is 0.05916740372776985\n",
      "epoch: 27 step: 820, loss is 0.000351652066456154\n",
      "epoch: 27 step: 821, loss is 0.0008832362364046276\n",
      "epoch: 27 step: 822, loss is 0.005879604257643223\n",
      "epoch: 27 step: 823, loss is 0.02516426518559456\n",
      "epoch: 27 step: 824, loss is 0.0008268050150945783\n",
      "epoch: 27 step: 825, loss is 0.0019930547568947077\n",
      "epoch: 27 step: 826, loss is 0.04709378629922867\n",
      "epoch: 27 step: 827, loss is 0.0012954510748386383\n",
      "epoch: 27 step: 828, loss is 0.010062658227980137\n",
      "epoch: 27 step: 829, loss is 0.001776786521077156\n",
      "epoch: 27 step: 830, loss is 0.0015091417590156198\n",
      "epoch: 27 step: 831, loss is 0.025464661419391632\n",
      "epoch: 27 step: 832, loss is 0.00012345951108727604\n",
      "epoch: 27 step: 833, loss is 0.005492993630468845\n",
      "epoch: 27 step: 834, loss is 0.0007437519961968064\n",
      "epoch: 27 step: 835, loss is 0.0005083239520899951\n",
      "epoch: 27 step: 836, loss is 0.03229989856481552\n",
      "epoch: 27 step: 837, loss is 0.008302081376314163\n",
      "epoch: 27 step: 838, loss is 0.0005778411286883056\n",
      "epoch: 27 step: 839, loss is 0.00050200498662889\n",
      "epoch: 27 step: 840, loss is 0.011336659081280231\n",
      "epoch: 27 step: 841, loss is 0.001400208449922502\n",
      "epoch: 27 step: 842, loss is 0.001770867151208222\n",
      "epoch: 27 step: 843, loss is 0.00010216682858299464\n",
      "epoch: 27 step: 844, loss is 0.0011948698665946722\n",
      "epoch: 27 step: 845, loss is 0.003211898496374488\n",
      "epoch: 27 step: 846, loss is 0.00014901673421263695\n",
      "epoch: 27 step: 847, loss is 0.10830578953027725\n",
      "epoch: 27 step: 848, loss is 0.004911069292575121\n",
      "epoch: 27 step: 849, loss is 0.0006584959337487817\n",
      "epoch: 27 step: 850, loss is 0.03706794232130051\n",
      "epoch: 27 step: 851, loss is 0.012037117965519428\n",
      "epoch: 27 step: 852, loss is 0.12045954167842865\n",
      "epoch: 27 step: 853, loss is 0.0014246734790503979\n",
      "epoch: 27 step: 854, loss is 0.24829423427581787\n",
      "epoch: 27 step: 855, loss is 0.055737655609846115\n",
      "epoch: 27 step: 856, loss is 0.005888067651540041\n",
      "epoch: 27 step: 857, loss is 0.0433843657374382\n",
      "epoch: 27 step: 858, loss is 0.014834701083600521\n",
      "epoch: 27 step: 859, loss is 3.74753653886728e-05\n",
      "epoch: 27 step: 860, loss is 0.003040869487449527\n",
      "epoch: 27 step: 861, loss is 0.0010598034132272005\n",
      "epoch: 27 step: 862, loss is 0.005936184898018837\n",
      "epoch: 27 step: 863, loss is 0.0021286713890731335\n",
      "epoch: 27 step: 864, loss is 0.005690812133252621\n",
      "epoch: 27 step: 865, loss is 0.002232593484222889\n",
      "epoch: 27 step: 866, loss is 0.00751233147457242\n",
      "epoch: 27 step: 867, loss is 0.011030820198357105\n",
      "epoch: 27 step: 868, loss is 0.007938624359667301\n",
      "epoch: 27 step: 869, loss is 0.00023217730631586164\n",
      "epoch: 27 step: 870, loss is 0.012117506936192513\n",
      "epoch: 27 step: 871, loss is 0.002269637305289507\n",
      "epoch: 27 step: 872, loss is 0.00019446079386398196\n",
      "epoch: 27 step: 873, loss is 0.005972796585410833\n",
      "epoch: 27 step: 874, loss is 0.024519329890608788\n",
      "epoch: 27 step: 875, loss is 0.04007737711071968\n",
      "epoch: 27 step: 876, loss is 0.05216560885310173\n",
      "epoch: 27 step: 877, loss is 0.02552025578916073\n",
      "epoch: 27 step: 878, loss is 0.001378814224153757\n",
      "epoch: 27 step: 879, loss is 0.010561417788267136\n",
      "epoch: 27 step: 880, loss is 0.0012422297149896622\n",
      "epoch: 27 step: 881, loss is 0.026035692542791367\n",
      "epoch: 27 step: 882, loss is 0.07659903168678284\n",
      "epoch: 27 step: 883, loss is 0.06341298669576645\n",
      "epoch: 27 step: 884, loss is 0.011201195418834686\n",
      "epoch: 27 step: 885, loss is 0.004077087622135878\n",
      "epoch: 27 step: 886, loss is 0.0012413888471201062\n",
      "epoch: 27 step: 887, loss is 0.003878942457959056\n",
      "epoch: 27 step: 888, loss is 0.013227642513811588\n",
      "epoch: 27 step: 889, loss is 0.025676490738987923\n",
      "epoch: 27 step: 890, loss is 0.0023748509120196104\n",
      "epoch: 27 step: 891, loss is 0.045388054102659225\n",
      "epoch: 27 step: 892, loss is 0.003644761862233281\n",
      "epoch: 27 step: 893, loss is 0.009258028119802475\n",
      "epoch: 27 step: 894, loss is 0.01080744806677103\n",
      "epoch: 27 step: 895, loss is 0.00019787174824159592\n",
      "epoch: 27 step: 896, loss is 0.0157802514731884\n",
      "epoch: 27 step: 897, loss is 0.00012369555770419538\n",
      "epoch: 27 step: 898, loss is 0.007351904176175594\n",
      "epoch: 27 step: 899, loss is 4.261520371073857e-05\n",
      "epoch: 27 step: 900, loss is 0.0006709145964123309\n",
      "epoch: 27 step: 901, loss is 0.058204833418130875\n",
      "epoch: 27 step: 902, loss is 0.03567776456475258\n",
      "epoch: 27 step: 903, loss is 0.08524724841117859\n",
      "epoch: 27 step: 904, loss is 0.00183578091673553\n",
      "epoch: 27 step: 905, loss is 0.0010680381674319506\n",
      "epoch: 27 step: 906, loss is 0.0015991475665941834\n",
      "epoch: 27 step: 907, loss is 0.10341803729534149\n",
      "epoch: 27 step: 908, loss is 0.0034594766329973936\n",
      "epoch: 27 step: 909, loss is 0.0028808615170419216\n",
      "epoch: 27 step: 910, loss is 0.10513070970773697\n",
      "epoch: 27 step: 911, loss is 0.004944154527038336\n",
      "epoch: 27 step: 912, loss is 0.03662826493382454\n",
      "epoch: 27 step: 913, loss is 0.01810896024107933\n",
      "epoch: 27 step: 914, loss is 0.0012003532610833645\n",
      "epoch: 27 step: 915, loss is 0.004728741943836212\n",
      "epoch: 27 step: 916, loss is 0.022976526990532875\n",
      "epoch: 27 step: 917, loss is 0.002809662837535143\n",
      "epoch: 27 step: 918, loss is 0.016644660383462906\n",
      "epoch: 27 step: 919, loss is 0.03016556426882744\n",
      "epoch: 27 step: 920, loss is 0.00022756395628675818\n",
      "epoch: 27 step: 921, loss is 0.0034493696875870228\n",
      "epoch: 27 step: 922, loss is 0.0003176205500494689\n",
      "epoch: 27 step: 923, loss is 0.009930942207574844\n",
      "epoch: 27 step: 924, loss is 0.014118368737399578\n",
      "epoch: 27 step: 925, loss is 0.06064616143703461\n",
      "epoch: 27 step: 926, loss is 0.013885428197681904\n",
      "epoch: 27 step: 927, loss is 0.08446826785802841\n",
      "epoch: 27 step: 928, loss is 0.0008184329490177333\n",
      "epoch: 27 step: 929, loss is 0.04016070067882538\n",
      "epoch: 27 step: 930, loss is 0.05318693444132805\n",
      "epoch: 27 step: 931, loss is 0.025036904960870743\n",
      "epoch: 27 step: 932, loss is 0.03799130395054817\n",
      "epoch: 27 step: 933, loss is 0.0616857185959816\n",
      "epoch: 27 step: 934, loss is 0.003311845473945141\n",
      "epoch: 27 step: 935, loss is 0.001364389550872147\n",
      "epoch: 27 step: 936, loss is 0.003572039306163788\n",
      "epoch: 27 step: 937, loss is 0.004670233465731144\n",
      "epoch: 28 step: 1, loss is 0.015132932923734188\n",
      "epoch: 28 step: 2, loss is 0.037279076874256134\n",
      "epoch: 28 step: 3, loss is 0.033665645867586136\n",
      "epoch: 28 step: 4, loss is 0.002547481097280979\n",
      "epoch: 28 step: 5, loss is 0.025531630963087082\n",
      "epoch: 28 step: 6, loss is 0.006827342323958874\n",
      "epoch: 28 step: 7, loss is 8.387964044231921e-05\n",
      "epoch: 28 step: 8, loss is 0.016057657077908516\n",
      "epoch: 28 step: 9, loss is 0.0004303178866393864\n",
      "epoch: 28 step: 10, loss is 0.0011021771933883429\n",
      "epoch: 28 step: 11, loss is 0.00342429056763649\n",
      "epoch: 28 step: 12, loss is 0.00040074693970382214\n",
      "epoch: 28 step: 13, loss is 0.0007066422840580344\n",
      "epoch: 28 step: 14, loss is 0.0017463816329836845\n",
      "epoch: 28 step: 15, loss is 0.005176573060452938\n",
      "epoch: 28 step: 16, loss is 0.01882193796336651\n",
      "epoch: 28 step: 17, loss is 0.0010979759972542524\n",
      "epoch: 28 step: 18, loss is 0.0038152309134602547\n",
      "epoch: 28 step: 19, loss is 0.008489580824971199\n",
      "epoch: 28 step: 20, loss is 0.0017019660444930196\n",
      "epoch: 28 step: 21, loss is 0.0002195150445913896\n",
      "epoch: 28 step: 22, loss is 0.00037163333036005497\n",
      "epoch: 28 step: 23, loss is 0.06998924165964127\n",
      "epoch: 28 step: 24, loss is 0.07327914983034134\n",
      "epoch: 28 step: 25, loss is 0.0034083211794495583\n",
      "epoch: 28 step: 26, loss is 0.0004332915414124727\n",
      "epoch: 28 step: 27, loss is 0.007788282353430986\n",
      "epoch: 28 step: 28, loss is 0.00430268794298172\n",
      "epoch: 28 step: 29, loss is 0.0038955039344727993\n",
      "epoch: 28 step: 30, loss is 0.0002376511401962489\n",
      "epoch: 28 step: 31, loss is 0.0001452577707823366\n",
      "epoch: 28 step: 32, loss is 0.012002060189843178\n",
      "epoch: 28 step: 33, loss is 0.002950016176328063\n",
      "epoch: 28 step: 34, loss is 0.0003967705415561795\n",
      "epoch: 28 step: 35, loss is 0.0007744749309495091\n",
      "epoch: 28 step: 36, loss is 0.003215876640751958\n",
      "epoch: 28 step: 37, loss is 0.06894489377737045\n",
      "epoch: 28 step: 38, loss is 0.00011331260611768812\n",
      "epoch: 28 step: 39, loss is 0.0015602440107613802\n",
      "epoch: 28 step: 40, loss is 0.002068746369332075\n",
      "epoch: 28 step: 41, loss is 0.0004973779432475567\n",
      "epoch: 28 step: 42, loss is 0.0036075126845389605\n",
      "epoch: 28 step: 43, loss is 0.00028730958001688123\n",
      "epoch: 28 step: 44, loss is 0.0067113712430000305\n",
      "epoch: 28 step: 45, loss is 0.00017108731844928116\n",
      "epoch: 28 step: 46, loss is 9.628485713619739e-05\n",
      "epoch: 28 step: 47, loss is 0.0898129865527153\n",
      "epoch: 28 step: 48, loss is 0.06646114587783813\n",
      "epoch: 28 step: 49, loss is 0.0007440180634148419\n",
      "epoch: 28 step: 50, loss is 0.001884268131107092\n",
      "epoch: 28 step: 51, loss is 4.8960802814690396e-05\n",
      "epoch: 28 step: 52, loss is 0.03959851711988449\n",
      "epoch: 28 step: 53, loss is 0.00020284780475776643\n",
      "epoch: 28 step: 54, loss is 0.00040368715417571366\n",
      "epoch: 28 step: 55, loss is 0.002079729223623872\n",
      "epoch: 28 step: 56, loss is 2.3467604478355497e-05\n",
      "epoch: 28 step: 57, loss is 0.0035404274240136147\n",
      "epoch: 28 step: 58, loss is 0.003252217546105385\n",
      "epoch: 28 step: 59, loss is 0.0061335316859185696\n",
      "epoch: 28 step: 60, loss is 0.02621031552553177\n",
      "epoch: 28 step: 61, loss is 0.003505237400531769\n",
      "epoch: 28 step: 62, loss is 0.008087982423603535\n",
      "epoch: 28 step: 63, loss is 0.11201261729001999\n",
      "epoch: 28 step: 64, loss is 0.004587762523442507\n",
      "epoch: 28 step: 65, loss is 0.003248180728405714\n",
      "epoch: 28 step: 66, loss is 0.0067822677083313465\n",
      "epoch: 28 step: 67, loss is 0.00014111596101429313\n",
      "epoch: 28 step: 68, loss is 0.033269740641117096\n",
      "epoch: 28 step: 69, loss is 0.0017675638664513826\n",
      "epoch: 28 step: 70, loss is 0.0010849407408386469\n",
      "epoch: 28 step: 71, loss is 0.17159196734428406\n",
      "epoch: 28 step: 72, loss is 0.05928008630871773\n",
      "epoch: 28 step: 73, loss is 0.005125584080815315\n",
      "epoch: 28 step: 74, loss is 0.0035824449732899666\n",
      "epoch: 28 step: 75, loss is 0.024288780987262726\n",
      "epoch: 28 step: 76, loss is 0.0001429137628292665\n",
      "epoch: 28 step: 77, loss is 0.0029146461747586727\n",
      "epoch: 28 step: 78, loss is 0.001020606025122106\n",
      "epoch: 28 step: 79, loss is 0.00011348416592227295\n",
      "epoch: 28 step: 80, loss is 0.004318724852055311\n",
      "epoch: 28 step: 81, loss is 0.03182066231966019\n",
      "epoch: 28 step: 82, loss is 4.963933315593749e-05\n",
      "epoch: 28 step: 83, loss is 0.0014965756563469768\n",
      "epoch: 28 step: 84, loss is 0.00025725364685058594\n",
      "epoch: 28 step: 85, loss is 0.0018517898861318827\n",
      "epoch: 28 step: 86, loss is 0.0013917838223278522\n",
      "epoch: 28 step: 87, loss is 0.035364069044589996\n",
      "epoch: 28 step: 88, loss is 0.009005177766084671\n",
      "epoch: 28 step: 89, loss is 0.02282053604722023\n",
      "epoch: 28 step: 90, loss is 0.0010653247591108084\n",
      "epoch: 28 step: 91, loss is 0.004802469629794359\n",
      "epoch: 28 step: 92, loss is 0.005156264640390873\n",
      "epoch: 28 step: 93, loss is 0.04772848263382912\n",
      "epoch: 28 step: 94, loss is 0.046450912952423096\n",
      "epoch: 28 step: 95, loss is 0.001508348505012691\n",
      "epoch: 28 step: 96, loss is 0.001853572204709053\n",
      "epoch: 28 step: 97, loss is 0.005357568617910147\n",
      "epoch: 28 step: 98, loss is 0.04022761806845665\n",
      "epoch: 28 step: 99, loss is 0.0004563652619253844\n",
      "epoch: 28 step: 100, loss is 0.0021147930528968573\n",
      "epoch: 28 step: 101, loss is 0.13569843769073486\n",
      "epoch: 28 step: 102, loss is 0.0007006932864896953\n",
      "epoch: 28 step: 103, loss is 0.001545696402899921\n",
      "epoch: 28 step: 104, loss is 0.00015042058657854795\n",
      "epoch: 28 step: 105, loss is 0.0005782522493973374\n",
      "epoch: 28 step: 106, loss is 0.036219652742147446\n",
      "epoch: 28 step: 107, loss is 0.0005492239724844694\n",
      "epoch: 28 step: 108, loss is 0.00028731717611663043\n",
      "epoch: 28 step: 109, loss is 0.014683425426483154\n",
      "epoch: 28 step: 110, loss is 0.007448291871696711\n",
      "epoch: 28 step: 111, loss is 0.039111312478780746\n",
      "epoch: 28 step: 112, loss is 0.005326662212610245\n",
      "epoch: 28 step: 113, loss is 0.0011903699487447739\n",
      "epoch: 28 step: 114, loss is 0.016688033938407898\n",
      "epoch: 28 step: 115, loss is 0.11888613551855087\n",
      "epoch: 28 step: 116, loss is 0.0010717334225773811\n",
      "epoch: 28 step: 117, loss is 0.02686447836458683\n",
      "epoch: 28 step: 118, loss is 0.06660137325525284\n",
      "epoch: 28 step: 119, loss is 0.0007571459864266217\n",
      "epoch: 28 step: 120, loss is 0.007020642049610615\n",
      "epoch: 28 step: 121, loss is 0.1333043873310089\n",
      "epoch: 28 step: 122, loss is 0.08911038935184479\n",
      "epoch: 28 step: 123, loss is 0.04347708448767662\n",
      "epoch: 28 step: 124, loss is 6.578570719284471e-06\n",
      "epoch: 28 step: 125, loss is 0.011781065724790096\n",
      "epoch: 28 step: 126, loss is 0.0011366716353222728\n",
      "epoch: 28 step: 127, loss is 0.1850384771823883\n",
      "epoch: 28 step: 128, loss is 0.016033288091421127\n",
      "epoch: 28 step: 129, loss is 0.0011952414643019438\n",
      "epoch: 28 step: 130, loss is 0.012371194548904896\n",
      "epoch: 28 step: 131, loss is 0.007443831767886877\n",
      "epoch: 28 step: 132, loss is 0.026448575779795647\n",
      "epoch: 28 step: 133, loss is 0.001458005397580564\n",
      "epoch: 28 step: 134, loss is 0.001512758550234139\n",
      "epoch: 28 step: 135, loss is 0.020407015457749367\n",
      "epoch: 28 step: 136, loss is 0.010129807516932487\n",
      "epoch: 28 step: 137, loss is 0.0333743691444397\n",
      "epoch: 28 step: 138, loss is 0.006489174906164408\n",
      "epoch: 28 step: 139, loss is 0.003293166169896722\n",
      "epoch: 28 step: 140, loss is 0.0007596940267831087\n",
      "epoch: 28 step: 141, loss is 0.0272841714322567\n",
      "epoch: 28 step: 142, loss is 0.004275034181773663\n",
      "epoch: 28 step: 143, loss is 0.0001353713741991669\n",
      "epoch: 28 step: 144, loss is 0.0053090560249984264\n",
      "epoch: 28 step: 145, loss is 0.0008019282249733806\n",
      "epoch: 28 step: 146, loss is 0.0031202880199998617\n",
      "epoch: 28 step: 147, loss is 0.0008681532344780862\n",
      "epoch: 28 step: 148, loss is 0.011282870545983315\n",
      "epoch: 28 step: 149, loss is 0.009640326723456383\n",
      "epoch: 28 step: 150, loss is 0.02391071431338787\n",
      "epoch: 28 step: 151, loss is 0.0044811381958425045\n",
      "epoch: 28 step: 152, loss is 0.0009402644936926663\n",
      "epoch: 28 step: 153, loss is 0.0017918638186529279\n",
      "epoch: 28 step: 154, loss is 0.045837368816137314\n",
      "epoch: 28 step: 155, loss is 0.004293337464332581\n",
      "epoch: 28 step: 156, loss is 3.9209182432387024e-05\n",
      "epoch: 28 step: 157, loss is 0.0007327079074457288\n",
      "epoch: 28 step: 158, loss is 0.012084498070180416\n",
      "epoch: 28 step: 159, loss is 0.0018112075049430132\n",
      "epoch: 28 step: 160, loss is 0.0008658115402795374\n",
      "epoch: 28 step: 161, loss is 0.0005077025853097439\n",
      "epoch: 28 step: 162, loss is 0.06591580808162689\n",
      "epoch: 28 step: 163, loss is 0.05583753436803818\n",
      "epoch: 28 step: 164, loss is 0.0012419720878824592\n",
      "epoch: 28 step: 165, loss is 0.022825244814157486\n",
      "epoch: 28 step: 166, loss is 0.019030196592211723\n",
      "epoch: 28 step: 167, loss is 0.0003667382407002151\n",
      "epoch: 28 step: 168, loss is 0.0011665208730846643\n",
      "epoch: 28 step: 169, loss is 0.04176789149641991\n",
      "epoch: 28 step: 170, loss is 0.005551942624151707\n",
      "epoch: 28 step: 171, loss is 0.06478556990623474\n",
      "epoch: 28 step: 172, loss is 0.001436263439245522\n",
      "epoch: 28 step: 173, loss is 0.02610848657786846\n",
      "epoch: 28 step: 174, loss is 0.006379843223839998\n",
      "epoch: 28 step: 175, loss is 0.0032272266689687967\n",
      "epoch: 28 step: 176, loss is 0.0025470128748565912\n",
      "epoch: 28 step: 177, loss is 0.020592963322997093\n",
      "epoch: 28 step: 178, loss is 0.019577641040086746\n",
      "epoch: 28 step: 179, loss is 0.007741026114672422\n",
      "epoch: 28 step: 180, loss is 0.018464967608451843\n",
      "epoch: 28 step: 181, loss is 0.007976206950843334\n",
      "epoch: 28 step: 182, loss is 0.0015148769598454237\n",
      "epoch: 28 step: 183, loss is 0.0015438890550285578\n",
      "epoch: 28 step: 184, loss is 0.005015620496124029\n",
      "epoch: 28 step: 185, loss is 0.0013582516694441438\n",
      "epoch: 28 step: 186, loss is 0.009345476515591145\n",
      "epoch: 28 step: 187, loss is 0.038864389061927795\n",
      "epoch: 28 step: 188, loss is 0.0036193947307765484\n",
      "epoch: 28 step: 189, loss is 0.19908161461353302\n",
      "epoch: 28 step: 190, loss is 0.03426932170987129\n",
      "epoch: 28 step: 191, loss is 0.015371494926512241\n",
      "epoch: 28 step: 192, loss is 0.00015551640535704792\n",
      "epoch: 28 step: 193, loss is 0.026952669024467468\n",
      "epoch: 28 step: 194, loss is 0.004562054295092821\n",
      "epoch: 28 step: 195, loss is 0.020533932372927666\n",
      "epoch: 28 step: 196, loss is 0.004775679670274258\n",
      "epoch: 28 step: 197, loss is 0.0001206680535688065\n",
      "epoch: 28 step: 198, loss is 0.0002126628387486562\n",
      "epoch: 28 step: 199, loss is 0.001145227113738656\n",
      "epoch: 28 step: 200, loss is 0.0006761114345863461\n",
      "epoch: 28 step: 201, loss is 0.06914114207029343\n",
      "epoch: 28 step: 202, loss is 2.8311374990153126e-05\n",
      "epoch: 28 step: 203, loss is 0.01700885035097599\n",
      "epoch: 28 step: 204, loss is 0.0031511178240180016\n",
      "epoch: 28 step: 205, loss is 0.006924839690327644\n",
      "epoch: 28 step: 206, loss is 0.0006301671382971108\n",
      "epoch: 28 step: 207, loss is 0.019274042919278145\n",
      "epoch: 28 step: 208, loss is 0.014010023325681686\n",
      "epoch: 28 step: 209, loss is 0.08488196134567261\n",
      "epoch: 28 step: 210, loss is 0.06899214535951614\n",
      "epoch: 28 step: 211, loss is 0.0001497352495789528\n",
      "epoch: 28 step: 212, loss is 0.10940564423799515\n",
      "epoch: 28 step: 213, loss is 0.0005609801737591624\n",
      "epoch: 28 step: 214, loss is 0.000328732916386798\n",
      "epoch: 28 step: 215, loss is 0.0008532741921953857\n",
      "epoch: 28 step: 216, loss is 0.004674095660448074\n",
      "epoch: 28 step: 217, loss is 0.023755719885230064\n",
      "epoch: 28 step: 218, loss is 0.005729091819375753\n",
      "epoch: 28 step: 219, loss is 0.0013736244291067123\n",
      "epoch: 28 step: 220, loss is 0.002150673884898424\n",
      "epoch: 28 step: 221, loss is 0.0013441505143418908\n",
      "epoch: 28 step: 222, loss is 0.05011223629117012\n",
      "epoch: 28 step: 223, loss is 0.00014280964387580752\n",
      "epoch: 28 step: 224, loss is 0.03353464975953102\n",
      "epoch: 28 step: 225, loss is 0.0024397864472121\n",
      "epoch: 28 step: 226, loss is 0.00020611952641047537\n",
      "epoch: 28 step: 227, loss is 0.03006722778081894\n",
      "epoch: 28 step: 228, loss is 0.07328992336988449\n",
      "epoch: 28 step: 229, loss is 0.019029229879379272\n",
      "epoch: 28 step: 230, loss is 0.005532102659344673\n",
      "epoch: 28 step: 231, loss is 0.002645471366122365\n",
      "epoch: 28 step: 232, loss is 0.04206370934844017\n",
      "epoch: 28 step: 233, loss is 0.0014647000934928656\n",
      "epoch: 28 step: 234, loss is 0.00012527442595455796\n",
      "epoch: 28 step: 235, loss is 0.13202711939811707\n",
      "epoch: 28 step: 236, loss is 0.0057787238620221615\n",
      "epoch: 28 step: 237, loss is 0.00018525384075473994\n",
      "epoch: 28 step: 238, loss is 0.027297884225845337\n",
      "epoch: 28 step: 239, loss is 0.005968137178570032\n",
      "epoch: 28 step: 240, loss is 0.005066786427050829\n",
      "epoch: 28 step: 241, loss is 0.0019481101771816611\n",
      "epoch: 28 step: 242, loss is 0.0017700219759717584\n",
      "epoch: 28 step: 243, loss is 0.01347280852496624\n",
      "epoch: 28 step: 244, loss is 0.007974743843078613\n",
      "epoch: 28 step: 245, loss is 0.0021974067203700542\n",
      "epoch: 28 step: 246, loss is 0.056499358266592026\n",
      "epoch: 28 step: 247, loss is 0.0002498582180123776\n",
      "epoch: 28 step: 248, loss is 0.011342201381921768\n",
      "epoch: 28 step: 249, loss is 0.14586728811264038\n",
      "epoch: 28 step: 250, loss is 0.0054895589128136635\n",
      "epoch: 28 step: 251, loss is 0.03957368806004524\n",
      "epoch: 28 step: 252, loss is 0.0006870653014630079\n",
      "epoch: 28 step: 253, loss is 0.009857267141342163\n",
      "epoch: 28 step: 254, loss is 0.0012985002249479294\n",
      "epoch: 28 step: 255, loss is 0.002067431341856718\n",
      "epoch: 28 step: 256, loss is 0.0651603415608406\n",
      "epoch: 28 step: 257, loss is 0.0010509116109460592\n",
      "epoch: 28 step: 258, loss is 0.003923153970390558\n",
      "epoch: 28 step: 259, loss is 0.006270866375416517\n",
      "epoch: 28 step: 260, loss is 0.027943700551986694\n",
      "epoch: 28 step: 261, loss is 0.007836085744202137\n",
      "epoch: 28 step: 262, loss is 0.0009838294936344028\n",
      "epoch: 28 step: 263, loss is 0.0063920090906322\n",
      "epoch: 28 step: 264, loss is 0.006973627023398876\n",
      "epoch: 28 step: 265, loss is 4.697242547990754e-05\n",
      "epoch: 28 step: 266, loss is 0.0015594802098348737\n",
      "epoch: 28 step: 267, loss is 0.0004024853988084942\n",
      "epoch: 28 step: 268, loss is 0.04979613423347473\n",
      "epoch: 28 step: 269, loss is 0.036640748381614685\n",
      "epoch: 28 step: 270, loss is 0.0014534974470734596\n",
      "epoch: 28 step: 271, loss is 0.00461975485086441\n",
      "epoch: 28 step: 272, loss is 0.003198659745976329\n",
      "epoch: 28 step: 273, loss is 0.05380690470337868\n",
      "epoch: 28 step: 274, loss is 0.004869385622441769\n",
      "epoch: 28 step: 275, loss is 0.025574712082743645\n",
      "epoch: 28 step: 276, loss is 0.013019698671996593\n",
      "epoch: 28 step: 277, loss is 0.016983475536108017\n",
      "epoch: 28 step: 278, loss is 0.03320213034749031\n",
      "epoch: 28 step: 279, loss is 0.006024631205946207\n",
      "epoch: 28 step: 280, loss is 0.0011119062546640635\n",
      "epoch: 28 step: 281, loss is 0.002475468907505274\n",
      "epoch: 28 step: 282, loss is 0.043022312223911285\n",
      "epoch: 28 step: 283, loss is 0.014492341317236423\n",
      "epoch: 28 step: 284, loss is 0.013296223245561123\n",
      "epoch: 28 step: 285, loss is 0.06212444230914116\n",
      "epoch: 28 step: 286, loss is 0.012228382751345634\n",
      "epoch: 28 step: 287, loss is 0.0014823873061686754\n",
      "epoch: 28 step: 288, loss is 0.005581573583185673\n",
      "epoch: 28 step: 289, loss is 0.0017142398282885551\n",
      "epoch: 28 step: 290, loss is 0.000617582758422941\n",
      "epoch: 28 step: 291, loss is 0.00495842844247818\n",
      "epoch: 28 step: 292, loss is 0.0006911554373800755\n",
      "epoch: 28 step: 293, loss is 0.00045244517968967557\n",
      "epoch: 28 step: 294, loss is 0.0005461715627461672\n",
      "epoch: 28 step: 295, loss is 0.013663901016116142\n",
      "epoch: 28 step: 296, loss is 6.767143349861726e-05\n",
      "epoch: 28 step: 297, loss is 0.0004670462803915143\n",
      "epoch: 28 step: 298, loss is 0.001065998338162899\n",
      "epoch: 28 step: 299, loss is 0.0008981312275864184\n",
      "epoch: 28 step: 300, loss is 0.0033374661579728127\n",
      "epoch: 28 step: 301, loss is 2.596413833089173e-05\n",
      "epoch: 28 step: 302, loss is 0.0011805822141468525\n",
      "epoch: 28 step: 303, loss is 0.0022583995014429092\n",
      "epoch: 28 step: 304, loss is 0.004507328383624554\n",
      "epoch: 28 step: 305, loss is 0.143606498837471\n",
      "epoch: 28 step: 306, loss is 0.00412739859893918\n",
      "epoch: 28 step: 307, loss is 0.00014106239541433752\n",
      "epoch: 28 step: 308, loss is 0.015204952098429203\n",
      "epoch: 28 step: 309, loss is 0.006499174516648054\n",
      "epoch: 28 step: 310, loss is 0.0022837298456579447\n",
      "epoch: 28 step: 311, loss is 0.003180412109941244\n",
      "epoch: 28 step: 312, loss is 0.0002044403227046132\n",
      "epoch: 28 step: 313, loss is 0.0005592834786511958\n",
      "epoch: 28 step: 314, loss is 0.0035192843060940504\n",
      "epoch: 28 step: 315, loss is 0.003950138576328754\n",
      "epoch: 28 step: 316, loss is 0.03847396373748779\n",
      "epoch: 28 step: 317, loss is 0.001447371207177639\n",
      "epoch: 28 step: 318, loss is 0.0003102053015027195\n",
      "epoch: 28 step: 319, loss is 0.008135467767715454\n",
      "epoch: 28 step: 320, loss is 0.11370928585529327\n",
      "epoch: 28 step: 321, loss is 0.0005779498023912311\n",
      "epoch: 28 step: 322, loss is 0.0003929163212887943\n",
      "epoch: 28 step: 323, loss is 0.004569306503981352\n",
      "epoch: 28 step: 324, loss is 0.0019281752174720168\n",
      "epoch: 28 step: 325, loss is 0.0058129034005105495\n",
      "epoch: 28 step: 326, loss is 0.0005963996518403292\n",
      "epoch: 28 step: 327, loss is 0.00214805593714118\n",
      "epoch: 28 step: 328, loss is 0.0049354685470461845\n",
      "epoch: 28 step: 329, loss is 0.0009774821810424328\n",
      "epoch: 28 step: 330, loss is 0.044950153678655624\n",
      "epoch: 28 step: 331, loss is 2.468680941092316e-05\n",
      "epoch: 28 step: 332, loss is 0.007184618152678013\n",
      "epoch: 28 step: 333, loss is 0.022311590611934662\n",
      "epoch: 28 step: 334, loss is 0.05347612872719765\n",
      "epoch: 28 step: 335, loss is 0.03381523862481117\n",
      "epoch: 28 step: 336, loss is 0.023493649438023567\n",
      "epoch: 28 step: 337, loss is 0.005202922038733959\n",
      "epoch: 28 step: 338, loss is 0.00037048381636850536\n",
      "epoch: 28 step: 339, loss is 0.01180269755423069\n",
      "epoch: 28 step: 340, loss is 0.0044833458960056305\n",
      "epoch: 28 step: 341, loss is 6.896948616486043e-05\n",
      "epoch: 28 step: 342, loss is 0.009151030331850052\n",
      "epoch: 28 step: 343, loss is 0.0015274110483005643\n",
      "epoch: 28 step: 344, loss is 0.012235223315656185\n",
      "epoch: 28 step: 345, loss is 0.019700029864907265\n",
      "epoch: 28 step: 346, loss is 0.001613982836715877\n",
      "epoch: 28 step: 347, loss is 0.11978811770677567\n",
      "epoch: 28 step: 348, loss is 0.00011513995559653267\n",
      "epoch: 28 step: 349, loss is 0.007003274746239185\n",
      "epoch: 28 step: 350, loss is 0.0007785409688949585\n",
      "epoch: 28 step: 351, loss is 0.018084142357110977\n",
      "epoch: 28 step: 352, loss is 0.00037203048123046756\n",
      "epoch: 28 step: 353, loss is 0.0019800509326159954\n",
      "epoch: 28 step: 354, loss is 0.0029376433230936527\n",
      "epoch: 28 step: 355, loss is 0.00969018042087555\n",
      "epoch: 28 step: 356, loss is 0.004441588185727596\n",
      "epoch: 28 step: 357, loss is 0.0003613393346313387\n",
      "epoch: 28 step: 358, loss is 0.006221002899110317\n",
      "epoch: 28 step: 359, loss is 0.0011000075610354543\n",
      "epoch: 28 step: 360, loss is 0.008403428830206394\n",
      "epoch: 28 step: 361, loss is 0.00014402389933820814\n",
      "epoch: 28 step: 362, loss is 0.0006123979110270739\n",
      "epoch: 28 step: 363, loss is 0.001762755447998643\n",
      "epoch: 28 step: 364, loss is 0.00014280216419138014\n",
      "epoch: 28 step: 365, loss is 0.0011735103325918317\n",
      "epoch: 28 step: 366, loss is 0.0016912848223000765\n",
      "epoch: 28 step: 367, loss is 0.008762086741626263\n",
      "epoch: 28 step: 368, loss is 0.011039643548429012\n",
      "epoch: 28 step: 369, loss is 0.008090285584330559\n",
      "epoch: 28 step: 370, loss is 9.556064469506964e-05\n",
      "epoch: 28 step: 371, loss is 0.003261869540438056\n",
      "epoch: 28 step: 372, loss is 2.2143012756714597e-05\n",
      "epoch: 28 step: 373, loss is 0.04375167936086655\n",
      "epoch: 28 step: 374, loss is 0.0030492632649838924\n",
      "epoch: 28 step: 375, loss is 0.0003912380780093372\n",
      "epoch: 28 step: 376, loss is 0.012557732872664928\n",
      "epoch: 28 step: 377, loss is 0.009835938923060894\n",
      "epoch: 28 step: 378, loss is 0.0018128209048882127\n",
      "epoch: 28 step: 379, loss is 0.02510504052042961\n",
      "epoch: 28 step: 380, loss is 0.0009979947935789824\n",
      "epoch: 28 step: 381, loss is 0.0006197566399350762\n",
      "epoch: 28 step: 382, loss is 0.00017679357551969588\n",
      "epoch: 28 step: 383, loss is 0.0012384656583890319\n",
      "epoch: 28 step: 384, loss is 0.00974268652498722\n",
      "epoch: 28 step: 385, loss is 0.002597186714410782\n",
      "epoch: 28 step: 386, loss is 0.0042193285189569\n",
      "epoch: 28 step: 387, loss is 0.02498660795390606\n",
      "epoch: 28 step: 388, loss is 0.0020329412072896957\n",
      "epoch: 28 step: 389, loss is 0.001333844382315874\n",
      "epoch: 28 step: 390, loss is 0.02357787825167179\n",
      "epoch: 28 step: 391, loss is 1.2624201190192252e-05\n",
      "epoch: 28 step: 392, loss is 0.0012622172944247723\n",
      "epoch: 28 step: 393, loss is 2.6760862965602428e-05\n",
      "epoch: 28 step: 394, loss is 0.017545878887176514\n",
      "epoch: 28 step: 395, loss is 0.0032704549375921488\n",
      "epoch: 28 step: 396, loss is 0.015759440138936043\n",
      "epoch: 28 step: 397, loss is 0.004475353751331568\n",
      "epoch: 28 step: 398, loss is 0.04862573370337486\n",
      "epoch: 28 step: 399, loss is 0.0001650452904868871\n",
      "epoch: 28 step: 400, loss is 0.00013998209033161402\n",
      "epoch: 28 step: 401, loss is 0.00022319624258670956\n",
      "epoch: 28 step: 402, loss is 0.042494744062423706\n",
      "epoch: 28 step: 403, loss is 0.015149715356528759\n",
      "epoch: 28 step: 404, loss is 0.0004587562580127269\n",
      "epoch: 28 step: 405, loss is 0.0018641581991687417\n",
      "epoch: 28 step: 406, loss is 0.0002571159275248647\n",
      "epoch: 28 step: 407, loss is 0.012743733823299408\n",
      "epoch: 28 step: 408, loss is 0.0004782108007930219\n",
      "epoch: 28 step: 409, loss is 0.0009408289333805442\n",
      "epoch: 28 step: 410, loss is 0.02872486039996147\n",
      "epoch: 28 step: 411, loss is 0.01949722319841385\n",
      "epoch: 28 step: 412, loss is 0.009916136972606182\n",
      "epoch: 28 step: 413, loss is 0.0004553532344289124\n",
      "epoch: 28 step: 414, loss is 0.0001253028749488294\n",
      "epoch: 28 step: 415, loss is 0.008197590708732605\n",
      "epoch: 28 step: 416, loss is 0.015887826681137085\n",
      "epoch: 28 step: 417, loss is 0.006330122705549002\n",
      "epoch: 28 step: 418, loss is 0.0006574036087840796\n",
      "epoch: 28 step: 419, loss is 0.0011162330629304051\n",
      "epoch: 28 step: 420, loss is 0.0021078435238450766\n",
      "epoch: 28 step: 421, loss is 0.0004580001113936305\n",
      "epoch: 28 step: 422, loss is 0.002189795020967722\n",
      "epoch: 28 step: 423, loss is 0.026080213487148285\n",
      "epoch: 28 step: 424, loss is 8.394834731006995e-05\n",
      "epoch: 28 step: 425, loss is 0.005630002822726965\n",
      "epoch: 28 step: 426, loss is 0.007609092630445957\n",
      "epoch: 28 step: 427, loss is 0.00033907152828760445\n",
      "epoch: 28 step: 428, loss is 0.0004447379324119538\n",
      "epoch: 28 step: 429, loss is 0.02133941277861595\n",
      "epoch: 28 step: 430, loss is 0.0020008599385619164\n",
      "epoch: 28 step: 431, loss is 0.004024845082312822\n",
      "epoch: 28 step: 432, loss is 0.0003711622557602823\n",
      "epoch: 28 step: 433, loss is 0.00011243665358051658\n",
      "epoch: 28 step: 434, loss is 0.0026189920026808977\n",
      "epoch: 28 step: 435, loss is 0.001148187555372715\n",
      "epoch: 28 step: 436, loss is 1.0572368410066701e-05\n",
      "epoch: 28 step: 437, loss is 0.007730456069111824\n",
      "epoch: 28 step: 438, loss is 0.008812922984361649\n",
      "epoch: 28 step: 439, loss is 0.002246817573904991\n",
      "epoch: 28 step: 440, loss is 0.09888021647930145\n",
      "epoch: 28 step: 441, loss is 0.008649195544421673\n",
      "epoch: 28 step: 442, loss is 0.0002494668588042259\n",
      "epoch: 28 step: 443, loss is 0.0005858369404450059\n",
      "epoch: 28 step: 444, loss is 0.0008132535731419921\n",
      "epoch: 28 step: 445, loss is 0.002166397636756301\n",
      "epoch: 28 step: 446, loss is 0.0027988278307020664\n",
      "epoch: 28 step: 447, loss is 0.0025442279875278473\n",
      "epoch: 28 step: 448, loss is 0.00024006275634746999\n",
      "epoch: 28 step: 449, loss is 0.0035294494591653347\n",
      "epoch: 28 step: 450, loss is 0.01453403476625681\n",
      "epoch: 28 step: 451, loss is 0.0001849455147748813\n",
      "epoch: 28 step: 452, loss is 0.00633016275241971\n",
      "epoch: 28 step: 453, loss is 0.0027267741970717907\n",
      "epoch: 28 step: 454, loss is 0.0034148036502301693\n",
      "epoch: 28 step: 455, loss is 0.0006424674647860229\n",
      "epoch: 28 step: 456, loss is 0.00014689404633827507\n",
      "epoch: 28 step: 457, loss is 0.05197886377573013\n",
      "epoch: 28 step: 458, loss is 0.00013539371138904244\n",
      "epoch: 28 step: 459, loss is 0.00012370094191282988\n",
      "epoch: 28 step: 460, loss is 0.011050278320908546\n",
      "epoch: 28 step: 461, loss is 0.016560150310397148\n",
      "epoch: 28 step: 462, loss is 0.06010718271136284\n",
      "epoch: 28 step: 463, loss is 0.00027082732412964106\n",
      "epoch: 28 step: 464, loss is 0.0001733119279379025\n",
      "epoch: 28 step: 465, loss is 0.0028307614848017693\n",
      "epoch: 28 step: 466, loss is 0.0035710958763957024\n",
      "epoch: 28 step: 467, loss is 0.0018931312952190638\n",
      "epoch: 28 step: 468, loss is 0.12430384755134583\n",
      "epoch: 28 step: 469, loss is 0.008967231027781963\n",
      "epoch: 28 step: 470, loss is 0.00046766974264755845\n",
      "epoch: 28 step: 471, loss is 0.009493401274085045\n",
      "epoch: 28 step: 472, loss is 0.0004289166536182165\n",
      "epoch: 28 step: 473, loss is 0.0001476314791943878\n",
      "epoch: 28 step: 474, loss is 0.01161259040236473\n",
      "epoch: 28 step: 475, loss is 0.0008809969294816256\n",
      "epoch: 28 step: 476, loss is 0.00037624099059030414\n",
      "epoch: 28 step: 477, loss is 0.0007482549990527332\n",
      "epoch: 28 step: 478, loss is 0.00032148699392564595\n",
      "epoch: 28 step: 479, loss is 0.0040964181534945965\n",
      "epoch: 28 step: 480, loss is 0.0052169435657560825\n",
      "epoch: 28 step: 481, loss is 0.0011610194342210889\n",
      "epoch: 28 step: 482, loss is 0.007326800841838121\n",
      "epoch: 28 step: 483, loss is 0.0030790038872510195\n",
      "epoch: 28 step: 484, loss is 0.02586740255355835\n",
      "epoch: 28 step: 485, loss is 0.007142654620110989\n",
      "epoch: 28 step: 486, loss is 0.002565467730164528\n",
      "epoch: 28 step: 487, loss is 0.00027860497357323766\n",
      "epoch: 28 step: 488, loss is 0.004482397343963385\n",
      "epoch: 28 step: 489, loss is 0.0018135530408471823\n",
      "epoch: 28 step: 490, loss is 0.009778063744306564\n",
      "epoch: 28 step: 491, loss is 0.002818789565935731\n",
      "epoch: 28 step: 492, loss is 0.00021016737446188927\n",
      "epoch: 28 step: 493, loss is 0.013782232068479061\n",
      "epoch: 28 step: 494, loss is 0.006119952537119389\n",
      "epoch: 28 step: 495, loss is 0.004589717369526625\n",
      "epoch: 28 step: 496, loss is 0.0004199996474198997\n",
      "epoch: 28 step: 497, loss is 0.004453836940228939\n",
      "epoch: 28 step: 498, loss is 0.1472225934267044\n",
      "epoch: 28 step: 499, loss is 0.004087856505066156\n",
      "epoch: 28 step: 500, loss is 0.0008343036752194166\n",
      "epoch: 28 step: 501, loss is 0.00030581402825191617\n",
      "epoch: 28 step: 502, loss is 0.07602559030056\n",
      "epoch: 28 step: 503, loss is 0.0013132960302755237\n",
      "epoch: 28 step: 504, loss is 0.0028094123117625713\n",
      "epoch: 28 step: 505, loss is 0.0013100658543407917\n",
      "epoch: 28 step: 506, loss is 0.0003430949873290956\n",
      "epoch: 28 step: 507, loss is 0.06268276274204254\n",
      "epoch: 28 step: 508, loss is 0.0004732448433060199\n",
      "epoch: 28 step: 509, loss is 0.0006607185350731015\n",
      "epoch: 28 step: 510, loss is 1.0468013897479977e-05\n",
      "epoch: 28 step: 511, loss is 0.05571978539228439\n",
      "epoch: 28 step: 512, loss is 0.0012832171050831676\n",
      "epoch: 28 step: 513, loss is 0.001603654120117426\n",
      "epoch: 28 step: 514, loss is 0.004294141195714474\n",
      "epoch: 28 step: 515, loss is 0.0056908875703811646\n",
      "epoch: 28 step: 516, loss is 0.001598105882294476\n",
      "epoch: 28 step: 517, loss is 0.002005580347031355\n",
      "epoch: 28 step: 518, loss is 0.01784864254295826\n",
      "epoch: 28 step: 519, loss is 0.0001427682291250676\n",
      "epoch: 28 step: 520, loss is 0.017822498455643654\n",
      "epoch: 28 step: 521, loss is 0.0025589356664568186\n",
      "epoch: 28 step: 522, loss is 0.00025315157836303115\n",
      "epoch: 28 step: 523, loss is 1.3401314390648622e-05\n",
      "epoch: 28 step: 524, loss is 0.0052147298119962215\n",
      "epoch: 28 step: 525, loss is 8.481655095238239e-05\n",
      "epoch: 28 step: 526, loss is 0.0019958787597715855\n",
      "epoch: 28 step: 527, loss is 0.012432056479156017\n",
      "epoch: 28 step: 528, loss is 0.0004491757426876575\n",
      "epoch: 28 step: 529, loss is 0.0005070907063782215\n",
      "epoch: 28 step: 530, loss is 0.00881167221814394\n",
      "epoch: 28 step: 531, loss is 0.005060215014964342\n",
      "epoch: 28 step: 532, loss is 0.01980314590036869\n",
      "epoch: 28 step: 533, loss is 0.009642747230827808\n",
      "epoch: 28 step: 534, loss is 0.0021633682772517204\n",
      "epoch: 28 step: 535, loss is 0.00526692857965827\n",
      "epoch: 28 step: 536, loss is 0.0004214641230646521\n",
      "epoch: 28 step: 537, loss is 0.005900301970541477\n",
      "epoch: 28 step: 538, loss is 0.000853955396451056\n",
      "epoch: 28 step: 539, loss is 0.003078923560678959\n",
      "epoch: 28 step: 540, loss is 0.006765631027519703\n",
      "epoch: 28 step: 541, loss is 0.005609672516584396\n",
      "epoch: 28 step: 542, loss is 0.0013173891929909587\n",
      "epoch: 28 step: 543, loss is 0.07036644220352173\n",
      "epoch: 28 step: 544, loss is 0.013295354321599007\n",
      "epoch: 28 step: 545, loss is 0.05471863970160484\n",
      "epoch: 28 step: 546, loss is 0.003374208929017186\n",
      "epoch: 28 step: 547, loss is 3.082737748627551e-05\n",
      "epoch: 28 step: 548, loss is 5.729939221055247e-05\n",
      "epoch: 28 step: 549, loss is 0.0008507840684615076\n",
      "epoch: 28 step: 550, loss is 0.039520200341939926\n",
      "epoch: 28 step: 551, loss is 0.020847758278250694\n",
      "epoch: 28 step: 552, loss is 0.00038871861761435866\n",
      "epoch: 28 step: 553, loss is 0.00406463211402297\n",
      "epoch: 28 step: 554, loss is 0.026652133092284203\n",
      "epoch: 28 step: 555, loss is 0.0015901799779385328\n",
      "epoch: 28 step: 556, loss is 0.01972087100148201\n",
      "epoch: 28 step: 557, loss is 0.0377533845603466\n",
      "epoch: 28 step: 558, loss is 0.001467349473387003\n",
      "epoch: 28 step: 559, loss is 0.0007298889104276896\n",
      "epoch: 28 step: 560, loss is 0.0007134084589779377\n",
      "epoch: 28 step: 561, loss is 0.001373106730170548\n",
      "epoch: 28 step: 562, loss is 0.025034552440047264\n",
      "epoch: 28 step: 563, loss is 0.0029071797616779804\n",
      "epoch: 28 step: 564, loss is 0.10031340271234512\n",
      "epoch: 28 step: 565, loss is 0.07231179624795914\n",
      "epoch: 28 step: 566, loss is 0.011012048460543156\n",
      "epoch: 28 step: 567, loss is 0.0004740393487736583\n",
      "epoch: 28 step: 568, loss is 0.000133843335788697\n",
      "epoch: 28 step: 569, loss is 0.01978948898613453\n",
      "epoch: 28 step: 570, loss is 8.150137000484392e-05\n",
      "epoch: 28 step: 571, loss is 0.0006427509360946715\n",
      "epoch: 28 step: 572, loss is 0.00939992256462574\n",
      "epoch: 28 step: 573, loss is 0.019473914057016373\n",
      "epoch: 28 step: 574, loss is 0.09467273205518723\n",
      "epoch: 28 step: 575, loss is 0.013323799706995487\n",
      "epoch: 28 step: 576, loss is 0.00315857888199389\n",
      "epoch: 28 step: 577, loss is 0.008126607164740562\n",
      "epoch: 28 step: 578, loss is 0.00031524093355983496\n",
      "epoch: 28 step: 579, loss is 0.0019032240379601717\n",
      "epoch: 28 step: 580, loss is 0.13975687325000763\n",
      "epoch: 28 step: 581, loss is 0.023807132616639137\n",
      "epoch: 28 step: 582, loss is 0.0008834135951474309\n",
      "epoch: 28 step: 583, loss is 0.020628303289413452\n",
      "epoch: 28 step: 584, loss is 0.027342047542333603\n",
      "epoch: 28 step: 585, loss is 0.0038098881486803293\n",
      "epoch: 28 step: 586, loss is 0.013159041292965412\n",
      "epoch: 28 step: 587, loss is 0.00015995948342606425\n",
      "epoch: 28 step: 588, loss is 0.01934494636952877\n",
      "epoch: 28 step: 589, loss is 0.0012087204959243536\n",
      "epoch: 28 step: 590, loss is 0.043830785900354385\n",
      "epoch: 28 step: 591, loss is 0.005823678802698851\n",
      "epoch: 28 step: 592, loss is 0.0007771752425469458\n",
      "epoch: 28 step: 593, loss is 0.11149400472640991\n",
      "epoch: 28 step: 594, loss is 0.0012876007240265608\n",
      "epoch: 28 step: 595, loss is 7.279033889062703e-05\n",
      "epoch: 28 step: 596, loss is 0.018479367718100548\n",
      "epoch: 28 step: 597, loss is 0.1580539494752884\n",
      "epoch: 28 step: 598, loss is 0.00015092198736965656\n",
      "epoch: 28 step: 599, loss is 0.007328959181904793\n",
      "epoch: 28 step: 600, loss is 3.1405211302626412e-06\n",
      "epoch: 28 step: 601, loss is 0.01481732726097107\n",
      "epoch: 28 step: 602, loss is 0.022511988878250122\n",
      "epoch: 28 step: 603, loss is 0.010945585556328297\n",
      "epoch: 28 step: 604, loss is 0.019251951947808266\n",
      "epoch: 28 step: 605, loss is 0.0034062450286000967\n",
      "epoch: 28 step: 606, loss is 0.00323632825165987\n",
      "epoch: 28 step: 607, loss is 0.036482516676187515\n",
      "epoch: 28 step: 608, loss is 0.00031405032495968044\n",
      "epoch: 28 step: 609, loss is 0.0005974685191176832\n",
      "epoch: 28 step: 610, loss is 0.006926456466317177\n",
      "epoch: 28 step: 611, loss is 0.09571485221385956\n",
      "epoch: 28 step: 612, loss is 0.05560019612312317\n",
      "epoch: 28 step: 613, loss is 0.02096744254231453\n",
      "epoch: 28 step: 614, loss is 0.0008098540129140019\n",
      "epoch: 28 step: 615, loss is 0.020248625427484512\n",
      "epoch: 28 step: 616, loss is 0.0035925167612731457\n",
      "epoch: 28 step: 617, loss is 0.00013595180644188076\n",
      "epoch: 28 step: 618, loss is 0.006674984470009804\n",
      "epoch: 28 step: 619, loss is 0.04540109634399414\n",
      "epoch: 28 step: 620, loss is 0.00012687448179349303\n",
      "epoch: 28 step: 621, loss is 0.048110321164131165\n",
      "epoch: 28 step: 622, loss is 0.0080686304718256\n",
      "epoch: 28 step: 623, loss is 0.0007288922206498682\n",
      "epoch: 28 step: 624, loss is 0.005687354132533073\n",
      "epoch: 28 step: 625, loss is 0.05859026312828064\n",
      "epoch: 28 step: 626, loss is 0.026600580662488937\n",
      "epoch: 28 step: 627, loss is 0.0019829794764518738\n",
      "epoch: 28 step: 628, loss is 0.0013191135367378592\n",
      "epoch: 28 step: 629, loss is 0.05060385912656784\n",
      "epoch: 28 step: 630, loss is 0.06105176731944084\n",
      "epoch: 28 step: 631, loss is 0.09087780863046646\n",
      "epoch: 28 step: 632, loss is 0.09703362733125687\n",
      "epoch: 28 step: 633, loss is 0.006288872100412846\n",
      "epoch: 28 step: 634, loss is 0.002855619415640831\n",
      "epoch: 28 step: 635, loss is 0.00242427084594965\n",
      "epoch: 28 step: 636, loss is 0.000255392660619691\n",
      "epoch: 28 step: 637, loss is 0.016526231542229652\n",
      "epoch: 28 step: 638, loss is 0.004160747863352299\n",
      "epoch: 28 step: 639, loss is 0.0028751795180141926\n",
      "epoch: 28 step: 640, loss is 0.15917444229125977\n",
      "epoch: 28 step: 641, loss is 0.0023389789275825024\n",
      "epoch: 28 step: 642, loss is 0.018984243273735046\n",
      "epoch: 28 step: 643, loss is 0.007657371461391449\n",
      "epoch: 28 step: 644, loss is 0.00016281344869639724\n",
      "epoch: 28 step: 645, loss is 4.567314681480639e-05\n",
      "epoch: 28 step: 646, loss is 0.00726122222840786\n",
      "epoch: 28 step: 647, loss is 0.10731035470962524\n",
      "epoch: 28 step: 648, loss is 0.16265536844730377\n",
      "epoch: 28 step: 649, loss is 0.05002927407622337\n",
      "epoch: 28 step: 650, loss is 0.03830615431070328\n",
      "epoch: 28 step: 651, loss is 0.02841085195541382\n",
      "epoch: 28 step: 652, loss is 0.20071542263031006\n",
      "epoch: 28 step: 653, loss is 0.015817422419786453\n",
      "epoch: 28 step: 654, loss is 0.035035815089941025\n",
      "epoch: 28 step: 655, loss is 0.01337932888418436\n",
      "epoch: 28 step: 656, loss is 0.12926509976387024\n",
      "epoch: 28 step: 657, loss is 0.015838783234357834\n",
      "epoch: 28 step: 658, loss is 0.010899994522333145\n",
      "epoch: 28 step: 659, loss is 0.0002305764937773347\n",
      "epoch: 28 step: 660, loss is 0.11150836199522018\n",
      "epoch: 28 step: 661, loss is 0.004797076340764761\n",
      "epoch: 28 step: 662, loss is 0.063463494181633\n",
      "epoch: 28 step: 663, loss is 0.12461091578006744\n",
      "epoch: 28 step: 664, loss is 0.010243802331387997\n",
      "epoch: 28 step: 665, loss is 0.01921417936682701\n",
      "epoch: 28 step: 666, loss is 0.009941188618540764\n",
      "epoch: 28 step: 667, loss is 0.020630264654755592\n",
      "epoch: 28 step: 668, loss is 0.018150676041841507\n",
      "epoch: 28 step: 669, loss is 0.00910965446382761\n",
      "epoch: 28 step: 670, loss is 0.0014761616475880146\n",
      "epoch: 28 step: 671, loss is 0.00013252698408905417\n",
      "epoch: 28 step: 672, loss is 0.10875698179006577\n",
      "epoch: 28 step: 673, loss is 0.001856005284935236\n",
      "epoch: 28 step: 674, loss is 0.0049284412525594234\n",
      "epoch: 28 step: 675, loss is 5.928675091126934e-05\n",
      "epoch: 28 step: 676, loss is 0.014849954284727573\n",
      "epoch: 28 step: 677, loss is 0.0023660166189074516\n",
      "epoch: 28 step: 678, loss is 0.0018307470018044114\n",
      "epoch: 28 step: 679, loss is 0.06903725117444992\n",
      "epoch: 28 step: 680, loss is 0.001799881225451827\n",
      "epoch: 28 step: 681, loss is 0.029728533700108528\n",
      "epoch: 28 step: 682, loss is 0.0036734584718942642\n",
      "epoch: 28 step: 683, loss is 0.031332649290561676\n",
      "epoch: 28 step: 684, loss is 0.00012962825712747872\n",
      "epoch: 28 step: 685, loss is 0.0015590653056278825\n",
      "epoch: 28 step: 686, loss is 0.0011035531060770154\n",
      "epoch: 28 step: 687, loss is 0.00036979035940021276\n",
      "epoch: 28 step: 688, loss is 2.9316259315237403e-05\n",
      "epoch: 28 step: 689, loss is 0.0011220787419006228\n",
      "epoch: 28 step: 690, loss is 0.0461244136095047\n",
      "epoch: 28 step: 691, loss is 0.00011149745841976255\n",
      "epoch: 28 step: 692, loss is 5.8321471442468464e-05\n",
      "epoch: 28 step: 693, loss is 0.004068424925208092\n",
      "epoch: 28 step: 694, loss is 0.0005503096617758274\n",
      "epoch: 28 step: 695, loss is 0.0037768930196762085\n",
      "epoch: 28 step: 696, loss is 0.0024193408899009228\n",
      "epoch: 28 step: 697, loss is 0.011385535821318626\n",
      "epoch: 28 step: 698, loss is 0.0023627786431461573\n",
      "epoch: 28 step: 699, loss is 0.01706932857632637\n",
      "epoch: 28 step: 700, loss is 0.001201306818984449\n",
      "epoch: 28 step: 701, loss is 0.024947751313447952\n",
      "epoch: 28 step: 702, loss is 0.07460585981607437\n",
      "epoch: 28 step: 703, loss is 0.0006580689223483205\n",
      "epoch: 28 step: 704, loss is 0.003308315761387348\n",
      "epoch: 28 step: 705, loss is 0.0012946940260007977\n",
      "epoch: 28 step: 706, loss is 0.010614088736474514\n",
      "epoch: 28 step: 707, loss is 0.038812343031167984\n",
      "epoch: 28 step: 708, loss is 0.07626207917928696\n",
      "epoch: 28 step: 709, loss is 0.15179800987243652\n",
      "epoch: 28 step: 710, loss is 0.00025016750441864133\n",
      "epoch: 28 step: 711, loss is 0.0037020777817815542\n",
      "epoch: 28 step: 712, loss is 3.063722760998644e-05\n",
      "epoch: 28 step: 713, loss is 0.0010983526008203626\n",
      "epoch: 28 step: 714, loss is 0.0026926377322524786\n",
      "epoch: 28 step: 715, loss is 0.0023814861197024584\n",
      "epoch: 28 step: 716, loss is 0.037868939340114594\n",
      "epoch: 28 step: 717, loss is 0.00456016743555665\n",
      "epoch: 28 step: 718, loss is 0.11829154193401337\n",
      "epoch: 28 step: 719, loss is 0.01963701844215393\n",
      "epoch: 28 step: 720, loss is 0.0001377396984025836\n",
      "epoch: 28 step: 721, loss is 0.028221499174833298\n",
      "epoch: 28 step: 722, loss is 0.0019415293354541063\n",
      "epoch: 28 step: 723, loss is 0.0007287815678864717\n",
      "epoch: 28 step: 724, loss is 0.002417124342173338\n",
      "epoch: 28 step: 725, loss is 0.01598331891000271\n",
      "epoch: 28 step: 726, loss is 0.0331977978348732\n",
      "epoch: 28 step: 727, loss is 0.0005894978530704975\n",
      "epoch: 28 step: 728, loss is 0.016702789813280106\n",
      "epoch: 28 step: 729, loss is 0.0025848860386759043\n",
      "epoch: 28 step: 730, loss is 0.00045332126319408417\n",
      "epoch: 28 step: 731, loss is 0.005622723139822483\n",
      "epoch: 28 step: 732, loss is 0.03759392723441124\n",
      "epoch: 28 step: 733, loss is 0.11233527213335037\n",
      "epoch: 28 step: 734, loss is 0.032377563416957855\n",
      "epoch: 28 step: 735, loss is 0.001439213752746582\n",
      "epoch: 28 step: 736, loss is 0.02612655609846115\n",
      "epoch: 28 step: 737, loss is 0.054883986711502075\n",
      "epoch: 28 step: 738, loss is 0.0005301889032125473\n",
      "epoch: 28 step: 739, loss is 0.0017815049504861236\n",
      "epoch: 28 step: 740, loss is 0.0004740680451504886\n",
      "epoch: 28 step: 741, loss is 0.013152668252587318\n",
      "epoch: 28 step: 742, loss is 0.015538130886852741\n",
      "epoch: 28 step: 743, loss is 0.040804602205753326\n",
      "epoch: 28 step: 744, loss is 0.006244327872991562\n",
      "epoch: 28 step: 745, loss is 0.017867309972643852\n",
      "epoch: 28 step: 746, loss is 0.009097160771489143\n",
      "epoch: 28 step: 747, loss is 0.01132990512996912\n",
      "epoch: 28 step: 748, loss is 0.005037415772676468\n",
      "epoch: 28 step: 749, loss is 0.008381953462958336\n",
      "epoch: 28 step: 750, loss is 0.0016608383739367127\n",
      "epoch: 28 step: 751, loss is 0.003736125538125634\n",
      "epoch: 28 step: 752, loss is 0.015642670914530754\n",
      "epoch: 28 step: 753, loss is 0.08464831113815308\n",
      "epoch: 28 step: 754, loss is 0.012663168832659721\n",
      "epoch: 28 step: 755, loss is 0.00015376346709672362\n",
      "epoch: 28 step: 756, loss is 0.1151922196149826\n",
      "epoch: 28 step: 757, loss is 0.011408249847590923\n",
      "epoch: 28 step: 758, loss is 0.003277451265603304\n",
      "epoch: 28 step: 759, loss is 0.005878380499780178\n",
      "epoch: 28 step: 760, loss is 0.03865644708275795\n",
      "epoch: 28 step: 761, loss is 5.996130857965909e-05\n",
      "epoch: 28 step: 762, loss is 0.007303175050765276\n",
      "epoch: 28 step: 763, loss is 0.0016008812235668302\n",
      "epoch: 28 step: 764, loss is 0.00019288703333586454\n",
      "epoch: 28 step: 765, loss is 0.04264795035123825\n",
      "epoch: 28 step: 766, loss is 0.029290741309523582\n",
      "epoch: 28 step: 767, loss is 0.029371177777647972\n",
      "epoch: 28 step: 768, loss is 0.01670900173485279\n",
      "epoch: 28 step: 769, loss is 0.005945812910795212\n",
      "epoch: 28 step: 770, loss is 0.004853782244026661\n",
      "epoch: 28 step: 771, loss is 0.03615136072039604\n",
      "epoch: 28 step: 772, loss is 0.017035674303770065\n",
      "epoch: 28 step: 773, loss is 0.019880764186382294\n",
      "epoch: 28 step: 774, loss is 0.004599601496011019\n",
      "epoch: 28 step: 775, loss is 0.017374688759446144\n",
      "epoch: 28 step: 776, loss is 0.07275441288948059\n",
      "epoch: 28 step: 777, loss is 0.005298835225403309\n",
      "epoch: 28 step: 778, loss is 0.0008302641217596829\n",
      "epoch: 28 step: 779, loss is 0.00011311477283015847\n",
      "epoch: 28 step: 780, loss is 0.003116491250693798\n",
      "epoch: 28 step: 781, loss is 0.0010094711324200034\n",
      "epoch: 28 step: 782, loss is 0.00034061403130181134\n",
      "epoch: 28 step: 783, loss is 0.06892989575862885\n",
      "epoch: 28 step: 784, loss is 0.0006387241883203387\n",
      "epoch: 28 step: 785, loss is 0.04236367717385292\n",
      "epoch: 28 step: 786, loss is 0.01248281542211771\n",
      "epoch: 28 step: 787, loss is 0.03613720089197159\n",
      "epoch: 28 step: 788, loss is 0.00953912828117609\n",
      "epoch: 28 step: 789, loss is 0.0015551127726212144\n",
      "epoch: 28 step: 790, loss is 0.08535566926002502\n",
      "epoch: 28 step: 791, loss is 0.00011934030044358224\n",
      "epoch: 28 step: 792, loss is 0.014662327244877815\n",
      "epoch: 28 step: 793, loss is 0.006620987318456173\n",
      "epoch: 28 step: 794, loss is 0.0036611026152968407\n",
      "epoch: 28 step: 795, loss is 0.0021438798867166042\n",
      "epoch: 28 step: 796, loss is 0.03707139566540718\n",
      "epoch: 28 step: 797, loss is 0.00035761611070483923\n",
      "epoch: 28 step: 798, loss is 0.00017702991317491978\n",
      "epoch: 28 step: 799, loss is 0.009850112721323967\n",
      "epoch: 28 step: 800, loss is 0.10236940532922745\n",
      "epoch: 28 step: 801, loss is 0.010414798744022846\n",
      "epoch: 28 step: 802, loss is 0.003970570862293243\n",
      "epoch: 28 step: 803, loss is 0.05407652258872986\n",
      "epoch: 28 step: 804, loss is 0.0013642908306792378\n",
      "epoch: 28 step: 805, loss is 0.012010343372821808\n",
      "epoch: 28 step: 806, loss is 0.04625154286623001\n",
      "epoch: 28 step: 807, loss is 0.006234592758119106\n",
      "epoch: 28 step: 808, loss is 0.024119634181261063\n",
      "epoch: 28 step: 809, loss is 0.0032658111304044724\n",
      "epoch: 28 step: 810, loss is 0.003965727053582668\n",
      "epoch: 28 step: 811, loss is 0.0032027731649577618\n",
      "epoch: 28 step: 812, loss is 0.011317894794046879\n",
      "epoch: 28 step: 813, loss is 0.005564484745264053\n",
      "epoch: 28 step: 814, loss is 0.005846953950822353\n",
      "epoch: 28 step: 815, loss is 0.022283002734184265\n",
      "epoch: 28 step: 816, loss is 0.0008364913519471884\n",
      "epoch: 28 step: 817, loss is 0.0003387394826859236\n",
      "epoch: 28 step: 818, loss is 0.0009442412992939353\n",
      "epoch: 28 step: 819, loss is 0.0017693054396659136\n",
      "epoch: 28 step: 820, loss is 0.0015732130268588662\n",
      "epoch: 28 step: 821, loss is 0.00596713786944747\n",
      "epoch: 28 step: 822, loss is 0.0033840083051472902\n",
      "epoch: 28 step: 823, loss is 0.0018955174600705504\n",
      "epoch: 28 step: 824, loss is 0.050787441432476044\n",
      "epoch: 28 step: 825, loss is 0.06891598552465439\n",
      "epoch: 28 step: 826, loss is 0.005632991436868906\n",
      "epoch: 28 step: 827, loss is 0.017759356647729874\n",
      "epoch: 28 step: 828, loss is 0.012730917893350124\n",
      "epoch: 28 step: 829, loss is 0.026379136368632317\n",
      "epoch: 28 step: 830, loss is 0.000354064570274204\n",
      "epoch: 28 step: 831, loss is 0.0026538576930761337\n",
      "epoch: 28 step: 832, loss is 0.02182052470743656\n",
      "epoch: 28 step: 833, loss is 0.015385333448648453\n",
      "epoch: 28 step: 834, loss is 0.00978274829685688\n",
      "epoch: 28 step: 835, loss is 0.0010594215709716082\n",
      "epoch: 28 step: 836, loss is 0.02938499115407467\n",
      "epoch: 28 step: 837, loss is 0.0002850299351848662\n",
      "epoch: 28 step: 838, loss is 0.0035879272036254406\n",
      "epoch: 28 step: 839, loss is 0.014708384871482849\n",
      "epoch: 28 step: 840, loss is 0.0006519139860756695\n",
      "epoch: 28 step: 841, loss is 0.10637985914945602\n",
      "epoch: 28 step: 842, loss is 0.005157896783202887\n",
      "epoch: 28 step: 843, loss is 0.0024581637699157\n",
      "epoch: 28 step: 844, loss is 0.005542356055229902\n",
      "epoch: 28 step: 845, loss is 0.0015109435189515352\n",
      "epoch: 28 step: 846, loss is 0.004996880888938904\n",
      "epoch: 28 step: 847, loss is 0.005948734935373068\n",
      "epoch: 28 step: 848, loss is 0.015431668609380722\n",
      "epoch: 28 step: 849, loss is 0.0008200726006180048\n",
      "epoch: 28 step: 850, loss is 0.000970078632235527\n",
      "epoch: 28 step: 851, loss is 0.04105705767869949\n",
      "epoch: 28 step: 852, loss is 0.004131835885345936\n",
      "epoch: 28 step: 853, loss is 0.0024869898334145546\n",
      "epoch: 28 step: 854, loss is 0.013555776327848434\n",
      "epoch: 28 step: 855, loss is 0.000323134649079293\n",
      "epoch: 28 step: 856, loss is 0.017291247844696045\n",
      "epoch: 28 step: 857, loss is 0.00547688128426671\n",
      "epoch: 28 step: 858, loss is 0.0010360138257965446\n",
      "epoch: 28 step: 859, loss is 0.0005715217557735741\n",
      "epoch: 28 step: 860, loss is 0.014852541498839855\n",
      "epoch: 28 step: 861, loss is 0.04480691999197006\n",
      "epoch: 28 step: 862, loss is 0.019697505980730057\n",
      "epoch: 28 step: 863, loss is 0.011507094837725163\n",
      "epoch: 28 step: 864, loss is 0.014078464359045029\n",
      "epoch: 28 step: 865, loss is 0.0027242908254265785\n",
      "epoch: 28 step: 866, loss is 0.009999850764870644\n",
      "epoch: 28 step: 867, loss is 0.00046826584730297327\n",
      "epoch: 28 step: 868, loss is 1.5164225260377862e-05\n",
      "epoch: 28 step: 869, loss is 0.09694076329469681\n",
      "epoch: 28 step: 870, loss is 0.0044727083295583725\n",
      "epoch: 28 step: 871, loss is 0.0014511537738144398\n",
      "epoch: 28 step: 872, loss is 0.001745014451444149\n",
      "epoch: 28 step: 873, loss is 0.00976678915321827\n",
      "epoch: 28 step: 874, loss is 0.0036198908928781748\n",
      "epoch: 28 step: 875, loss is 0.06348273903131485\n",
      "epoch: 28 step: 876, loss is 8.785700629232451e-05\n",
      "epoch: 28 step: 877, loss is 0.0007932978332974017\n",
      "epoch: 28 step: 878, loss is 0.007809052709490061\n",
      "epoch: 28 step: 879, loss is 0.050323858857154846\n",
      "epoch: 28 step: 880, loss is 0.0013795332051813602\n",
      "epoch: 28 step: 881, loss is 0.038851894438266754\n",
      "epoch: 28 step: 882, loss is 0.006017074920237064\n",
      "epoch: 28 step: 883, loss is 0.0003707467403728515\n",
      "epoch: 28 step: 884, loss is 0.05111091211438179\n",
      "epoch: 28 step: 885, loss is 0.012582528404891491\n",
      "epoch: 28 step: 886, loss is 0.0020315945148468018\n",
      "epoch: 28 step: 887, loss is 0.046936262398958206\n",
      "epoch: 28 step: 888, loss is 0.031515054404735565\n",
      "epoch: 28 step: 889, loss is 0.0001986211573239416\n",
      "epoch: 28 step: 890, loss is 7.227242167573422e-05\n",
      "epoch: 28 step: 891, loss is 0.001517206896096468\n",
      "epoch: 28 step: 892, loss is 0.0020115587394684553\n",
      "epoch: 28 step: 893, loss is 0.0019747863989323378\n",
      "epoch: 28 step: 894, loss is 0.006285178475081921\n",
      "epoch: 28 step: 895, loss is 0.1278909593820572\n",
      "epoch: 28 step: 896, loss is 8.205399353755638e-05\n",
      "epoch: 28 step: 897, loss is 0.018651388585567474\n",
      "epoch: 28 step: 898, loss is 0.002714905422180891\n",
      "epoch: 28 step: 899, loss is 0.03072553314268589\n",
      "epoch: 28 step: 900, loss is 0.005702198017388582\n",
      "epoch: 28 step: 901, loss is 0.007563690654933453\n",
      "epoch: 28 step: 902, loss is 0.005422615446150303\n",
      "epoch: 28 step: 903, loss is 0.017218133434653282\n",
      "epoch: 28 step: 904, loss is 0.0009780817199498415\n",
      "epoch: 28 step: 905, loss is 0.04485791549086571\n",
      "epoch: 28 step: 906, loss is 0.00014476620708592236\n",
      "epoch: 28 step: 907, loss is 0.00041075871558859944\n",
      "epoch: 28 step: 908, loss is 0.03795371577143669\n",
      "epoch: 28 step: 909, loss is 0.004320008680224419\n",
      "epoch: 28 step: 910, loss is 0.0020076045766472816\n",
      "epoch: 28 step: 911, loss is 0.0011455253697931767\n",
      "epoch: 28 step: 912, loss is 0.1259281188249588\n",
      "epoch: 28 step: 913, loss is 0.01211688295006752\n",
      "epoch: 28 step: 914, loss is 0.021816615015268326\n",
      "epoch: 28 step: 915, loss is 0.04230932891368866\n",
      "epoch: 28 step: 916, loss is 0.017120085656642914\n",
      "epoch: 28 step: 917, loss is 0.0001290000363951549\n",
      "epoch: 28 step: 918, loss is 5.548736953642219e-05\n",
      "epoch: 28 step: 919, loss is 0.12004730850458145\n",
      "epoch: 28 step: 920, loss is 0.02602338418364525\n",
      "epoch: 28 step: 921, loss is 0.0007760128937661648\n",
      "epoch: 28 step: 922, loss is 0.00020284776110202074\n",
      "epoch: 28 step: 923, loss is 0.0005849622539244592\n",
      "epoch: 28 step: 924, loss is 0.00020027921709697694\n",
      "epoch: 28 step: 925, loss is 0.07007018476724625\n",
      "epoch: 28 step: 926, loss is 0.10423054546117783\n",
      "epoch: 28 step: 927, loss is 0.025715406984090805\n",
      "epoch: 28 step: 928, loss is 0.016100307926535606\n",
      "epoch: 28 step: 929, loss is 0.02900726906955242\n",
      "epoch: 28 step: 930, loss is 0.0027963302563875914\n",
      "epoch: 28 step: 931, loss is 0.004166475031524897\n",
      "epoch: 28 step: 932, loss is 0.02098807506263256\n",
      "epoch: 28 step: 933, loss is 0.001097261207178235\n",
      "epoch: 28 step: 934, loss is 0.05903080850839615\n",
      "epoch: 28 step: 935, loss is 0.0387331061065197\n",
      "epoch: 28 step: 936, loss is 0.022469695657491684\n",
      "epoch: 28 step: 937, loss is 0.0015533139230683446\n",
      "epoch: 29 step: 1, loss is 0.0011367909610271454\n",
      "epoch: 29 step: 2, loss is 0.0009052826790139079\n",
      "epoch: 29 step: 3, loss is 0.00903732143342495\n",
      "epoch: 29 step: 4, loss is 0.0022717700339853764\n",
      "epoch: 29 step: 5, loss is 0.04761079326272011\n",
      "epoch: 29 step: 6, loss is 0.006682949606329203\n",
      "epoch: 29 step: 7, loss is 0.0002811090962495655\n",
      "epoch: 29 step: 8, loss is 0.00806063786149025\n",
      "epoch: 29 step: 9, loss is 0.0019117037300020456\n",
      "epoch: 29 step: 10, loss is 0.0785473957657814\n",
      "epoch: 29 step: 11, loss is 0.0008825248805806041\n",
      "epoch: 29 step: 12, loss is 0.0017349817790091038\n",
      "epoch: 29 step: 13, loss is 0.00946896243840456\n",
      "epoch: 29 step: 14, loss is 0.010223363526165485\n",
      "epoch: 29 step: 15, loss is 0.004915351513773203\n",
      "epoch: 29 step: 16, loss is 0.002081327373161912\n",
      "epoch: 29 step: 17, loss is 0.000278232095297426\n",
      "epoch: 29 step: 18, loss is 0.0001904004893731326\n",
      "epoch: 29 step: 19, loss is 0.004122751299291849\n",
      "epoch: 29 step: 20, loss is 0.00468086265027523\n",
      "epoch: 29 step: 21, loss is 8.302231435663998e-05\n",
      "epoch: 29 step: 22, loss is 0.0001796077995095402\n",
      "epoch: 29 step: 23, loss is 0.042761437594890594\n",
      "epoch: 29 step: 24, loss is 0.06311288475990295\n",
      "epoch: 29 step: 25, loss is 0.0013992841122671962\n",
      "epoch: 29 step: 26, loss is 0.0012892186641693115\n",
      "epoch: 29 step: 27, loss is 0.0018088873475790024\n",
      "epoch: 29 step: 28, loss is 0.004518340807408094\n",
      "epoch: 29 step: 29, loss is 3.5836703318636864e-05\n",
      "epoch: 29 step: 30, loss is 0.00023094411881174892\n",
      "epoch: 29 step: 31, loss is 0.0013669179752469063\n",
      "epoch: 29 step: 32, loss is 0.0022973460145294666\n",
      "epoch: 29 step: 33, loss is 0.0016418159939348698\n",
      "epoch: 29 step: 34, loss is 0.0013716677203774452\n",
      "epoch: 29 step: 35, loss is 0.0022722582798451185\n",
      "epoch: 29 step: 36, loss is 0.012542059645056725\n",
      "epoch: 29 step: 37, loss is 0.0003530226240400225\n",
      "epoch: 29 step: 38, loss is 0.050219763070344925\n",
      "epoch: 29 step: 39, loss is 0.00036267575342208147\n",
      "epoch: 29 step: 40, loss is 0.05076628923416138\n",
      "epoch: 29 step: 41, loss is 0.009258236736059189\n",
      "epoch: 29 step: 42, loss is 0.0008708325331099331\n",
      "epoch: 29 step: 43, loss is 0.013807295821607113\n",
      "epoch: 29 step: 44, loss is 0.0045700971968472\n",
      "epoch: 29 step: 45, loss is 0.0024755937047302723\n",
      "epoch: 29 step: 46, loss is 0.007543300744146109\n",
      "epoch: 29 step: 47, loss is 0.0031557728070765734\n",
      "epoch: 29 step: 48, loss is 0.0041940901428461075\n",
      "epoch: 29 step: 49, loss is 0.008667213842272758\n",
      "epoch: 29 step: 50, loss is 0.0017292465781792998\n",
      "epoch: 29 step: 51, loss is 0.002687909407541156\n",
      "epoch: 29 step: 52, loss is 9.829095506574959e-05\n",
      "epoch: 29 step: 53, loss is 0.002643490908667445\n",
      "epoch: 29 step: 54, loss is 0.00015050631191115826\n",
      "epoch: 29 step: 55, loss is 0.003077430184930563\n",
      "epoch: 29 step: 56, loss is 0.0014575092354789376\n",
      "epoch: 29 step: 57, loss is 0.019653717055916786\n",
      "epoch: 29 step: 58, loss is 0.0008321330533362925\n",
      "epoch: 29 step: 59, loss is 0.0005047200247645378\n",
      "epoch: 29 step: 60, loss is 0.004395290743559599\n",
      "epoch: 29 step: 61, loss is 0.0002538102271500975\n",
      "epoch: 29 step: 62, loss is 0.00013083477097097784\n",
      "epoch: 29 step: 63, loss is 0.010003184899687767\n",
      "epoch: 29 step: 64, loss is 0.01046475488692522\n",
      "epoch: 29 step: 65, loss is 0.0004626865265890956\n",
      "epoch: 29 step: 66, loss is 0.007290864363312721\n",
      "epoch: 29 step: 67, loss is 0.004125985782593489\n",
      "epoch: 29 step: 68, loss is 0.00037377537228167057\n",
      "epoch: 29 step: 69, loss is 0.002043380169197917\n",
      "epoch: 29 step: 70, loss is 0.0014318127650767565\n",
      "epoch: 29 step: 71, loss is 0.019120285287499428\n",
      "epoch: 29 step: 72, loss is 0.01121129933744669\n",
      "epoch: 29 step: 73, loss is 0.0005978047265671194\n",
      "epoch: 29 step: 74, loss is 0.005441425833851099\n",
      "epoch: 29 step: 75, loss is 0.001047748257406056\n",
      "epoch: 29 step: 76, loss is 0.00033029087353497744\n",
      "epoch: 29 step: 77, loss is 0.005648765247315168\n",
      "epoch: 29 step: 78, loss is 0.003932423423975706\n",
      "epoch: 29 step: 79, loss is 0.00018508661014493555\n",
      "epoch: 29 step: 80, loss is 0.017105713486671448\n",
      "epoch: 29 step: 81, loss is 0.013047362677752972\n",
      "epoch: 29 step: 82, loss is 0.00034915938158519566\n",
      "epoch: 29 step: 83, loss is 0.0011752850841730833\n",
      "epoch: 29 step: 84, loss is 0.00042643368942663074\n",
      "epoch: 29 step: 85, loss is 0.014014575630426407\n",
      "epoch: 29 step: 86, loss is 1.9461031115497462e-05\n",
      "epoch: 29 step: 87, loss is 0.00023401658108923584\n",
      "epoch: 29 step: 88, loss is 2.476001827744767e-05\n",
      "epoch: 29 step: 89, loss is 0.0007943740929476917\n",
      "epoch: 29 step: 90, loss is 0.00040424970211461186\n",
      "epoch: 29 step: 91, loss is 0.002254800172522664\n",
      "epoch: 29 step: 92, loss is 0.00044147015432827175\n",
      "epoch: 29 step: 93, loss is 5.011296525481157e-05\n",
      "epoch: 29 step: 94, loss is 0.00035324852797202766\n",
      "epoch: 29 step: 95, loss is 0.0001498164056101814\n",
      "epoch: 29 step: 96, loss is 0.002302225911989808\n",
      "epoch: 29 step: 97, loss is 0.0054812501184642315\n",
      "epoch: 29 step: 98, loss is 0.000704023172147572\n",
      "epoch: 29 step: 99, loss is 0.03323700651526451\n",
      "epoch: 29 step: 100, loss is 0.0005335360765457153\n",
      "epoch: 29 step: 101, loss is 0.0002299044281244278\n",
      "epoch: 29 step: 102, loss is 0.00019971601432189345\n",
      "epoch: 29 step: 103, loss is 0.012705398723483086\n",
      "epoch: 29 step: 104, loss is 0.011606122367084026\n",
      "epoch: 29 step: 105, loss is 0.00036763655953109264\n",
      "epoch: 29 step: 106, loss is 0.0007135036285035312\n",
      "epoch: 29 step: 107, loss is 0.003376750508323312\n",
      "epoch: 29 step: 108, loss is 0.028910620138049126\n",
      "epoch: 29 step: 109, loss is 0.002686368068680167\n",
      "epoch: 29 step: 110, loss is 0.011405464261770248\n",
      "epoch: 29 step: 111, loss is 0.07749385386705399\n",
      "epoch: 29 step: 112, loss is 0.0014914865605533123\n",
      "epoch: 29 step: 113, loss is 0.11590345203876495\n",
      "epoch: 29 step: 114, loss is 0.0050464351661503315\n",
      "epoch: 29 step: 115, loss is 0.030654555186629295\n",
      "epoch: 29 step: 116, loss is 0.01294450368732214\n",
      "epoch: 29 step: 117, loss is 4.362166509963572e-05\n",
      "epoch: 29 step: 118, loss is 9.048983338288963e-05\n",
      "epoch: 29 step: 119, loss is 0.0007021405617706478\n",
      "epoch: 29 step: 120, loss is 0.006647519767284393\n",
      "epoch: 29 step: 121, loss is 0.04934808984398842\n",
      "epoch: 29 step: 122, loss is 5.5557229643454775e-05\n",
      "epoch: 29 step: 123, loss is 0.0004963790997862816\n",
      "epoch: 29 step: 124, loss is 0.0008005242561921477\n",
      "epoch: 29 step: 125, loss is 0.0010804157936945558\n",
      "epoch: 29 step: 126, loss is 0.0007866412051953375\n",
      "epoch: 29 step: 127, loss is 0.0007781346794217825\n",
      "epoch: 29 step: 128, loss is 0.0006036676932126284\n",
      "epoch: 29 step: 129, loss is 0.0028541143983602524\n",
      "epoch: 29 step: 130, loss is 0.00011868948786286637\n",
      "epoch: 29 step: 131, loss is 0.00015920496662147343\n",
      "epoch: 29 step: 132, loss is 0.001399531029164791\n",
      "epoch: 29 step: 133, loss is 0.00239190342836082\n",
      "epoch: 29 step: 134, loss is 0.00016260733536910266\n",
      "epoch: 29 step: 135, loss is 0.002164860488846898\n",
      "epoch: 29 step: 136, loss is 0.0012631913414224982\n",
      "epoch: 29 step: 137, loss is 8.799841452855617e-05\n",
      "epoch: 29 step: 138, loss is 0.0017311740666627884\n",
      "epoch: 29 step: 139, loss is 0.0032959729433059692\n",
      "epoch: 29 step: 140, loss is 0.00950802955776453\n",
      "epoch: 29 step: 141, loss is 0.0007677252287976444\n",
      "epoch: 29 step: 142, loss is 0.0012416355311870575\n",
      "epoch: 29 step: 143, loss is 0.011479812674224377\n",
      "epoch: 29 step: 144, loss is 0.030606897547841072\n",
      "epoch: 29 step: 145, loss is 0.07967477291822433\n",
      "epoch: 29 step: 146, loss is 0.0005688946694135666\n",
      "epoch: 29 step: 147, loss is 0.0015872479416429996\n",
      "epoch: 29 step: 148, loss is 0.0011632876703515649\n",
      "epoch: 29 step: 149, loss is 0.0006036182749085128\n",
      "epoch: 29 step: 150, loss is 0.0028542238287627697\n",
      "epoch: 29 step: 151, loss is 0.0027447319589555264\n",
      "epoch: 29 step: 152, loss is 0.01331330370157957\n",
      "epoch: 29 step: 153, loss is 0.0014389512361958623\n",
      "epoch: 29 step: 154, loss is 0.0009835580131039023\n",
      "epoch: 29 step: 155, loss is 0.026010842993855476\n",
      "epoch: 29 step: 156, loss is 0.005026118829846382\n",
      "epoch: 29 step: 157, loss is 0.005458175670355558\n",
      "epoch: 29 step: 158, loss is 0.08379880338907242\n",
      "epoch: 29 step: 159, loss is 0.01802549883723259\n",
      "epoch: 29 step: 160, loss is 0.015241241082549095\n",
      "epoch: 29 step: 161, loss is 0.06032414361834526\n",
      "epoch: 29 step: 162, loss is 0.0001232605573022738\n",
      "epoch: 29 step: 163, loss is 0.07947013527154922\n",
      "epoch: 29 step: 164, loss is 0.0007259692647494376\n",
      "epoch: 29 step: 165, loss is 0.0027628846000880003\n",
      "epoch: 29 step: 166, loss is 0.0008849671576172113\n",
      "epoch: 29 step: 167, loss is 0.008170117624104023\n",
      "epoch: 29 step: 168, loss is 0.0006590440752916038\n",
      "epoch: 29 step: 169, loss is 0.00013356977433431894\n",
      "epoch: 29 step: 170, loss is 0.0009882901795208454\n",
      "epoch: 29 step: 171, loss is 0.032071035355329514\n",
      "epoch: 29 step: 172, loss is 0.018248409032821655\n",
      "epoch: 29 step: 173, loss is 0.007982785813510418\n",
      "epoch: 29 step: 174, loss is 0.009338214062154293\n",
      "epoch: 29 step: 175, loss is 0.00041950467857532203\n",
      "epoch: 29 step: 176, loss is 0.002003079280257225\n",
      "epoch: 29 step: 177, loss is 0.00331795122474432\n",
      "epoch: 29 step: 178, loss is 0.0008834745385684073\n",
      "epoch: 29 step: 179, loss is 0.02313043549656868\n",
      "epoch: 29 step: 180, loss is 0.00034121607313863933\n",
      "epoch: 29 step: 181, loss is 0.050485759973526\n",
      "epoch: 29 step: 182, loss is 0.01627759449183941\n",
      "epoch: 29 step: 183, loss is 0.09712570160627365\n",
      "epoch: 29 step: 184, loss is 0.0001500975340604782\n",
      "epoch: 29 step: 185, loss is 0.0019869061652570963\n",
      "epoch: 29 step: 186, loss is 0.0004917742917314172\n",
      "epoch: 29 step: 187, loss is 0.011816686950623989\n",
      "epoch: 29 step: 188, loss is 0.0009705147240310907\n",
      "epoch: 29 step: 189, loss is 0.0008605225593782961\n",
      "epoch: 29 step: 190, loss is 0.00017199981084559113\n",
      "epoch: 29 step: 191, loss is 0.1398770809173584\n",
      "epoch: 29 step: 192, loss is 0.012180819176137447\n",
      "epoch: 29 step: 193, loss is 0.0011893551563844085\n",
      "epoch: 29 step: 194, loss is 0.06635645776987076\n",
      "epoch: 29 step: 195, loss is 0.0009252553572878242\n",
      "epoch: 29 step: 196, loss is 0.06039862334728241\n",
      "epoch: 29 step: 197, loss is 0.00012049511860823259\n",
      "epoch: 29 step: 198, loss is 0.05215763300657272\n",
      "epoch: 29 step: 199, loss is 0.02092689462006092\n",
      "epoch: 29 step: 200, loss is 0.0016827014042064548\n",
      "epoch: 29 step: 201, loss is 0.00040124886436387897\n",
      "epoch: 29 step: 202, loss is 0.0002218037989223376\n",
      "epoch: 29 step: 203, loss is 0.041857365518808365\n",
      "epoch: 29 step: 204, loss is 0.0011112636420875788\n",
      "epoch: 29 step: 205, loss is 0.0007934351451694965\n",
      "epoch: 29 step: 206, loss is 0.0003879581345245242\n",
      "epoch: 29 step: 207, loss is 0.0020158630795776844\n",
      "epoch: 29 step: 208, loss is 0.025301501154899597\n",
      "epoch: 29 step: 209, loss is 0.027354368939995766\n",
      "epoch: 29 step: 210, loss is 0.00012000095011899248\n",
      "epoch: 29 step: 211, loss is 0.0024501392617821693\n",
      "epoch: 29 step: 212, loss is 0.003569867229089141\n",
      "epoch: 29 step: 213, loss is 0.038674261420965195\n",
      "epoch: 29 step: 214, loss is 0.0015561339678242803\n",
      "epoch: 29 step: 215, loss is 0.0032747972290962934\n",
      "epoch: 29 step: 216, loss is 0.0014460331294685602\n",
      "epoch: 29 step: 217, loss is 1.2554959539556876e-05\n",
      "epoch: 29 step: 218, loss is 0.0008840786176733673\n",
      "epoch: 29 step: 219, loss is 0.002696659415960312\n",
      "epoch: 29 step: 220, loss is 0.0007702842704020441\n",
      "epoch: 29 step: 221, loss is 0.00018113339319825172\n",
      "epoch: 29 step: 222, loss is 0.00017842759552877396\n",
      "epoch: 29 step: 223, loss is 0.01432823482900858\n",
      "epoch: 29 step: 224, loss is 0.0230112187564373\n",
      "epoch: 29 step: 225, loss is 0.0040421816520392895\n",
      "epoch: 29 step: 226, loss is 0.0003243382088840008\n",
      "epoch: 29 step: 227, loss is 0.057318903505802155\n",
      "epoch: 29 step: 228, loss is 0.01159529760479927\n",
      "epoch: 29 step: 229, loss is 0.0040481979958713055\n",
      "epoch: 29 step: 230, loss is 0.006177661474794149\n",
      "epoch: 29 step: 231, loss is 0.0011054700007662177\n",
      "epoch: 29 step: 232, loss is 0.024823438376188278\n",
      "epoch: 29 step: 233, loss is 0.013751727528870106\n",
      "epoch: 29 step: 234, loss is 8.205069752875715e-05\n",
      "epoch: 29 step: 235, loss is 0.0005702225025743246\n",
      "epoch: 29 step: 236, loss is 0.0048862360417842865\n",
      "epoch: 29 step: 237, loss is 0.06119835004210472\n",
      "epoch: 29 step: 238, loss is 0.00016373292601201683\n",
      "epoch: 29 step: 239, loss is 0.00031761874561198056\n",
      "epoch: 29 step: 240, loss is 0.0003383647126611322\n",
      "epoch: 29 step: 241, loss is 1.2012290426355321e-05\n",
      "epoch: 29 step: 242, loss is 0.00038543471600860357\n",
      "epoch: 29 step: 243, loss is 0.013936433009803295\n",
      "epoch: 29 step: 244, loss is 0.0006022022571414709\n",
      "epoch: 29 step: 245, loss is 6.882796151330695e-05\n",
      "epoch: 29 step: 246, loss is 0.0006284972769208252\n",
      "epoch: 29 step: 247, loss is 0.0017267403891310096\n",
      "epoch: 29 step: 248, loss is 0.0003180487547069788\n",
      "epoch: 29 step: 249, loss is 0.0022762382868677378\n",
      "epoch: 29 step: 250, loss is 0.04858886078000069\n",
      "epoch: 29 step: 251, loss is 0.008882712572813034\n",
      "epoch: 29 step: 252, loss is 0.010277797468006611\n",
      "epoch: 29 step: 253, loss is 0.0004348633228801191\n",
      "epoch: 29 step: 254, loss is 0.0046506221406161785\n",
      "epoch: 29 step: 255, loss is 4.8594127292744815e-05\n",
      "epoch: 29 step: 256, loss is 0.010435251519083977\n",
      "epoch: 29 step: 257, loss is 0.005843983497470617\n",
      "epoch: 29 step: 258, loss is 0.000260265136603266\n",
      "epoch: 29 step: 259, loss is 0.0031100157648324966\n",
      "epoch: 29 step: 260, loss is 0.00010862510680453852\n",
      "epoch: 29 step: 261, loss is 0.0014931467594578862\n",
      "epoch: 29 step: 262, loss is 0.0011291173286736012\n",
      "epoch: 29 step: 263, loss is 0.0005578254349529743\n",
      "epoch: 29 step: 264, loss is 0.00014469721645582467\n",
      "epoch: 29 step: 265, loss is 0.003616118337959051\n",
      "epoch: 29 step: 266, loss is 0.005127452313899994\n",
      "epoch: 29 step: 267, loss is 0.0005128355696797371\n",
      "epoch: 29 step: 268, loss is 0.03109743632376194\n",
      "epoch: 29 step: 269, loss is 0.004713921807706356\n",
      "epoch: 29 step: 270, loss is 0.0024612892884761095\n",
      "epoch: 29 step: 271, loss is 0.00013106079131830484\n",
      "epoch: 29 step: 272, loss is 0.00021295284386724234\n",
      "epoch: 29 step: 273, loss is 0.03991401940584183\n",
      "epoch: 29 step: 274, loss is 0.008552703075110912\n",
      "epoch: 29 step: 275, loss is 0.00010639036918291822\n",
      "epoch: 29 step: 276, loss is 7.43305790820159e-05\n",
      "epoch: 29 step: 277, loss is 0.002693530637770891\n",
      "epoch: 29 step: 278, loss is 1.6722675354685634e-05\n",
      "epoch: 29 step: 279, loss is 0.0011839332291856408\n",
      "epoch: 29 step: 280, loss is 0.0002698963799048215\n",
      "epoch: 29 step: 281, loss is 0.00017313574790023267\n",
      "epoch: 29 step: 282, loss is 0.02077591046690941\n",
      "epoch: 29 step: 283, loss is 0.008453510701656342\n",
      "epoch: 29 step: 284, loss is 0.00034962937934324145\n",
      "epoch: 29 step: 285, loss is 0.005356097128242254\n",
      "epoch: 29 step: 286, loss is 0.042282916605472565\n",
      "epoch: 29 step: 287, loss is 0.006383462343364954\n",
      "epoch: 29 step: 288, loss is 0.00040797219844534993\n",
      "epoch: 29 step: 289, loss is 0.037832438945770264\n",
      "epoch: 29 step: 290, loss is 0.0022815782576799393\n",
      "epoch: 29 step: 291, loss is 0.0028621735982596874\n",
      "epoch: 29 step: 292, loss is 0.000251048244535923\n",
      "epoch: 29 step: 293, loss is 0.06202233210206032\n",
      "epoch: 29 step: 294, loss is 0.015602249652147293\n",
      "epoch: 29 step: 295, loss is 0.008081933483481407\n",
      "epoch: 29 step: 296, loss is 0.02203737013041973\n",
      "epoch: 29 step: 297, loss is 0.0002077159588225186\n",
      "epoch: 29 step: 298, loss is 0.0009884381433948874\n",
      "epoch: 29 step: 299, loss is 0.0022861147299408913\n",
      "epoch: 29 step: 300, loss is 0.0008777404436841607\n",
      "epoch: 29 step: 301, loss is 0.0012256938498467207\n",
      "epoch: 29 step: 302, loss is 0.020353665575385094\n",
      "epoch: 29 step: 303, loss is 0.0003282902471255511\n",
      "epoch: 29 step: 304, loss is 0.022493084892630577\n",
      "epoch: 29 step: 305, loss is 0.0006045683403499424\n",
      "epoch: 29 step: 306, loss is 0.0016507366672158241\n",
      "epoch: 29 step: 307, loss is 0.0061262380331754684\n",
      "epoch: 29 step: 308, loss is 0.00014347879914566875\n",
      "epoch: 29 step: 309, loss is 0.00013361097080633044\n",
      "epoch: 29 step: 310, loss is 0.027871794998645782\n",
      "epoch: 29 step: 311, loss is 0.00383764854632318\n",
      "epoch: 29 step: 312, loss is 0.0053970664739608765\n",
      "epoch: 29 step: 313, loss is 0.0006529919919557869\n",
      "epoch: 29 step: 314, loss is 0.0018270820146426558\n",
      "epoch: 29 step: 315, loss is 0.004287490621209145\n",
      "epoch: 29 step: 316, loss is 0.0005296361050568521\n",
      "epoch: 29 step: 317, loss is 0.06777334213256836\n",
      "epoch: 29 step: 318, loss is 0.00838417373597622\n",
      "epoch: 29 step: 319, loss is 0.0002604848996270448\n",
      "epoch: 29 step: 320, loss is 0.001907747471705079\n",
      "epoch: 29 step: 321, loss is 0.011545735411345959\n",
      "epoch: 29 step: 322, loss is 0.007313793525099754\n",
      "epoch: 29 step: 323, loss is 0.002466911682859063\n",
      "epoch: 29 step: 324, loss is 0.0008574910461902618\n",
      "epoch: 29 step: 325, loss is 0.01017655711621046\n",
      "epoch: 29 step: 326, loss is 8.055235957726836e-05\n",
      "epoch: 29 step: 327, loss is 0.0006520379101857543\n",
      "epoch: 29 step: 328, loss is 0.053980425000190735\n",
      "epoch: 29 step: 329, loss is 0.0017070295289158821\n",
      "epoch: 29 step: 330, loss is 0.0007458646432496607\n",
      "epoch: 29 step: 331, loss is 0.0037586952093988657\n",
      "epoch: 29 step: 332, loss is 0.0002447047736495733\n",
      "epoch: 29 step: 333, loss is 0.003246460808441043\n",
      "epoch: 29 step: 334, loss is 0.00032455605105496943\n",
      "epoch: 29 step: 335, loss is 0.0007792449905537069\n",
      "epoch: 29 step: 336, loss is 0.0002634216216392815\n",
      "epoch: 29 step: 337, loss is 0.0022775130346417427\n",
      "epoch: 29 step: 338, loss is 0.01336307916790247\n",
      "epoch: 29 step: 339, loss is 0.00018113647820428014\n",
      "epoch: 29 step: 340, loss is 0.017626680433750153\n",
      "epoch: 29 step: 341, loss is 0.0002375453623244539\n",
      "epoch: 29 step: 342, loss is 2.173700340790674e-05\n",
      "epoch: 29 step: 343, loss is 0.0034300629049539566\n",
      "epoch: 29 step: 344, loss is 0.0025339447893202305\n",
      "epoch: 29 step: 345, loss is 0.0018988515948876739\n",
      "epoch: 29 step: 346, loss is 0.0005288226529955864\n",
      "epoch: 29 step: 347, loss is 2.8535716410260648e-05\n",
      "epoch: 29 step: 348, loss is 0.06379035860300064\n",
      "epoch: 29 step: 349, loss is 0.0030583441257476807\n",
      "epoch: 29 step: 350, loss is 0.0005994432140141726\n",
      "epoch: 29 step: 351, loss is 0.00045766643597744405\n",
      "epoch: 29 step: 352, loss is 0.002612275304272771\n",
      "epoch: 29 step: 353, loss is 0.001391841215081513\n",
      "epoch: 29 step: 354, loss is 0.009157664142549038\n",
      "epoch: 29 step: 355, loss is 0.006854154169559479\n",
      "epoch: 29 step: 356, loss is 0.0008746778476051986\n",
      "epoch: 29 step: 357, loss is 0.03493804484605789\n",
      "epoch: 29 step: 358, loss is 0.01531003974378109\n",
      "epoch: 29 step: 359, loss is 0.0016294375527650118\n",
      "epoch: 29 step: 360, loss is 1.3262011634651572e-05\n",
      "epoch: 29 step: 361, loss is 0.00048564490862190723\n",
      "epoch: 29 step: 362, loss is 0.0011672875843942165\n",
      "epoch: 29 step: 363, loss is 0.0074155316688120365\n",
      "epoch: 29 step: 364, loss is 0.0032527095172554255\n",
      "epoch: 29 step: 365, loss is 0.00024963036412373185\n",
      "epoch: 29 step: 366, loss is 0.00018711082520894706\n",
      "epoch: 29 step: 367, loss is 0.0010216939263045788\n",
      "epoch: 29 step: 368, loss is 0.0016278342809528112\n",
      "epoch: 29 step: 369, loss is 0.00014792423462495208\n",
      "epoch: 29 step: 370, loss is 0.0010722915176302195\n",
      "epoch: 29 step: 371, loss is 0.0002634432748891413\n",
      "epoch: 29 step: 372, loss is 0.0006791832856833935\n",
      "epoch: 29 step: 373, loss is 0.0066259573213756084\n",
      "epoch: 29 step: 374, loss is 0.001814889837987721\n",
      "epoch: 29 step: 375, loss is 0.0021284236572682858\n",
      "epoch: 29 step: 376, loss is 0.011778261512517929\n",
      "epoch: 29 step: 377, loss is 5.3463358199223876e-05\n",
      "epoch: 29 step: 378, loss is 0.0013228163588792086\n",
      "epoch: 29 step: 379, loss is 0.0004451879358384758\n",
      "epoch: 29 step: 380, loss is 0.033523958176374435\n",
      "epoch: 29 step: 381, loss is 4.4866545067634434e-05\n",
      "epoch: 29 step: 382, loss is 0.000164934026543051\n",
      "epoch: 29 step: 383, loss is 0.0020996427629143\n",
      "epoch: 29 step: 384, loss is 0.004968645051121712\n",
      "epoch: 29 step: 385, loss is 0.0003589643747545779\n",
      "epoch: 29 step: 386, loss is 0.010286245495080948\n",
      "epoch: 29 step: 387, loss is 5.546371539821848e-05\n",
      "epoch: 29 step: 388, loss is 1.1346957762725651e-05\n",
      "epoch: 29 step: 389, loss is 0.001453434699214995\n",
      "epoch: 29 step: 390, loss is 0.00033149708178825676\n",
      "epoch: 29 step: 391, loss is 0.03216524422168732\n",
      "epoch: 29 step: 392, loss is 0.029645003378391266\n",
      "epoch: 29 step: 393, loss is 0.001993902027606964\n",
      "epoch: 29 step: 394, loss is 0.0011323437793180346\n",
      "epoch: 29 step: 395, loss is 0.0068033249117434025\n",
      "epoch: 29 step: 396, loss is 0.00012656391481868923\n",
      "epoch: 29 step: 397, loss is 0.000575665442738682\n",
      "epoch: 29 step: 398, loss is 2.894805948017165e-05\n",
      "epoch: 29 step: 399, loss is 0.001848992076702416\n",
      "epoch: 29 step: 400, loss is 0.00034133688313886523\n",
      "epoch: 29 step: 401, loss is 8.602697198512033e-05\n",
      "epoch: 29 step: 402, loss is 0.002355586038902402\n",
      "epoch: 29 step: 403, loss is 0.00019042572239413857\n",
      "epoch: 29 step: 404, loss is 0.0003525352803990245\n",
      "epoch: 29 step: 405, loss is 0.012642662040889263\n",
      "epoch: 29 step: 406, loss is 0.0006449611391872168\n",
      "epoch: 29 step: 407, loss is 0.00037669992889277637\n",
      "epoch: 29 step: 408, loss is 0.005065586883574724\n",
      "epoch: 29 step: 409, loss is 0.0019871043041348457\n",
      "epoch: 29 step: 410, loss is 0.0001747199275996536\n",
      "epoch: 29 step: 411, loss is 0.00297566712833941\n",
      "epoch: 29 step: 412, loss is 0.0009983532363548875\n",
      "epoch: 29 step: 413, loss is 4.277037805877626e-05\n",
      "epoch: 29 step: 414, loss is 0.00015537157014477998\n",
      "epoch: 29 step: 415, loss is 0.0039985934272408485\n",
      "epoch: 29 step: 416, loss is 0.001841200515627861\n",
      "epoch: 29 step: 417, loss is 2.832385325746145e-05\n",
      "epoch: 29 step: 418, loss is 0.0032853842712938786\n",
      "epoch: 29 step: 419, loss is 0.00038882176158949733\n",
      "epoch: 29 step: 420, loss is 0.010207549668848515\n",
      "epoch: 29 step: 421, loss is 0.00018964997434522957\n",
      "epoch: 29 step: 422, loss is 0.00028384948382154107\n",
      "epoch: 29 step: 423, loss is 8.104834705591202e-05\n",
      "epoch: 29 step: 424, loss is 0.0002123401063727215\n",
      "epoch: 29 step: 425, loss is 0.0011878889054059982\n",
      "epoch: 29 step: 426, loss is 0.05924390256404877\n",
      "epoch: 29 step: 427, loss is 0.00019980697834398597\n",
      "epoch: 29 step: 428, loss is 7.024869410088286e-05\n",
      "epoch: 29 step: 429, loss is 0.000493158120661974\n",
      "epoch: 29 step: 430, loss is 0.0016993791796267033\n",
      "epoch: 29 step: 431, loss is 0.0002896393707487732\n",
      "epoch: 29 step: 432, loss is 0.004805905744433403\n",
      "epoch: 29 step: 433, loss is 0.007835068739950657\n",
      "epoch: 29 step: 434, loss is 0.006219608709216118\n",
      "epoch: 29 step: 435, loss is 0.0006771100452169776\n",
      "epoch: 29 step: 436, loss is 0.018503034487366676\n",
      "epoch: 29 step: 437, loss is 0.007403169292956591\n",
      "epoch: 29 step: 438, loss is 0.0008399885264225304\n",
      "epoch: 29 step: 439, loss is 0.0018947815988212824\n",
      "epoch: 29 step: 440, loss is 0.0035673934035003185\n",
      "epoch: 29 step: 441, loss is 7.692516373936087e-05\n",
      "epoch: 29 step: 442, loss is 0.003733416087925434\n",
      "epoch: 29 step: 443, loss is 0.0006182940560393035\n",
      "epoch: 29 step: 444, loss is 0.017773406580090523\n",
      "epoch: 29 step: 445, loss is 1.776815770426765e-05\n",
      "epoch: 29 step: 446, loss is 0.0010901326313614845\n",
      "epoch: 29 step: 447, loss is 0.003958391956984997\n",
      "epoch: 29 step: 448, loss is 0.0016550736036151648\n",
      "epoch: 29 step: 449, loss is 0.0021174727007746696\n",
      "epoch: 29 step: 450, loss is 0.0042845942080020905\n",
      "epoch: 29 step: 451, loss is 1.6384650734835304e-05\n",
      "epoch: 29 step: 452, loss is 2.2135312974569388e-05\n",
      "epoch: 29 step: 453, loss is 0.001557784853503108\n",
      "epoch: 29 step: 454, loss is 0.0019792066887021065\n",
      "epoch: 29 step: 455, loss is 0.0027630506083369255\n",
      "epoch: 29 step: 456, loss is 0.0037071919068694115\n",
      "epoch: 29 step: 457, loss is 0.001182022737339139\n",
      "epoch: 29 step: 458, loss is 0.0003269786247983575\n",
      "epoch: 29 step: 459, loss is 0.01579207554459572\n",
      "epoch: 29 step: 460, loss is 0.009968549013137817\n",
      "epoch: 29 step: 461, loss is 0.003073187079280615\n",
      "epoch: 29 step: 462, loss is 0.003036260372027755\n",
      "epoch: 29 step: 463, loss is 0.0008199703297577798\n",
      "epoch: 29 step: 464, loss is 0.00011250497482251376\n",
      "epoch: 29 step: 465, loss is 0.0020852694287896156\n",
      "epoch: 29 step: 466, loss is 2.2979729692451656e-05\n",
      "epoch: 29 step: 467, loss is 0.0034632831811904907\n",
      "epoch: 29 step: 468, loss is 0.001125985523685813\n",
      "epoch: 29 step: 469, loss is 0.0011402240488678217\n",
      "epoch: 29 step: 470, loss is 0.0002340618084417656\n",
      "epoch: 29 step: 471, loss is 0.018002692610025406\n",
      "epoch: 29 step: 472, loss is 0.0007882809150032699\n",
      "epoch: 29 step: 473, loss is 0.013089681975543499\n",
      "epoch: 29 step: 474, loss is 0.0065791490487754345\n",
      "epoch: 29 step: 475, loss is 7.621318945894018e-05\n",
      "epoch: 29 step: 476, loss is 0.3559912443161011\n",
      "epoch: 29 step: 477, loss is 0.0018669276032596827\n",
      "epoch: 29 step: 478, loss is 0.002139115473255515\n",
      "epoch: 29 step: 479, loss is 0.004487380851060152\n",
      "epoch: 29 step: 480, loss is 0.0009199110791087151\n",
      "epoch: 29 step: 481, loss is 0.0072889369912445545\n",
      "epoch: 29 step: 482, loss is 0.0008380814106203616\n",
      "epoch: 29 step: 483, loss is 0.0462271124124527\n",
      "epoch: 29 step: 484, loss is 0.0010925650130957365\n",
      "epoch: 29 step: 485, loss is 0.0008471561595797539\n",
      "epoch: 29 step: 486, loss is 0.036999013274908066\n",
      "epoch: 29 step: 487, loss is 0.00111907918471843\n",
      "epoch: 29 step: 488, loss is 0.017792366445064545\n",
      "epoch: 29 step: 489, loss is 0.0025244399439543486\n",
      "epoch: 29 step: 490, loss is 0.000998087809421122\n",
      "epoch: 29 step: 491, loss is 0.0013302898732945323\n",
      "epoch: 29 step: 492, loss is 0.01359063945710659\n",
      "epoch: 29 step: 493, loss is 0.008231986314058304\n",
      "epoch: 29 step: 494, loss is 0.015284077264368534\n",
      "epoch: 29 step: 495, loss is 0.01715243048965931\n",
      "epoch: 29 step: 496, loss is 0.00212914333678782\n",
      "epoch: 29 step: 497, loss is 0.002687123604118824\n",
      "epoch: 29 step: 498, loss is 0.0013688827166333795\n",
      "epoch: 29 step: 499, loss is 0.0026607499457895756\n",
      "epoch: 29 step: 500, loss is 0.0022995523177087307\n",
      "epoch: 29 step: 501, loss is 0.010074206627905369\n",
      "epoch: 29 step: 502, loss is 0.0004756784765049815\n",
      "epoch: 29 step: 503, loss is 0.0029267682693898678\n",
      "epoch: 29 step: 504, loss is 0.007085030432790518\n",
      "epoch: 29 step: 505, loss is 0.0012603196082636714\n",
      "epoch: 29 step: 506, loss is 0.031042875722050667\n",
      "epoch: 29 step: 507, loss is 4.1983756091212854e-05\n",
      "epoch: 29 step: 508, loss is 0.004811034072190523\n",
      "epoch: 29 step: 509, loss is 0.007945558056235313\n",
      "epoch: 29 step: 510, loss is 0.0003256530035287142\n",
      "epoch: 29 step: 511, loss is 0.000798902299720794\n",
      "epoch: 29 step: 512, loss is 0.00026080076349899173\n",
      "epoch: 29 step: 513, loss is 0.00046239825314842165\n",
      "epoch: 29 step: 514, loss is 0.004376182332634926\n",
      "epoch: 29 step: 515, loss is 0.011737831868231297\n",
      "epoch: 29 step: 516, loss is 0.0006218406488187611\n",
      "epoch: 29 step: 517, loss is 0.005808476358652115\n",
      "epoch: 29 step: 518, loss is 0.01644524373114109\n",
      "epoch: 29 step: 519, loss is 0.00044213872752152383\n",
      "epoch: 29 step: 520, loss is 0.002539300359785557\n",
      "epoch: 29 step: 521, loss is 0.0047460272908210754\n",
      "epoch: 29 step: 522, loss is 0.0014263377524912357\n",
      "epoch: 29 step: 523, loss is 0.020501090213656425\n",
      "epoch: 29 step: 524, loss is 0.001379725057631731\n",
      "epoch: 29 step: 525, loss is 0.030173009261488914\n",
      "epoch: 29 step: 526, loss is 0.005577028729021549\n",
      "epoch: 29 step: 527, loss is 0.008504769764840603\n",
      "epoch: 29 step: 528, loss is 0.010604901239275932\n",
      "epoch: 29 step: 529, loss is 0.0026558968238532543\n",
      "epoch: 29 step: 530, loss is 0.01904878579080105\n",
      "epoch: 29 step: 531, loss is 0.008044023998081684\n",
      "epoch: 29 step: 532, loss is 0.0010400922037661076\n",
      "epoch: 29 step: 533, loss is 0.0016952730948105454\n",
      "epoch: 29 step: 534, loss is 0.004732088651508093\n",
      "epoch: 29 step: 535, loss is 0.0008986072498373687\n",
      "epoch: 29 step: 536, loss is 0.0007790630916133523\n",
      "epoch: 29 step: 537, loss is 0.0015955206472426653\n",
      "epoch: 29 step: 538, loss is 0.0001755621051415801\n",
      "epoch: 29 step: 539, loss is 0.00020652558305300772\n",
      "epoch: 29 step: 540, loss is 0.00012925828923471272\n",
      "epoch: 29 step: 541, loss is 0.015869515016674995\n",
      "epoch: 29 step: 542, loss is 0.000373089627828449\n",
      "epoch: 29 step: 543, loss is 0.001623908756300807\n",
      "epoch: 29 step: 544, loss is 0.05587727576494217\n",
      "epoch: 29 step: 545, loss is 0.00407029315829277\n",
      "epoch: 29 step: 546, loss is 0.001333787920884788\n",
      "epoch: 29 step: 547, loss is 8.453168447886128e-06\n",
      "epoch: 29 step: 548, loss is 1.774019983713515e-05\n",
      "epoch: 29 step: 549, loss is 0.004473377019166946\n",
      "epoch: 29 step: 550, loss is 0.0017984302248805761\n",
      "epoch: 29 step: 551, loss is 0.00024945230688899755\n",
      "epoch: 29 step: 552, loss is 0.02588716335594654\n",
      "epoch: 29 step: 553, loss is 0.01654493249952793\n",
      "epoch: 29 step: 554, loss is 0.0053085521794855595\n",
      "epoch: 29 step: 555, loss is 5.2921292081009597e-05\n",
      "epoch: 29 step: 556, loss is 0.006959977559745312\n",
      "epoch: 29 step: 557, loss is 0.0021684078965336084\n",
      "epoch: 29 step: 558, loss is 0.0026756112929433584\n",
      "epoch: 29 step: 559, loss is 0.0007374787237495184\n",
      "epoch: 29 step: 560, loss is 0.0033611031249165535\n",
      "epoch: 29 step: 561, loss is 0.0006277486099861562\n",
      "epoch: 29 step: 562, loss is 0.0005776017205789685\n",
      "epoch: 29 step: 563, loss is 0.0001585433928994462\n",
      "epoch: 29 step: 564, loss is 0.0005843642866238952\n",
      "epoch: 29 step: 565, loss is 0.0008555420208722353\n",
      "epoch: 29 step: 566, loss is 0.00023479977971874177\n",
      "epoch: 29 step: 567, loss is 0.00024706069962121546\n",
      "epoch: 29 step: 568, loss is 6.35581964161247e-05\n",
      "epoch: 29 step: 569, loss is 0.1964767724275589\n",
      "epoch: 29 step: 570, loss is 2.6394740416435525e-05\n",
      "epoch: 29 step: 571, loss is 0.013618539087474346\n",
      "epoch: 29 step: 572, loss is 0.00029733413248322904\n",
      "epoch: 29 step: 573, loss is 0.03290597349405289\n",
      "epoch: 29 step: 574, loss is 0.012803175486624241\n",
      "epoch: 29 step: 575, loss is 0.009688116610050201\n",
      "epoch: 29 step: 576, loss is 0.00034836778650060296\n",
      "epoch: 29 step: 577, loss is 0.0003314454515930265\n",
      "epoch: 29 step: 578, loss is 0.0001353098195977509\n",
      "epoch: 29 step: 579, loss is 0.002277348656207323\n",
      "epoch: 29 step: 580, loss is 0.00015498556604143232\n",
      "epoch: 29 step: 581, loss is 0.00030461829737760127\n",
      "epoch: 29 step: 582, loss is 0.011268996633589268\n",
      "epoch: 29 step: 583, loss is 0.0352579802274704\n",
      "epoch: 29 step: 584, loss is 0.002286548726260662\n",
      "epoch: 29 step: 585, loss is 0.00020174977544229478\n",
      "epoch: 29 step: 586, loss is 0.0059243179857730865\n",
      "epoch: 29 step: 587, loss is 0.0007823669584468007\n",
      "epoch: 29 step: 588, loss is 0.0036303179804235697\n",
      "epoch: 29 step: 589, loss is 0.005571981426328421\n",
      "epoch: 29 step: 590, loss is 0.0017208271892741323\n",
      "epoch: 29 step: 591, loss is 0.004883440677076578\n",
      "epoch: 29 step: 592, loss is 0.011086066253483295\n",
      "epoch: 29 step: 593, loss is 0.0022301177959889174\n",
      "epoch: 29 step: 594, loss is 0.000660232559312135\n",
      "epoch: 29 step: 595, loss is 0.0877501517534256\n",
      "epoch: 29 step: 596, loss is 0.0002984840830322355\n",
      "epoch: 29 step: 597, loss is 0.0006603792426176369\n",
      "epoch: 29 step: 598, loss is 0.0003978721215389669\n",
      "epoch: 29 step: 599, loss is 0.0005707103409804404\n",
      "epoch: 29 step: 600, loss is 0.00016948566189967096\n",
      "epoch: 29 step: 601, loss is 0.000294393248623237\n",
      "epoch: 29 step: 602, loss is 0.002568660769611597\n",
      "epoch: 29 step: 603, loss is 0.0003887821512762457\n",
      "epoch: 29 step: 604, loss is 0.00016625937132630497\n",
      "epoch: 29 step: 605, loss is 0.0272586178034544\n",
      "epoch: 29 step: 606, loss is 0.018766626715660095\n",
      "epoch: 29 step: 607, loss is 0.002381083322688937\n",
      "epoch: 29 step: 608, loss is 0.014652815647423267\n",
      "epoch: 29 step: 609, loss is 0.08879023045301437\n",
      "epoch: 29 step: 610, loss is 0.003651004284620285\n",
      "epoch: 29 step: 611, loss is 0.0341113805770874\n",
      "epoch: 29 step: 612, loss is 0.006350821815431118\n",
      "epoch: 29 step: 613, loss is 0.03676368668675423\n",
      "epoch: 29 step: 614, loss is 0.0006944593624211848\n",
      "epoch: 29 step: 615, loss is 0.017283713445067406\n",
      "epoch: 29 step: 616, loss is 7.22647673683241e-05\n",
      "epoch: 29 step: 617, loss is 0.00014131722855381668\n",
      "epoch: 29 step: 618, loss is 0.0009108168305829167\n",
      "epoch: 29 step: 619, loss is 0.010636369697749615\n",
      "epoch: 29 step: 620, loss is 0.0036005405709147453\n",
      "epoch: 29 step: 621, loss is 0.0002670353860594332\n",
      "epoch: 29 step: 622, loss is 0.0009237710619345307\n",
      "epoch: 29 step: 623, loss is 0.0015053765382617712\n",
      "epoch: 29 step: 624, loss is 0.0009448499185964465\n",
      "epoch: 29 step: 625, loss is 0.016760094091296196\n",
      "epoch: 29 step: 626, loss is 0.0014975415542721748\n",
      "epoch: 29 step: 627, loss is 0.004382171668112278\n",
      "epoch: 29 step: 628, loss is 0.04911930114030838\n",
      "epoch: 29 step: 629, loss is 0.05041965842247009\n",
      "epoch: 29 step: 630, loss is 0.0031808982603251934\n",
      "epoch: 29 step: 631, loss is 0.004602182190865278\n",
      "epoch: 29 step: 632, loss is 0.010843095369637012\n",
      "epoch: 29 step: 633, loss is 0.005411974154412746\n",
      "epoch: 29 step: 634, loss is 0.014346730895340443\n",
      "epoch: 29 step: 635, loss is 0.014673953875899315\n",
      "epoch: 29 step: 636, loss is 0.004384550731629133\n",
      "epoch: 29 step: 637, loss is 5.9588746808003634e-05\n",
      "epoch: 29 step: 638, loss is 0.0038639374542981386\n",
      "epoch: 29 step: 639, loss is 0.0002486667945049703\n",
      "epoch: 29 step: 640, loss is 3.3216027077287436e-05\n",
      "epoch: 29 step: 641, loss is 0.027472207322716713\n",
      "epoch: 29 step: 642, loss is 0.00013550119183491915\n",
      "epoch: 29 step: 643, loss is 9.979707829188555e-05\n",
      "epoch: 29 step: 644, loss is 0.001287068473175168\n",
      "epoch: 29 step: 645, loss is 5.0576341891428456e-05\n",
      "epoch: 29 step: 646, loss is 0.03116779215633869\n",
      "epoch: 29 step: 647, loss is 0.007622212637215853\n",
      "epoch: 29 step: 648, loss is 0.00011622283636825159\n",
      "epoch: 29 step: 649, loss is 0.0035108698066323996\n",
      "epoch: 29 step: 650, loss is 0.00679794792085886\n",
      "epoch: 29 step: 651, loss is 0.0028627871070057154\n",
      "epoch: 29 step: 652, loss is 0.02921908162534237\n",
      "epoch: 29 step: 653, loss is 0.002788342535495758\n",
      "epoch: 29 step: 654, loss is 0.01146218553185463\n",
      "epoch: 29 step: 655, loss is 0.0006356308003887534\n",
      "epoch: 29 step: 656, loss is 2.181359741371125e-05\n",
      "epoch: 29 step: 657, loss is 0.001580834505148232\n",
      "epoch: 29 step: 658, loss is 0.0018643740331754088\n",
      "epoch: 29 step: 659, loss is 0.0023080070968717337\n",
      "epoch: 29 step: 660, loss is 0.015333609655499458\n",
      "epoch: 29 step: 661, loss is 0.020436950027942657\n",
      "epoch: 29 step: 662, loss is 0.06108158454298973\n",
      "epoch: 29 step: 663, loss is 0.11393452435731888\n",
      "epoch: 29 step: 664, loss is 0.004192267544567585\n",
      "epoch: 29 step: 665, loss is 0.0008619611035101116\n",
      "epoch: 29 step: 666, loss is 0.0007970837759785354\n",
      "epoch: 29 step: 667, loss is 0.013322841376066208\n",
      "epoch: 29 step: 668, loss is 0.0004613279888872057\n",
      "epoch: 29 step: 669, loss is 0.005734140053391457\n",
      "epoch: 29 step: 670, loss is 0.013072805479168892\n",
      "epoch: 29 step: 671, loss is 0.0022463377099484205\n",
      "epoch: 29 step: 672, loss is 0.001651855418458581\n",
      "epoch: 29 step: 673, loss is 0.0021537230350077152\n",
      "epoch: 29 step: 674, loss is 0.067502960562706\n",
      "epoch: 29 step: 675, loss is 0.00258180545642972\n",
      "epoch: 29 step: 676, loss is 0.006282815709710121\n",
      "epoch: 29 step: 677, loss is 0.0003999832842964679\n",
      "epoch: 29 step: 678, loss is 0.09463398903608322\n",
      "epoch: 29 step: 679, loss is 0.0007216665544547141\n",
      "epoch: 29 step: 680, loss is 0.057178106158971786\n",
      "epoch: 29 step: 681, loss is 0.010417220182716846\n",
      "epoch: 29 step: 682, loss is 0.0027909199707210064\n",
      "epoch: 29 step: 683, loss is 0.00011707918019965291\n",
      "epoch: 29 step: 684, loss is 0.07269040495157242\n",
      "epoch: 29 step: 685, loss is 0.0013637817464768887\n",
      "epoch: 29 step: 686, loss is 0.0028415494598448277\n",
      "epoch: 29 step: 687, loss is 0.011169329285621643\n",
      "epoch: 29 step: 688, loss is 0.0004043052904307842\n",
      "epoch: 29 step: 689, loss is 9.41525140660815e-05\n",
      "epoch: 29 step: 690, loss is 0.0003743518318515271\n",
      "epoch: 29 step: 691, loss is 0.0012230458669364452\n",
      "epoch: 29 step: 692, loss is 0.00027700330247171223\n",
      "epoch: 29 step: 693, loss is 0.0004262504226062447\n",
      "epoch: 29 step: 694, loss is 0.0023280258756130934\n",
      "epoch: 29 step: 695, loss is 0.0015218723565340042\n",
      "epoch: 29 step: 696, loss is 0.022003090009093285\n",
      "epoch: 29 step: 697, loss is 0.00039127698983065784\n",
      "epoch: 29 step: 698, loss is 0.01204683817923069\n",
      "epoch: 29 step: 699, loss is 0.0019254798535257578\n",
      "epoch: 29 step: 700, loss is 0.005045517347753048\n",
      "epoch: 29 step: 701, loss is 0.0002100669516948983\n",
      "epoch: 29 step: 702, loss is 0.006101958453655243\n",
      "epoch: 29 step: 703, loss is 0.0002513796789571643\n",
      "epoch: 29 step: 704, loss is 0.0018884303281083703\n",
      "epoch: 29 step: 705, loss is 0.11177461594343185\n",
      "epoch: 29 step: 706, loss is 0.00742801558226347\n",
      "epoch: 29 step: 707, loss is 0.0006127836531959474\n",
      "epoch: 29 step: 708, loss is 0.00027036172104999423\n",
      "epoch: 29 step: 709, loss is 0.00014970111078582704\n",
      "epoch: 29 step: 710, loss is 0.0049669817090034485\n",
      "epoch: 29 step: 711, loss is 0.006354647688567638\n",
      "epoch: 29 step: 712, loss is 0.0002672612899914384\n",
      "epoch: 29 step: 713, loss is 0.00042815072811208665\n",
      "epoch: 29 step: 714, loss is 0.0032115872018039227\n",
      "epoch: 29 step: 715, loss is 0.007826123386621475\n",
      "epoch: 29 step: 716, loss is 0.0010478482581675053\n",
      "epoch: 29 step: 717, loss is 0.0021053259260952473\n",
      "epoch: 29 step: 718, loss is 0.0007655437220819294\n",
      "epoch: 29 step: 719, loss is 0.003109897254034877\n",
      "epoch: 29 step: 720, loss is 0.0004128843720536679\n",
      "epoch: 29 step: 721, loss is 0.001137858722358942\n",
      "epoch: 29 step: 722, loss is 0.012749400921165943\n",
      "epoch: 29 step: 723, loss is 0.013880359940230846\n",
      "epoch: 29 step: 724, loss is 0.003212306648492813\n",
      "epoch: 29 step: 725, loss is 0.0014192049857228994\n",
      "epoch: 29 step: 726, loss is 0.006345501635223627\n",
      "epoch: 29 step: 727, loss is 0.015322817489504814\n",
      "epoch: 29 step: 728, loss is 0.0005828613066114485\n",
      "epoch: 29 step: 729, loss is 0.03299212083220482\n",
      "epoch: 29 step: 730, loss is 0.0005960920243524015\n",
      "epoch: 29 step: 731, loss is 0.00020183056767564267\n",
      "epoch: 29 step: 732, loss is 0.0009397273533977568\n",
      "epoch: 29 step: 733, loss is 0.0013320364523679018\n",
      "epoch: 29 step: 734, loss is 0.00043724168790504336\n",
      "epoch: 29 step: 735, loss is 0.005874481052160263\n",
      "epoch: 29 step: 736, loss is 5.080609844299033e-05\n",
      "epoch: 29 step: 737, loss is 0.0002278200554428622\n",
      "epoch: 29 step: 738, loss is 0.014452911913394928\n",
      "epoch: 29 step: 739, loss is 0.01946425251662731\n",
      "epoch: 29 step: 740, loss is 0.0006145441439002752\n",
      "epoch: 29 step: 741, loss is 0.00029823678778484464\n",
      "epoch: 29 step: 742, loss is 0.03414272889494896\n",
      "epoch: 29 step: 743, loss is 0.0048092943616211414\n",
      "epoch: 29 step: 744, loss is 0.0013070069253444672\n",
      "epoch: 29 step: 745, loss is 0.0031811220105737448\n",
      "epoch: 29 step: 746, loss is 0.003111228346824646\n",
      "epoch: 29 step: 747, loss is 0.0001700347347650677\n",
      "epoch: 29 step: 748, loss is 0.023664578795433044\n",
      "epoch: 29 step: 749, loss is 0.0027547101490199566\n",
      "epoch: 29 step: 750, loss is 0.0010136528871953487\n",
      "epoch: 29 step: 751, loss is 0.0045494395308196545\n",
      "epoch: 29 step: 752, loss is 0.00021047299378551543\n",
      "epoch: 29 step: 753, loss is 0.003378440858796239\n",
      "epoch: 29 step: 754, loss is 0.00045727717224508524\n",
      "epoch: 29 step: 755, loss is 0.0018083667382597923\n",
      "epoch: 29 step: 756, loss is 0.01200944371521473\n",
      "epoch: 29 step: 757, loss is 0.0157819502055645\n",
      "epoch: 29 step: 758, loss is 0.13288652896881104\n",
      "epoch: 29 step: 759, loss is 0.007506929337978363\n",
      "epoch: 29 step: 760, loss is 0.06004123389720917\n",
      "epoch: 29 step: 761, loss is 0.00028081017080694437\n",
      "epoch: 29 step: 762, loss is 0.0012768905144184828\n",
      "epoch: 29 step: 763, loss is 0.002298564650118351\n",
      "epoch: 29 step: 764, loss is 0.01652623899281025\n",
      "epoch: 29 step: 765, loss is 0.0005411583697423339\n",
      "epoch: 29 step: 766, loss is 0.00931825116276741\n",
      "epoch: 29 step: 767, loss is 0.0034659949596971273\n",
      "epoch: 29 step: 768, loss is 0.011446904391050339\n",
      "epoch: 29 step: 769, loss is 0.0006793689681217074\n",
      "epoch: 29 step: 770, loss is 0.0013216269435361028\n",
      "epoch: 29 step: 771, loss is 0.021288644522428513\n",
      "epoch: 29 step: 772, loss is 0.0026936803478747606\n",
      "epoch: 29 step: 773, loss is 0.004192163702100515\n",
      "epoch: 29 step: 774, loss is 0.0010008066892623901\n",
      "epoch: 29 step: 775, loss is 0.00010575375199550763\n",
      "epoch: 29 step: 776, loss is 0.000367588218068704\n",
      "epoch: 29 step: 777, loss is 0.0007960616494528949\n",
      "epoch: 29 step: 778, loss is 0.0006805885932408273\n",
      "epoch: 29 step: 779, loss is 0.1282103806734085\n",
      "epoch: 29 step: 780, loss is 4.7219749831128865e-05\n",
      "epoch: 29 step: 781, loss is 0.00018016838293988258\n",
      "epoch: 29 step: 782, loss is 0.0005034675705246627\n",
      "epoch: 29 step: 783, loss is 0.00700648408383131\n",
      "epoch: 29 step: 784, loss is 0.008237915113568306\n",
      "epoch: 29 step: 785, loss is 0.007273449562489986\n",
      "epoch: 29 step: 786, loss is 7.340281445067376e-05\n",
      "epoch: 29 step: 787, loss is 0.00016684092406649143\n",
      "epoch: 29 step: 788, loss is 0.004475101828575134\n",
      "epoch: 29 step: 789, loss is 0.00074627751018852\n",
      "epoch: 29 step: 790, loss is 0.0005281470366753638\n",
      "epoch: 29 step: 791, loss is 0.0185603778809309\n",
      "epoch: 29 step: 792, loss is 0.007339851465076208\n",
      "epoch: 29 step: 793, loss is 0.018495948985219002\n",
      "epoch: 29 step: 794, loss is 0.04472305253148079\n",
      "epoch: 29 step: 795, loss is 0.00011046949657611549\n",
      "epoch: 29 step: 796, loss is 0.0040815710090100765\n",
      "epoch: 29 step: 797, loss is 8.790300489636138e-05\n",
      "epoch: 29 step: 798, loss is 0.013835465535521507\n",
      "epoch: 29 step: 799, loss is 0.008056013844907284\n",
      "epoch: 29 step: 800, loss is 0.005126310512423515\n",
      "epoch: 29 step: 801, loss is 0.02757871150970459\n",
      "epoch: 29 step: 802, loss is 0.024458937346935272\n",
      "epoch: 29 step: 803, loss is 0.007886962965130806\n",
      "epoch: 29 step: 804, loss is 0.01205059327185154\n",
      "epoch: 29 step: 805, loss is 0.00010579760419204831\n",
      "epoch: 29 step: 806, loss is 0.0011836495250463486\n",
      "epoch: 29 step: 807, loss is 0.001995601924136281\n",
      "epoch: 29 step: 808, loss is 0.007450192701071501\n",
      "epoch: 29 step: 809, loss is 0.003457962768152356\n",
      "epoch: 29 step: 810, loss is 0.000723778095562011\n",
      "epoch: 29 step: 811, loss is 0.006927778013050556\n",
      "epoch: 29 step: 812, loss is 0.00115727458614856\n",
      "epoch: 29 step: 813, loss is 0.008766425773501396\n",
      "epoch: 29 step: 814, loss is 0.03870880976319313\n",
      "epoch: 29 step: 815, loss is 0.0005379599169827998\n",
      "epoch: 29 step: 816, loss is 0.002173851942643523\n",
      "epoch: 29 step: 817, loss is 0.008693341165781021\n",
      "epoch: 29 step: 818, loss is 1.6005806173780002e-05\n",
      "epoch: 29 step: 819, loss is 0.00014261445903684944\n",
      "epoch: 29 step: 820, loss is 8.587392949266359e-05\n",
      "epoch: 29 step: 821, loss is 0.0023564475122839212\n",
      "epoch: 29 step: 822, loss is 0.0016258128453046083\n",
      "epoch: 29 step: 823, loss is 0.0007076428737491369\n",
      "epoch: 29 step: 824, loss is 0.002323616063222289\n",
      "epoch: 29 step: 825, loss is 0.0003483258478809148\n",
      "epoch: 29 step: 826, loss is 0.0012022836599498987\n",
      "epoch: 29 step: 827, loss is 0.009046105667948723\n",
      "epoch: 29 step: 828, loss is 0.0001504155807197094\n",
      "epoch: 29 step: 829, loss is 0.0029672111850231886\n",
      "epoch: 29 step: 830, loss is 0.0016320619033649564\n",
      "epoch: 29 step: 831, loss is 0.007827693596482277\n",
      "epoch: 29 step: 832, loss is 0.0006521775503642857\n",
      "epoch: 29 step: 833, loss is 0.0005931296036578715\n",
      "epoch: 29 step: 834, loss is 0.0001899538328871131\n",
      "epoch: 29 step: 835, loss is 0.00011473971244413406\n",
      "epoch: 29 step: 836, loss is 0.0008513074135407805\n",
      "epoch: 29 step: 837, loss is 0.008542162366211414\n",
      "epoch: 29 step: 838, loss is 0.000875383208040148\n",
      "epoch: 29 step: 839, loss is 0.006574725732207298\n",
      "epoch: 29 step: 840, loss is 0.02059370093047619\n",
      "epoch: 29 step: 841, loss is 0.02792491763830185\n",
      "epoch: 29 step: 842, loss is 2.640740422066301e-05\n",
      "epoch: 29 step: 843, loss is 0.04293234273791313\n",
      "epoch: 29 step: 844, loss is 0.0010078217601403594\n",
      "epoch: 29 step: 845, loss is 0.015236375853419304\n",
      "epoch: 29 step: 846, loss is 0.0007442511850968003\n",
      "epoch: 29 step: 847, loss is 0.03621998429298401\n",
      "epoch: 29 step: 848, loss is 3.1441486498806626e-05\n",
      "epoch: 29 step: 849, loss is 0.05545296519994736\n",
      "epoch: 29 step: 850, loss is 0.0008466772269457579\n",
      "epoch: 29 step: 851, loss is 0.014336650259792805\n",
      "epoch: 29 step: 852, loss is 0.0002789233694784343\n",
      "epoch: 29 step: 853, loss is 0.0026679085567593575\n",
      "epoch: 29 step: 854, loss is 0.008404404856264591\n",
      "epoch: 29 step: 855, loss is 0.010596007108688354\n",
      "epoch: 29 step: 856, loss is 0.003663781564682722\n",
      "epoch: 29 step: 857, loss is 0.0001612988271517679\n",
      "epoch: 29 step: 858, loss is 0.0748831033706665\n",
      "epoch: 29 step: 859, loss is 0.00032315810676664114\n",
      "epoch: 29 step: 860, loss is 0.03986493498086929\n",
      "epoch: 29 step: 861, loss is 0.0012817411916330457\n",
      "epoch: 29 step: 862, loss is 0.00010264159936923534\n",
      "epoch: 29 step: 863, loss is 9.853618394117802e-05\n",
      "epoch: 29 step: 864, loss is 0.004178924951702356\n",
      "epoch: 29 step: 865, loss is 0.005482387728989124\n",
      "epoch: 29 step: 866, loss is 0.00023940530081745237\n",
      "epoch: 29 step: 867, loss is 0.0014983287546783686\n",
      "epoch: 29 step: 868, loss is 0.0013010128168389201\n",
      "epoch: 29 step: 869, loss is 0.005218896549195051\n",
      "epoch: 29 step: 870, loss is 0.00627182750031352\n",
      "epoch: 29 step: 871, loss is 0.07317373901605606\n",
      "epoch: 29 step: 872, loss is 0.003585794707760215\n",
      "epoch: 29 step: 873, loss is 0.009475394152104855\n",
      "epoch: 29 step: 874, loss is 0.0004482452350202948\n",
      "epoch: 29 step: 875, loss is 0.0014009788865223527\n",
      "epoch: 29 step: 876, loss is 0.0006226633558981121\n",
      "epoch: 29 step: 877, loss is 0.003226259257644415\n",
      "epoch: 29 step: 878, loss is 0.00014321092749014497\n",
      "epoch: 29 step: 879, loss is 0.0014257058501243591\n",
      "epoch: 29 step: 880, loss is 0.0018094858387485147\n",
      "epoch: 29 step: 881, loss is 3.426133480388671e-05\n",
      "epoch: 29 step: 882, loss is 0.01776876673102379\n",
      "epoch: 29 step: 883, loss is 0.0020582086872309446\n",
      "epoch: 29 step: 884, loss is 0.004717481322586536\n",
      "epoch: 29 step: 885, loss is 0.0031373482197523117\n",
      "epoch: 29 step: 886, loss is 0.0027138523291796446\n",
      "epoch: 29 step: 887, loss is 0.007453826256096363\n",
      "epoch: 29 step: 888, loss is 0.0016703297151252627\n",
      "epoch: 29 step: 889, loss is 0.01448109745979309\n",
      "epoch: 29 step: 890, loss is 0.006447006482630968\n",
      "epoch: 29 step: 891, loss is 0.0017317446181550622\n",
      "epoch: 29 step: 892, loss is 0.0011165678733959794\n",
      "epoch: 29 step: 893, loss is 0.0122956782579422\n",
      "epoch: 29 step: 894, loss is 0.00015681186050642282\n",
      "epoch: 29 step: 895, loss is 0.012355415150523186\n",
      "epoch: 29 step: 896, loss is 6.065818524803035e-05\n",
      "epoch: 29 step: 897, loss is 0.028898298740386963\n",
      "epoch: 29 step: 898, loss is 0.0025412687100470066\n",
      "epoch: 29 step: 899, loss is 0.000266850198386237\n",
      "epoch: 29 step: 900, loss is 0.0008261373732239008\n",
      "epoch: 29 step: 901, loss is 0.0005183707689866424\n",
      "epoch: 29 step: 902, loss is 0.002147773979231715\n",
      "epoch: 29 step: 903, loss is 0.0009383524302393198\n",
      "epoch: 29 step: 904, loss is 0.04436736926436424\n",
      "epoch: 29 step: 905, loss is 0.02365938201546669\n",
      "epoch: 29 step: 906, loss is 4.5590888475999236e-05\n",
      "epoch: 29 step: 907, loss is 0.024989552795886993\n",
      "epoch: 29 step: 908, loss is 0.002064554952085018\n",
      "epoch: 29 step: 909, loss is 0.005653494969010353\n",
      "epoch: 29 step: 910, loss is 0.006008617579936981\n",
      "epoch: 29 step: 911, loss is 0.010610968805849552\n",
      "epoch: 29 step: 912, loss is 1.5109974810911808e-05\n",
      "epoch: 29 step: 913, loss is 0.0012742902617901564\n",
      "epoch: 29 step: 914, loss is 0.00025837289285846055\n",
      "epoch: 29 step: 915, loss is 0.02316194772720337\n",
      "epoch: 29 step: 916, loss is 5.989081546431407e-05\n",
      "epoch: 29 step: 917, loss is 0.0003218871424905956\n",
      "epoch: 29 step: 918, loss is 0.0010153941111639142\n",
      "epoch: 29 step: 919, loss is 0.04236556217074394\n",
      "epoch: 29 step: 920, loss is 0.0001778135192580521\n",
      "epoch: 29 step: 921, loss is 0.04144806042313576\n",
      "epoch: 29 step: 922, loss is 0.0014396064216271043\n",
      "epoch: 29 step: 923, loss is 0.0004570592427626252\n",
      "epoch: 29 step: 924, loss is 0.017894405871629715\n",
      "epoch: 29 step: 925, loss is 0.004671762231737375\n",
      "epoch: 29 step: 926, loss is 0.032850079238414764\n",
      "epoch: 29 step: 927, loss is 0.002982498612254858\n",
      "epoch: 29 step: 928, loss is 0.010785245336592197\n",
      "epoch: 29 step: 929, loss is 0.0002223800984211266\n",
      "epoch: 29 step: 930, loss is 9.355184374726377e-06\n",
      "epoch: 29 step: 931, loss is 0.06732894480228424\n",
      "epoch: 29 step: 932, loss is 0.01569337211549282\n",
      "epoch: 29 step: 933, loss is 0.03394481912255287\n",
      "epoch: 29 step: 934, loss is 0.0006304889102466404\n",
      "epoch: 29 step: 935, loss is 0.17939746379852295\n",
      "epoch: 29 step: 936, loss is 0.009451488964259624\n",
      "epoch: 29 step: 937, loss is 0.003353096777573228\n",
      "epoch: 30 step: 1, loss is 9.401008719578385e-05\n",
      "epoch: 30 step: 2, loss is 0.0003081322065554559\n",
      "epoch: 30 step: 3, loss is 0.0019422853365540504\n",
      "epoch: 30 step: 4, loss is 4.137184077990241e-06\n",
      "epoch: 30 step: 5, loss is 0.03182419016957283\n",
      "epoch: 30 step: 6, loss is 0.012167832814157009\n",
      "epoch: 30 step: 7, loss is 0.005740936379879713\n",
      "epoch: 30 step: 8, loss is 0.0012236956972628832\n",
      "epoch: 30 step: 9, loss is 0.0016966209514066577\n",
      "epoch: 30 step: 10, loss is 0.05771281570196152\n",
      "epoch: 30 step: 11, loss is 0.00022941957286093384\n",
      "epoch: 30 step: 12, loss is 7.296693365788087e-05\n",
      "epoch: 30 step: 13, loss is 0.005760545842349529\n",
      "epoch: 30 step: 14, loss is 0.0030139994341880083\n",
      "epoch: 30 step: 15, loss is 0.0012626857496798038\n",
      "epoch: 30 step: 16, loss is 0.010586065240204334\n",
      "epoch: 30 step: 17, loss is 0.0001488831330789253\n",
      "epoch: 30 step: 18, loss is 0.020421070978045464\n",
      "epoch: 30 step: 19, loss is 0.12749627232551575\n",
      "epoch: 30 step: 20, loss is 0.002260521287098527\n",
      "epoch: 30 step: 21, loss is 0.002106873784214258\n",
      "epoch: 30 step: 22, loss is 4.713690577773377e-05\n",
      "epoch: 30 step: 23, loss is 5.376436092774384e-05\n",
      "epoch: 30 step: 24, loss is 0.0011023759143427014\n",
      "epoch: 30 step: 25, loss is 0.07685694098472595\n",
      "epoch: 30 step: 26, loss is 0.0104179373010993\n",
      "epoch: 30 step: 27, loss is 0.042672861367464066\n",
      "epoch: 30 step: 28, loss is 0.0033225594088435173\n",
      "epoch: 30 step: 29, loss is 0.0005333072622306645\n",
      "epoch: 30 step: 30, loss is 0.006174437701702118\n",
      "epoch: 30 step: 31, loss is 0.00011718957830453292\n",
      "epoch: 30 step: 32, loss is 0.04407130181789398\n",
      "epoch: 30 step: 33, loss is 0.000126301558339037\n",
      "epoch: 30 step: 34, loss is 0.0355035699903965\n",
      "epoch: 30 step: 35, loss is 2.0014651454403065e-05\n",
      "epoch: 30 step: 36, loss is 0.03971567004919052\n",
      "epoch: 30 step: 37, loss is 0.0013047638349235058\n",
      "epoch: 30 step: 38, loss is 0.018683038651943207\n",
      "epoch: 30 step: 39, loss is 0.015389188192784786\n",
      "epoch: 30 step: 40, loss is 0.0010670153424143791\n",
      "epoch: 30 step: 41, loss is 0.0004670604248531163\n",
      "epoch: 30 step: 42, loss is 0.02398078329861164\n",
      "epoch: 30 step: 43, loss is 0.07572305202484131\n",
      "epoch: 30 step: 44, loss is 0.1135551705956459\n",
      "epoch: 30 step: 45, loss is 0.005748326424509287\n",
      "epoch: 30 step: 46, loss is 0.011799895204603672\n",
      "epoch: 30 step: 47, loss is 0.00011729926336556673\n",
      "epoch: 30 step: 48, loss is 0.025482529774308205\n",
      "epoch: 30 step: 49, loss is 0.0005912131164222956\n",
      "epoch: 30 step: 50, loss is 0.05953715741634369\n",
      "epoch: 30 step: 51, loss is 0.0005322779761627316\n",
      "epoch: 30 step: 52, loss is 0.015360663644969463\n",
      "epoch: 30 step: 53, loss is 0.02328134886920452\n",
      "epoch: 30 step: 54, loss is 0.0731443241238594\n",
      "epoch: 30 step: 55, loss is 0.000979833654128015\n",
      "epoch: 30 step: 56, loss is 0.011569088324904442\n",
      "epoch: 30 step: 57, loss is 0.010423599742352962\n",
      "epoch: 30 step: 58, loss is 0.06187007948756218\n",
      "epoch: 30 step: 59, loss is 0.007621590048074722\n",
      "epoch: 30 step: 60, loss is 0.10762106627225876\n",
      "epoch: 30 step: 61, loss is 0.011247803457081318\n",
      "epoch: 30 step: 62, loss is 0.023791689425706863\n",
      "epoch: 30 step: 63, loss is 6.570135155925527e-05\n",
      "epoch: 30 step: 64, loss is 0.013893816620111465\n",
      "epoch: 30 step: 65, loss is 0.021041521802544594\n",
      "epoch: 30 step: 66, loss is 0.00014085121802054346\n",
      "epoch: 30 step: 67, loss is 0.001799741992726922\n",
      "epoch: 30 step: 68, loss is 0.002026492264121771\n",
      "epoch: 30 step: 69, loss is 0.0019354420946910977\n",
      "epoch: 30 step: 70, loss is 0.00016977277118712664\n",
      "epoch: 30 step: 71, loss is 0.00010458609904162586\n",
      "epoch: 30 step: 72, loss is 0.010382440872490406\n",
      "epoch: 30 step: 73, loss is 0.005648242775350809\n",
      "epoch: 30 step: 74, loss is 0.0015849367482587695\n",
      "epoch: 30 step: 75, loss is 0.0004598899104166776\n",
      "epoch: 30 step: 76, loss is 0.0002795189502649009\n",
      "epoch: 30 step: 77, loss is 0.002852920675650239\n",
      "epoch: 30 step: 78, loss is 0.026041526347398758\n",
      "epoch: 30 step: 79, loss is 0.0019988855347037315\n",
      "epoch: 30 step: 80, loss is 0.0199277326464653\n",
      "epoch: 30 step: 81, loss is 0.0005620186566375196\n",
      "epoch: 30 step: 82, loss is 0.014856173656880856\n",
      "epoch: 30 step: 83, loss is 0.0008463506819680333\n",
      "epoch: 30 step: 84, loss is 0.004423495382070541\n",
      "epoch: 30 step: 85, loss is 8.585432078689337e-05\n",
      "epoch: 30 step: 86, loss is 0.01720607466995716\n",
      "epoch: 30 step: 87, loss is 0.05065714567899704\n",
      "epoch: 30 step: 88, loss is 0.00785906333476305\n",
      "epoch: 30 step: 89, loss is 0.00040029012598097324\n",
      "epoch: 30 step: 90, loss is 1.6004980352590792e-05\n",
      "epoch: 30 step: 91, loss is 4.385873762657866e-05\n",
      "epoch: 30 step: 92, loss is 0.0004860426124650985\n",
      "epoch: 30 step: 93, loss is 0.01621055416762829\n",
      "epoch: 30 step: 94, loss is 0.00016210469766519964\n",
      "epoch: 30 step: 95, loss is 0.0010048409458249807\n",
      "epoch: 30 step: 96, loss is 0.0013477492611855268\n",
      "epoch: 30 step: 97, loss is 9.439024870516732e-05\n",
      "epoch: 30 step: 98, loss is 0.00016010136459954083\n",
      "epoch: 30 step: 99, loss is 0.0012685690307989717\n",
      "epoch: 30 step: 100, loss is 0.00024787173606455326\n",
      "epoch: 30 step: 101, loss is 0.00507217925041914\n",
      "epoch: 30 step: 102, loss is 0.004562362562865019\n",
      "epoch: 30 step: 103, loss is 0.0017679519951343536\n",
      "epoch: 30 step: 104, loss is 0.014361235313117504\n",
      "epoch: 30 step: 105, loss is 0.00749027356505394\n",
      "epoch: 30 step: 106, loss is 0.03855561837553978\n",
      "epoch: 30 step: 107, loss is 0.0018412614008411765\n",
      "epoch: 30 step: 108, loss is 0.0004856816667597741\n",
      "epoch: 30 step: 109, loss is 0.020141001790761948\n",
      "epoch: 30 step: 110, loss is 0.004215606488287449\n",
      "epoch: 30 step: 111, loss is 0.0027909562923014164\n",
      "epoch: 30 step: 112, loss is 0.001163642155006528\n",
      "epoch: 30 step: 113, loss is 0.0003442252054810524\n",
      "epoch: 30 step: 114, loss is 0.0022219137754291296\n",
      "epoch: 30 step: 115, loss is 0.002315844874829054\n",
      "epoch: 30 step: 116, loss is 0.0005267760716378689\n",
      "epoch: 30 step: 117, loss is 0.01465209573507309\n",
      "epoch: 30 step: 118, loss is 0.008465897291898727\n",
      "epoch: 30 step: 119, loss is 7.601647666888312e-05\n",
      "epoch: 30 step: 120, loss is 0.0005172299570403993\n",
      "epoch: 30 step: 121, loss is 0.009489133022725582\n",
      "epoch: 30 step: 122, loss is 0.0007010394474491477\n",
      "epoch: 30 step: 123, loss is 0.006853659171611071\n",
      "epoch: 30 step: 124, loss is 0.029344890266656876\n",
      "epoch: 30 step: 125, loss is 0.0006492944667115808\n",
      "epoch: 30 step: 126, loss is 0.09105545282363892\n",
      "epoch: 30 step: 127, loss is 0.0006931882235221565\n",
      "epoch: 30 step: 128, loss is 0.0003626715042628348\n",
      "epoch: 30 step: 129, loss is 0.058617331087589264\n",
      "epoch: 30 step: 130, loss is 0.013537490740418434\n",
      "epoch: 30 step: 131, loss is 0.0003228629648219794\n",
      "epoch: 30 step: 132, loss is 0.0028892895206809044\n",
      "epoch: 30 step: 133, loss is 0.00028877955628558993\n",
      "epoch: 30 step: 134, loss is 0.023775214329361916\n",
      "epoch: 30 step: 135, loss is 0.019724523648619652\n",
      "epoch: 30 step: 136, loss is 0.0022513410076498985\n",
      "epoch: 30 step: 137, loss is 1.995767161133699e-05\n",
      "epoch: 30 step: 138, loss is 0.00042983051389455795\n",
      "epoch: 30 step: 139, loss is 0.000506929645780474\n",
      "epoch: 30 step: 140, loss is 0.0008029371383599937\n",
      "epoch: 30 step: 141, loss is 0.0009227964328601956\n",
      "epoch: 30 step: 142, loss is 0.09791787713766098\n",
      "epoch: 30 step: 143, loss is 0.0025316684041172266\n",
      "epoch: 30 step: 144, loss is 0.0005005145794712007\n",
      "epoch: 30 step: 145, loss is 4.772852116730064e-05\n",
      "epoch: 30 step: 146, loss is 0.0002284318470628932\n",
      "epoch: 30 step: 147, loss is 0.0001093284590751864\n",
      "epoch: 30 step: 148, loss is 2.408562613709364e-05\n",
      "epoch: 30 step: 149, loss is 0.0003691559250000864\n",
      "epoch: 30 step: 150, loss is 0.007690467871725559\n",
      "epoch: 30 step: 151, loss is 0.00024288905842695385\n",
      "epoch: 30 step: 152, loss is 0.0050470419228076935\n",
      "epoch: 30 step: 153, loss is 1.047231944539817e-05\n",
      "epoch: 30 step: 154, loss is 0.016626663506031036\n",
      "epoch: 30 step: 155, loss is 0.0004131075693294406\n",
      "epoch: 30 step: 156, loss is 0.008411571383476257\n",
      "epoch: 30 step: 157, loss is 0.0007100508664734662\n",
      "epoch: 30 step: 158, loss is 0.007087794132530689\n",
      "epoch: 30 step: 159, loss is 0.0006279177614487708\n",
      "epoch: 30 step: 160, loss is 0.012625368312001228\n",
      "epoch: 30 step: 161, loss is 0.006625492125749588\n",
      "epoch: 30 step: 162, loss is 0.005035457666963339\n",
      "epoch: 30 step: 163, loss is 0.0015163846546784043\n",
      "epoch: 30 step: 164, loss is 0.014485545456409454\n",
      "epoch: 30 step: 165, loss is 0.0014525952283293009\n",
      "epoch: 30 step: 166, loss is 8.888187585398555e-05\n",
      "epoch: 30 step: 167, loss is 0.0016334314132109284\n",
      "epoch: 30 step: 168, loss is 0.0016721290303394198\n",
      "epoch: 30 step: 169, loss is 0.00017200346337631345\n",
      "epoch: 30 step: 170, loss is 9.701504313852638e-05\n",
      "epoch: 30 step: 171, loss is 0.0262152086943388\n",
      "epoch: 30 step: 172, loss is 9.939489245880395e-05\n",
      "epoch: 30 step: 173, loss is 0.00010739721619756892\n",
      "epoch: 30 step: 174, loss is 0.0012379150139167905\n",
      "epoch: 30 step: 175, loss is 0.01684359274804592\n",
      "epoch: 30 step: 176, loss is 0.0007736840634606779\n",
      "epoch: 30 step: 177, loss is 0.0007551360758952796\n",
      "epoch: 30 step: 178, loss is 0.00011441852257121354\n",
      "epoch: 30 step: 179, loss is 0.0005481800762936473\n",
      "epoch: 30 step: 180, loss is 0.0010003085481002927\n",
      "epoch: 30 step: 181, loss is 0.013024311512708664\n",
      "epoch: 30 step: 182, loss is 0.0008308049291372299\n",
      "epoch: 30 step: 183, loss is 0.014256567694246769\n",
      "epoch: 30 step: 184, loss is 0.0002812966995406896\n",
      "epoch: 30 step: 185, loss is 0.0011796560138463974\n",
      "epoch: 30 step: 186, loss is 0.0016754086827859282\n",
      "epoch: 30 step: 187, loss is 0.009060033597052097\n",
      "epoch: 30 step: 188, loss is 0.0020994062069803476\n",
      "epoch: 30 step: 189, loss is 0.003810408990830183\n",
      "epoch: 30 step: 190, loss is 0.00017025689885485917\n",
      "epoch: 30 step: 191, loss is 0.10650341212749481\n",
      "epoch: 30 step: 192, loss is 0.0015880862483754754\n",
      "epoch: 30 step: 193, loss is 0.06055271252989769\n",
      "epoch: 30 step: 194, loss is 0.0027489536441862583\n",
      "epoch: 30 step: 195, loss is 0.005256970413029194\n",
      "epoch: 30 step: 196, loss is 0.006011974532157183\n",
      "epoch: 30 step: 197, loss is 0.009212590754032135\n",
      "epoch: 30 step: 198, loss is 4.850791810895316e-05\n",
      "epoch: 30 step: 199, loss is 0.008652500808238983\n",
      "epoch: 30 step: 200, loss is 0.0013605791609734297\n",
      "epoch: 30 step: 201, loss is 0.007642954122275114\n",
      "epoch: 30 step: 202, loss is 5.4938220273470506e-05\n",
      "epoch: 30 step: 203, loss is 0.0671771913766861\n",
      "epoch: 30 step: 204, loss is 0.0007091738516464829\n",
      "epoch: 30 step: 205, loss is 0.010978057980537415\n",
      "epoch: 30 step: 206, loss is 0.012082243338227272\n",
      "epoch: 30 step: 207, loss is 0.004461502656340599\n",
      "epoch: 30 step: 208, loss is 5.3025818488094956e-05\n",
      "epoch: 30 step: 209, loss is 0.004343980923295021\n",
      "epoch: 30 step: 210, loss is 8.538101974409074e-05\n",
      "epoch: 30 step: 211, loss is 0.00047452107537537813\n",
      "epoch: 30 step: 212, loss is 0.0037675704807043076\n",
      "epoch: 30 step: 213, loss is 0.00040315600926987827\n",
      "epoch: 30 step: 214, loss is 0.0006337562808766961\n",
      "epoch: 30 step: 215, loss is 0.00038467696867883205\n",
      "epoch: 30 step: 216, loss is 0.000815128325484693\n",
      "epoch: 30 step: 217, loss is 7.37460286472924e-05\n",
      "epoch: 30 step: 218, loss is 0.04527980461716652\n",
      "epoch: 30 step: 219, loss is 0.00014616985572502017\n",
      "epoch: 30 step: 220, loss is 0.0018186550587415695\n",
      "epoch: 30 step: 221, loss is 0.001261242781765759\n",
      "epoch: 30 step: 222, loss is 6.479718285845593e-05\n",
      "epoch: 30 step: 223, loss is 0.03644438832998276\n",
      "epoch: 30 step: 224, loss is 0.002621962921693921\n",
      "epoch: 30 step: 225, loss is 0.0007873986614868045\n",
      "epoch: 30 step: 226, loss is 0.014075493440032005\n",
      "epoch: 30 step: 227, loss is 0.06665098667144775\n",
      "epoch: 30 step: 228, loss is 0.00044239789713174105\n",
      "epoch: 30 step: 229, loss is 0.0004963699611835182\n",
      "epoch: 30 step: 230, loss is 0.0024198093451559544\n",
      "epoch: 30 step: 231, loss is 0.03145243227481842\n",
      "epoch: 30 step: 232, loss is 0.0013720823917537928\n",
      "epoch: 30 step: 233, loss is 0.001304885372519493\n",
      "epoch: 30 step: 234, loss is 0.00010435555304866284\n",
      "epoch: 30 step: 235, loss is 8.54357349453494e-05\n",
      "epoch: 30 step: 236, loss is 0.0012095714919269085\n",
      "epoch: 30 step: 237, loss is 0.0011827906128019094\n",
      "epoch: 30 step: 238, loss is 0.00021665349777322263\n",
      "epoch: 30 step: 239, loss is 0.00844164751470089\n",
      "epoch: 30 step: 240, loss is 0.030579404905438423\n",
      "epoch: 30 step: 241, loss is 0.00010620021203067154\n",
      "epoch: 30 step: 242, loss is 0.0007906858809292316\n",
      "epoch: 30 step: 243, loss is 0.003938153851777315\n",
      "epoch: 30 step: 244, loss is 0.03802718222141266\n",
      "epoch: 30 step: 245, loss is 0.02357642911374569\n",
      "epoch: 30 step: 246, loss is 0.004244395066052675\n",
      "epoch: 30 step: 247, loss is 0.0003074601700063795\n",
      "epoch: 30 step: 248, loss is 0.003450896358117461\n",
      "epoch: 30 step: 249, loss is 0.000496711116284132\n",
      "epoch: 30 step: 250, loss is 0.002561841858550906\n",
      "epoch: 30 step: 251, loss is 0.0021975659765303135\n",
      "epoch: 30 step: 252, loss is 0.0005040014511905611\n",
      "epoch: 30 step: 253, loss is 0.013076149858534336\n",
      "epoch: 30 step: 254, loss is 0.0006000284920446575\n",
      "epoch: 30 step: 255, loss is 2.5521427232888527e-05\n",
      "epoch: 30 step: 256, loss is 0.004829886369407177\n",
      "epoch: 30 step: 257, loss is 0.0018891483778133988\n",
      "epoch: 30 step: 258, loss is 0.0013350802473723888\n",
      "epoch: 30 step: 259, loss is 0.003958229906857014\n",
      "epoch: 30 step: 260, loss is 0.0031011581886559725\n",
      "epoch: 30 step: 261, loss is 0.025361331179738045\n",
      "epoch: 30 step: 262, loss is 0.0005475411307998002\n",
      "epoch: 30 step: 263, loss is 0.0017338244942948222\n",
      "epoch: 30 step: 264, loss is 6.945887434994802e-05\n",
      "epoch: 30 step: 265, loss is 3.435605685808696e-05\n",
      "epoch: 30 step: 266, loss is 0.0011604776373133063\n",
      "epoch: 30 step: 267, loss is 7.308730710064992e-05\n",
      "epoch: 30 step: 268, loss is 0.014801268465816975\n",
      "epoch: 30 step: 269, loss is 0.012965183705091476\n",
      "epoch: 30 step: 270, loss is 0.0052491300739347935\n",
      "epoch: 30 step: 271, loss is 0.0016877909656614065\n",
      "epoch: 30 step: 272, loss is 0.0005148726049810648\n",
      "epoch: 30 step: 273, loss is 0.0003009683277923614\n",
      "epoch: 30 step: 274, loss is 0.019105471670627594\n",
      "epoch: 30 step: 275, loss is 4.355310738901608e-05\n",
      "epoch: 30 step: 276, loss is 0.007812857627868652\n",
      "epoch: 30 step: 277, loss is 5.276272349874489e-05\n",
      "epoch: 30 step: 278, loss is 0.006551225669682026\n",
      "epoch: 30 step: 279, loss is 1.050596620189026e-05\n",
      "epoch: 30 step: 280, loss is 0.0002698330790735781\n",
      "epoch: 30 step: 281, loss is 0.0019282203866168857\n",
      "epoch: 30 step: 282, loss is 0.0016476004384458065\n",
      "epoch: 30 step: 283, loss is 0.0014358897460624576\n",
      "epoch: 30 step: 284, loss is 3.170932541252114e-05\n",
      "epoch: 30 step: 285, loss is 0.004583143629133701\n",
      "epoch: 30 step: 286, loss is 0.003753671422600746\n",
      "epoch: 30 step: 287, loss is 0.0010655838996171951\n",
      "epoch: 30 step: 288, loss is 0.023372352123260498\n",
      "epoch: 30 step: 289, loss is 0.0031754327937960625\n",
      "epoch: 30 step: 290, loss is 0.0001524122926639393\n",
      "epoch: 30 step: 291, loss is 0.008836404420435429\n",
      "epoch: 30 step: 292, loss is 3.6416895454749465e-05\n",
      "epoch: 30 step: 293, loss is 0.05136675760149956\n",
      "epoch: 30 step: 294, loss is 0.012756350450217724\n",
      "epoch: 30 step: 295, loss is 9.530876559438184e-05\n",
      "epoch: 30 step: 296, loss is 0.00013112953456584364\n",
      "epoch: 30 step: 297, loss is 0.0003149501862935722\n",
      "epoch: 30 step: 298, loss is 0.0006393719231709838\n",
      "epoch: 30 step: 299, loss is 0.019465025514364243\n",
      "epoch: 30 step: 300, loss is 1.208057892654324e-05\n",
      "epoch: 30 step: 301, loss is 0.00032287908834405243\n",
      "epoch: 30 step: 302, loss is 6.447140185628086e-05\n",
      "epoch: 30 step: 303, loss is 0.0016889184480533004\n",
      "epoch: 30 step: 304, loss is 0.01084852498024702\n",
      "epoch: 30 step: 305, loss is 0.0007285246392711997\n",
      "epoch: 30 step: 306, loss is 0.0011955124791711569\n",
      "epoch: 30 step: 307, loss is 0.004572346806526184\n",
      "epoch: 30 step: 308, loss is 0.00045569526264443994\n",
      "epoch: 30 step: 309, loss is 2.8992948500672355e-05\n",
      "epoch: 30 step: 310, loss is 0.00012288351717870682\n",
      "epoch: 30 step: 311, loss is 0.0033195833675563335\n",
      "epoch: 30 step: 312, loss is 0.0988779067993164\n",
      "epoch: 30 step: 313, loss is 0.001886378275230527\n",
      "epoch: 30 step: 314, loss is 0.039679642766714096\n",
      "epoch: 30 step: 315, loss is 0.02786100283265114\n",
      "epoch: 30 step: 316, loss is 0.04747096076607704\n",
      "epoch: 30 step: 317, loss is 0.0004042056098114699\n",
      "epoch: 30 step: 318, loss is 0.006691503804177046\n",
      "epoch: 30 step: 319, loss is 0.005692839622497559\n",
      "epoch: 30 step: 320, loss is 0.006471206899732351\n",
      "epoch: 30 step: 321, loss is 0.0004950721049681306\n",
      "epoch: 30 step: 322, loss is 0.023651694878935814\n",
      "epoch: 30 step: 323, loss is 0.00027259273338131607\n",
      "epoch: 30 step: 324, loss is 0.0021006858441978693\n",
      "epoch: 30 step: 325, loss is 0.0014396291226148605\n",
      "epoch: 30 step: 326, loss is 0.003443492343649268\n",
      "epoch: 30 step: 327, loss is 0.06288307160139084\n",
      "epoch: 30 step: 328, loss is 0.014480634592473507\n",
      "epoch: 30 step: 329, loss is 0.0034000466112047434\n",
      "epoch: 30 step: 330, loss is 0.006520475260913372\n",
      "epoch: 30 step: 331, loss is 0.0005550620844587684\n",
      "epoch: 30 step: 332, loss is 0.029018856585025787\n",
      "epoch: 30 step: 333, loss is 0.008181658573448658\n",
      "epoch: 30 step: 334, loss is 0.06371832638978958\n",
      "epoch: 30 step: 335, loss is 0.003336928552016616\n",
      "epoch: 30 step: 336, loss is 0.005760509520769119\n",
      "epoch: 30 step: 337, loss is 0.0007144598057493567\n",
      "epoch: 30 step: 338, loss is 0.0019034596625715494\n",
      "epoch: 30 step: 339, loss is 0.0048178862780332565\n",
      "epoch: 30 step: 340, loss is 0.00036189224920235574\n",
      "epoch: 30 step: 341, loss is 7.945037941681221e-05\n",
      "epoch: 30 step: 342, loss is 0.06920422613620758\n",
      "epoch: 30 step: 343, loss is 0.03543204069137573\n",
      "epoch: 30 step: 344, loss is 0.05939663201570511\n",
      "epoch: 30 step: 345, loss is 0.0010382055770605803\n",
      "epoch: 30 step: 346, loss is 8.921383414417505e-05\n",
      "epoch: 30 step: 347, loss is 0.00032075162744149566\n",
      "epoch: 30 step: 348, loss is 4.9529327952768654e-05\n",
      "epoch: 30 step: 349, loss is 0.00794740580022335\n",
      "epoch: 30 step: 350, loss is 0.0030577322468161583\n",
      "epoch: 30 step: 351, loss is 0.012112420983612537\n",
      "epoch: 30 step: 352, loss is 0.004055776633322239\n",
      "epoch: 30 step: 353, loss is 2.832345853676088e-05\n",
      "epoch: 30 step: 354, loss is 0.0005465769208967686\n",
      "epoch: 30 step: 355, loss is 5.6217349992948584e-06\n",
      "epoch: 30 step: 356, loss is 0.0031693819910287857\n",
      "epoch: 30 step: 357, loss is 9.55530849751085e-05\n",
      "epoch: 30 step: 358, loss is 0.08214924484491348\n",
      "epoch: 30 step: 359, loss is 0.023636486381292343\n",
      "epoch: 30 step: 360, loss is 0.06682803481817245\n",
      "epoch: 30 step: 361, loss is 0.002202952979132533\n",
      "epoch: 30 step: 362, loss is 0.0017567476024851203\n",
      "epoch: 30 step: 363, loss is 0.11290045827627182\n",
      "epoch: 30 step: 364, loss is 0.0009448950877413154\n",
      "epoch: 30 step: 365, loss is 0.0010183885460719466\n",
      "epoch: 30 step: 366, loss is 0.0039617037400603294\n",
      "epoch: 30 step: 367, loss is 0.004384154453873634\n",
      "epoch: 30 step: 368, loss is 0.0020490093156695366\n",
      "epoch: 30 step: 369, loss is 0.011648502200841904\n",
      "epoch: 30 step: 370, loss is 0.022289620712399483\n",
      "epoch: 30 step: 371, loss is 0.002347326837480068\n",
      "epoch: 30 step: 372, loss is 0.1334245353937149\n",
      "epoch: 30 step: 373, loss is 0.0012935013510286808\n",
      "epoch: 30 step: 374, loss is 0.0002876828075386584\n",
      "epoch: 30 step: 375, loss is 0.0052221291698515415\n",
      "epoch: 30 step: 376, loss is 0.014077844098210335\n",
      "epoch: 30 step: 377, loss is 0.009041288867592812\n",
      "epoch: 30 step: 378, loss is 0.005592836067080498\n",
      "epoch: 30 step: 379, loss is 0.007382586132735014\n",
      "epoch: 30 step: 380, loss is 0.0019974461756646633\n",
      "epoch: 30 step: 381, loss is 0.0048500485718250275\n",
      "epoch: 30 step: 382, loss is 6.718176155118272e-05\n",
      "epoch: 30 step: 383, loss is 0.004546016920357943\n",
      "epoch: 30 step: 384, loss is 0.002688026987016201\n",
      "epoch: 30 step: 385, loss is 0.006953230127692223\n",
      "epoch: 30 step: 386, loss is 0.010109996423125267\n",
      "epoch: 30 step: 387, loss is 0.0031012019608169794\n",
      "epoch: 30 step: 388, loss is 0.0005029531312175095\n",
      "epoch: 30 step: 389, loss is 0.003734293859452009\n",
      "epoch: 30 step: 390, loss is 0.0006367449532262981\n",
      "epoch: 30 step: 391, loss is 0.005138819571584463\n",
      "epoch: 30 step: 392, loss is 0.08240243047475815\n",
      "epoch: 30 step: 393, loss is 0.012513319961726665\n",
      "epoch: 30 step: 394, loss is 0.005520382430404425\n",
      "epoch: 30 step: 395, loss is 0.0011582280276343226\n",
      "epoch: 30 step: 396, loss is 0.003758507315069437\n",
      "epoch: 30 step: 397, loss is 0.0007464581867679954\n",
      "epoch: 30 step: 398, loss is 0.0006762994453310966\n",
      "epoch: 30 step: 399, loss is 0.0002313457807758823\n",
      "epoch: 30 step: 400, loss is 0.0007868182728998363\n",
      "epoch: 30 step: 401, loss is 0.046479713171720505\n",
      "epoch: 30 step: 402, loss is 0.0032330695539712906\n",
      "epoch: 30 step: 403, loss is 0.00031285465229302645\n",
      "epoch: 30 step: 404, loss is 9.636093454901129e-05\n",
      "epoch: 30 step: 405, loss is 0.00820273905992508\n",
      "epoch: 30 step: 406, loss is 0.0017048708396032453\n",
      "epoch: 30 step: 407, loss is 4.7750418161740527e-05\n",
      "epoch: 30 step: 408, loss is 0.016461968421936035\n",
      "epoch: 30 step: 409, loss is 0.06594910472631454\n",
      "epoch: 30 step: 410, loss is 0.007757395040243864\n",
      "epoch: 30 step: 411, loss is 0.0001343775657005608\n",
      "epoch: 30 step: 412, loss is 0.0019613306503742933\n",
      "epoch: 30 step: 413, loss is 0.0004030650306958705\n",
      "epoch: 30 step: 414, loss is 0.005155012011528015\n",
      "epoch: 30 step: 415, loss is 0.0009976888541132212\n",
      "epoch: 30 step: 416, loss is 0.0003889147483278066\n",
      "epoch: 30 step: 417, loss is 0.0002971866342704743\n",
      "epoch: 30 step: 418, loss is 6.996010051807389e-05\n",
      "epoch: 30 step: 419, loss is 0.00021670674323104322\n",
      "epoch: 30 step: 420, loss is 0.00013515973114408553\n",
      "epoch: 30 step: 421, loss is 0.013609865680336952\n",
      "epoch: 30 step: 422, loss is 0.005040891468524933\n",
      "epoch: 30 step: 423, loss is 0.0001952960155904293\n",
      "epoch: 30 step: 424, loss is 0.0005082719726487994\n",
      "epoch: 30 step: 425, loss is 4.417716263560578e-05\n",
      "epoch: 30 step: 426, loss is 7.403080235235393e-05\n",
      "epoch: 30 step: 427, loss is 0.0007595078204758465\n",
      "epoch: 30 step: 428, loss is 0.0020268564112484455\n",
      "epoch: 30 step: 429, loss is 0.0004073872696608305\n",
      "epoch: 30 step: 430, loss is 0.00982601847499609\n",
      "epoch: 30 step: 431, loss is 0.008330250158905983\n",
      "epoch: 30 step: 432, loss is 0.009777186438441277\n",
      "epoch: 30 step: 433, loss is 0.0009396374807693064\n",
      "epoch: 30 step: 434, loss is 0.00019602381507866085\n",
      "epoch: 30 step: 435, loss is 0.028637144714593887\n",
      "epoch: 30 step: 436, loss is 0.0026488606818020344\n",
      "epoch: 30 step: 437, loss is 0.009535068646073341\n",
      "epoch: 30 step: 438, loss is 0.008004976436495781\n",
      "epoch: 30 step: 439, loss is 0.0005640832823701203\n",
      "epoch: 30 step: 440, loss is 0.003769806120544672\n",
      "epoch: 30 step: 441, loss is 0.001154822064563632\n",
      "epoch: 30 step: 442, loss is 0.11784529685974121\n",
      "epoch: 30 step: 443, loss is 0.013196631334722042\n",
      "epoch: 30 step: 444, loss is 0.00038045222754590213\n",
      "epoch: 30 step: 445, loss is 0.008265704847872257\n",
      "epoch: 30 step: 446, loss is 0.05141184851527214\n",
      "epoch: 30 step: 447, loss is 0.0004362456384114921\n",
      "epoch: 30 step: 448, loss is 0.0013389848172664642\n",
      "epoch: 30 step: 449, loss is 0.005028736777603626\n",
      "epoch: 30 step: 450, loss is 0.0001254199305549264\n",
      "epoch: 30 step: 451, loss is 0.0016810119850561023\n",
      "epoch: 30 step: 452, loss is 0.00013541581574827433\n",
      "epoch: 30 step: 453, loss is 0.002178586320951581\n",
      "epoch: 30 step: 454, loss is 0.042747411876916885\n",
      "epoch: 30 step: 455, loss is 0.016054963693022728\n",
      "epoch: 30 step: 456, loss is 0.002435889793559909\n",
      "epoch: 30 step: 457, loss is 0.018538877367973328\n",
      "epoch: 30 step: 458, loss is 0.00012680134386755526\n",
      "epoch: 30 step: 459, loss is 0.0032605454325675964\n",
      "epoch: 30 step: 460, loss is 0.2688635587692261\n",
      "epoch: 30 step: 461, loss is 0.058458395302295685\n",
      "epoch: 30 step: 462, loss is 0.003425316885113716\n",
      "epoch: 30 step: 463, loss is 0.00042859086534008384\n",
      "epoch: 30 step: 464, loss is 0.00553804449737072\n",
      "epoch: 30 step: 465, loss is 0.00044656568206846714\n",
      "epoch: 30 step: 466, loss is 0.002262027468532324\n",
      "epoch: 30 step: 467, loss is 0.0017635574331507087\n",
      "epoch: 30 step: 468, loss is 0.0010251628700643778\n",
      "epoch: 30 step: 469, loss is 0.001015799934975803\n",
      "epoch: 30 step: 470, loss is 0.007498615887016058\n",
      "epoch: 30 step: 471, loss is 0.0003525374922901392\n",
      "epoch: 30 step: 472, loss is 0.02295866794884205\n",
      "epoch: 30 step: 473, loss is 0.0002633480471558869\n",
      "epoch: 30 step: 474, loss is 0.0014136405661702156\n",
      "epoch: 30 step: 475, loss is 0.0027884349692612886\n",
      "epoch: 30 step: 476, loss is 0.0002525818126741797\n",
      "epoch: 30 step: 477, loss is 0.012565660290420055\n",
      "epoch: 30 step: 478, loss is 0.011957723647356033\n",
      "epoch: 30 step: 479, loss is 0.0028148111887276173\n",
      "epoch: 30 step: 480, loss is 0.004537361674010754\n",
      "epoch: 30 step: 481, loss is 0.0006054582772776484\n",
      "epoch: 30 step: 482, loss is 8.295087354781572e-06\n",
      "epoch: 30 step: 483, loss is 0.05190318077802658\n",
      "epoch: 30 step: 484, loss is 0.004356954246759415\n",
      "epoch: 30 step: 485, loss is 0.020407365635037422\n",
      "epoch: 30 step: 486, loss is 0.005482349079102278\n",
      "epoch: 30 step: 487, loss is 0.07138977944850922\n",
      "epoch: 30 step: 488, loss is 0.06424538046121597\n",
      "epoch: 30 step: 489, loss is 0.013549546711146832\n",
      "epoch: 30 step: 490, loss is 0.0006385727319866419\n",
      "epoch: 30 step: 491, loss is 0.002225614385679364\n",
      "epoch: 30 step: 492, loss is 0.00495552085340023\n",
      "epoch: 30 step: 493, loss is 0.014575759880244732\n",
      "epoch: 30 step: 494, loss is 0.01107284240424633\n",
      "epoch: 30 step: 495, loss is 3.8847771065775305e-05\n",
      "epoch: 30 step: 496, loss is 0.0464186854660511\n",
      "epoch: 30 step: 497, loss is 0.0720367282629013\n",
      "epoch: 30 step: 498, loss is 0.0009400602430105209\n",
      "epoch: 30 step: 499, loss is 0.007636777590960264\n",
      "epoch: 30 step: 500, loss is 0.0027802689000964165\n",
      "epoch: 30 step: 501, loss is 0.00014891059254296124\n",
      "epoch: 30 step: 502, loss is 0.0011134784435853362\n",
      "epoch: 30 step: 503, loss is 0.0039007856976240873\n",
      "epoch: 30 step: 504, loss is 0.0017765206284821033\n",
      "epoch: 30 step: 505, loss is 0.0014422337990254164\n",
      "epoch: 30 step: 506, loss is 0.10860221087932587\n",
      "epoch: 30 step: 507, loss is 0.0103897824883461\n",
      "epoch: 30 step: 508, loss is 0.005188950337469578\n",
      "epoch: 30 step: 509, loss is 0.00032917436328716576\n",
      "epoch: 30 step: 510, loss is 0.026750117540359497\n",
      "epoch: 30 step: 511, loss is 0.002770716091617942\n",
      "epoch: 30 step: 512, loss is 0.006420541554689407\n",
      "epoch: 30 step: 513, loss is 0.002461194759234786\n",
      "epoch: 30 step: 514, loss is 0.0017167292535305023\n",
      "epoch: 30 step: 515, loss is 0.0002438450464978814\n",
      "epoch: 30 step: 516, loss is 0.013875684700906277\n",
      "epoch: 30 step: 517, loss is 0.00047265508328564465\n",
      "epoch: 30 step: 518, loss is 0.0003901541349478066\n",
      "epoch: 30 step: 519, loss is 0.0002130575303453952\n",
      "epoch: 30 step: 520, loss is 0.0004543907125480473\n",
      "epoch: 30 step: 521, loss is 0.1000887081027031\n",
      "epoch: 30 step: 522, loss is 0.03012600727379322\n",
      "epoch: 30 step: 523, loss is 0.026467744261026382\n",
      "epoch: 30 step: 524, loss is 0.05117911472916603\n",
      "epoch: 30 step: 525, loss is 0.0074173929169774055\n",
      "epoch: 30 step: 526, loss is 0.14101064205169678\n",
      "epoch: 30 step: 527, loss is 0.00026671381783671677\n",
      "epoch: 30 step: 528, loss is 0.04467703774571419\n",
      "epoch: 30 step: 529, loss is 0.0062884073704481125\n",
      "epoch: 30 step: 530, loss is 0.0019208486191928387\n",
      "epoch: 30 step: 531, loss is 0.0005071191117167473\n",
      "epoch: 30 step: 532, loss is 0.003501252504065633\n",
      "epoch: 30 step: 533, loss is 0.0011685608187690377\n",
      "epoch: 30 step: 534, loss is 0.0012590843252837658\n",
      "epoch: 30 step: 535, loss is 0.057307105511426926\n",
      "epoch: 30 step: 536, loss is 0.00835650134831667\n",
      "epoch: 30 step: 537, loss is 0.0009002859587781131\n",
      "epoch: 30 step: 538, loss is 0.0005546505562961102\n",
      "epoch: 30 step: 539, loss is 0.06367174535989761\n",
      "epoch: 30 step: 540, loss is 0.00044964399421587586\n",
      "epoch: 30 step: 541, loss is 0.0012982769403606653\n",
      "epoch: 30 step: 542, loss is 0.060955993831157684\n",
      "epoch: 30 step: 543, loss is 0.019549403339624405\n",
      "epoch: 30 step: 544, loss is 0.00011828477727249265\n",
      "epoch: 30 step: 545, loss is 0.003845911705866456\n",
      "epoch: 30 step: 546, loss is 0.09948080033063889\n",
      "epoch: 30 step: 547, loss is 0.11060412973165512\n",
      "epoch: 30 step: 548, loss is 0.005012812092900276\n",
      "epoch: 30 step: 549, loss is 0.003909696824848652\n",
      "epoch: 30 step: 550, loss is 0.0017748054815456271\n",
      "epoch: 30 step: 551, loss is 0.0005325948586687446\n",
      "epoch: 30 step: 552, loss is 0.01013147085905075\n",
      "epoch: 30 step: 553, loss is 0.021611439064145088\n",
      "epoch: 30 step: 554, loss is 0.003755039069801569\n",
      "epoch: 30 step: 555, loss is 0.0004446428210940212\n",
      "epoch: 30 step: 556, loss is 0.0014436646597459912\n",
      "epoch: 30 step: 557, loss is 0.033860523253679276\n",
      "epoch: 30 step: 558, loss is 0.00427586492151022\n",
      "epoch: 30 step: 559, loss is 0.06677539646625519\n",
      "epoch: 30 step: 560, loss is 0.00010422577179269865\n",
      "epoch: 30 step: 561, loss is 0.01277272216975689\n",
      "epoch: 30 step: 562, loss is 0.0010528527200222015\n",
      "epoch: 30 step: 563, loss is 0.006703188177198172\n",
      "epoch: 30 step: 564, loss is 0.0025108628906309605\n",
      "epoch: 30 step: 565, loss is 0.0005010516033507884\n",
      "epoch: 30 step: 566, loss is 0.07242126017808914\n",
      "epoch: 30 step: 567, loss is 0.061168212443590164\n",
      "epoch: 30 step: 568, loss is 0.02183903194963932\n",
      "epoch: 30 step: 569, loss is 0.03948049619793892\n",
      "epoch: 30 step: 570, loss is 0.0003141123161185533\n",
      "epoch: 30 step: 571, loss is 0.005754409823566675\n",
      "epoch: 30 step: 572, loss is 0.002736929804086685\n",
      "epoch: 30 step: 573, loss is 0.00287786521948874\n",
      "epoch: 30 step: 574, loss is 0.0030800055246800184\n",
      "epoch: 30 step: 575, loss is 0.00434845732524991\n",
      "epoch: 30 step: 576, loss is 0.01902460865676403\n",
      "epoch: 30 step: 577, loss is 0.00816155131906271\n",
      "epoch: 30 step: 578, loss is 0.0012179408222436905\n",
      "epoch: 30 step: 579, loss is 0.002723707351833582\n",
      "epoch: 30 step: 580, loss is 0.007239829748868942\n",
      "epoch: 30 step: 581, loss is 0.0007032360881567001\n",
      "epoch: 30 step: 582, loss is 0.0062753669917583466\n",
      "epoch: 30 step: 583, loss is 0.005930890329182148\n",
      "epoch: 30 step: 584, loss is 0.00010475066665094346\n",
      "epoch: 30 step: 585, loss is 0.013482670299708843\n",
      "epoch: 30 step: 586, loss is 0.009683916345238686\n",
      "epoch: 30 step: 587, loss is 0.00021610110707115382\n",
      "epoch: 30 step: 588, loss is 0.0044972593896090984\n",
      "epoch: 30 step: 589, loss is 0.017265871167182922\n",
      "epoch: 30 step: 590, loss is 0.007128129247575998\n",
      "epoch: 30 step: 591, loss is 0.0001099355285987258\n",
      "epoch: 30 step: 592, loss is 0.0002443199045956135\n",
      "epoch: 30 step: 593, loss is 0.004957923199981451\n",
      "epoch: 30 step: 594, loss is 0.11299972981214523\n",
      "epoch: 30 step: 595, loss is 0.0004324870533309877\n",
      "epoch: 30 step: 596, loss is 0.06569353491067886\n",
      "epoch: 30 step: 597, loss is 0.0024682977236807346\n",
      "epoch: 30 step: 598, loss is 0.0021647014655172825\n",
      "epoch: 30 step: 599, loss is 0.0007212519412860274\n",
      "epoch: 30 step: 600, loss is 0.031341303139925\n",
      "epoch: 30 step: 601, loss is 0.07486854493618011\n",
      "epoch: 30 step: 602, loss is 0.0005933220381848514\n",
      "epoch: 30 step: 603, loss is 0.008365736342966557\n",
      "epoch: 30 step: 604, loss is 0.06945029646158218\n",
      "epoch: 30 step: 605, loss is 0.03435056284070015\n",
      "epoch: 30 step: 606, loss is 0.05293136462569237\n",
      "epoch: 30 step: 607, loss is 0.00025746773462742567\n",
      "epoch: 30 step: 608, loss is 0.09901215136051178\n",
      "epoch: 30 step: 609, loss is 0.08084744960069656\n",
      "epoch: 30 step: 610, loss is 0.06001029908657074\n",
      "epoch: 30 step: 611, loss is 0.013694606721401215\n",
      "epoch: 30 step: 612, loss is 0.05282682925462723\n",
      "epoch: 30 step: 613, loss is 0.0002478918177075684\n",
      "epoch: 30 step: 614, loss is 0.04849778115749359\n",
      "epoch: 30 step: 615, loss is 0.0004957787459716201\n",
      "epoch: 30 step: 616, loss is 0.01915581524372101\n",
      "epoch: 30 step: 617, loss is 0.019700895994901657\n",
      "epoch: 30 step: 618, loss is 0.011420994065701962\n",
      "epoch: 30 step: 619, loss is 0.12184001505374908\n",
      "epoch: 30 step: 620, loss is 0.015399663709104061\n",
      "epoch: 30 step: 621, loss is 0.006793798878788948\n",
      "epoch: 30 step: 622, loss is 0.0027693447191268206\n",
      "epoch: 30 step: 623, loss is 9.71940899034962e-05\n",
      "epoch: 30 step: 624, loss is 0.01817982643842697\n",
      "epoch: 30 step: 625, loss is 0.00020478037185966969\n",
      "epoch: 30 step: 626, loss is 0.0005080481059849262\n",
      "epoch: 30 step: 627, loss is 0.001309658051468432\n",
      "epoch: 30 step: 628, loss is 0.0013830952811986208\n",
      "epoch: 30 step: 629, loss is 0.05413014814257622\n",
      "epoch: 30 step: 630, loss is 0.0046852692030370235\n",
      "epoch: 30 step: 631, loss is 0.009761979803442955\n",
      "epoch: 30 step: 632, loss is 0.0012156866723671556\n",
      "epoch: 30 step: 633, loss is 0.014686629176139832\n",
      "epoch: 30 step: 634, loss is 0.02189326286315918\n",
      "epoch: 30 step: 635, loss is 0.006067583803087473\n",
      "epoch: 30 step: 636, loss is 0.04317580908536911\n",
      "epoch: 30 step: 637, loss is 0.0018679490312933922\n",
      "epoch: 30 step: 638, loss is 0.060687001794576645\n",
      "epoch: 30 step: 639, loss is 0.0010234082583338022\n",
      "epoch: 30 step: 640, loss is 0.02845994383096695\n",
      "epoch: 30 step: 641, loss is 0.0012093898840248585\n",
      "epoch: 30 step: 642, loss is 0.011681877076625824\n",
      "epoch: 30 step: 643, loss is 0.0028208233416080475\n",
      "epoch: 30 step: 644, loss is 0.01299524586647749\n",
      "epoch: 30 step: 645, loss is 0.00466286251321435\n",
      "epoch: 30 step: 646, loss is 0.000727226899471134\n",
      "epoch: 30 step: 647, loss is 0.0179105494171381\n",
      "epoch: 30 step: 648, loss is 0.005643214099109173\n",
      "epoch: 30 step: 649, loss is 0.00461043743416667\n",
      "epoch: 30 step: 650, loss is 0.0001299545547226444\n",
      "epoch: 30 step: 651, loss is 0.00020500364189501852\n",
      "epoch: 30 step: 652, loss is 0.0038246263284236193\n",
      "epoch: 30 step: 653, loss is 0.06357582658529282\n",
      "epoch: 30 step: 654, loss is 0.048239219933748245\n",
      "epoch: 30 step: 655, loss is 0.0031116781756281853\n",
      "epoch: 30 step: 656, loss is 0.062479328364133835\n",
      "epoch: 30 step: 657, loss is 0.0011961637064814568\n",
      "epoch: 30 step: 658, loss is 0.0001564551203045994\n",
      "epoch: 30 step: 659, loss is 0.0022995222825556993\n",
      "epoch: 30 step: 660, loss is 0.0008924864232540131\n",
      "epoch: 30 step: 661, loss is 0.006329716648906469\n",
      "epoch: 30 step: 662, loss is 0.00044539576629176736\n",
      "epoch: 30 step: 663, loss is 0.0076911840587854385\n",
      "epoch: 30 step: 664, loss is 0.025605276226997375\n",
      "epoch: 30 step: 665, loss is 0.0009935008129104972\n",
      "epoch: 30 step: 666, loss is 0.002220507711172104\n",
      "epoch: 30 step: 667, loss is 0.000397215539123863\n",
      "epoch: 30 step: 668, loss is 0.014126883819699287\n",
      "epoch: 30 step: 669, loss is 0.01629505306482315\n",
      "epoch: 30 step: 670, loss is 0.017713729292154312\n",
      "epoch: 30 step: 671, loss is 0.0028569716960191727\n",
      "epoch: 30 step: 672, loss is 0.0018940167501568794\n",
      "epoch: 30 step: 673, loss is 0.0005888523301109672\n",
      "epoch: 30 step: 674, loss is 0.0020805583335459232\n",
      "epoch: 30 step: 675, loss is 0.05877881497144699\n",
      "epoch: 30 step: 676, loss is 0.09743978083133698\n",
      "epoch: 30 step: 677, loss is 0.005400256719440222\n",
      "epoch: 30 step: 678, loss is 0.005291551351547241\n",
      "epoch: 30 step: 679, loss is 2.6792016797116958e-05\n",
      "epoch: 30 step: 680, loss is 0.06491537392139435\n",
      "epoch: 30 step: 681, loss is 0.03315209224820137\n",
      "epoch: 30 step: 682, loss is 0.0008814094471745193\n",
      "epoch: 30 step: 683, loss is 0.05342337489128113\n",
      "epoch: 30 step: 684, loss is 0.009307011030614376\n",
      "epoch: 30 step: 685, loss is 0.00866322685033083\n",
      "epoch: 30 step: 686, loss is 0.0009703905670903623\n",
      "epoch: 30 step: 687, loss is 0.0284157395362854\n",
      "epoch: 30 step: 688, loss is 0.02071962133049965\n",
      "epoch: 30 step: 689, loss is 0.005354305263608694\n",
      "epoch: 30 step: 690, loss is 0.018026456236839294\n",
      "epoch: 30 step: 691, loss is 0.020174380391836166\n",
      "epoch: 30 step: 692, loss is 0.00634200032800436\n",
      "epoch: 30 step: 693, loss is 0.033034782856702805\n",
      "epoch: 30 step: 694, loss is 0.01405242457985878\n",
      "epoch: 30 step: 695, loss is 0.000957307405769825\n",
      "epoch: 30 step: 696, loss is 0.03790000081062317\n",
      "epoch: 30 step: 697, loss is 0.006217064801603556\n",
      "epoch: 30 step: 698, loss is 0.005015213508158922\n",
      "epoch: 30 step: 699, loss is 0.07612280547618866\n",
      "epoch: 30 step: 700, loss is 0.0013553457101806998\n",
      "epoch: 30 step: 701, loss is 0.048054762184619904\n",
      "epoch: 30 step: 702, loss is 0.0132548026740551\n",
      "epoch: 30 step: 703, loss is 0.025475982576608658\n",
      "epoch: 30 step: 704, loss is 8.410480950260535e-05\n",
      "epoch: 30 step: 705, loss is 0.00456701684743166\n",
      "epoch: 30 step: 706, loss is 0.00029815061134286225\n",
      "epoch: 30 step: 707, loss is 0.0007323831086978316\n",
      "epoch: 30 step: 708, loss is 0.008554376661777496\n",
      "epoch: 30 step: 709, loss is 0.00390773918479681\n",
      "epoch: 30 step: 710, loss is 0.0010845664655789733\n",
      "epoch: 30 step: 711, loss is 0.03218838572502136\n",
      "epoch: 30 step: 712, loss is 0.01561284065246582\n",
      "epoch: 30 step: 713, loss is 0.0033112838864326477\n",
      "epoch: 30 step: 714, loss is 0.011471983976662159\n",
      "epoch: 30 step: 715, loss is 0.007781656458973885\n",
      "epoch: 30 step: 716, loss is 0.0041008275002241135\n",
      "epoch: 30 step: 717, loss is 0.00015852607612032443\n",
      "epoch: 30 step: 718, loss is 0.0009705423726700246\n",
      "epoch: 30 step: 719, loss is 0.009392265230417252\n",
      "epoch: 30 step: 720, loss is 0.154403954744339\n",
      "epoch: 30 step: 721, loss is 0.0009118052548728883\n",
      "epoch: 30 step: 722, loss is 5.897081791772507e-05\n",
      "epoch: 30 step: 723, loss is 0.0010383420158177614\n",
      "epoch: 30 step: 724, loss is 0.003954785875976086\n",
      "epoch: 30 step: 725, loss is 0.04216992110013962\n",
      "epoch: 30 step: 726, loss is 6.020350156177301e-06\n",
      "epoch: 30 step: 727, loss is 0.016695227473974228\n",
      "epoch: 30 step: 728, loss is 0.008641569875180721\n",
      "epoch: 30 step: 729, loss is 0.025859862565994263\n",
      "epoch: 30 step: 730, loss is 0.003861421952024102\n",
      "epoch: 30 step: 731, loss is 0.0001375785213895142\n",
      "epoch: 30 step: 732, loss is 0.001800112659111619\n",
      "epoch: 30 step: 733, loss is 0.02102787047624588\n",
      "epoch: 30 step: 734, loss is 0.00848032720386982\n",
      "epoch: 30 step: 735, loss is 0.008996429853141308\n",
      "epoch: 30 step: 736, loss is 0.02705966681241989\n",
      "epoch: 30 step: 737, loss is 0.002209671074524522\n",
      "epoch: 30 step: 738, loss is 0.0004647758323699236\n",
      "epoch: 30 step: 739, loss is 0.0882054790854454\n",
      "epoch: 30 step: 740, loss is 0.06531070917844772\n",
      "epoch: 30 step: 741, loss is 0.0014041096437722445\n",
      "epoch: 30 step: 742, loss is 0.0009453047532588243\n",
      "epoch: 30 step: 743, loss is 0.012196002528071404\n",
      "epoch: 30 step: 744, loss is 0.0019556372426450253\n",
      "epoch: 30 step: 745, loss is 0.018194997683167458\n",
      "epoch: 30 step: 746, loss is 0.002659884048625827\n",
      "epoch: 30 step: 747, loss is 0.00010067909170174971\n",
      "epoch: 30 step: 748, loss is 0.00612794142216444\n",
      "epoch: 30 step: 749, loss is 0.0020444230176508427\n",
      "epoch: 30 step: 750, loss is 0.001378805609419942\n",
      "epoch: 30 step: 751, loss is 0.0016171232564374804\n",
      "epoch: 30 step: 752, loss is 2.754919250946841e-06\n",
      "epoch: 30 step: 753, loss is 0.00018063880270347\n",
      "epoch: 30 step: 754, loss is 0.01926845870912075\n",
      "epoch: 30 step: 755, loss is 0.002269892953336239\n",
      "epoch: 30 step: 756, loss is 0.026848098263144493\n",
      "epoch: 30 step: 757, loss is 0.001868403167463839\n",
      "epoch: 30 step: 758, loss is 0.0023787652608007193\n",
      "epoch: 30 step: 759, loss is 0.04982103407382965\n",
      "epoch: 30 step: 760, loss is 0.0008400416700169444\n",
      "epoch: 30 step: 761, loss is 0.03911932185292244\n",
      "epoch: 30 step: 762, loss is 0.0005033466150052845\n",
      "epoch: 30 step: 763, loss is 0.06707514822483063\n",
      "epoch: 30 step: 764, loss is 0.02969331294298172\n",
      "epoch: 30 step: 765, loss is 0.004571821074932814\n",
      "epoch: 30 step: 766, loss is 0.0005750862183049321\n",
      "epoch: 30 step: 767, loss is 0.13546516001224518\n",
      "epoch: 30 step: 768, loss is 0.013084585778415203\n",
      "epoch: 30 step: 769, loss is 0.010086612775921822\n",
      "epoch: 30 step: 770, loss is 0.00167913269251585\n",
      "epoch: 30 step: 771, loss is 0.031159499660134315\n",
      "epoch: 30 step: 772, loss is 0.0036020984407514334\n",
      "epoch: 30 step: 773, loss is 0.0002635263663250953\n",
      "epoch: 30 step: 774, loss is 0.0880969911813736\n",
      "epoch: 30 step: 775, loss is 1.0657173334038816e-05\n",
      "epoch: 30 step: 776, loss is 0.005477966740727425\n",
      "epoch: 30 step: 777, loss is 0.01274645235389471\n",
      "epoch: 30 step: 778, loss is 0.007006674073636532\n",
      "epoch: 30 step: 779, loss is 0.14054538309574127\n",
      "epoch: 30 step: 780, loss is 0.0009693145984783769\n",
      "epoch: 30 step: 781, loss is 0.00018525932682678103\n",
      "epoch: 30 step: 782, loss is 0.05887623131275177\n",
      "epoch: 30 step: 783, loss is 0.003017234615981579\n",
      "epoch: 30 step: 784, loss is 0.005142752081155777\n",
      "epoch: 30 step: 785, loss is 0.0019238556269556284\n",
      "epoch: 30 step: 786, loss is 0.0055819321423769\n",
      "epoch: 30 step: 787, loss is 0.010763159953057766\n",
      "epoch: 30 step: 788, loss is 0.06827308982610703\n",
      "epoch: 30 step: 789, loss is 0.0011720871552824974\n",
      "epoch: 30 step: 790, loss is 0.0005828772555105388\n",
      "epoch: 30 step: 791, loss is 0.01227439846843481\n",
      "epoch: 30 step: 792, loss is 0.0034985546953976154\n",
      "epoch: 30 step: 793, loss is 0.04670296981930733\n",
      "epoch: 30 step: 794, loss is 0.07960284501314163\n",
      "epoch: 30 step: 795, loss is 0.005015193484723568\n",
      "epoch: 30 step: 796, loss is 0.007777445018291473\n",
      "epoch: 30 step: 797, loss is 6.911116361152381e-05\n",
      "epoch: 30 step: 798, loss is 0.04881540685892105\n",
      "epoch: 30 step: 799, loss is 0.00450672535225749\n",
      "epoch: 30 step: 800, loss is 0.09679880738258362\n",
      "epoch: 30 step: 801, loss is 0.0032374351285398006\n",
      "epoch: 30 step: 802, loss is 0.001689260476268828\n",
      "epoch: 30 step: 803, loss is 0.014466499909758568\n",
      "epoch: 30 step: 804, loss is 0.026307998225092888\n",
      "epoch: 30 step: 805, loss is 0.01708407513797283\n",
      "epoch: 30 step: 806, loss is 0.0002761264331638813\n",
      "epoch: 30 step: 807, loss is 0.0013107334962114692\n",
      "epoch: 30 step: 808, loss is 0.11266137659549713\n",
      "epoch: 30 step: 809, loss is 0.001502402825281024\n",
      "epoch: 30 step: 810, loss is 0.002388319931924343\n",
      "epoch: 30 step: 811, loss is 0.00016951607540249825\n",
      "epoch: 30 step: 812, loss is 0.085589200258255\n",
      "epoch: 30 step: 813, loss is 0.0014437929494306445\n",
      "epoch: 30 step: 814, loss is 0.016875531524419785\n",
      "epoch: 30 step: 815, loss is 0.048663124442100525\n",
      "epoch: 30 step: 816, loss is 0.0006333201890811324\n",
      "epoch: 30 step: 817, loss is 0.07278021425008774\n",
      "epoch: 30 step: 818, loss is 0.0059465086087584496\n",
      "epoch: 30 step: 819, loss is 0.04060661420226097\n",
      "epoch: 30 step: 820, loss is 0.0031783387530595064\n",
      "epoch: 30 step: 821, loss is 0.000880531850270927\n",
      "epoch: 30 step: 822, loss is 0.08852563053369522\n",
      "epoch: 30 step: 823, loss is 0.021685995161533356\n",
      "epoch: 30 step: 824, loss is 0.08092678338289261\n",
      "epoch: 30 step: 825, loss is 0.0018878295086324215\n",
      "epoch: 30 step: 826, loss is 0.010077586397528648\n",
      "epoch: 30 step: 827, loss is 0.009953399188816547\n",
      "epoch: 30 step: 828, loss is 0.0005737301544286311\n",
      "epoch: 30 step: 829, loss is 0.06946274638175964\n",
      "epoch: 30 step: 830, loss is 0.060059644281864166\n",
      "epoch: 30 step: 831, loss is 0.0032521854154765606\n",
      "epoch: 30 step: 832, loss is 0.12229639291763306\n",
      "epoch: 30 step: 833, loss is 0.0202566459774971\n",
      "epoch: 30 step: 834, loss is 0.008180348202586174\n",
      "epoch: 30 step: 835, loss is 0.0007360549061559141\n",
      "epoch: 30 step: 836, loss is 0.0006576420273631811\n",
      "epoch: 30 step: 837, loss is 0.0009279554942622781\n",
      "epoch: 30 step: 838, loss is 0.10933268815279007\n",
      "epoch: 30 step: 839, loss is 0.059821099042892456\n",
      "epoch: 30 step: 840, loss is 0.00012188656546641141\n",
      "epoch: 30 step: 841, loss is 0.005994169972836971\n",
      "epoch: 30 step: 842, loss is 0.04282156750559807\n",
      "epoch: 30 step: 843, loss is 0.00977101270109415\n",
      "epoch: 30 step: 844, loss is 0.004509514197707176\n",
      "epoch: 30 step: 845, loss is 0.00019554175378289074\n",
      "epoch: 30 step: 846, loss is 0.018180035054683685\n",
      "epoch: 30 step: 847, loss is 0.04405355826020241\n",
      "epoch: 30 step: 848, loss is 0.009151119738817215\n",
      "epoch: 30 step: 849, loss is 0.3168281614780426\n",
      "epoch: 30 step: 850, loss is 0.12426269799470901\n",
      "epoch: 30 step: 851, loss is 0.0009367411839775741\n",
      "epoch: 30 step: 852, loss is 0.0015782690607011318\n",
      "epoch: 30 step: 853, loss is 0.0008491025655530393\n",
      "epoch: 30 step: 854, loss is 0.0029732640832662582\n",
      "epoch: 30 step: 855, loss is 0.013690434396266937\n",
      "epoch: 30 step: 856, loss is 0.006683070212602615\n",
      "epoch: 30 step: 857, loss is 0.0011494011851027608\n",
      "epoch: 30 step: 858, loss is 0.004892495460808277\n",
      "epoch: 30 step: 859, loss is 0.016764523461461067\n",
      "epoch: 30 step: 860, loss is 0.005980291869491339\n",
      "epoch: 30 step: 861, loss is 0.07014428079128265\n",
      "epoch: 30 step: 862, loss is 0.012723403051495552\n",
      "epoch: 30 step: 863, loss is 0.015534603036940098\n",
      "epoch: 30 step: 864, loss is 0.004947193898260593\n",
      "epoch: 30 step: 865, loss is 0.016335932537913322\n",
      "epoch: 30 step: 866, loss is 0.0026831282302737236\n",
      "epoch: 30 step: 867, loss is 0.0017077018273994327\n",
      "epoch: 30 step: 868, loss is 0.00031623084214515984\n",
      "epoch: 30 step: 869, loss is 0.00040218871436081827\n",
      "epoch: 30 step: 870, loss is 0.013803957030177116\n",
      "epoch: 30 step: 871, loss is 0.024295298382639885\n",
      "epoch: 30 step: 872, loss is 0.005671611987054348\n",
      "epoch: 30 step: 873, loss is 0.004319251514971256\n",
      "epoch: 30 step: 874, loss is 0.012880206108093262\n",
      "epoch: 30 step: 875, loss is 0.04240681231021881\n",
      "epoch: 30 step: 876, loss is 0.010928574949502945\n",
      "epoch: 30 step: 877, loss is 0.04373740032315254\n",
      "epoch: 30 step: 878, loss is 0.0011254817945882678\n",
      "epoch: 30 step: 879, loss is 0.060850776731967926\n",
      "epoch: 30 step: 880, loss is 0.0028789418283849955\n",
      "epoch: 30 step: 881, loss is 0.00046562819625250995\n",
      "epoch: 30 step: 882, loss is 0.0056177182123064995\n",
      "epoch: 30 step: 883, loss is 0.002114300848916173\n",
      "epoch: 30 step: 884, loss is 0.0613219253718853\n",
      "epoch: 30 step: 885, loss is 2.6171226636506617e-05\n",
      "epoch: 30 step: 886, loss is 0.009571211412549019\n",
      "epoch: 30 step: 887, loss is 0.010497357696294785\n",
      "epoch: 30 step: 888, loss is 0.004503494128584862\n",
      "epoch: 30 step: 889, loss is 0.012814217247068882\n",
      "epoch: 30 step: 890, loss is 0.0019374820403754711\n",
      "epoch: 30 step: 891, loss is 0.0007515038596466184\n",
      "epoch: 30 step: 892, loss is 0.00030109097133390605\n",
      "epoch: 30 step: 893, loss is 0.0010102170053869486\n",
      "epoch: 30 step: 894, loss is 0.0004785743076354265\n",
      "epoch: 30 step: 895, loss is 0.0002115740062436089\n",
      "epoch: 30 step: 896, loss is 0.005336563102900982\n",
      "epoch: 30 step: 897, loss is 0.02980375476181507\n",
      "epoch: 30 step: 898, loss is 0.00061987922526896\n",
      "epoch: 30 step: 899, loss is 0.08502248674631119\n",
      "epoch: 30 step: 900, loss is 0.000919863348826766\n",
      "epoch: 30 step: 901, loss is 0.0017921942053362727\n",
      "epoch: 30 step: 902, loss is 0.014940546825528145\n",
      "epoch: 30 step: 903, loss is 0.002858560299500823\n",
      "epoch: 30 step: 904, loss is 0.02520040050148964\n",
      "epoch: 30 step: 905, loss is 0.00624094670638442\n",
      "epoch: 30 step: 906, loss is 0.13654139637947083\n",
      "epoch: 30 step: 907, loss is 0.005732867866754532\n",
      "epoch: 30 step: 908, loss is 0.00769115099683404\n",
      "epoch: 30 step: 909, loss is 0.0001875938760349527\n",
      "epoch: 30 step: 910, loss is 0.00018720049411058426\n",
      "epoch: 30 step: 911, loss is 0.07901335507631302\n",
      "epoch: 30 step: 912, loss is 0.011903123930096626\n",
      "epoch: 30 step: 913, loss is 0.09029388427734375\n",
      "epoch: 30 step: 914, loss is 9.971068357117474e-05\n",
      "epoch: 30 step: 915, loss is 0.003979478497058153\n",
      "epoch: 30 step: 916, loss is 0.006705249194055796\n",
      "epoch: 30 step: 917, loss is 0.002933692652732134\n",
      "epoch: 30 step: 918, loss is 0.10443998873233795\n",
      "epoch: 30 step: 919, loss is 0.0037495887372642756\n",
      "epoch: 30 step: 920, loss is 0.0742524042725563\n",
      "epoch: 30 step: 921, loss is 0.016626084223389626\n",
      "epoch: 30 step: 922, loss is 0.007896747440099716\n",
      "epoch: 30 step: 923, loss is 0.0001163322594948113\n",
      "epoch: 30 step: 924, loss is 0.0003479039005469531\n",
      "epoch: 30 step: 925, loss is 0.002006360562518239\n",
      "epoch: 30 step: 926, loss is 2.408726140856743e-05\n",
      "epoch: 30 step: 927, loss is 0.07631285488605499\n",
      "epoch: 30 step: 928, loss is 0.0036050709895789623\n",
      "epoch: 30 step: 929, loss is 0.20681564509868622\n",
      "epoch: 30 step: 930, loss is 0.0006247213459573686\n",
      "epoch: 30 step: 931, loss is 0.023646468296647072\n",
      "epoch: 30 step: 932, loss is 0.009417058899998665\n",
      "epoch: 30 step: 933, loss is 5.907345985178836e-05\n",
      "epoch: 30 step: 934, loss is 0.0014293321873992682\n",
      "epoch: 30 step: 935, loss is 0.008793040178716183\n",
      "epoch: 30 step: 936, loss is 0.038763102144002914\n",
      "epoch: 30 step: 937, loss is 0.04481698200106621\n",
      "{'acc': 0.9091546474358975}\n"
     ]
    }
   ],
   "source": [
    "# 训练无正则化的网络\n",
    "model = train(ForwardFashion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11628:33592,MainProcess):2023-05-07-23:13:09.651.305 [mindspore\\nn\\layer\\basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 2.3033084869384766\n",
      "epoch: 1 step: 2, loss is 2.2912983894348145\n",
      "epoch: 1 step: 3, loss is 2.259307622909546\n",
      "epoch: 1 step: 4, loss is 2.2221856117248535\n",
      "epoch: 1 step: 5, loss is 2.155104875564575\n",
      "epoch: 1 step: 6, loss is 2.0913286209106445\n",
      "epoch: 1 step: 7, loss is 2.0250165462493896\n",
      "epoch: 1 step: 8, loss is 2.0342304706573486\n",
      "epoch: 1 step: 9, loss is 2.0312376022338867\n",
      "epoch: 1 step: 10, loss is 1.9301564693450928\n",
      "epoch: 1 step: 11, loss is 1.9516725540161133\n",
      "epoch: 1 step: 12, loss is 1.8849729299545288\n",
      "epoch: 1 step: 13, loss is 1.8157083988189697\n",
      "epoch: 1 step: 14, loss is 1.8661521673202515\n",
      "epoch: 1 step: 15, loss is 1.786258578300476\n",
      "epoch: 1 step: 16, loss is 1.760751485824585\n",
      "epoch: 1 step: 17, loss is 1.739661455154419\n",
      "epoch: 1 step: 18, loss is 1.773848056793213\n",
      "epoch: 1 step: 19, loss is 1.6044656038284302\n",
      "epoch: 1 step: 20, loss is 1.6934984922409058\n",
      "epoch: 1 step: 21, loss is 1.6572368144989014\n",
      "epoch: 1 step: 22, loss is 1.5966156721115112\n",
      "epoch: 1 step: 23, loss is 1.6660839319229126\n",
      "epoch: 1 step: 24, loss is 1.4997341632843018\n",
      "epoch: 1 step: 25, loss is 1.5217690467834473\n",
      "epoch: 1 step: 26, loss is 1.4932681322097778\n",
      "epoch: 1 step: 27, loss is 1.5374207496643066\n",
      "epoch: 1 step: 28, loss is 1.573889970779419\n",
      "epoch: 1 step: 29, loss is 1.3751357793807983\n",
      "epoch: 1 step: 30, loss is 1.443774700164795\n",
      "epoch: 1 step: 31, loss is 1.5377665758132935\n",
      "epoch: 1 step: 32, loss is 1.3386508226394653\n",
      "epoch: 1 step: 33, loss is 1.3873200416564941\n",
      "epoch: 1 step: 34, loss is 1.3323155641555786\n",
      "epoch: 1 step: 35, loss is 1.328580379486084\n",
      "epoch: 1 step: 36, loss is 1.272659420967102\n",
      "epoch: 1 step: 37, loss is 1.319295883178711\n",
      "epoch: 1 step: 38, loss is 1.40371835231781\n",
      "epoch: 1 step: 39, loss is 1.24977707862854\n",
      "epoch: 1 step: 40, loss is 1.33805251121521\n",
      "epoch: 1 step: 41, loss is 1.3094146251678467\n",
      "epoch: 1 step: 42, loss is 1.2906441688537598\n",
      "epoch: 1 step: 43, loss is 1.3015180826187134\n",
      "epoch: 1 step: 44, loss is 1.364505648612976\n",
      "epoch: 1 step: 45, loss is 1.2887588739395142\n",
      "epoch: 1 step: 46, loss is 1.1408846378326416\n",
      "epoch: 1 step: 47, loss is 1.1661521196365356\n",
      "epoch: 1 step: 48, loss is 1.3580762147903442\n",
      "epoch: 1 step: 49, loss is 1.1376214027404785\n",
      "epoch: 1 step: 50, loss is 1.0001848936080933\n",
      "epoch: 1 step: 51, loss is 0.9973628520965576\n",
      "epoch: 1 step: 52, loss is 1.2404814958572388\n",
      "epoch: 1 step: 53, loss is 1.1013493537902832\n",
      "epoch: 1 step: 54, loss is 1.0861176252365112\n",
      "epoch: 1 step: 55, loss is 0.9640684723854065\n",
      "epoch: 1 step: 56, loss is 1.004626750946045\n",
      "epoch: 1 step: 57, loss is 0.9912627935409546\n",
      "epoch: 1 step: 58, loss is 1.0726871490478516\n",
      "epoch: 1 step: 59, loss is 1.0653343200683594\n",
      "epoch: 1 step: 60, loss is 1.0772618055343628\n",
      "epoch: 1 step: 61, loss is 1.0166747570037842\n",
      "epoch: 1 step: 62, loss is 1.0234959125518799\n",
      "epoch: 1 step: 63, loss is 0.9524876475334167\n",
      "epoch: 1 step: 64, loss is 0.9604887962341309\n",
      "epoch: 1 step: 65, loss is 0.9433775544166565\n",
      "epoch: 1 step: 66, loss is 1.0697883367538452\n",
      "epoch: 1 step: 67, loss is 1.0241525173187256\n",
      "epoch: 1 step: 68, loss is 0.873054563999176\n",
      "epoch: 1 step: 69, loss is 0.9699782729148865\n",
      "epoch: 1 step: 70, loss is 0.763526976108551\n",
      "epoch: 1 step: 71, loss is 1.0174530744552612\n",
      "epoch: 1 step: 72, loss is 1.0127822160720825\n",
      "epoch: 1 step: 73, loss is 0.8307448625564575\n",
      "epoch: 1 step: 74, loss is 0.8721055388450623\n",
      "epoch: 1 step: 75, loss is 0.9340368509292603\n",
      "epoch: 1 step: 76, loss is 0.8403089642524719\n",
      "epoch: 1 step: 77, loss is 0.957640528678894\n",
      "epoch: 1 step: 78, loss is 0.825240433216095\n",
      "epoch: 1 step: 79, loss is 0.8931412696838379\n",
      "epoch: 1 step: 80, loss is 0.7089846730232239\n",
      "epoch: 1 step: 81, loss is 0.6566417217254639\n",
      "epoch: 1 step: 82, loss is 0.9707500338554382\n",
      "epoch: 1 step: 83, loss is 0.8724446296691895\n",
      "epoch: 1 step: 84, loss is 0.7740784883499146\n",
      "epoch: 1 step: 85, loss is 0.8653754591941833\n",
      "epoch: 1 step: 86, loss is 0.8466867804527283\n",
      "epoch: 1 step: 87, loss is 0.7855318188667297\n",
      "epoch: 1 step: 88, loss is 0.878181517124176\n",
      "epoch: 1 step: 89, loss is 0.8143012523651123\n",
      "epoch: 1 step: 90, loss is 0.7227540016174316\n",
      "epoch: 1 step: 91, loss is 0.7747106552124023\n",
      "epoch: 1 step: 92, loss is 0.7587388157844543\n",
      "epoch: 1 step: 93, loss is 0.7814462780952454\n",
      "epoch: 1 step: 94, loss is 0.6400535702705383\n",
      "epoch: 1 step: 95, loss is 0.7609431147575378\n",
      "epoch: 1 step: 96, loss is 0.8851820230484009\n",
      "epoch: 1 step: 97, loss is 0.9295158386230469\n",
      "epoch: 1 step: 98, loss is 0.8228305578231812\n",
      "epoch: 1 step: 99, loss is 0.759220540523529\n",
      "epoch: 1 step: 100, loss is 0.704814612865448\n",
      "epoch: 1 step: 101, loss is 0.8876160383224487\n",
      "epoch: 1 step: 102, loss is 0.8561869263648987\n",
      "epoch: 1 step: 103, loss is 0.8341086506843567\n",
      "epoch: 1 step: 104, loss is 0.6117450594902039\n",
      "epoch: 1 step: 105, loss is 0.6558822989463806\n",
      "epoch: 1 step: 106, loss is 0.8235476016998291\n",
      "epoch: 1 step: 107, loss is 0.6960670948028564\n",
      "epoch: 1 step: 108, loss is 0.6287662386894226\n",
      "epoch: 1 step: 109, loss is 0.7319211959838867\n",
      "epoch: 1 step: 110, loss is 0.7534497976303101\n",
      "epoch: 1 step: 111, loss is 0.9588432312011719\n",
      "epoch: 1 step: 112, loss is 0.7587708234786987\n",
      "epoch: 1 step: 113, loss is 0.6469250917434692\n",
      "epoch: 1 step: 114, loss is 0.7840585112571716\n",
      "epoch: 1 step: 115, loss is 0.6926997303962708\n",
      "epoch: 1 step: 116, loss is 0.8590111136436462\n",
      "epoch: 1 step: 117, loss is 0.8296195864677429\n",
      "epoch: 1 step: 118, loss is 0.650941789150238\n",
      "epoch: 1 step: 119, loss is 0.8673703670501709\n",
      "epoch: 1 step: 120, loss is 0.7355528473854065\n",
      "epoch: 1 step: 121, loss is 0.5662399530410767\n",
      "epoch: 1 step: 122, loss is 0.8082036972045898\n",
      "epoch: 1 step: 123, loss is 0.5874025225639343\n",
      "epoch: 1 step: 124, loss is 0.7389861345291138\n",
      "epoch: 1 step: 125, loss is 0.6466505527496338\n",
      "epoch: 1 step: 126, loss is 0.5960373282432556\n",
      "epoch: 1 step: 127, loss is 0.6998077630996704\n",
      "epoch: 1 step: 128, loss is 0.6685923337936401\n",
      "epoch: 1 step: 129, loss is 0.6114891767501831\n",
      "epoch: 1 step: 130, loss is 0.797447919845581\n",
      "epoch: 1 step: 131, loss is 0.6984149217605591\n",
      "epoch: 1 step: 132, loss is 0.8001754283905029\n",
      "epoch: 1 step: 133, loss is 0.7029373049736023\n",
      "epoch: 1 step: 134, loss is 0.7115188241004944\n",
      "epoch: 1 step: 135, loss is 0.9788309335708618\n",
      "epoch: 1 step: 136, loss is 0.9530706405639648\n",
      "epoch: 1 step: 137, loss is 0.8870624303817749\n",
      "epoch: 1 step: 138, loss is 0.7616801261901855\n",
      "epoch: 1 step: 139, loss is 0.7294917106628418\n",
      "epoch: 1 step: 140, loss is 0.8438906669616699\n",
      "epoch: 1 step: 141, loss is 0.6031439900398254\n",
      "epoch: 1 step: 142, loss is 0.8122309446334839\n",
      "epoch: 1 step: 143, loss is 0.8399403691291809\n",
      "epoch: 1 step: 144, loss is 0.8564499616622925\n",
      "epoch: 1 step: 145, loss is 0.6262512803077698\n",
      "epoch: 1 step: 146, loss is 0.786774754524231\n",
      "epoch: 1 step: 147, loss is 0.6523295640945435\n",
      "epoch: 1 step: 148, loss is 0.7198846340179443\n",
      "epoch: 1 step: 149, loss is 0.8141584992408752\n",
      "epoch: 1 step: 150, loss is 0.6595156192779541\n",
      "epoch: 1 step: 151, loss is 0.6695541143417358\n",
      "epoch: 1 step: 152, loss is 0.5836772918701172\n",
      "epoch: 1 step: 153, loss is 0.5856205821037292\n",
      "epoch: 1 step: 154, loss is 0.5107686519622803\n",
      "epoch: 1 step: 155, loss is 0.48364150524139404\n",
      "epoch: 1 step: 156, loss is 0.658270001411438\n",
      "epoch: 1 step: 157, loss is 0.6998368501663208\n",
      "epoch: 1 step: 158, loss is 0.7941182851791382\n",
      "epoch: 1 step: 159, loss is 0.6328625082969666\n",
      "epoch: 1 step: 160, loss is 0.7130305767059326\n",
      "epoch: 1 step: 161, loss is 0.6838651299476624\n",
      "epoch: 1 step: 162, loss is 0.709292471408844\n",
      "epoch: 1 step: 163, loss is 0.5308914184570312\n",
      "epoch: 1 step: 164, loss is 0.7070664167404175\n",
      "epoch: 1 step: 165, loss is 0.6649038195610046\n",
      "epoch: 1 step: 166, loss is 0.6188201308250427\n",
      "epoch: 1 step: 167, loss is 0.7472101449966431\n",
      "epoch: 1 step: 168, loss is 0.612530529499054\n",
      "epoch: 1 step: 169, loss is 0.6692261695861816\n",
      "epoch: 1 step: 170, loss is 0.5163692235946655\n",
      "epoch: 1 step: 171, loss is 0.5419915914535522\n",
      "epoch: 1 step: 172, loss is 0.6891424059867859\n",
      "epoch: 1 step: 173, loss is 0.6409580707550049\n",
      "epoch: 1 step: 174, loss is 0.7549872994422913\n",
      "epoch: 1 step: 175, loss is 0.5249603986740112\n",
      "epoch: 1 step: 176, loss is 0.767436683177948\n",
      "epoch: 1 step: 177, loss is 0.603757381439209\n",
      "epoch: 1 step: 178, loss is 0.542860746383667\n",
      "epoch: 1 step: 179, loss is 0.5710679888725281\n",
      "epoch: 1 step: 180, loss is 0.6549072265625\n",
      "epoch: 1 step: 181, loss is 0.551700234413147\n",
      "epoch: 1 step: 182, loss is 0.5507374405860901\n",
      "epoch: 1 step: 183, loss is 0.7379090189933777\n",
      "epoch: 1 step: 184, loss is 0.642833411693573\n",
      "epoch: 1 step: 185, loss is 0.6766247749328613\n",
      "epoch: 1 step: 186, loss is 0.6621915102005005\n",
      "epoch: 1 step: 187, loss is 0.6149587631225586\n",
      "epoch: 1 step: 188, loss is 0.6028936505317688\n",
      "epoch: 1 step: 189, loss is 0.7830656170845032\n",
      "epoch: 1 step: 190, loss is 0.5776125192642212\n",
      "epoch: 1 step: 191, loss is 0.7033936977386475\n",
      "epoch: 1 step: 192, loss is 0.5591040253639221\n",
      "epoch: 1 step: 193, loss is 0.5839878916740417\n",
      "epoch: 1 step: 194, loss is 0.5799925327301025\n",
      "epoch: 1 step: 195, loss is 0.7992850542068481\n",
      "epoch: 1 step: 196, loss is 0.7050189971923828\n",
      "epoch: 1 step: 197, loss is 0.5430728793144226\n",
      "epoch: 1 step: 198, loss is 0.619609534740448\n",
      "epoch: 1 step: 199, loss is 0.6359764337539673\n",
      "epoch: 1 step: 200, loss is 0.668053925037384\n",
      "epoch: 1 step: 201, loss is 0.5079798102378845\n",
      "epoch: 1 step: 202, loss is 0.6858286261558533\n",
      "epoch: 1 step: 203, loss is 0.646307110786438\n",
      "epoch: 1 step: 204, loss is 0.520154595375061\n",
      "epoch: 1 step: 205, loss is 0.7261051535606384\n",
      "epoch: 1 step: 206, loss is 0.5530792474746704\n",
      "epoch: 1 step: 207, loss is 0.526218056678772\n",
      "epoch: 1 step: 208, loss is 0.6656922698020935\n",
      "epoch: 1 step: 209, loss is 0.6059851050376892\n",
      "epoch: 1 step: 210, loss is 0.6676272749900818\n",
      "epoch: 1 step: 211, loss is 0.5074433088302612\n",
      "epoch: 1 step: 212, loss is 0.640438973903656\n",
      "epoch: 1 step: 213, loss is 0.5861106514930725\n",
      "epoch: 1 step: 214, loss is 0.710961103439331\n",
      "epoch: 1 step: 215, loss is 0.4527831971645355\n",
      "epoch: 1 step: 216, loss is 0.5648279190063477\n",
      "epoch: 1 step: 217, loss is 0.5823121666908264\n",
      "epoch: 1 step: 218, loss is 0.6126566529273987\n",
      "epoch: 1 step: 219, loss is 0.43316659331321716\n",
      "epoch: 1 step: 220, loss is 0.6223984360694885\n",
      "epoch: 1 step: 221, loss is 0.5391267538070679\n",
      "epoch: 1 step: 222, loss is 0.4601510763168335\n",
      "epoch: 1 step: 223, loss is 0.5269326567649841\n",
      "epoch: 1 step: 224, loss is 0.6261268854141235\n",
      "epoch: 1 step: 225, loss is 0.5395373702049255\n",
      "epoch: 1 step: 226, loss is 0.7968310117721558\n",
      "epoch: 1 step: 227, loss is 0.5591614842414856\n",
      "epoch: 1 step: 228, loss is 0.5166530013084412\n",
      "epoch: 1 step: 229, loss is 0.6135626435279846\n",
      "epoch: 1 step: 230, loss is 0.6095688939094543\n",
      "epoch: 1 step: 231, loss is 0.5648043751716614\n",
      "epoch: 1 step: 232, loss is 0.5917198061943054\n",
      "epoch: 1 step: 233, loss is 0.5067534446716309\n",
      "epoch: 1 step: 234, loss is 0.40956971049308777\n",
      "epoch: 1 step: 235, loss is 0.5396709442138672\n",
      "epoch: 1 step: 236, loss is 0.7291741371154785\n",
      "epoch: 1 step: 237, loss is 0.5618541240692139\n",
      "epoch: 1 step: 238, loss is 0.47558316588401794\n",
      "epoch: 1 step: 239, loss is 0.6754003763198853\n",
      "epoch: 1 step: 240, loss is 0.5941951870918274\n",
      "epoch: 1 step: 241, loss is 0.5476987361907959\n",
      "epoch: 1 step: 242, loss is 0.7156100869178772\n",
      "epoch: 1 step: 243, loss is 0.5928297638893127\n",
      "epoch: 1 step: 244, loss is 0.5166337490081787\n",
      "epoch: 1 step: 245, loss is 0.4707415699958801\n",
      "epoch: 1 step: 246, loss is 0.44823703169822693\n",
      "epoch: 1 step: 247, loss is 0.6518151164054871\n",
      "epoch: 1 step: 248, loss is 0.630608856678009\n",
      "epoch: 1 step: 249, loss is 0.4552707076072693\n",
      "epoch: 1 step: 250, loss is 0.5897242426872253\n",
      "epoch: 1 step: 251, loss is 0.8042630553245544\n",
      "epoch: 1 step: 252, loss is 0.49604836106300354\n",
      "epoch: 1 step: 253, loss is 0.5128123760223389\n",
      "epoch: 1 step: 254, loss is 0.5060638785362244\n",
      "epoch: 1 step: 255, loss is 0.44010886549949646\n",
      "epoch: 1 step: 256, loss is 0.468508243560791\n",
      "epoch: 1 step: 257, loss is 0.5651213526725769\n",
      "epoch: 1 step: 258, loss is 0.4248429834842682\n",
      "epoch: 1 step: 259, loss is 0.6893313527107239\n",
      "epoch: 1 step: 260, loss is 0.7102922797203064\n",
      "epoch: 1 step: 261, loss is 0.7504362463951111\n",
      "epoch: 1 step: 262, loss is 0.42367038130760193\n",
      "epoch: 1 step: 263, loss is 0.5707444548606873\n",
      "epoch: 1 step: 264, loss is 0.5496528744697571\n",
      "epoch: 1 step: 265, loss is 0.4916542172431946\n",
      "epoch: 1 step: 266, loss is 0.6667388081550598\n",
      "epoch: 1 step: 267, loss is 0.49993863701820374\n",
      "epoch: 1 step: 268, loss is 0.4773944318294525\n",
      "epoch: 1 step: 269, loss is 0.38133686780929565\n",
      "epoch: 1 step: 270, loss is 0.6786631345748901\n",
      "epoch: 1 step: 271, loss is 0.41119083762168884\n",
      "epoch: 1 step: 272, loss is 0.580814003944397\n",
      "epoch: 1 step: 273, loss is 0.5578857660293579\n",
      "epoch: 1 step: 274, loss is 0.6514258980751038\n",
      "epoch: 1 step: 275, loss is 0.5075199007987976\n",
      "epoch: 1 step: 276, loss is 0.46577560901641846\n",
      "epoch: 1 step: 277, loss is 0.5571478605270386\n",
      "epoch: 1 step: 278, loss is 0.651921808719635\n",
      "epoch: 1 step: 279, loss is 0.44522982835769653\n",
      "epoch: 1 step: 280, loss is 0.5366873145103455\n",
      "epoch: 1 step: 281, loss is 0.49648231267929077\n",
      "epoch: 1 step: 282, loss is 0.5142693519592285\n",
      "epoch: 1 step: 283, loss is 0.5746228098869324\n",
      "epoch: 1 step: 284, loss is 0.6424062848091125\n",
      "epoch: 1 step: 285, loss is 0.531181812286377\n",
      "epoch: 1 step: 286, loss is 0.6775255799293518\n",
      "epoch: 1 step: 287, loss is 0.693869411945343\n",
      "epoch: 1 step: 288, loss is 0.5483437180519104\n",
      "epoch: 1 step: 289, loss is 0.6660967469215393\n",
      "epoch: 1 step: 290, loss is 0.530489981174469\n",
      "epoch: 1 step: 291, loss is 0.5510081052780151\n",
      "epoch: 1 step: 292, loss is 0.5852241516113281\n",
      "epoch: 1 step: 293, loss is 0.42095744609832764\n",
      "epoch: 1 step: 294, loss is 0.6249361038208008\n",
      "epoch: 1 step: 295, loss is 0.6760113835334778\n",
      "epoch: 1 step: 296, loss is 0.5190817713737488\n",
      "epoch: 1 step: 297, loss is 0.61269211769104\n",
      "epoch: 1 step: 298, loss is 0.753232479095459\n",
      "epoch: 1 step: 299, loss is 0.47659093141555786\n",
      "epoch: 1 step: 300, loss is 0.5440809726715088\n",
      "epoch: 1 step: 301, loss is 0.3832670748233795\n",
      "epoch: 1 step: 302, loss is 0.5513920187950134\n",
      "epoch: 1 step: 303, loss is 0.5945976376533508\n",
      "epoch: 1 step: 304, loss is 0.5823091864585876\n",
      "epoch: 1 step: 305, loss is 0.5831994414329529\n",
      "epoch: 1 step: 306, loss is 0.600879967212677\n",
      "epoch: 1 step: 307, loss is 0.6492664813995361\n",
      "epoch: 1 step: 308, loss is 0.6285352110862732\n",
      "epoch: 1 step: 309, loss is 0.6167533993721008\n",
      "epoch: 1 step: 310, loss is 0.6777517199516296\n",
      "epoch: 1 step: 311, loss is 0.7090703248977661\n",
      "epoch: 1 step: 312, loss is 0.496229350566864\n",
      "epoch: 1 step: 313, loss is 0.4635061025619507\n",
      "epoch: 1 step: 314, loss is 0.6523191332817078\n",
      "epoch: 1 step: 315, loss is 0.5687681436538696\n",
      "epoch: 1 step: 316, loss is 0.6184585094451904\n",
      "epoch: 1 step: 317, loss is 0.6597533822059631\n",
      "epoch: 1 step: 318, loss is 0.7834663391113281\n",
      "epoch: 1 step: 319, loss is 0.5996564030647278\n",
      "epoch: 1 step: 320, loss is 0.5914909243583679\n",
      "epoch: 1 step: 321, loss is 0.635174036026001\n",
      "epoch: 1 step: 322, loss is 0.5203193426132202\n",
      "epoch: 1 step: 323, loss is 0.6642624735832214\n",
      "epoch: 1 step: 324, loss is 0.4453216791152954\n",
      "epoch: 1 step: 325, loss is 0.621063768863678\n",
      "epoch: 1 step: 326, loss is 0.5496291518211365\n",
      "epoch: 1 step: 327, loss is 0.530159056186676\n",
      "epoch: 1 step: 328, loss is 0.6683912873268127\n",
      "epoch: 1 step: 329, loss is 0.8391059041023254\n",
      "epoch: 1 step: 330, loss is 0.5793076753616333\n",
      "epoch: 1 step: 331, loss is 0.5180012583732605\n",
      "epoch: 1 step: 332, loss is 0.45756906270980835\n",
      "epoch: 1 step: 333, loss is 0.43192926049232483\n",
      "epoch: 1 step: 334, loss is 0.5523061752319336\n",
      "epoch: 1 step: 335, loss is 0.5840752124786377\n",
      "epoch: 1 step: 336, loss is 0.4125686287879944\n",
      "epoch: 1 step: 337, loss is 0.5205383896827698\n",
      "epoch: 1 step: 338, loss is 0.6784340143203735\n",
      "epoch: 1 step: 339, loss is 0.429706335067749\n",
      "epoch: 1 step: 340, loss is 0.4856410622596741\n",
      "epoch: 1 step: 341, loss is 0.6654556393623352\n",
      "epoch: 1 step: 342, loss is 0.5392837524414062\n",
      "epoch: 1 step: 343, loss is 0.6060534715652466\n",
      "epoch: 1 step: 344, loss is 0.5259262323379517\n",
      "epoch: 1 step: 345, loss is 0.4925110340118408\n",
      "epoch: 1 step: 346, loss is 0.703264057636261\n",
      "epoch: 1 step: 347, loss is 0.5571931600570679\n",
      "epoch: 1 step: 348, loss is 0.5366581082344055\n",
      "epoch: 1 step: 349, loss is 0.4220070540904999\n",
      "epoch: 1 step: 350, loss is 0.4632472097873688\n",
      "epoch: 1 step: 351, loss is 0.4367982745170593\n",
      "epoch: 1 step: 352, loss is 0.593572735786438\n",
      "epoch: 1 step: 353, loss is 0.48649656772613525\n",
      "epoch: 1 step: 354, loss is 0.4865894317626953\n",
      "epoch: 1 step: 355, loss is 0.5157145857810974\n",
      "epoch: 1 step: 356, loss is 0.33764082193374634\n",
      "epoch: 1 step: 357, loss is 0.5180094242095947\n",
      "epoch: 1 step: 358, loss is 0.551102340221405\n",
      "epoch: 1 step: 359, loss is 0.4377753734588623\n",
      "epoch: 1 step: 360, loss is 0.6699396967887878\n",
      "epoch: 1 step: 361, loss is 0.6088426113128662\n",
      "epoch: 1 step: 362, loss is 0.3991941213607788\n",
      "epoch: 1 step: 363, loss is 0.3970833718776703\n",
      "epoch: 1 step: 364, loss is 0.43126288056373596\n",
      "epoch: 1 step: 365, loss is 0.5560365319252014\n",
      "epoch: 1 step: 366, loss is 0.5450569987297058\n",
      "epoch: 1 step: 367, loss is 0.4180944859981537\n",
      "epoch: 1 step: 368, loss is 0.5314127206802368\n",
      "epoch: 1 step: 369, loss is 0.6008134484291077\n",
      "epoch: 1 step: 370, loss is 0.45699021220207214\n",
      "epoch: 1 step: 371, loss is 0.2988181710243225\n",
      "epoch: 1 step: 372, loss is 0.6855711936950684\n",
      "epoch: 1 step: 373, loss is 0.48543795943260193\n",
      "epoch: 1 step: 374, loss is 0.4984549582004547\n",
      "epoch: 1 step: 375, loss is 0.4418046176433563\n",
      "epoch: 1 step: 376, loss is 0.5273146033287048\n",
      "epoch: 1 step: 377, loss is 0.5517125129699707\n",
      "epoch: 1 step: 378, loss is 0.5911495685577393\n",
      "epoch: 1 step: 379, loss is 0.5662381052970886\n",
      "epoch: 1 step: 380, loss is 0.5311065316200256\n",
      "epoch: 1 step: 381, loss is 0.5575304627418518\n",
      "epoch: 1 step: 382, loss is 0.3869251012802124\n",
      "epoch: 1 step: 383, loss is 0.31331369280815125\n",
      "epoch: 1 step: 384, loss is 0.46540695428848267\n",
      "epoch: 1 step: 385, loss is 0.5224447846412659\n",
      "epoch: 1 step: 386, loss is 0.5439213514328003\n",
      "epoch: 1 step: 387, loss is 0.38606539368629456\n",
      "epoch: 1 step: 388, loss is 0.4720461070537567\n",
      "epoch: 1 step: 389, loss is 0.48854872584342957\n",
      "epoch: 1 step: 390, loss is 0.5224863290786743\n",
      "epoch: 1 step: 391, loss is 0.5551239848136902\n",
      "epoch: 1 step: 392, loss is 0.4714564383029938\n",
      "epoch: 1 step: 393, loss is 0.4455164670944214\n",
      "epoch: 1 step: 394, loss is 0.4356868863105774\n",
      "epoch: 1 step: 395, loss is 0.4962378144264221\n",
      "epoch: 1 step: 396, loss is 0.6577069163322449\n",
      "epoch: 1 step: 397, loss is 0.47159716486930847\n",
      "epoch: 1 step: 398, loss is 0.434617817401886\n",
      "epoch: 1 step: 399, loss is 0.45441606640815735\n",
      "epoch: 1 step: 400, loss is 0.48658040165901184\n",
      "epoch: 1 step: 401, loss is 0.5891075134277344\n",
      "epoch: 1 step: 402, loss is 0.6303519606590271\n",
      "epoch: 1 step: 403, loss is 0.5757031440734863\n",
      "epoch: 1 step: 404, loss is 0.63700270652771\n",
      "epoch: 1 step: 405, loss is 0.5013301968574524\n",
      "epoch: 1 step: 406, loss is 0.704599916934967\n",
      "epoch: 1 step: 407, loss is 0.42689526081085205\n",
      "epoch: 1 step: 408, loss is 0.3856135606765747\n",
      "epoch: 1 step: 409, loss is 0.3943852484226227\n",
      "epoch: 1 step: 410, loss is 0.5235924124717712\n",
      "epoch: 1 step: 411, loss is 0.3621336817741394\n",
      "epoch: 1 step: 412, loss is 0.45903268456459045\n",
      "epoch: 1 step: 413, loss is 0.4807933270931244\n",
      "epoch: 1 step: 414, loss is 0.5203524231910706\n",
      "epoch: 1 step: 415, loss is 0.5294023156166077\n",
      "epoch: 1 step: 416, loss is 0.6150240302085876\n",
      "epoch: 1 step: 417, loss is 0.5388509631156921\n",
      "epoch: 1 step: 418, loss is 0.7275946140289307\n",
      "epoch: 1 step: 419, loss is 0.4735654890537262\n",
      "epoch: 1 step: 420, loss is 0.7003387212753296\n",
      "epoch: 1 step: 421, loss is 0.6438491344451904\n",
      "epoch: 1 step: 422, loss is 0.6062538623809814\n",
      "epoch: 1 step: 423, loss is 0.5262132883071899\n",
      "epoch: 1 step: 424, loss is 0.4466108977794647\n",
      "epoch: 1 step: 425, loss is 0.5416851043701172\n",
      "epoch: 1 step: 426, loss is 0.46700841188430786\n",
      "epoch: 1 step: 427, loss is 0.27787816524505615\n",
      "epoch: 1 step: 428, loss is 0.7497695684432983\n",
      "epoch: 1 step: 429, loss is 0.5575721859931946\n",
      "epoch: 1 step: 430, loss is 0.45326101779937744\n",
      "epoch: 1 step: 431, loss is 0.5319918394088745\n",
      "epoch: 1 step: 432, loss is 0.699047863483429\n",
      "epoch: 1 step: 433, loss is 0.5623424649238586\n",
      "epoch: 1 step: 434, loss is 0.5514816045761108\n",
      "epoch: 1 step: 435, loss is 0.3543984293937683\n",
      "epoch: 1 step: 436, loss is 0.43647611141204834\n",
      "epoch: 1 step: 437, loss is 0.6444807648658752\n",
      "epoch: 1 step: 438, loss is 0.5347383618354797\n",
      "epoch: 1 step: 439, loss is 0.45112207531929016\n",
      "epoch: 1 step: 440, loss is 0.45506754517555237\n",
      "epoch: 1 step: 441, loss is 0.33734941482543945\n",
      "epoch: 1 step: 442, loss is 0.6080840826034546\n",
      "epoch: 1 step: 443, loss is 0.5568806529045105\n",
      "epoch: 1 step: 444, loss is 0.47476834058761597\n",
      "epoch: 1 step: 445, loss is 0.5435217022895813\n",
      "epoch: 1 step: 446, loss is 0.5700722932815552\n",
      "epoch: 1 step: 447, loss is 0.3525906503200531\n",
      "epoch: 1 step: 448, loss is 0.6158062815666199\n",
      "epoch: 1 step: 449, loss is 0.5124384164810181\n",
      "epoch: 1 step: 450, loss is 0.4167793095111847\n",
      "epoch: 1 step: 451, loss is 0.32106852531433105\n",
      "epoch: 1 step: 452, loss is 0.3743526339530945\n",
      "epoch: 1 step: 453, loss is 0.5328587293624878\n",
      "epoch: 1 step: 454, loss is 0.3766581118106842\n",
      "epoch: 1 step: 455, loss is 0.6269282102584839\n",
      "epoch: 1 step: 456, loss is 0.40091678500175476\n",
      "epoch: 1 step: 457, loss is 0.5982627272605896\n",
      "epoch: 1 step: 458, loss is 0.4583244025707245\n",
      "epoch: 1 step: 459, loss is 0.33033981919288635\n",
      "epoch: 1 step: 460, loss is 0.48121920228004456\n",
      "epoch: 1 step: 461, loss is 0.31975388526916504\n",
      "epoch: 1 step: 462, loss is 0.5016005635261536\n",
      "epoch: 1 step: 463, loss is 0.44897863268852234\n",
      "epoch: 1 step: 464, loss is 0.4715801775455475\n",
      "epoch: 1 step: 465, loss is 0.473966121673584\n",
      "epoch: 1 step: 466, loss is 0.5427387952804565\n",
      "epoch: 1 step: 467, loss is 0.493413507938385\n",
      "epoch: 1 step: 468, loss is 0.5639205574989319\n",
      "epoch: 1 step: 469, loss is 0.3874293863773346\n",
      "epoch: 1 step: 470, loss is 0.5027636885643005\n",
      "epoch: 1 step: 471, loss is 0.5869157314300537\n",
      "epoch: 1 step: 472, loss is 0.6971096396446228\n",
      "epoch: 1 step: 473, loss is 0.415291965007782\n",
      "epoch: 1 step: 474, loss is 0.6107326745986938\n",
      "epoch: 1 step: 475, loss is 0.5860377550125122\n",
      "epoch: 1 step: 476, loss is 0.6238973140716553\n",
      "epoch: 1 step: 477, loss is 0.5459592938423157\n",
      "epoch: 1 step: 478, loss is 0.3954800069332123\n",
      "epoch: 1 step: 479, loss is 0.6020299196243286\n",
      "epoch: 1 step: 480, loss is 0.594042956829071\n",
      "epoch: 1 step: 481, loss is 0.562238335609436\n",
      "epoch: 1 step: 482, loss is 0.4483925700187683\n",
      "epoch: 1 step: 483, loss is 0.34499868750572205\n",
      "epoch: 1 step: 484, loss is 0.40393996238708496\n",
      "epoch: 1 step: 485, loss is 0.5473000407218933\n",
      "epoch: 1 step: 486, loss is 0.4935491681098938\n",
      "epoch: 1 step: 487, loss is 0.5104847550392151\n",
      "epoch: 1 step: 488, loss is 0.41873252391815186\n",
      "epoch: 1 step: 489, loss is 0.485397070646286\n",
      "epoch: 1 step: 490, loss is 0.48315194249153137\n",
      "epoch: 1 step: 491, loss is 0.5450640320777893\n",
      "epoch: 1 step: 492, loss is 0.3243098556995392\n",
      "epoch: 1 step: 493, loss is 0.600001871585846\n",
      "epoch: 1 step: 494, loss is 0.24175900220870972\n",
      "epoch: 1 step: 495, loss is 0.41886961460113525\n",
      "epoch: 1 step: 496, loss is 0.3294520974159241\n",
      "epoch: 1 step: 497, loss is 0.4182669520378113\n",
      "epoch: 1 step: 498, loss is 0.4197273552417755\n",
      "epoch: 1 step: 499, loss is 0.717594563961029\n",
      "epoch: 1 step: 500, loss is 0.42380160093307495\n",
      "epoch: 1 step: 501, loss is 0.4984264373779297\n",
      "epoch: 1 step: 502, loss is 0.6860296726226807\n",
      "epoch: 1 step: 503, loss is 0.45318689942359924\n",
      "epoch: 1 step: 504, loss is 0.35057011246681213\n",
      "epoch: 1 step: 505, loss is 0.499022513628006\n",
      "epoch: 1 step: 506, loss is 0.45201414823532104\n",
      "epoch: 1 step: 507, loss is 0.5525385737419128\n",
      "epoch: 1 step: 508, loss is 0.5648859143257141\n",
      "epoch: 1 step: 509, loss is 0.3686080574989319\n",
      "epoch: 1 step: 510, loss is 0.38421955704689026\n",
      "epoch: 1 step: 511, loss is 0.5342653393745422\n",
      "epoch: 1 step: 512, loss is 0.5315178036689758\n",
      "epoch: 1 step: 513, loss is 0.5133808255195618\n",
      "epoch: 1 step: 514, loss is 0.3826163113117218\n",
      "epoch: 1 step: 515, loss is 0.43121567368507385\n",
      "epoch: 1 step: 516, loss is 0.5399459600448608\n",
      "epoch: 1 step: 517, loss is 0.41543155908584595\n",
      "epoch: 1 step: 518, loss is 0.441008061170578\n",
      "epoch: 1 step: 519, loss is 0.4944964051246643\n",
      "epoch: 1 step: 520, loss is 0.3993048071861267\n",
      "epoch: 1 step: 521, loss is 0.3940671980381012\n",
      "epoch: 1 step: 522, loss is 0.5737298130989075\n",
      "epoch: 1 step: 523, loss is 0.40101590752601624\n",
      "epoch: 1 step: 524, loss is 0.43190449476242065\n",
      "epoch: 1 step: 525, loss is 0.4892745912075043\n",
      "epoch: 1 step: 526, loss is 0.48070138692855835\n",
      "epoch: 1 step: 527, loss is 0.39172226190567017\n",
      "epoch: 1 step: 528, loss is 0.355852335691452\n",
      "epoch: 1 step: 529, loss is 0.30308660864830017\n",
      "epoch: 1 step: 530, loss is 0.6400946378707886\n",
      "epoch: 1 step: 531, loss is 0.6061209440231323\n",
      "epoch: 1 step: 532, loss is 0.3980819582939148\n",
      "epoch: 1 step: 533, loss is 0.44044777750968933\n",
      "epoch: 1 step: 534, loss is 0.3975825905799866\n",
      "epoch: 1 step: 535, loss is 0.3105818033218384\n",
      "epoch: 1 step: 536, loss is 0.485185831785202\n",
      "epoch: 1 step: 537, loss is 0.46287718415260315\n",
      "epoch: 1 step: 538, loss is 0.4219568073749542\n",
      "epoch: 1 step: 539, loss is 0.43868038058280945\n",
      "epoch: 1 step: 540, loss is 0.4439453184604645\n",
      "epoch: 1 step: 541, loss is 0.5358335375785828\n",
      "epoch: 1 step: 542, loss is 0.42045703530311584\n",
      "epoch: 1 step: 543, loss is 0.41825050115585327\n",
      "epoch: 1 step: 544, loss is 0.48935770988464355\n",
      "epoch: 1 step: 545, loss is 0.6059035062789917\n",
      "epoch: 1 step: 546, loss is 0.5195122957229614\n",
      "epoch: 1 step: 547, loss is 0.39207723736763\n",
      "epoch: 1 step: 548, loss is 0.49824464321136475\n",
      "epoch: 1 step: 549, loss is 0.39729079604148865\n",
      "epoch: 1 step: 550, loss is 0.42445647716522217\n",
      "epoch: 1 step: 551, loss is 0.4033929109573364\n",
      "epoch: 1 step: 552, loss is 0.4297681152820587\n",
      "epoch: 1 step: 553, loss is 0.6115930676460266\n",
      "epoch: 1 step: 554, loss is 0.7778823375701904\n",
      "epoch: 1 step: 555, loss is 0.42874613404273987\n",
      "epoch: 1 step: 556, loss is 0.3849527835845947\n",
      "epoch: 1 step: 557, loss is 0.39655232429504395\n",
      "epoch: 1 step: 558, loss is 0.6797283291816711\n",
      "epoch: 1 step: 559, loss is 0.5483356714248657\n",
      "epoch: 1 step: 560, loss is 0.4055434465408325\n",
      "epoch: 1 step: 561, loss is 0.35899344086647034\n",
      "epoch: 1 step: 562, loss is 0.40188536047935486\n",
      "epoch: 1 step: 563, loss is 0.32784783840179443\n",
      "epoch: 1 step: 564, loss is 0.4613094925880432\n",
      "epoch: 1 step: 565, loss is 0.556594729423523\n",
      "epoch: 1 step: 566, loss is 0.44138312339782715\n",
      "epoch: 1 step: 567, loss is 0.2603180706501007\n",
      "epoch: 1 step: 568, loss is 0.5002744197845459\n",
      "epoch: 1 step: 569, loss is 0.43737074732780457\n",
      "epoch: 1 step: 570, loss is 0.45533397793769836\n",
      "epoch: 1 step: 571, loss is 0.6816654801368713\n",
      "epoch: 1 step: 572, loss is 0.7789781093597412\n",
      "epoch: 1 step: 573, loss is 0.3521836996078491\n",
      "epoch: 1 step: 574, loss is 0.4460102915763855\n",
      "epoch: 1 step: 575, loss is 0.4575025737285614\n",
      "epoch: 1 step: 576, loss is 0.4160432517528534\n",
      "epoch: 1 step: 577, loss is 0.40193212032318115\n",
      "epoch: 1 step: 578, loss is 0.288151353597641\n",
      "epoch: 1 step: 579, loss is 0.3676437735557556\n",
      "epoch: 1 step: 580, loss is 0.5523189902305603\n",
      "epoch: 1 step: 581, loss is 0.4355860650539398\n",
      "epoch: 1 step: 582, loss is 0.5373522639274597\n",
      "epoch: 1 step: 583, loss is 0.4860357344150543\n",
      "epoch: 1 step: 584, loss is 0.4173317551612854\n",
      "epoch: 1 step: 585, loss is 0.4225772023200989\n",
      "epoch: 1 step: 586, loss is 0.3454480767250061\n",
      "epoch: 1 step: 587, loss is 0.5056054592132568\n",
      "epoch: 1 step: 588, loss is 0.5240651369094849\n",
      "epoch: 1 step: 589, loss is 0.4585413634777069\n",
      "epoch: 1 step: 590, loss is 0.5993708372116089\n",
      "epoch: 1 step: 591, loss is 0.3966708779335022\n",
      "epoch: 1 step: 592, loss is 0.5944580435752869\n",
      "epoch: 1 step: 593, loss is 0.5082582831382751\n",
      "epoch: 1 step: 594, loss is 0.3961041569709778\n",
      "epoch: 1 step: 595, loss is 0.5499704480171204\n",
      "epoch: 1 step: 596, loss is 0.31881183385849\n",
      "epoch: 1 step: 597, loss is 0.6167764067649841\n",
      "epoch: 1 step: 598, loss is 0.45693889260292053\n",
      "epoch: 1 step: 599, loss is 0.4885682761669159\n",
      "epoch: 1 step: 600, loss is 0.43251878023147583\n",
      "epoch: 1 step: 601, loss is 0.47154518961906433\n",
      "epoch: 1 step: 602, loss is 0.588019073009491\n",
      "epoch: 1 step: 603, loss is 0.4573915898799896\n",
      "epoch: 1 step: 604, loss is 0.45071786642074585\n",
      "epoch: 1 step: 605, loss is 0.422265887260437\n",
      "epoch: 1 step: 606, loss is 0.4911815822124481\n",
      "epoch: 1 step: 607, loss is 0.2853877544403076\n",
      "epoch: 1 step: 608, loss is 0.3968309760093689\n",
      "epoch: 1 step: 609, loss is 0.2789856493473053\n",
      "epoch: 1 step: 610, loss is 0.6650799512863159\n",
      "epoch: 1 step: 611, loss is 0.29879993200302124\n",
      "epoch: 1 step: 612, loss is 0.6829891204833984\n",
      "epoch: 1 step: 613, loss is 0.5383983254432678\n",
      "epoch: 1 step: 614, loss is 0.5417346954345703\n",
      "epoch: 1 step: 615, loss is 0.5023854970932007\n",
      "epoch: 1 step: 616, loss is 0.4148082435131073\n",
      "epoch: 1 step: 617, loss is 0.28381970524787903\n",
      "epoch: 1 step: 618, loss is 0.5988603830337524\n",
      "epoch: 1 step: 619, loss is 0.44544851779937744\n",
      "epoch: 1 step: 620, loss is 0.4659428894519806\n",
      "epoch: 1 step: 621, loss is 0.45059600472450256\n",
      "epoch: 1 step: 622, loss is 0.4366832375526428\n",
      "epoch: 1 step: 623, loss is 0.41075798869132996\n",
      "epoch: 1 step: 624, loss is 0.5697632431983948\n",
      "epoch: 1 step: 625, loss is 0.3202914893627167\n",
      "epoch: 1 step: 626, loss is 0.5862836837768555\n",
      "epoch: 1 step: 627, loss is 0.6442663073539734\n",
      "epoch: 1 step: 628, loss is 0.43757644295692444\n",
      "epoch: 1 step: 629, loss is 0.5996076464653015\n",
      "epoch: 1 step: 630, loss is 0.3254430890083313\n",
      "epoch: 1 step: 631, loss is 0.7008762359619141\n",
      "epoch: 1 step: 632, loss is 0.5433717966079712\n",
      "epoch: 1 step: 633, loss is 0.4042867124080658\n",
      "epoch: 1 step: 634, loss is 0.5482848286628723\n",
      "epoch: 1 step: 635, loss is 0.45867791771888733\n",
      "epoch: 1 step: 636, loss is 0.724907398223877\n",
      "epoch: 1 step: 637, loss is 0.4740365743637085\n",
      "epoch: 1 step: 638, loss is 0.30731117725372314\n",
      "epoch: 1 step: 639, loss is 0.2791385352611542\n",
      "epoch: 1 step: 640, loss is 0.708633542060852\n",
      "epoch: 1 step: 641, loss is 0.5312418937683105\n",
      "epoch: 1 step: 642, loss is 0.27386540174484253\n",
      "epoch: 1 step: 643, loss is 0.45262375473976135\n",
      "epoch: 1 step: 644, loss is 0.30040138959884644\n",
      "epoch: 1 step: 645, loss is 0.5779647827148438\n",
      "epoch: 1 step: 646, loss is 0.548103928565979\n",
      "epoch: 1 step: 647, loss is 0.3536148965358734\n",
      "epoch: 1 step: 648, loss is 0.33472567796707153\n",
      "epoch: 1 step: 649, loss is 0.3529909551143646\n",
      "epoch: 1 step: 650, loss is 0.6033729314804077\n",
      "epoch: 1 step: 651, loss is 0.6485865712165833\n",
      "epoch: 1 step: 652, loss is 0.40448203682899475\n",
      "epoch: 1 step: 653, loss is 0.4490724205970764\n",
      "epoch: 1 step: 654, loss is 0.37715837359428406\n",
      "epoch: 1 step: 655, loss is 0.38379403948783875\n",
      "epoch: 1 step: 656, loss is 0.48261457681655884\n",
      "epoch: 1 step: 657, loss is 0.3501785397529602\n",
      "epoch: 1 step: 658, loss is 0.374142050743103\n",
      "epoch: 1 step: 659, loss is 0.8156684637069702\n",
      "epoch: 1 step: 660, loss is 0.49929946660995483\n",
      "epoch: 1 step: 661, loss is 0.5122807621955872\n",
      "epoch: 1 step: 662, loss is 0.5407674312591553\n",
      "epoch: 1 step: 663, loss is 0.6749551892280579\n",
      "epoch: 1 step: 664, loss is 0.39437538385391235\n",
      "epoch: 1 step: 665, loss is 0.6226904392242432\n",
      "epoch: 1 step: 666, loss is 0.5181691646575928\n",
      "epoch: 1 step: 667, loss is 0.43983545899391174\n",
      "epoch: 1 step: 668, loss is 0.5183748602867126\n",
      "epoch: 1 step: 669, loss is 0.45996010303497314\n",
      "epoch: 1 step: 670, loss is 0.35672464966773987\n",
      "epoch: 1 step: 671, loss is 0.4691891670227051\n",
      "epoch: 1 step: 672, loss is 0.49681633710861206\n",
      "epoch: 1 step: 673, loss is 0.5869668126106262\n",
      "epoch: 1 step: 674, loss is 0.5974765419960022\n",
      "epoch: 1 step: 675, loss is 0.7114228010177612\n",
      "epoch: 1 step: 676, loss is 0.4644424021244049\n",
      "epoch: 1 step: 677, loss is 0.40333089232444763\n",
      "epoch: 1 step: 678, loss is 0.45640358328819275\n",
      "epoch: 1 step: 679, loss is 0.37237852811813354\n",
      "epoch: 1 step: 680, loss is 0.33337274193763733\n",
      "epoch: 1 step: 681, loss is 0.2863856852054596\n",
      "epoch: 1 step: 682, loss is 0.5072002410888672\n",
      "epoch: 1 step: 683, loss is 0.5194621086120605\n",
      "epoch: 1 step: 684, loss is 0.5793402194976807\n",
      "epoch: 1 step: 685, loss is 0.39637964963912964\n",
      "epoch: 1 step: 686, loss is 0.3790709972381592\n",
      "epoch: 1 step: 687, loss is 0.5098750591278076\n",
      "epoch: 1 step: 688, loss is 0.3610355257987976\n",
      "epoch: 1 step: 689, loss is 0.35501956939697266\n",
      "epoch: 1 step: 690, loss is 0.4448878765106201\n",
      "epoch: 1 step: 691, loss is 0.34177467226982117\n",
      "epoch: 1 step: 692, loss is 0.2598044276237488\n",
      "epoch: 1 step: 693, loss is 0.2546875476837158\n",
      "epoch: 1 step: 694, loss is 0.2496347278356552\n",
      "epoch: 1 step: 695, loss is 0.3412039875984192\n",
      "epoch: 1 step: 696, loss is 0.38813766837120056\n",
      "epoch: 1 step: 697, loss is 0.4551405608654022\n",
      "epoch: 1 step: 698, loss is 0.3661068081855774\n",
      "epoch: 1 step: 699, loss is 0.48936402797698975\n",
      "epoch: 1 step: 700, loss is 0.5323798060417175\n",
      "epoch: 1 step: 701, loss is 0.47650739550590515\n",
      "epoch: 1 step: 702, loss is 0.3638874888420105\n",
      "epoch: 1 step: 703, loss is 0.3507515788078308\n",
      "epoch: 1 step: 704, loss is 0.4703430235385895\n",
      "epoch: 1 step: 705, loss is 0.625710666179657\n",
      "epoch: 1 step: 706, loss is 0.4720577299594879\n",
      "epoch: 1 step: 707, loss is 0.37023916840553284\n",
      "epoch: 1 step: 708, loss is 0.28446486592292786\n",
      "epoch: 1 step: 709, loss is 0.2941262423992157\n",
      "epoch: 1 step: 710, loss is 0.28952062129974365\n",
      "epoch: 1 step: 711, loss is 0.4122627079486847\n",
      "epoch: 1 step: 712, loss is 0.3643359839916229\n",
      "epoch: 1 step: 713, loss is 0.5125337243080139\n",
      "epoch: 1 step: 714, loss is 0.4920755922794342\n",
      "epoch: 1 step: 715, loss is 0.49826592206954956\n",
      "epoch: 1 step: 716, loss is 0.31069713830947876\n",
      "epoch: 1 step: 717, loss is 0.4747089743614197\n",
      "epoch: 1 step: 718, loss is 0.41312044858932495\n",
      "epoch: 1 step: 719, loss is 0.33673563599586487\n",
      "epoch: 1 step: 720, loss is 0.567582905292511\n",
      "epoch: 1 step: 721, loss is 0.24653689563274384\n",
      "epoch: 1 step: 722, loss is 0.3618810772895813\n",
      "epoch: 1 step: 723, loss is 0.5482936501502991\n",
      "epoch: 1 step: 724, loss is 0.3991309702396393\n",
      "epoch: 1 step: 725, loss is 0.41516342759132385\n",
      "epoch: 1 step: 726, loss is 0.41122397780418396\n",
      "epoch: 1 step: 727, loss is 0.4307635724544525\n",
      "epoch: 1 step: 728, loss is 0.3766404986381531\n",
      "epoch: 1 step: 729, loss is 0.33634793758392334\n",
      "epoch: 1 step: 730, loss is 0.6955264806747437\n",
      "epoch: 1 step: 731, loss is 0.5595656037330627\n",
      "epoch: 1 step: 732, loss is 0.5415540337562561\n",
      "epoch: 1 step: 733, loss is 0.39872491359710693\n",
      "epoch: 1 step: 734, loss is 0.4393303096294403\n",
      "epoch: 1 step: 735, loss is 0.4758211374282837\n",
      "epoch: 1 step: 736, loss is 0.35479018092155457\n",
      "epoch: 1 step: 737, loss is 0.432163268327713\n",
      "epoch: 1 step: 738, loss is 0.3099854290485382\n",
      "epoch: 1 step: 739, loss is 0.4605960249900818\n",
      "epoch: 1 step: 740, loss is 0.5136741399765015\n",
      "epoch: 1 step: 741, loss is 0.4216289818286896\n",
      "epoch: 1 step: 742, loss is 0.37045249342918396\n",
      "epoch: 1 step: 743, loss is 0.6399327516555786\n",
      "epoch: 1 step: 744, loss is 0.40759050846099854\n",
      "epoch: 1 step: 745, loss is 0.35805726051330566\n",
      "epoch: 1 step: 746, loss is 0.32805052399635315\n",
      "epoch: 1 step: 747, loss is 0.416152685880661\n",
      "epoch: 1 step: 748, loss is 0.5048309564590454\n",
      "epoch: 1 step: 749, loss is 0.35494956374168396\n",
      "epoch: 1 step: 750, loss is 0.4444141089916229\n",
      "epoch: 1 step: 751, loss is 0.448665976524353\n",
      "epoch: 1 step: 752, loss is 0.41687169671058655\n",
      "epoch: 1 step: 753, loss is 0.5240934491157532\n",
      "epoch: 1 step: 754, loss is 0.5616848468780518\n",
      "epoch: 1 step: 755, loss is 0.4022936522960663\n",
      "epoch: 1 step: 756, loss is 0.3377804160118103\n",
      "epoch: 1 step: 757, loss is 0.33249661326408386\n",
      "epoch: 1 step: 758, loss is 0.4832819402217865\n",
      "epoch: 1 step: 759, loss is 0.34216341376304626\n",
      "epoch: 1 step: 760, loss is 0.2825241684913635\n",
      "epoch: 1 step: 761, loss is 0.4747495651245117\n",
      "epoch: 1 step: 762, loss is 0.47672563791275024\n",
      "epoch: 1 step: 763, loss is 0.4295332133769989\n",
      "epoch: 1 step: 764, loss is 0.6908764243125916\n",
      "epoch: 1 step: 765, loss is 0.4770800471305847\n",
      "epoch: 1 step: 766, loss is 0.4036632478237152\n",
      "epoch: 1 step: 767, loss is 0.3552360236644745\n",
      "epoch: 1 step: 768, loss is 0.5527185797691345\n",
      "epoch: 1 step: 769, loss is 0.5344177484512329\n",
      "epoch: 1 step: 770, loss is 0.41497915983200073\n",
      "epoch: 1 step: 771, loss is 0.4010539948940277\n",
      "epoch: 1 step: 772, loss is 0.3665291965007782\n",
      "epoch: 1 step: 773, loss is 0.5542307496070862\n",
      "epoch: 1 step: 774, loss is 0.49243369698524475\n",
      "epoch: 1 step: 775, loss is 0.5405917167663574\n",
      "epoch: 1 step: 776, loss is 0.42840537428855896\n",
      "epoch: 1 step: 777, loss is 0.602728009223938\n",
      "epoch: 1 step: 778, loss is 0.33628758788108826\n",
      "epoch: 1 step: 779, loss is 0.5877518057823181\n",
      "epoch: 1 step: 780, loss is 0.5695604681968689\n",
      "epoch: 1 step: 781, loss is 0.577303946018219\n",
      "epoch: 1 step: 782, loss is 0.6272227168083191\n",
      "epoch: 1 step: 783, loss is 0.4199448227882385\n",
      "epoch: 1 step: 784, loss is 0.3587627112865448\n",
      "epoch: 1 step: 785, loss is 0.2983684837818146\n",
      "epoch: 1 step: 786, loss is 0.4103057086467743\n",
      "epoch: 1 step: 787, loss is 0.49410852789878845\n",
      "epoch: 1 step: 788, loss is 0.49449485540390015\n",
      "epoch: 1 step: 789, loss is 0.4272879958152771\n",
      "epoch: 1 step: 790, loss is 0.4009058475494385\n",
      "epoch: 1 step: 791, loss is 0.34771397709846497\n",
      "epoch: 1 step: 792, loss is 0.4000611901283264\n",
      "epoch: 1 step: 793, loss is 0.40960195660591125\n",
      "epoch: 1 step: 794, loss is 0.3790394365787506\n",
      "epoch: 1 step: 795, loss is 0.24669550359249115\n",
      "epoch: 1 step: 796, loss is 0.4455071985721588\n",
      "epoch: 1 step: 797, loss is 0.7259458303451538\n",
      "epoch: 1 step: 798, loss is 0.42986828088760376\n",
      "epoch: 1 step: 799, loss is 0.47225505113601685\n",
      "epoch: 1 step: 800, loss is 0.5380823612213135\n",
      "epoch: 1 step: 801, loss is 0.42167744040489197\n",
      "epoch: 1 step: 802, loss is 0.45744088292121887\n",
      "epoch: 1 step: 803, loss is 0.46759214997291565\n",
      "epoch: 1 step: 804, loss is 0.4536815881729126\n",
      "epoch: 1 step: 805, loss is 0.6697259545326233\n",
      "epoch: 1 step: 806, loss is 0.4356110692024231\n",
      "epoch: 1 step: 807, loss is 0.3688085079193115\n",
      "epoch: 1 step: 808, loss is 0.44545891880989075\n",
      "epoch: 1 step: 809, loss is 0.4006046950817108\n",
      "epoch: 1 step: 810, loss is 0.394609659910202\n",
      "epoch: 1 step: 811, loss is 0.2112295925617218\n",
      "epoch: 1 step: 812, loss is 0.3400711119174957\n",
      "epoch: 1 step: 813, loss is 0.6697919964790344\n",
      "epoch: 1 step: 814, loss is 0.5582597851753235\n",
      "epoch: 1 step: 815, loss is 0.5858667492866516\n",
      "epoch: 1 step: 816, loss is 0.3257962763309479\n",
      "epoch: 1 step: 817, loss is 0.47799795866012573\n",
      "epoch: 1 step: 818, loss is 0.39086753129959106\n",
      "epoch: 1 step: 819, loss is 0.27252060174942017\n",
      "epoch: 1 step: 820, loss is 0.46016740798950195\n",
      "epoch: 1 step: 821, loss is 0.45138099789619446\n",
      "epoch: 1 step: 822, loss is 0.5145811438560486\n",
      "epoch: 1 step: 823, loss is 0.5914002060890198\n",
      "epoch: 1 step: 824, loss is 0.32301953434944153\n",
      "epoch: 1 step: 825, loss is 0.3061428666114807\n",
      "epoch: 1 step: 826, loss is 0.30170297622680664\n",
      "epoch: 1 step: 827, loss is 0.2815083861351013\n",
      "epoch: 1 step: 828, loss is 0.4959448277950287\n",
      "epoch: 1 step: 829, loss is 0.4922018051147461\n",
      "epoch: 1 step: 830, loss is 0.6610931158065796\n",
      "epoch: 1 step: 831, loss is 0.3238903880119324\n",
      "epoch: 1 step: 832, loss is 0.43895965814590454\n",
      "epoch: 1 step: 833, loss is 0.4759409725666046\n",
      "epoch: 1 step: 834, loss is 0.35600030422210693\n",
      "epoch: 1 step: 835, loss is 0.2734483480453491\n",
      "epoch: 1 step: 836, loss is 0.31752637028694153\n",
      "epoch: 1 step: 837, loss is 0.5199548006057739\n",
      "epoch: 1 step: 838, loss is 0.7852495908737183\n",
      "epoch: 1 step: 839, loss is 0.4931550920009613\n",
      "epoch: 1 step: 840, loss is 0.437770277261734\n",
      "epoch: 1 step: 841, loss is 0.41042229533195496\n",
      "epoch: 1 step: 842, loss is 0.3796378970146179\n",
      "epoch: 1 step: 843, loss is 0.44487959146499634\n",
      "epoch: 1 step: 844, loss is 0.49820777773857117\n",
      "epoch: 1 step: 845, loss is 0.48049959540367126\n",
      "epoch: 1 step: 846, loss is 0.5447111129760742\n",
      "epoch: 1 step: 847, loss is 0.41709476709365845\n",
      "epoch: 1 step: 848, loss is 0.4339689612388611\n",
      "epoch: 1 step: 849, loss is 0.3845559358596802\n",
      "epoch: 1 step: 850, loss is 0.5843732357025146\n",
      "epoch: 1 step: 851, loss is 0.48821312189102173\n",
      "epoch: 1 step: 852, loss is 0.39488476514816284\n",
      "epoch: 1 step: 853, loss is 0.33912393450737\n",
      "epoch: 1 step: 854, loss is 0.6365823745727539\n",
      "epoch: 1 step: 855, loss is 0.5332275629043579\n",
      "epoch: 1 step: 856, loss is 0.5162066221237183\n",
      "epoch: 1 step: 857, loss is 0.4265452027320862\n",
      "epoch: 1 step: 858, loss is 0.4865480065345764\n",
      "epoch: 1 step: 859, loss is 0.42163318395614624\n",
      "epoch: 1 step: 860, loss is 0.5764263868331909\n",
      "epoch: 1 step: 861, loss is 0.4282928705215454\n",
      "epoch: 1 step: 862, loss is 0.46928510069847107\n",
      "epoch: 1 step: 863, loss is 0.4458563029766083\n",
      "epoch: 1 step: 864, loss is 0.42781078815460205\n",
      "epoch: 1 step: 865, loss is 0.5351345539093018\n",
      "epoch: 1 step: 866, loss is 0.3569616675376892\n",
      "epoch: 1 step: 867, loss is 0.37547439336776733\n",
      "epoch: 1 step: 868, loss is 0.30390262603759766\n",
      "epoch: 1 step: 869, loss is 0.6918423771858215\n",
      "epoch: 1 step: 870, loss is 0.46157994866371155\n",
      "epoch: 1 step: 871, loss is 0.41193199157714844\n",
      "epoch: 1 step: 872, loss is 0.39936381578445435\n",
      "epoch: 1 step: 873, loss is 0.5299728512763977\n",
      "epoch: 1 step: 874, loss is 0.40486693382263184\n",
      "epoch: 1 step: 875, loss is 0.6022235155105591\n",
      "epoch: 1 step: 876, loss is 0.42788177728652954\n",
      "epoch: 1 step: 877, loss is 0.43749937415122986\n",
      "epoch: 1 step: 878, loss is 0.4225028157234192\n",
      "epoch: 1 step: 879, loss is 0.6173826456069946\n",
      "epoch: 1 step: 880, loss is 0.5020166039466858\n",
      "epoch: 1 step: 881, loss is 0.4414626955986023\n",
      "epoch: 1 step: 882, loss is 0.4538128674030304\n",
      "epoch: 1 step: 883, loss is 0.32199040055274963\n",
      "epoch: 1 step: 884, loss is 0.4998856484889984\n",
      "epoch: 1 step: 885, loss is 0.33780986070632935\n",
      "epoch: 1 step: 886, loss is 0.6278194785118103\n",
      "epoch: 1 step: 887, loss is 0.5057567358016968\n",
      "epoch: 1 step: 888, loss is 0.480805367231369\n",
      "epoch: 1 step: 889, loss is 0.3699737787246704\n",
      "epoch: 1 step: 890, loss is 0.44153812527656555\n",
      "epoch: 1 step: 891, loss is 0.42142051458358765\n",
      "epoch: 1 step: 892, loss is 0.38586580753326416\n",
      "epoch: 1 step: 893, loss is 0.45465975999832153\n",
      "epoch: 1 step: 894, loss is 0.37477433681488037\n",
      "epoch: 1 step: 895, loss is 0.4450562000274658\n",
      "epoch: 1 step: 896, loss is 0.4209342896938324\n",
      "epoch: 1 step: 897, loss is 0.3597412109375\n",
      "epoch: 1 step: 898, loss is 0.5245890021324158\n",
      "epoch: 1 step: 899, loss is 0.3385463058948517\n",
      "epoch: 1 step: 900, loss is 0.5974285006523132\n",
      "epoch: 1 step: 901, loss is 0.4659832715988159\n",
      "epoch: 1 step: 902, loss is 0.6176547408103943\n",
      "epoch: 1 step: 903, loss is 0.35845306515693665\n",
      "epoch: 1 step: 904, loss is 0.5934303402900696\n",
      "epoch: 1 step: 905, loss is 0.33512449264526367\n",
      "epoch: 1 step: 906, loss is 0.40739864110946655\n",
      "epoch: 1 step: 907, loss is 0.4548259675502777\n",
      "epoch: 1 step: 908, loss is 0.5281697511672974\n",
      "epoch: 1 step: 909, loss is 0.3779670000076294\n",
      "epoch: 1 step: 910, loss is 0.4468326270580292\n",
      "epoch: 1 step: 911, loss is 0.4226733446121216\n",
      "epoch: 1 step: 912, loss is 0.4055290222167969\n",
      "epoch: 1 step: 913, loss is 0.5001094341278076\n",
      "epoch: 1 step: 914, loss is 0.3882112503051758\n",
      "epoch: 1 step: 915, loss is 0.3872498571872711\n",
      "epoch: 1 step: 916, loss is 0.327181339263916\n",
      "epoch: 1 step: 917, loss is 0.38980215787887573\n",
      "epoch: 1 step: 918, loss is 0.2527620196342468\n",
      "epoch: 1 step: 919, loss is 0.43286412954330444\n",
      "epoch: 1 step: 920, loss is 0.42846083641052246\n",
      "epoch: 1 step: 921, loss is 0.6722451448440552\n",
      "epoch: 1 step: 922, loss is 0.4146610200405121\n",
      "epoch: 1 step: 923, loss is 0.39226266741752625\n",
      "epoch: 1 step: 924, loss is 0.3603730797767639\n",
      "epoch: 1 step: 925, loss is 0.32925945520401\n",
      "epoch: 1 step: 926, loss is 0.48965293169021606\n",
      "epoch: 1 step: 927, loss is 0.580121636390686\n",
      "epoch: 1 step: 928, loss is 0.4366658627986908\n",
      "epoch: 1 step: 929, loss is 0.48568668961524963\n",
      "epoch: 1 step: 930, loss is 0.2736539840698242\n",
      "epoch: 1 step: 931, loss is 0.49326834082603455\n",
      "epoch: 1 step: 932, loss is 0.4086645841598511\n",
      "epoch: 1 step: 933, loss is 0.3292859196662903\n",
      "epoch: 1 step: 934, loss is 0.5373608469963074\n",
      "epoch: 1 step: 935, loss is 0.34080126881599426\n",
      "epoch: 1 step: 936, loss is 0.547487735748291\n",
      "epoch: 1 step: 937, loss is 0.358318567276001\n",
      "epoch: 2 step: 1, loss is 0.2639942467212677\n",
      "epoch: 2 step: 2, loss is 0.4418179392814636\n",
      "epoch: 2 step: 3, loss is 0.4527361989021301\n",
      "epoch: 2 step: 4, loss is 0.4691298305988312\n",
      "epoch: 2 step: 5, loss is 0.3933514654636383\n",
      "epoch: 2 step: 6, loss is 0.4529862701892853\n",
      "epoch: 2 step: 7, loss is 0.5233531594276428\n",
      "epoch: 2 step: 8, loss is 0.45390304923057556\n",
      "epoch: 2 step: 9, loss is 0.2940007150173187\n",
      "epoch: 2 step: 10, loss is 0.3138335049152374\n",
      "epoch: 2 step: 11, loss is 0.3789970874786377\n",
      "epoch: 2 step: 12, loss is 0.3518335223197937\n",
      "epoch: 2 step: 13, loss is 0.44722333550453186\n",
      "epoch: 2 step: 14, loss is 0.3317679464817047\n",
      "epoch: 2 step: 15, loss is 0.4167233109474182\n",
      "epoch: 2 step: 16, loss is 0.4311855733394623\n",
      "epoch: 2 step: 17, loss is 0.43712159991264343\n",
      "epoch: 2 step: 18, loss is 0.42059630155563354\n",
      "epoch: 2 step: 19, loss is 0.3754512667655945\n",
      "epoch: 2 step: 20, loss is 0.3742334842681885\n",
      "epoch: 2 step: 21, loss is 0.3752863109111786\n",
      "epoch: 2 step: 22, loss is 0.43575868010520935\n",
      "epoch: 2 step: 23, loss is 0.516867995262146\n",
      "epoch: 2 step: 24, loss is 0.3433862030506134\n",
      "epoch: 2 step: 25, loss is 0.5911536812782288\n",
      "epoch: 2 step: 26, loss is 0.37254518270492554\n",
      "epoch: 2 step: 27, loss is 0.3698992133140564\n",
      "epoch: 2 step: 28, loss is 0.39743077754974365\n",
      "epoch: 2 step: 29, loss is 0.7032502293586731\n",
      "epoch: 2 step: 30, loss is 0.3730509877204895\n",
      "epoch: 2 step: 31, loss is 0.41081005334854126\n",
      "epoch: 2 step: 32, loss is 0.39511358737945557\n",
      "epoch: 2 step: 33, loss is 0.4120958149433136\n",
      "epoch: 2 step: 34, loss is 0.4732666313648224\n",
      "epoch: 2 step: 35, loss is 0.6027442216873169\n",
      "epoch: 2 step: 36, loss is 0.3868005573749542\n",
      "epoch: 2 step: 37, loss is 0.40775996446609497\n",
      "epoch: 2 step: 38, loss is 0.515816330909729\n",
      "epoch: 2 step: 39, loss is 0.3436128497123718\n",
      "epoch: 2 step: 40, loss is 0.37209266424179077\n",
      "epoch: 2 step: 41, loss is 0.43562835454940796\n",
      "epoch: 2 step: 42, loss is 0.4758378267288208\n",
      "epoch: 2 step: 43, loss is 0.32333073019981384\n",
      "epoch: 2 step: 44, loss is 0.38334912061691284\n",
      "epoch: 2 step: 45, loss is 0.4003845751285553\n",
      "epoch: 2 step: 46, loss is 0.4796300232410431\n",
      "epoch: 2 step: 47, loss is 0.3758995234966278\n",
      "epoch: 2 step: 48, loss is 0.30669549107551575\n",
      "epoch: 2 step: 49, loss is 0.4454917907714844\n",
      "epoch: 2 step: 50, loss is 0.4425930380821228\n",
      "epoch: 2 step: 51, loss is 0.38408637046813965\n",
      "epoch: 2 step: 52, loss is 0.3815847933292389\n",
      "epoch: 2 step: 53, loss is 0.2712593376636505\n",
      "epoch: 2 step: 54, loss is 0.5278830528259277\n",
      "epoch: 2 step: 55, loss is 0.5762008428573608\n",
      "epoch: 2 step: 56, loss is 0.26909905672073364\n",
      "epoch: 2 step: 57, loss is 0.3658279776573181\n",
      "epoch: 2 step: 58, loss is 0.22424238920211792\n",
      "epoch: 2 step: 59, loss is 0.3134078085422516\n",
      "epoch: 2 step: 60, loss is 0.42860543727874756\n",
      "epoch: 2 step: 61, loss is 0.3021141588687897\n",
      "epoch: 2 step: 62, loss is 0.2899121642112732\n",
      "epoch: 2 step: 63, loss is 0.37672868371009827\n",
      "epoch: 2 step: 64, loss is 0.3975878357887268\n",
      "epoch: 2 step: 65, loss is 0.3423994183540344\n",
      "epoch: 2 step: 66, loss is 0.34502503275871277\n",
      "epoch: 2 step: 67, loss is 0.4107590317726135\n",
      "epoch: 2 step: 68, loss is 0.4016725718975067\n",
      "epoch: 2 step: 69, loss is 0.4905984103679657\n",
      "epoch: 2 step: 70, loss is 0.3986356258392334\n",
      "epoch: 2 step: 71, loss is 0.32208722829818726\n",
      "epoch: 2 step: 72, loss is 0.5170748829841614\n",
      "epoch: 2 step: 73, loss is 0.49512791633605957\n",
      "epoch: 2 step: 74, loss is 0.32622984051704407\n",
      "epoch: 2 step: 75, loss is 0.48440349102020264\n",
      "epoch: 2 step: 76, loss is 0.4778812825679779\n",
      "epoch: 2 step: 77, loss is 0.5355291366577148\n",
      "epoch: 2 step: 78, loss is 0.3135914206504822\n",
      "epoch: 2 step: 79, loss is 0.3399718105792999\n",
      "epoch: 2 step: 80, loss is 0.297679603099823\n",
      "epoch: 2 step: 81, loss is 0.5053866505622864\n",
      "epoch: 2 step: 82, loss is 0.3365393579006195\n",
      "epoch: 2 step: 83, loss is 0.4783560037612915\n",
      "epoch: 2 step: 84, loss is 0.45588892698287964\n",
      "epoch: 2 step: 85, loss is 0.432655930519104\n",
      "epoch: 2 step: 86, loss is 0.32584407925605774\n",
      "epoch: 2 step: 87, loss is 0.37298959493637085\n",
      "epoch: 2 step: 88, loss is 0.4782472848892212\n",
      "epoch: 2 step: 89, loss is 0.38727426528930664\n",
      "epoch: 2 step: 90, loss is 0.25028881430625916\n",
      "epoch: 2 step: 91, loss is 0.3842848837375641\n",
      "epoch: 2 step: 92, loss is 0.38023626804351807\n",
      "epoch: 2 step: 93, loss is 0.5337202548980713\n",
      "epoch: 2 step: 94, loss is 0.3668871521949768\n",
      "epoch: 2 step: 95, loss is 0.41403070092201233\n",
      "epoch: 2 step: 96, loss is 0.73780357837677\n",
      "epoch: 2 step: 97, loss is 0.38279062509536743\n",
      "epoch: 2 step: 98, loss is 0.353240966796875\n",
      "epoch: 2 step: 99, loss is 0.3258335292339325\n",
      "epoch: 2 step: 100, loss is 0.31557387113571167\n",
      "epoch: 2 step: 101, loss is 0.4809363782405853\n",
      "epoch: 2 step: 102, loss is 0.6060388088226318\n",
      "epoch: 2 step: 103, loss is 0.41017061471939087\n",
      "epoch: 2 step: 104, loss is 0.47353631258010864\n",
      "epoch: 2 step: 105, loss is 0.5090348124504089\n",
      "epoch: 2 step: 106, loss is 0.3881461024284363\n",
      "epoch: 2 step: 107, loss is 0.33548060059547424\n",
      "epoch: 2 step: 108, loss is 0.41717061400413513\n",
      "epoch: 2 step: 109, loss is 0.46196454763412476\n",
      "epoch: 2 step: 110, loss is 0.3307877480983734\n",
      "epoch: 2 step: 111, loss is 0.3496708273887634\n",
      "epoch: 2 step: 112, loss is 0.5274330377578735\n",
      "epoch: 2 step: 113, loss is 0.33107712864875793\n",
      "epoch: 2 step: 114, loss is 0.3089233934879303\n",
      "epoch: 2 step: 115, loss is 0.3706772029399872\n",
      "epoch: 2 step: 116, loss is 0.40817469358444214\n",
      "epoch: 2 step: 117, loss is 0.350925087928772\n",
      "epoch: 2 step: 118, loss is 0.4113081395626068\n",
      "epoch: 2 step: 119, loss is 0.6938645243644714\n",
      "epoch: 2 step: 120, loss is 0.27121034264564514\n",
      "epoch: 2 step: 121, loss is 0.6017609238624573\n",
      "epoch: 2 step: 122, loss is 0.549497663974762\n",
      "epoch: 2 step: 123, loss is 0.22201256453990936\n",
      "epoch: 2 step: 124, loss is 0.3866620361804962\n",
      "epoch: 2 step: 125, loss is 0.28524184226989746\n",
      "epoch: 2 step: 126, loss is 0.3377531170845032\n",
      "epoch: 2 step: 127, loss is 0.3433399498462677\n",
      "epoch: 2 step: 128, loss is 0.3331763446331024\n",
      "epoch: 2 step: 129, loss is 0.33546942472457886\n",
      "epoch: 2 step: 130, loss is 0.2751476764678955\n",
      "epoch: 2 step: 131, loss is 0.4666605293750763\n",
      "epoch: 2 step: 132, loss is 0.3769100606441498\n",
      "epoch: 2 step: 133, loss is 0.5252168774604797\n",
      "epoch: 2 step: 134, loss is 0.5971503853797913\n",
      "epoch: 2 step: 135, loss is 0.40841150283813477\n",
      "epoch: 2 step: 136, loss is 0.5899673104286194\n",
      "epoch: 2 step: 137, loss is 0.2920396029949188\n",
      "epoch: 2 step: 138, loss is 0.3403396010398865\n",
      "epoch: 2 step: 139, loss is 0.27872052788734436\n",
      "epoch: 2 step: 140, loss is 0.552566647529602\n",
      "epoch: 2 step: 141, loss is 0.6009698510169983\n",
      "epoch: 2 step: 142, loss is 0.5003281831741333\n",
      "epoch: 2 step: 143, loss is 0.3692106008529663\n",
      "epoch: 2 step: 144, loss is 0.3047902286052704\n",
      "epoch: 2 step: 145, loss is 0.544560968875885\n",
      "epoch: 2 step: 146, loss is 0.28585824370384216\n",
      "epoch: 2 step: 147, loss is 0.33286699652671814\n",
      "epoch: 2 step: 148, loss is 0.27894458174705505\n",
      "epoch: 2 step: 149, loss is 0.2790311574935913\n",
      "epoch: 2 step: 150, loss is 0.38451874256134033\n",
      "epoch: 2 step: 151, loss is 0.24617081880569458\n",
      "epoch: 2 step: 152, loss is 0.3592318892478943\n",
      "epoch: 2 step: 153, loss is 0.6037831902503967\n",
      "epoch: 2 step: 154, loss is 0.302755743265152\n",
      "epoch: 2 step: 155, loss is 0.42699307203292847\n",
      "epoch: 2 step: 156, loss is 0.46744251251220703\n",
      "epoch: 2 step: 157, loss is 0.45944541692733765\n",
      "epoch: 2 step: 158, loss is 0.3861904442310333\n",
      "epoch: 2 step: 159, loss is 0.4464414715766907\n",
      "epoch: 2 step: 160, loss is 0.47270792722702026\n",
      "epoch: 2 step: 161, loss is 0.3410494327545166\n",
      "epoch: 2 step: 162, loss is 0.47818440198898315\n",
      "epoch: 2 step: 163, loss is 0.46206921339035034\n",
      "epoch: 2 step: 164, loss is 0.6458014249801636\n",
      "epoch: 2 step: 165, loss is 0.46118801832199097\n",
      "epoch: 2 step: 166, loss is 0.40503162145614624\n",
      "epoch: 2 step: 167, loss is 0.3253900706768036\n",
      "epoch: 2 step: 168, loss is 0.5782663822174072\n",
      "epoch: 2 step: 169, loss is 0.3229333162307739\n",
      "epoch: 2 step: 170, loss is 0.4253411889076233\n",
      "epoch: 2 step: 171, loss is 0.4652400612831116\n",
      "epoch: 2 step: 172, loss is 0.2048894315958023\n",
      "epoch: 2 step: 173, loss is 0.25649598240852356\n",
      "epoch: 2 step: 174, loss is 0.41912031173706055\n",
      "epoch: 2 step: 175, loss is 0.36422857642173767\n",
      "epoch: 2 step: 176, loss is 0.49190667271614075\n",
      "epoch: 2 step: 177, loss is 0.5597665309906006\n",
      "epoch: 2 step: 178, loss is 0.48552170395851135\n",
      "epoch: 2 step: 179, loss is 0.24922175705432892\n",
      "epoch: 2 step: 180, loss is 0.3162698447704315\n",
      "epoch: 2 step: 181, loss is 0.5245288014411926\n",
      "epoch: 2 step: 182, loss is 0.46883654594421387\n",
      "epoch: 2 step: 183, loss is 0.439677894115448\n",
      "epoch: 2 step: 184, loss is 0.4164563715457916\n",
      "epoch: 2 step: 185, loss is 0.5752571821212769\n",
      "epoch: 2 step: 186, loss is 0.48737460374832153\n",
      "epoch: 2 step: 187, loss is 0.31658223271369934\n",
      "epoch: 2 step: 188, loss is 0.43939176201820374\n",
      "epoch: 2 step: 189, loss is 0.3384780287742615\n",
      "epoch: 2 step: 190, loss is 0.36340445280075073\n",
      "epoch: 2 step: 191, loss is 0.4144711494445801\n",
      "epoch: 2 step: 192, loss is 0.5860109925270081\n",
      "epoch: 2 step: 193, loss is 0.2132577896118164\n",
      "epoch: 2 step: 194, loss is 0.3096567988395691\n",
      "epoch: 2 step: 195, loss is 0.48337259888648987\n",
      "epoch: 2 step: 196, loss is 0.5384219288825989\n",
      "epoch: 2 step: 197, loss is 0.39754197001457214\n",
      "epoch: 2 step: 198, loss is 0.34875255823135376\n",
      "epoch: 2 step: 199, loss is 0.3994332551956177\n",
      "epoch: 2 step: 200, loss is 0.27957189083099365\n",
      "epoch: 2 step: 201, loss is 0.3404398560523987\n",
      "epoch: 2 step: 202, loss is 0.3635476231575012\n",
      "epoch: 2 step: 203, loss is 0.45149701833724976\n",
      "epoch: 2 step: 204, loss is 0.4606623947620392\n",
      "epoch: 2 step: 205, loss is 0.3148627281188965\n",
      "epoch: 2 step: 206, loss is 0.21473199129104614\n",
      "epoch: 2 step: 207, loss is 0.6356741786003113\n",
      "epoch: 2 step: 208, loss is 0.46349650621414185\n",
      "epoch: 2 step: 209, loss is 0.42369672656059265\n",
      "epoch: 2 step: 210, loss is 0.42637741565704346\n",
      "epoch: 2 step: 211, loss is 0.39222902059555054\n",
      "epoch: 2 step: 212, loss is 0.2758912444114685\n",
      "epoch: 2 step: 213, loss is 0.5045484304428101\n",
      "epoch: 2 step: 214, loss is 0.2910855710506439\n",
      "epoch: 2 step: 215, loss is 0.25969359278678894\n",
      "epoch: 2 step: 216, loss is 0.43267032504081726\n",
      "epoch: 2 step: 217, loss is 0.48159974813461304\n",
      "epoch: 2 step: 218, loss is 0.3767896294593811\n",
      "epoch: 2 step: 219, loss is 0.4708678424358368\n",
      "epoch: 2 step: 220, loss is 0.41356924176216125\n",
      "epoch: 2 step: 221, loss is 0.7248948812484741\n",
      "epoch: 2 step: 222, loss is 0.37799569964408875\n",
      "epoch: 2 step: 223, loss is 0.3740297555923462\n",
      "epoch: 2 step: 224, loss is 0.30157148838043213\n",
      "epoch: 2 step: 225, loss is 0.2870689630508423\n",
      "epoch: 2 step: 226, loss is 0.642525315284729\n",
      "epoch: 2 step: 227, loss is 0.45628589391708374\n",
      "epoch: 2 step: 228, loss is 0.44379037618637085\n",
      "epoch: 2 step: 229, loss is 0.28322476148605347\n",
      "epoch: 2 step: 230, loss is 0.4708760976791382\n",
      "epoch: 2 step: 231, loss is 0.42210280895233154\n",
      "epoch: 2 step: 232, loss is 0.41826748847961426\n",
      "epoch: 2 step: 233, loss is 0.61838698387146\n",
      "epoch: 2 step: 234, loss is 0.3214058578014374\n",
      "epoch: 2 step: 235, loss is 0.25035935640335083\n",
      "epoch: 2 step: 236, loss is 0.44704529643058777\n",
      "epoch: 2 step: 237, loss is 0.4459059536457062\n",
      "epoch: 2 step: 238, loss is 0.2637883722782135\n",
      "epoch: 2 step: 239, loss is 0.3936781883239746\n",
      "epoch: 2 step: 240, loss is 0.44481760263442993\n",
      "epoch: 2 step: 241, loss is 0.396247535943985\n",
      "epoch: 2 step: 242, loss is 0.4568435251712799\n",
      "epoch: 2 step: 243, loss is 0.20931865274906158\n",
      "epoch: 2 step: 244, loss is 0.3840532600879669\n",
      "epoch: 2 step: 245, loss is 0.380057156085968\n",
      "epoch: 2 step: 246, loss is 0.40607088804244995\n",
      "epoch: 2 step: 247, loss is 0.30607500672340393\n",
      "epoch: 2 step: 248, loss is 0.509527862071991\n",
      "epoch: 2 step: 249, loss is 0.4040462374687195\n",
      "epoch: 2 step: 250, loss is 0.8094565868377686\n",
      "epoch: 2 step: 251, loss is 0.5214034914970398\n",
      "epoch: 2 step: 252, loss is 0.18786846101284027\n",
      "epoch: 2 step: 253, loss is 0.4096495807170868\n",
      "epoch: 2 step: 254, loss is 0.46475833654403687\n",
      "epoch: 2 step: 255, loss is 0.47593697905540466\n",
      "epoch: 2 step: 256, loss is 0.31942418217658997\n",
      "epoch: 2 step: 257, loss is 0.5301548838615417\n",
      "epoch: 2 step: 258, loss is 0.4173939824104309\n",
      "epoch: 2 step: 259, loss is 0.36964377760887146\n",
      "epoch: 2 step: 260, loss is 0.4275897741317749\n",
      "epoch: 2 step: 261, loss is 0.409426212310791\n",
      "epoch: 2 step: 262, loss is 0.26505497097969055\n",
      "epoch: 2 step: 263, loss is 0.36148858070373535\n",
      "epoch: 2 step: 264, loss is 0.5501481890678406\n",
      "epoch: 2 step: 265, loss is 0.3166182041168213\n",
      "epoch: 2 step: 266, loss is 0.4393274188041687\n",
      "epoch: 2 step: 267, loss is 0.42984411120414734\n",
      "epoch: 2 step: 268, loss is 0.40107718110084534\n",
      "epoch: 2 step: 269, loss is 0.2714536190032959\n",
      "epoch: 2 step: 270, loss is 0.3095092177391052\n",
      "epoch: 2 step: 271, loss is 0.26108333468437195\n",
      "epoch: 2 step: 272, loss is 0.41396549344062805\n",
      "epoch: 2 step: 273, loss is 0.41190144419670105\n",
      "epoch: 2 step: 274, loss is 0.3573404550552368\n",
      "epoch: 2 step: 275, loss is 0.3219681680202484\n",
      "epoch: 2 step: 276, loss is 0.45690038800239563\n",
      "epoch: 2 step: 277, loss is 0.2980768382549286\n",
      "epoch: 2 step: 278, loss is 0.3953380286693573\n",
      "epoch: 2 step: 279, loss is 0.4841977059841156\n",
      "epoch: 2 step: 280, loss is 0.41225722432136536\n",
      "epoch: 2 step: 281, loss is 0.4450083076953888\n",
      "epoch: 2 step: 282, loss is 0.29741016030311584\n",
      "epoch: 2 step: 283, loss is 0.4984591007232666\n",
      "epoch: 2 step: 284, loss is 0.3987860083580017\n",
      "epoch: 2 step: 285, loss is 0.4991522431373596\n",
      "epoch: 2 step: 286, loss is 0.5585011839866638\n",
      "epoch: 2 step: 287, loss is 0.299939900636673\n",
      "epoch: 2 step: 288, loss is 0.5646846890449524\n",
      "epoch: 2 step: 289, loss is 0.47607603669166565\n",
      "epoch: 2 step: 290, loss is 0.3706792891025543\n",
      "epoch: 2 step: 291, loss is 0.5371971130371094\n",
      "epoch: 2 step: 292, loss is 0.36382097005844116\n",
      "epoch: 2 step: 293, loss is 0.4249814450740814\n",
      "epoch: 2 step: 294, loss is 0.3653102219104767\n",
      "epoch: 2 step: 295, loss is 0.27258574962615967\n",
      "epoch: 2 step: 296, loss is 0.2546614110469818\n",
      "epoch: 2 step: 297, loss is 0.5632067322731018\n",
      "epoch: 2 step: 298, loss is 0.5594980716705322\n",
      "epoch: 2 step: 299, loss is 0.2888856530189514\n",
      "epoch: 2 step: 300, loss is 0.26768621802330017\n",
      "epoch: 2 step: 301, loss is 0.4006741940975189\n",
      "epoch: 2 step: 302, loss is 0.3843417763710022\n",
      "epoch: 2 step: 303, loss is 0.3864985406398773\n",
      "epoch: 2 step: 304, loss is 0.290289044380188\n",
      "epoch: 2 step: 305, loss is 0.40716537833213806\n",
      "epoch: 2 step: 306, loss is 0.33812540769577026\n",
      "epoch: 2 step: 307, loss is 0.5264796018600464\n",
      "epoch: 2 step: 308, loss is 0.4279038608074188\n",
      "epoch: 2 step: 309, loss is 0.511711061000824\n",
      "epoch: 2 step: 310, loss is 0.464272677898407\n",
      "epoch: 2 step: 311, loss is 0.30583900213241577\n",
      "epoch: 2 step: 312, loss is 0.5551523566246033\n",
      "epoch: 2 step: 313, loss is 0.2675004005432129\n",
      "epoch: 2 step: 314, loss is 0.31752529740333557\n",
      "epoch: 2 step: 315, loss is 0.3306156098842621\n",
      "epoch: 2 step: 316, loss is 0.3798641264438629\n",
      "epoch: 2 step: 317, loss is 0.5307164192199707\n",
      "epoch: 2 step: 318, loss is 0.35406211018562317\n",
      "epoch: 2 step: 319, loss is 0.4576382040977478\n",
      "epoch: 2 step: 320, loss is 0.3418787121772766\n",
      "epoch: 2 step: 321, loss is 0.2722451686859131\n",
      "epoch: 2 step: 322, loss is 0.31672903895378113\n",
      "epoch: 2 step: 323, loss is 0.28538745641708374\n",
      "epoch: 2 step: 324, loss is 0.34464800357818604\n",
      "epoch: 2 step: 325, loss is 0.3991166353225708\n",
      "epoch: 2 step: 326, loss is 0.5327752828598022\n",
      "epoch: 2 step: 327, loss is 0.4717705249786377\n",
      "epoch: 2 step: 328, loss is 0.47311532497406006\n",
      "epoch: 2 step: 329, loss is 0.3769466280937195\n",
      "epoch: 2 step: 330, loss is 0.3110431730747223\n",
      "epoch: 2 step: 331, loss is 0.37496447563171387\n",
      "epoch: 2 step: 332, loss is 0.2842879295349121\n",
      "epoch: 2 step: 333, loss is 0.7063515782356262\n",
      "epoch: 2 step: 334, loss is 0.43084707856178284\n",
      "epoch: 2 step: 335, loss is 0.39305850863456726\n",
      "epoch: 2 step: 336, loss is 0.30736345052719116\n",
      "epoch: 2 step: 337, loss is 0.2766227126121521\n",
      "epoch: 2 step: 338, loss is 0.4598235487937927\n",
      "epoch: 2 step: 339, loss is 0.42835313081741333\n",
      "epoch: 2 step: 340, loss is 0.5449650287628174\n",
      "epoch: 2 step: 341, loss is 0.3880751132965088\n",
      "epoch: 2 step: 342, loss is 0.27730482816696167\n",
      "epoch: 2 step: 343, loss is 0.5213443636894226\n",
      "epoch: 2 step: 344, loss is 0.6435922384262085\n",
      "epoch: 2 step: 345, loss is 0.44827139377593994\n",
      "epoch: 2 step: 346, loss is 0.3207409083843231\n",
      "epoch: 2 step: 347, loss is 0.35340654850006104\n",
      "epoch: 2 step: 348, loss is 0.32041993737220764\n",
      "epoch: 2 step: 349, loss is 0.32323142886161804\n",
      "epoch: 2 step: 350, loss is 0.31534937024116516\n",
      "epoch: 2 step: 351, loss is 0.4225211441516876\n",
      "epoch: 2 step: 352, loss is 0.29067590832710266\n",
      "epoch: 2 step: 353, loss is 0.5791010856628418\n",
      "epoch: 2 step: 354, loss is 0.3535408675670624\n",
      "epoch: 2 step: 355, loss is 0.6328924894332886\n",
      "epoch: 2 step: 356, loss is 0.369063138961792\n",
      "epoch: 2 step: 357, loss is 0.22715210914611816\n",
      "epoch: 2 step: 358, loss is 0.2593301236629486\n",
      "epoch: 2 step: 359, loss is 0.5307413935661316\n",
      "epoch: 2 step: 360, loss is 0.5575799345970154\n",
      "epoch: 2 step: 361, loss is 0.5386466979980469\n",
      "epoch: 2 step: 362, loss is 0.34304121136665344\n",
      "epoch: 2 step: 363, loss is 0.2802833020687103\n",
      "epoch: 2 step: 364, loss is 0.343135267496109\n",
      "epoch: 2 step: 365, loss is 0.4146059453487396\n",
      "epoch: 2 step: 366, loss is 0.2186623364686966\n",
      "epoch: 2 step: 367, loss is 0.31353873014450073\n",
      "epoch: 2 step: 368, loss is 0.3487743139266968\n",
      "epoch: 2 step: 369, loss is 0.44249600172042847\n",
      "epoch: 2 step: 370, loss is 0.4571094810962677\n",
      "epoch: 2 step: 371, loss is 0.4790659248828888\n",
      "epoch: 2 step: 372, loss is 0.3017718195915222\n",
      "epoch: 2 step: 373, loss is 0.2795710563659668\n",
      "epoch: 2 step: 374, loss is 0.4713793396949768\n",
      "epoch: 2 step: 375, loss is 0.32182547450065613\n",
      "epoch: 2 step: 376, loss is 0.2328123301267624\n",
      "epoch: 2 step: 377, loss is 0.48329925537109375\n",
      "epoch: 2 step: 378, loss is 0.4396403431892395\n",
      "epoch: 2 step: 379, loss is 0.41445648670196533\n",
      "epoch: 2 step: 380, loss is 0.4124130606651306\n",
      "epoch: 2 step: 381, loss is 0.40799251198768616\n",
      "epoch: 2 step: 382, loss is 0.41121843457221985\n",
      "epoch: 2 step: 383, loss is 0.46392831206321716\n",
      "epoch: 2 step: 384, loss is 0.308455228805542\n",
      "epoch: 2 step: 385, loss is 0.40536925196647644\n",
      "epoch: 2 step: 386, loss is 0.5269145369529724\n",
      "epoch: 2 step: 387, loss is 0.3239423334598541\n",
      "epoch: 2 step: 388, loss is 0.6018149852752686\n",
      "epoch: 2 step: 389, loss is 0.3178597390651703\n",
      "epoch: 2 step: 390, loss is 0.28263115882873535\n",
      "epoch: 2 step: 391, loss is 0.30216968059539795\n",
      "epoch: 2 step: 392, loss is 0.3315616846084595\n",
      "epoch: 2 step: 393, loss is 0.447446346282959\n",
      "epoch: 2 step: 394, loss is 0.32236436009407043\n",
      "epoch: 2 step: 395, loss is 0.35401439666748047\n",
      "epoch: 2 step: 396, loss is 0.43975064158439636\n",
      "epoch: 2 step: 397, loss is 0.38098859786987305\n",
      "epoch: 2 step: 398, loss is 0.25685369968414307\n",
      "epoch: 2 step: 399, loss is 0.34390345215797424\n",
      "epoch: 2 step: 400, loss is 0.3784497380256653\n",
      "epoch: 2 step: 401, loss is 0.4477516710758209\n",
      "epoch: 2 step: 402, loss is 0.38294488191604614\n",
      "epoch: 2 step: 403, loss is 0.3415367007255554\n",
      "epoch: 2 step: 404, loss is 0.38158100843429565\n",
      "epoch: 2 step: 405, loss is 0.35227012634277344\n",
      "epoch: 2 step: 406, loss is 0.4058630168437958\n",
      "epoch: 2 step: 407, loss is 0.5194461941719055\n",
      "epoch: 2 step: 408, loss is 0.3374495804309845\n",
      "epoch: 2 step: 409, loss is 0.33040091395378113\n",
      "epoch: 2 step: 410, loss is 0.27939489483833313\n",
      "epoch: 2 step: 411, loss is 0.3445419371128082\n",
      "epoch: 2 step: 412, loss is 0.6137789487838745\n",
      "epoch: 2 step: 413, loss is 0.5657585859298706\n",
      "epoch: 2 step: 414, loss is 0.46529340744018555\n",
      "epoch: 2 step: 415, loss is 0.4004143476486206\n",
      "epoch: 2 step: 416, loss is 0.39331841468811035\n",
      "epoch: 2 step: 417, loss is 0.45253899693489075\n",
      "epoch: 2 step: 418, loss is 0.4436597228050232\n",
      "epoch: 2 step: 419, loss is 0.30703043937683105\n",
      "epoch: 2 step: 420, loss is 0.6987862586975098\n",
      "epoch: 2 step: 421, loss is 0.34323859214782715\n",
      "epoch: 2 step: 422, loss is 0.3237015902996063\n",
      "epoch: 2 step: 423, loss is 0.4167393743991852\n",
      "epoch: 2 step: 424, loss is 0.4302971661090851\n",
      "epoch: 2 step: 425, loss is 0.5962510704994202\n",
      "epoch: 2 step: 426, loss is 0.32293596863746643\n",
      "epoch: 2 step: 427, loss is 0.3076538145542145\n",
      "epoch: 2 step: 428, loss is 0.3130960166454315\n",
      "epoch: 2 step: 429, loss is 0.3492788076400757\n",
      "epoch: 2 step: 430, loss is 0.4312300682067871\n",
      "epoch: 2 step: 431, loss is 0.6553462743759155\n",
      "epoch: 2 step: 432, loss is 0.3868366777896881\n",
      "epoch: 2 step: 433, loss is 0.6496057510375977\n",
      "epoch: 2 step: 434, loss is 0.34514036774635315\n",
      "epoch: 2 step: 435, loss is 0.6783977150917053\n",
      "epoch: 2 step: 436, loss is 0.3792612850666046\n",
      "epoch: 2 step: 437, loss is 0.47455859184265137\n",
      "epoch: 2 step: 438, loss is 0.3493027985095978\n",
      "epoch: 2 step: 439, loss is 0.32993748784065247\n",
      "epoch: 2 step: 440, loss is 0.32693731784820557\n",
      "epoch: 2 step: 441, loss is 0.6561874747276306\n",
      "epoch: 2 step: 442, loss is 0.5022681355476379\n",
      "epoch: 2 step: 443, loss is 0.3913792371749878\n",
      "epoch: 2 step: 444, loss is 0.523773193359375\n",
      "epoch: 2 step: 445, loss is 0.338899165391922\n",
      "epoch: 2 step: 446, loss is 0.37547188997268677\n",
      "epoch: 2 step: 447, loss is 0.3752608001232147\n",
      "epoch: 2 step: 448, loss is 0.3247247636318207\n",
      "epoch: 2 step: 449, loss is 0.22712922096252441\n",
      "epoch: 2 step: 450, loss is 0.4233771562576294\n",
      "epoch: 2 step: 451, loss is 0.21431486308574677\n",
      "epoch: 2 step: 452, loss is 0.3181607127189636\n",
      "epoch: 2 step: 453, loss is 0.5199256539344788\n",
      "epoch: 2 step: 454, loss is 0.5506443977355957\n",
      "epoch: 2 step: 455, loss is 0.28352081775665283\n",
      "epoch: 2 step: 456, loss is 0.5100349187850952\n",
      "epoch: 2 step: 457, loss is 0.3093569576740265\n",
      "epoch: 2 step: 458, loss is 0.7480167150497437\n",
      "epoch: 2 step: 459, loss is 0.312129944562912\n",
      "epoch: 2 step: 460, loss is 0.3588798940181732\n",
      "epoch: 2 step: 461, loss is 0.5793145895004272\n",
      "epoch: 2 step: 462, loss is 0.2936534285545349\n",
      "epoch: 2 step: 463, loss is 0.30075207352638245\n",
      "epoch: 2 step: 464, loss is 0.30089324712753296\n",
      "epoch: 2 step: 465, loss is 0.5568419098854065\n",
      "epoch: 2 step: 466, loss is 0.33305320143699646\n",
      "epoch: 2 step: 467, loss is 0.4044465720653534\n",
      "epoch: 2 step: 468, loss is 0.7288395762443542\n",
      "epoch: 2 step: 469, loss is 0.43336790800094604\n",
      "epoch: 2 step: 470, loss is 0.3565925359725952\n",
      "epoch: 2 step: 471, loss is 0.6039153933525085\n",
      "epoch: 2 step: 472, loss is 0.4028888940811157\n",
      "epoch: 2 step: 473, loss is 0.24719645082950592\n",
      "epoch: 2 step: 474, loss is 0.3742732107639313\n",
      "epoch: 2 step: 475, loss is 0.4652143120765686\n",
      "epoch: 2 step: 476, loss is 0.4104747176170349\n",
      "epoch: 2 step: 477, loss is 0.27064165472984314\n",
      "epoch: 2 step: 478, loss is 0.34252995252609253\n",
      "epoch: 2 step: 479, loss is 0.3928634524345398\n",
      "epoch: 2 step: 480, loss is 0.35452038049697876\n",
      "epoch: 2 step: 481, loss is 0.26978081464767456\n",
      "epoch: 2 step: 482, loss is 0.3107370138168335\n",
      "epoch: 2 step: 483, loss is 0.4811815023422241\n",
      "epoch: 2 step: 484, loss is 0.33233532309532166\n",
      "epoch: 2 step: 485, loss is 0.49968665838241577\n",
      "epoch: 2 step: 486, loss is 0.3694651424884796\n",
      "epoch: 2 step: 487, loss is 0.4760262370109558\n",
      "epoch: 2 step: 488, loss is 0.3049689829349518\n",
      "epoch: 2 step: 489, loss is 0.3363650441169739\n",
      "epoch: 2 step: 490, loss is 0.4761063754558563\n",
      "epoch: 2 step: 491, loss is 0.45968538522720337\n",
      "epoch: 2 step: 492, loss is 0.565895140171051\n",
      "epoch: 2 step: 493, loss is 0.3938480317592621\n",
      "epoch: 2 step: 494, loss is 0.49054408073425293\n",
      "epoch: 2 step: 495, loss is 0.549060046672821\n",
      "epoch: 2 step: 496, loss is 0.5186908841133118\n",
      "epoch: 2 step: 497, loss is 0.43808987736701965\n",
      "epoch: 2 step: 498, loss is 0.26993197202682495\n",
      "epoch: 2 step: 499, loss is 0.3086552619934082\n",
      "epoch: 2 step: 500, loss is 0.6566805243492126\n",
      "epoch: 2 step: 501, loss is 0.2755466103553772\n",
      "epoch: 2 step: 502, loss is 0.3438604474067688\n",
      "epoch: 2 step: 503, loss is 0.3387177884578705\n",
      "epoch: 2 step: 504, loss is 0.43137601017951965\n",
      "epoch: 2 step: 505, loss is 0.4249974191188812\n",
      "epoch: 2 step: 506, loss is 0.3929134011268616\n",
      "epoch: 2 step: 507, loss is 0.32446572184562683\n",
      "epoch: 2 step: 508, loss is 0.4010792374610901\n",
      "epoch: 2 step: 509, loss is 0.2919440269470215\n",
      "epoch: 2 step: 510, loss is 0.24029342830181122\n",
      "epoch: 2 step: 511, loss is 0.4074317514896393\n",
      "epoch: 2 step: 512, loss is 0.2922855317592621\n",
      "epoch: 2 step: 513, loss is 0.28466662764549255\n",
      "epoch: 2 step: 514, loss is 0.40425679087638855\n",
      "epoch: 2 step: 515, loss is 0.41665974259376526\n",
      "epoch: 2 step: 516, loss is 0.5495980381965637\n",
      "epoch: 2 step: 517, loss is 0.4462537169456482\n",
      "epoch: 2 step: 518, loss is 0.30127108097076416\n",
      "epoch: 2 step: 519, loss is 0.647551953792572\n",
      "epoch: 2 step: 520, loss is 0.589949905872345\n",
      "epoch: 2 step: 521, loss is 0.37499746680259705\n",
      "epoch: 2 step: 522, loss is 0.5631739497184753\n",
      "epoch: 2 step: 523, loss is 0.5238776206970215\n",
      "epoch: 2 step: 524, loss is 0.3779841363430023\n",
      "epoch: 2 step: 525, loss is 0.32846707105636597\n",
      "epoch: 2 step: 526, loss is 0.4384755790233612\n",
      "epoch: 2 step: 527, loss is 0.30569103360176086\n",
      "epoch: 2 step: 528, loss is 0.4488651752471924\n",
      "epoch: 2 step: 529, loss is 0.38850775361061096\n",
      "epoch: 2 step: 530, loss is 0.3995426595211029\n",
      "epoch: 2 step: 531, loss is 0.33783280849456787\n",
      "epoch: 2 step: 532, loss is 0.3208719491958618\n",
      "epoch: 2 step: 533, loss is 0.47205793857574463\n",
      "epoch: 2 step: 534, loss is 0.28242555260658264\n",
      "epoch: 2 step: 535, loss is 0.453702449798584\n",
      "epoch: 2 step: 536, loss is 0.4908365309238434\n",
      "epoch: 2 step: 537, loss is 0.6844419240951538\n",
      "epoch: 2 step: 538, loss is 0.3313175439834595\n",
      "epoch: 2 step: 539, loss is 0.3485780358314514\n",
      "epoch: 2 step: 540, loss is 0.5020686388015747\n",
      "epoch: 2 step: 541, loss is 0.38012754917144775\n",
      "epoch: 2 step: 542, loss is 0.44523584842681885\n",
      "epoch: 2 step: 543, loss is 0.34663355350494385\n",
      "epoch: 2 step: 544, loss is 0.3752548098564148\n",
      "epoch: 2 step: 545, loss is 0.5971747040748596\n",
      "epoch: 2 step: 546, loss is 0.5353913903236389\n",
      "epoch: 2 step: 547, loss is 0.44142308831214905\n",
      "epoch: 2 step: 548, loss is 0.23773205280303955\n",
      "epoch: 2 step: 549, loss is 0.33684173226356506\n",
      "epoch: 2 step: 550, loss is 0.4210791289806366\n",
      "epoch: 2 step: 551, loss is 0.3224726915359497\n",
      "epoch: 2 step: 552, loss is 0.27606865763664246\n",
      "epoch: 2 step: 553, loss is 0.4683293402194977\n",
      "epoch: 2 step: 554, loss is 0.4029655456542969\n",
      "epoch: 2 step: 555, loss is 0.4128534495830536\n",
      "epoch: 2 step: 556, loss is 0.33693942427635193\n",
      "epoch: 2 step: 557, loss is 0.2848511040210724\n",
      "epoch: 2 step: 558, loss is 0.46346738934516907\n",
      "epoch: 2 step: 559, loss is 0.5136740207672119\n",
      "epoch: 2 step: 560, loss is 0.3097034990787506\n",
      "epoch: 2 step: 561, loss is 0.39959660172462463\n",
      "epoch: 2 step: 562, loss is 0.432260662317276\n",
      "epoch: 2 step: 563, loss is 0.19989238679409027\n",
      "epoch: 2 step: 564, loss is 0.28824910521507263\n",
      "epoch: 2 step: 565, loss is 0.4632633328437805\n",
      "epoch: 2 step: 566, loss is 0.31956663727760315\n",
      "epoch: 2 step: 567, loss is 0.40279045701026917\n",
      "epoch: 2 step: 568, loss is 0.3061603307723999\n",
      "epoch: 2 step: 569, loss is 0.4146984815597534\n",
      "epoch: 2 step: 570, loss is 0.5166376233100891\n",
      "epoch: 2 step: 571, loss is 0.31077584624290466\n",
      "epoch: 2 step: 572, loss is 0.40370750427246094\n",
      "epoch: 2 step: 573, loss is 0.46206945180892944\n",
      "epoch: 2 step: 574, loss is 0.3728753328323364\n",
      "epoch: 2 step: 575, loss is 0.44766587018966675\n",
      "epoch: 2 step: 576, loss is 0.3929268717765808\n",
      "epoch: 2 step: 577, loss is 0.4211413860321045\n",
      "epoch: 2 step: 578, loss is 0.19744890928268433\n",
      "epoch: 2 step: 579, loss is 0.33937957882881165\n",
      "epoch: 2 step: 580, loss is 0.31544119119644165\n",
      "epoch: 2 step: 581, loss is 0.2384463995695114\n",
      "epoch: 2 step: 582, loss is 0.36095839738845825\n",
      "epoch: 2 step: 583, loss is 0.46056947112083435\n",
      "epoch: 2 step: 584, loss is 0.38784530758857727\n",
      "epoch: 2 step: 585, loss is 0.4967862069606781\n",
      "epoch: 2 step: 586, loss is 0.5442408323287964\n",
      "epoch: 2 step: 587, loss is 0.19388005137443542\n",
      "epoch: 2 step: 588, loss is 0.3258647620677948\n",
      "epoch: 2 step: 589, loss is 0.266399621963501\n",
      "epoch: 2 step: 590, loss is 0.3842538297176361\n",
      "epoch: 2 step: 591, loss is 0.4880768954753876\n",
      "epoch: 2 step: 592, loss is 0.3475309908390045\n",
      "epoch: 2 step: 593, loss is 0.20038318634033203\n",
      "epoch: 2 step: 594, loss is 0.34280508756637573\n",
      "epoch: 2 step: 595, loss is 0.2312384694814682\n",
      "epoch: 2 step: 596, loss is 0.47762948274612427\n",
      "epoch: 2 step: 597, loss is 0.46423307061195374\n",
      "epoch: 2 step: 598, loss is 0.3977211117744446\n",
      "epoch: 2 step: 599, loss is 0.2599083483219147\n",
      "epoch: 2 step: 600, loss is 0.3356602191925049\n",
      "epoch: 2 step: 601, loss is 0.5304452180862427\n",
      "epoch: 2 step: 602, loss is 0.39700329303741455\n",
      "epoch: 2 step: 603, loss is 0.5139588117599487\n",
      "epoch: 2 step: 604, loss is 0.3594815135002136\n",
      "epoch: 2 step: 605, loss is 0.4879259467124939\n",
      "epoch: 2 step: 606, loss is 0.6219438314437866\n",
      "epoch: 2 step: 607, loss is 0.42456603050231934\n",
      "epoch: 2 step: 608, loss is 0.4641754627227783\n",
      "epoch: 2 step: 609, loss is 0.42231306433677673\n",
      "epoch: 2 step: 610, loss is 0.5354740023612976\n",
      "epoch: 2 step: 611, loss is 0.3707767426967621\n",
      "epoch: 2 step: 612, loss is 0.45878303050994873\n",
      "epoch: 2 step: 613, loss is 0.5423187613487244\n",
      "epoch: 2 step: 614, loss is 0.46572330594062805\n",
      "epoch: 2 step: 615, loss is 0.5114208459854126\n",
      "epoch: 2 step: 616, loss is 0.3256227970123291\n",
      "epoch: 2 step: 617, loss is 0.5062772035598755\n",
      "epoch: 2 step: 618, loss is 0.4464164972305298\n",
      "epoch: 2 step: 619, loss is 0.3579988181591034\n",
      "epoch: 2 step: 620, loss is 0.3198769986629486\n",
      "epoch: 2 step: 621, loss is 0.35064923763275146\n",
      "epoch: 2 step: 622, loss is 0.24503330886363983\n",
      "epoch: 2 step: 623, loss is 0.46948984265327454\n",
      "epoch: 2 step: 624, loss is 0.41471561789512634\n",
      "epoch: 2 step: 625, loss is 0.2956741452217102\n",
      "epoch: 2 step: 626, loss is 0.40130817890167236\n",
      "epoch: 2 step: 627, loss is 0.29994311928749084\n",
      "epoch: 2 step: 628, loss is 0.6216908097267151\n",
      "epoch: 2 step: 629, loss is 0.41514697670936584\n",
      "epoch: 2 step: 630, loss is 0.5011263489723206\n",
      "epoch: 2 step: 631, loss is 0.5628061294555664\n",
      "epoch: 2 step: 632, loss is 0.34432899951934814\n",
      "epoch: 2 step: 633, loss is 0.31717467308044434\n",
      "epoch: 2 step: 634, loss is 0.34289950132369995\n",
      "epoch: 2 step: 635, loss is 0.32427850365638733\n",
      "epoch: 2 step: 636, loss is 0.27657440304756165\n",
      "epoch: 2 step: 637, loss is 0.24605132639408112\n",
      "epoch: 2 step: 638, loss is 0.27664923667907715\n",
      "epoch: 2 step: 639, loss is 0.3260582983493805\n",
      "epoch: 2 step: 640, loss is 0.2978893220424652\n",
      "epoch: 2 step: 641, loss is 0.504331111907959\n",
      "epoch: 2 step: 642, loss is 0.4717158079147339\n",
      "epoch: 2 step: 643, loss is 0.3394336402416229\n",
      "epoch: 2 step: 644, loss is 0.2723690867424011\n",
      "epoch: 2 step: 645, loss is 0.2876113951206207\n",
      "epoch: 2 step: 646, loss is 0.4414632320404053\n",
      "epoch: 2 step: 647, loss is 0.34321895241737366\n",
      "epoch: 2 step: 648, loss is 0.48332613706588745\n",
      "epoch: 2 step: 649, loss is 0.4852222204208374\n",
      "epoch: 2 step: 650, loss is 0.3694112300872803\n",
      "epoch: 2 step: 651, loss is 0.3683088719844818\n",
      "epoch: 2 step: 652, loss is 0.30228179693222046\n",
      "epoch: 2 step: 653, loss is 0.2912966310977936\n",
      "epoch: 2 step: 654, loss is 0.29771310091018677\n",
      "epoch: 2 step: 655, loss is 0.33418676257133484\n",
      "epoch: 2 step: 656, loss is 0.47950080037117004\n",
      "epoch: 2 step: 657, loss is 0.3950735032558441\n",
      "epoch: 2 step: 658, loss is 0.34724748134613037\n",
      "epoch: 2 step: 659, loss is 0.30218449234962463\n",
      "epoch: 2 step: 660, loss is 0.4478939473628998\n",
      "epoch: 2 step: 661, loss is 0.456803560256958\n",
      "epoch: 2 step: 662, loss is 0.43278467655181885\n",
      "epoch: 2 step: 663, loss is 0.2571508586406708\n",
      "epoch: 2 step: 664, loss is 0.36697033047676086\n",
      "epoch: 2 step: 665, loss is 0.4064219295978546\n",
      "epoch: 2 step: 666, loss is 0.32143434882164\n",
      "epoch: 2 step: 667, loss is 0.43494895100593567\n",
      "epoch: 2 step: 668, loss is 0.3812793493270874\n",
      "epoch: 2 step: 669, loss is 0.5739426016807556\n",
      "epoch: 2 step: 670, loss is 0.3478712737560272\n",
      "epoch: 2 step: 671, loss is 0.3225381374359131\n",
      "epoch: 2 step: 672, loss is 0.38575902581214905\n",
      "epoch: 2 step: 673, loss is 0.3074541389942169\n",
      "epoch: 2 step: 674, loss is 0.3495544493198395\n",
      "epoch: 2 step: 675, loss is 0.25014185905456543\n",
      "epoch: 2 step: 676, loss is 0.6466615796089172\n",
      "epoch: 2 step: 677, loss is 0.3242429196834564\n",
      "epoch: 2 step: 678, loss is 0.37321144342422485\n",
      "epoch: 2 step: 679, loss is 0.39545032382011414\n",
      "epoch: 2 step: 680, loss is 0.2971215546131134\n",
      "epoch: 2 step: 681, loss is 0.33661654591560364\n",
      "epoch: 2 step: 682, loss is 0.45783159136772156\n",
      "epoch: 2 step: 683, loss is 0.5929059386253357\n",
      "epoch: 2 step: 684, loss is 0.419183611869812\n",
      "epoch: 2 step: 685, loss is 0.5249278545379639\n",
      "epoch: 2 step: 686, loss is 0.5996756553649902\n",
      "epoch: 2 step: 687, loss is 0.2749593257904053\n",
      "epoch: 2 step: 688, loss is 0.4081093370914459\n",
      "epoch: 2 step: 689, loss is 0.32745060324668884\n",
      "epoch: 2 step: 690, loss is 0.44102951884269714\n",
      "epoch: 2 step: 691, loss is 0.4103728234767914\n",
      "epoch: 2 step: 692, loss is 0.417307585477829\n",
      "epoch: 2 step: 693, loss is 0.22328270971775055\n",
      "epoch: 2 step: 694, loss is 0.4731907844543457\n",
      "epoch: 2 step: 695, loss is 0.37144389748573303\n",
      "epoch: 2 step: 696, loss is 0.24193470180034637\n",
      "epoch: 2 step: 697, loss is 0.4669186472892761\n",
      "epoch: 2 step: 698, loss is 0.3531050682067871\n",
      "epoch: 2 step: 699, loss is 0.3606133460998535\n",
      "epoch: 2 step: 700, loss is 0.36378493905067444\n",
      "epoch: 2 step: 701, loss is 0.472949743270874\n",
      "epoch: 2 step: 702, loss is 0.21248799562454224\n",
      "epoch: 2 step: 703, loss is 0.33898410201072693\n",
      "epoch: 2 step: 704, loss is 0.5251495838165283\n",
      "epoch: 2 step: 705, loss is 0.632459282875061\n",
      "epoch: 2 step: 706, loss is 0.2597447633743286\n",
      "epoch: 2 step: 707, loss is 0.37990033626556396\n",
      "epoch: 2 step: 708, loss is 0.3017849624156952\n",
      "epoch: 2 step: 709, loss is 0.37386831641197205\n",
      "epoch: 2 step: 710, loss is 0.444835364818573\n",
      "epoch: 2 step: 711, loss is 0.30815863609313965\n",
      "epoch: 2 step: 712, loss is 0.32298633456230164\n",
      "epoch: 2 step: 713, loss is 0.5317666530609131\n",
      "epoch: 2 step: 714, loss is 0.4195796549320221\n",
      "epoch: 2 step: 715, loss is 0.3002326488494873\n",
      "epoch: 2 step: 716, loss is 0.3639339208602905\n",
      "epoch: 2 step: 717, loss is 0.50083988904953\n",
      "epoch: 2 step: 718, loss is 0.28371500968933105\n",
      "epoch: 2 step: 719, loss is 0.3081810474395752\n",
      "epoch: 2 step: 720, loss is 0.2883340120315552\n",
      "epoch: 2 step: 721, loss is 0.41021013259887695\n",
      "epoch: 2 step: 722, loss is 0.33681944012641907\n",
      "epoch: 2 step: 723, loss is 0.46815118193626404\n",
      "epoch: 2 step: 724, loss is 0.3150372803211212\n",
      "epoch: 2 step: 725, loss is 0.24246405065059662\n",
      "epoch: 2 step: 726, loss is 0.3530309796333313\n",
      "epoch: 2 step: 727, loss is 0.3981841504573822\n",
      "epoch: 2 step: 728, loss is 0.25362980365753174\n",
      "epoch: 2 step: 729, loss is 0.4386852979660034\n",
      "epoch: 2 step: 730, loss is 0.5262991189956665\n",
      "epoch: 2 step: 731, loss is 0.3555118441581726\n",
      "epoch: 2 step: 732, loss is 0.2879970967769623\n",
      "epoch: 2 step: 733, loss is 0.33516329526901245\n",
      "epoch: 2 step: 734, loss is 0.4305999279022217\n",
      "epoch: 2 step: 735, loss is 0.4708060026168823\n",
      "epoch: 2 step: 736, loss is 0.31376203894615173\n",
      "epoch: 2 step: 737, loss is 0.20278136432170868\n",
      "epoch: 2 step: 738, loss is 0.2968214750289917\n",
      "epoch: 2 step: 739, loss is 0.3826698958873749\n",
      "epoch: 2 step: 740, loss is 0.3743879795074463\n",
      "epoch: 2 step: 741, loss is 0.4661826491355896\n",
      "epoch: 2 step: 742, loss is 0.3633632957935333\n",
      "epoch: 2 step: 743, loss is 0.2904937267303467\n",
      "epoch: 2 step: 744, loss is 0.3678440451622009\n",
      "epoch: 2 step: 745, loss is 0.4770960211753845\n",
      "epoch: 2 step: 746, loss is 0.31161776185035706\n",
      "epoch: 2 step: 747, loss is 0.24680568277835846\n",
      "epoch: 2 step: 748, loss is 0.4840511679649353\n",
      "epoch: 2 step: 749, loss is 0.41704416275024414\n",
      "epoch: 2 step: 750, loss is 0.3476541042327881\n",
      "epoch: 2 step: 751, loss is 0.3513117730617523\n",
      "epoch: 2 step: 752, loss is 0.4777912199497223\n",
      "epoch: 2 step: 753, loss is 0.2587321400642395\n",
      "epoch: 2 step: 754, loss is 0.4766102433204651\n",
      "epoch: 2 step: 755, loss is 0.17157521843910217\n",
      "epoch: 2 step: 756, loss is 0.4694872796535492\n",
      "epoch: 2 step: 757, loss is 0.3613845407962799\n",
      "epoch: 2 step: 758, loss is 0.4294053614139557\n",
      "epoch: 2 step: 759, loss is 0.34602105617523193\n",
      "epoch: 2 step: 760, loss is 0.29117029905319214\n",
      "epoch: 2 step: 761, loss is 0.45602020621299744\n",
      "epoch: 2 step: 762, loss is 0.44038698077201843\n",
      "epoch: 2 step: 763, loss is 0.4079433083534241\n",
      "epoch: 2 step: 764, loss is 0.4290502667427063\n",
      "epoch: 2 step: 765, loss is 0.2466270923614502\n",
      "epoch: 2 step: 766, loss is 0.4782412648200989\n",
      "epoch: 2 step: 767, loss is 0.25892528891563416\n",
      "epoch: 2 step: 768, loss is 0.2705985903739929\n",
      "epoch: 2 step: 769, loss is 0.1860101819038391\n",
      "epoch: 2 step: 770, loss is 0.37388986349105835\n",
      "epoch: 2 step: 771, loss is 0.267645001411438\n",
      "epoch: 2 step: 772, loss is 0.3480776250362396\n",
      "epoch: 2 step: 773, loss is 0.3858785927295685\n",
      "epoch: 2 step: 774, loss is 0.2224048376083374\n",
      "epoch: 2 step: 775, loss is 0.27687519788742065\n",
      "epoch: 2 step: 776, loss is 0.4105377197265625\n",
      "epoch: 2 step: 777, loss is 0.3075147867202759\n",
      "epoch: 2 step: 778, loss is 0.3893866240978241\n",
      "epoch: 2 step: 779, loss is 0.33446449041366577\n",
      "epoch: 2 step: 780, loss is 0.5292457342147827\n",
      "epoch: 2 step: 781, loss is 0.31174135208129883\n",
      "epoch: 2 step: 782, loss is 0.21353739500045776\n",
      "epoch: 2 step: 783, loss is 0.30799150466918945\n",
      "epoch: 2 step: 784, loss is 0.48964768648147583\n",
      "epoch: 2 step: 785, loss is 0.3085428476333618\n",
      "epoch: 2 step: 786, loss is 0.6186997294425964\n",
      "epoch: 2 step: 787, loss is 0.37870824337005615\n",
      "epoch: 2 step: 788, loss is 0.31808003783226013\n",
      "epoch: 2 step: 789, loss is 0.34316524863243103\n",
      "epoch: 2 step: 790, loss is 0.39177218079566956\n",
      "epoch: 2 step: 791, loss is 0.35174107551574707\n",
      "epoch: 2 step: 792, loss is 0.39737311005592346\n",
      "epoch: 2 step: 793, loss is 0.3517421782016754\n",
      "epoch: 2 step: 794, loss is 0.4645702838897705\n",
      "epoch: 2 step: 795, loss is 0.27586978673934937\n",
      "epoch: 2 step: 796, loss is 0.39845162630081177\n",
      "epoch: 2 step: 797, loss is 0.25232943892478943\n",
      "epoch: 2 step: 798, loss is 0.37511202692985535\n",
      "epoch: 2 step: 799, loss is 0.18902207911014557\n",
      "epoch: 2 step: 800, loss is 0.32526275515556335\n",
      "epoch: 2 step: 801, loss is 0.3470035791397095\n",
      "epoch: 2 step: 802, loss is 0.33081015944480896\n",
      "epoch: 2 step: 803, loss is 0.3110862970352173\n",
      "epoch: 2 step: 804, loss is 0.33657369017601013\n",
      "epoch: 2 step: 805, loss is 0.4946015775203705\n",
      "epoch: 2 step: 806, loss is 0.2453557699918747\n",
      "epoch: 2 step: 807, loss is 0.32738375663757324\n",
      "epoch: 2 step: 808, loss is 0.34789350628852844\n",
      "epoch: 2 step: 809, loss is 0.39836594462394714\n",
      "epoch: 2 step: 810, loss is 0.32626375555992126\n",
      "epoch: 2 step: 811, loss is 0.41029176115989685\n",
      "epoch: 2 step: 812, loss is 0.6630387306213379\n",
      "epoch: 2 step: 813, loss is 0.28869694471359253\n",
      "epoch: 2 step: 814, loss is 0.24432694911956787\n",
      "epoch: 2 step: 815, loss is 0.35427990555763245\n",
      "epoch: 2 step: 816, loss is 0.34576308727264404\n",
      "epoch: 2 step: 817, loss is 0.4206967055797577\n",
      "epoch: 2 step: 818, loss is 0.33207058906555176\n",
      "epoch: 2 step: 819, loss is 0.4323558211326599\n",
      "epoch: 2 step: 820, loss is 0.2068309783935547\n",
      "epoch: 2 step: 821, loss is 0.2177373319864273\n",
      "epoch: 2 step: 822, loss is 0.28758957982063293\n",
      "epoch: 2 step: 823, loss is 0.24203023314476013\n",
      "epoch: 2 step: 824, loss is 0.4409729242324829\n",
      "epoch: 2 step: 825, loss is 0.3375341594219208\n",
      "epoch: 2 step: 826, loss is 0.25725042819976807\n",
      "epoch: 2 step: 827, loss is 0.3520781397819519\n",
      "epoch: 2 step: 828, loss is 0.4584720730781555\n",
      "epoch: 2 step: 829, loss is 0.34800463914871216\n",
      "epoch: 2 step: 830, loss is 0.5747295022010803\n",
      "epoch: 2 step: 831, loss is 0.30293533205986023\n",
      "epoch: 2 step: 832, loss is 0.2569211423397064\n",
      "epoch: 2 step: 833, loss is 0.30184462666511536\n",
      "epoch: 2 step: 834, loss is 0.5758658051490784\n",
      "epoch: 2 step: 835, loss is 0.3509850800037384\n",
      "epoch: 2 step: 836, loss is 0.3322591185569763\n",
      "epoch: 2 step: 837, loss is 0.3390369415283203\n",
      "epoch: 2 step: 838, loss is 0.3276571035385132\n",
      "epoch: 2 step: 839, loss is 0.468426913022995\n",
      "epoch: 2 step: 840, loss is 0.6508282423019409\n",
      "epoch: 2 step: 841, loss is 0.5198193788528442\n",
      "epoch: 2 step: 842, loss is 0.34239688515663147\n",
      "epoch: 2 step: 843, loss is 0.6249319314956665\n",
      "epoch: 2 step: 844, loss is 0.3649437129497528\n",
      "epoch: 2 step: 845, loss is 0.3730972707271576\n",
      "epoch: 2 step: 846, loss is 0.19959516823291779\n",
      "epoch: 2 step: 847, loss is 0.36295536160469055\n",
      "epoch: 2 step: 848, loss is 0.2669994831085205\n",
      "epoch: 2 step: 849, loss is 0.503408670425415\n",
      "epoch: 2 step: 850, loss is 0.3732970058917999\n",
      "epoch: 2 step: 851, loss is 0.4605114758014679\n",
      "epoch: 2 step: 852, loss is 0.21274493634700775\n",
      "epoch: 2 step: 853, loss is 0.39773377776145935\n",
      "epoch: 2 step: 854, loss is 0.3452400267124176\n",
      "epoch: 2 step: 855, loss is 0.32673823833465576\n",
      "epoch: 2 step: 856, loss is 0.3072211444377899\n",
      "epoch: 2 step: 857, loss is 0.3785010576248169\n",
      "epoch: 2 step: 858, loss is 0.2951412498950958\n",
      "epoch: 2 step: 859, loss is 0.5649238228797913\n",
      "epoch: 2 step: 860, loss is 0.2790851891040802\n",
      "epoch: 2 step: 861, loss is 0.21556967496871948\n",
      "epoch: 2 step: 862, loss is 0.46289318799972534\n",
      "epoch: 2 step: 863, loss is 0.20772333443164825\n",
      "epoch: 2 step: 864, loss is 0.3331952691078186\n",
      "epoch: 2 step: 865, loss is 0.18385687470436096\n",
      "epoch: 2 step: 866, loss is 0.4774877429008484\n",
      "epoch: 2 step: 867, loss is 0.3410398066043854\n",
      "epoch: 2 step: 868, loss is 0.2872699201107025\n",
      "epoch: 2 step: 869, loss is 0.4799085259437561\n",
      "epoch: 2 step: 870, loss is 0.33023324608802795\n",
      "epoch: 2 step: 871, loss is 0.3814692199230194\n",
      "epoch: 2 step: 872, loss is 0.486721009016037\n",
      "epoch: 2 step: 873, loss is 0.21381305158138275\n",
      "epoch: 2 step: 874, loss is 0.42492011189460754\n",
      "epoch: 2 step: 875, loss is 0.49555760622024536\n",
      "epoch: 2 step: 876, loss is 0.38518664240837097\n",
      "epoch: 2 step: 877, loss is 0.6837377548217773\n",
      "epoch: 2 step: 878, loss is 0.3121121823787689\n",
      "epoch: 2 step: 879, loss is 0.3141251802444458\n",
      "epoch: 2 step: 880, loss is 0.3572191894054413\n",
      "epoch: 2 step: 881, loss is 0.2714673578739166\n",
      "epoch: 2 step: 882, loss is 0.4099810719490051\n",
      "epoch: 2 step: 883, loss is 0.3785170018672943\n",
      "epoch: 2 step: 884, loss is 0.4814566373825073\n",
      "epoch: 2 step: 885, loss is 0.4194466173648834\n",
      "epoch: 2 step: 886, loss is 0.3544471561908722\n",
      "epoch: 2 step: 887, loss is 0.3800254166126251\n",
      "epoch: 2 step: 888, loss is 0.574838399887085\n",
      "epoch: 2 step: 889, loss is 0.30913245677948\n",
      "epoch: 2 step: 890, loss is 0.19853021204471588\n",
      "epoch: 2 step: 891, loss is 0.31808048486709595\n",
      "epoch: 2 step: 892, loss is 0.3578788936138153\n",
      "epoch: 2 step: 893, loss is 0.3187050521373749\n",
      "epoch: 2 step: 894, loss is 0.40532538294792175\n",
      "epoch: 2 step: 895, loss is 0.563158392906189\n",
      "epoch: 2 step: 896, loss is 0.4193543493747711\n",
      "epoch: 2 step: 897, loss is 0.2498420625925064\n",
      "epoch: 2 step: 898, loss is 0.26524439454078674\n",
      "epoch: 2 step: 899, loss is 0.19777171313762665\n",
      "epoch: 2 step: 900, loss is 0.40114322304725647\n",
      "epoch: 2 step: 901, loss is 0.46852371096611023\n",
      "epoch: 2 step: 902, loss is 0.5220268964767456\n",
      "epoch: 2 step: 903, loss is 0.4328901469707489\n",
      "epoch: 2 step: 904, loss is 0.34307366609573364\n",
      "epoch: 2 step: 905, loss is 0.3437481224536896\n",
      "epoch: 2 step: 906, loss is 0.21624134480953217\n",
      "epoch: 2 step: 907, loss is 0.3846549391746521\n",
      "epoch: 2 step: 908, loss is 0.5592218637466431\n",
      "epoch: 2 step: 909, loss is 0.3511832356452942\n",
      "epoch: 2 step: 910, loss is 0.4222457706928253\n",
      "epoch: 2 step: 911, loss is 0.6685351133346558\n",
      "epoch: 2 step: 912, loss is 0.35337188839912415\n",
      "epoch: 2 step: 913, loss is 0.3103378713130951\n",
      "epoch: 2 step: 914, loss is 0.3447716236114502\n",
      "epoch: 2 step: 915, loss is 0.6233581900596619\n",
      "epoch: 2 step: 916, loss is 0.3970378339290619\n",
      "epoch: 2 step: 917, loss is 0.49170631170272827\n",
      "epoch: 2 step: 918, loss is 0.3009297549724579\n",
      "epoch: 2 step: 919, loss is 0.49303969740867615\n",
      "epoch: 2 step: 920, loss is 0.3040624260902405\n",
      "epoch: 2 step: 921, loss is 0.4379426836967468\n",
      "epoch: 2 step: 922, loss is 0.3826068341732025\n",
      "epoch: 2 step: 923, loss is 0.27785176038742065\n",
      "epoch: 2 step: 924, loss is 0.48014286160469055\n",
      "epoch: 2 step: 925, loss is 0.34909680485725403\n",
      "epoch: 2 step: 926, loss is 0.28674936294555664\n",
      "epoch: 2 step: 927, loss is 0.34376999735832214\n",
      "epoch: 2 step: 928, loss is 0.32221144437789917\n",
      "epoch: 2 step: 929, loss is 0.40728893876075745\n",
      "epoch: 2 step: 930, loss is 0.33215710520744324\n",
      "epoch: 2 step: 931, loss is 0.36233383417129517\n",
      "epoch: 2 step: 932, loss is 0.5542885065078735\n",
      "epoch: 2 step: 933, loss is 0.33302047848701477\n",
      "epoch: 2 step: 934, loss is 0.2873988747596741\n",
      "epoch: 2 step: 935, loss is 0.3541598618030548\n",
      "epoch: 2 step: 936, loss is 0.3089953362941742\n",
      "epoch: 2 step: 937, loss is 0.4366152882575989\n",
      "epoch: 3 step: 1, loss is 0.5450757741928101\n",
      "epoch: 3 step: 2, loss is 0.38301435112953186\n",
      "epoch: 3 step: 3, loss is 0.20205676555633545\n",
      "epoch: 3 step: 4, loss is 0.3683640956878662\n",
      "epoch: 3 step: 5, loss is 0.41574016213417053\n",
      "epoch: 3 step: 6, loss is 0.2500581741333008\n",
      "epoch: 3 step: 7, loss is 0.5356442332267761\n",
      "epoch: 3 step: 8, loss is 0.36254775524139404\n",
      "epoch: 3 step: 9, loss is 0.35317572951316833\n",
      "epoch: 3 step: 10, loss is 0.3342771828174591\n",
      "epoch: 3 step: 11, loss is 0.4958855211734772\n",
      "epoch: 3 step: 12, loss is 0.4189794659614563\n",
      "epoch: 3 step: 13, loss is 0.4298180341720581\n",
      "epoch: 3 step: 14, loss is 0.3640735149383545\n",
      "epoch: 3 step: 15, loss is 0.5844885110855103\n",
      "epoch: 3 step: 16, loss is 0.39294207096099854\n",
      "epoch: 3 step: 17, loss is 0.24219688773155212\n",
      "epoch: 3 step: 18, loss is 0.32206717133522034\n",
      "epoch: 3 step: 19, loss is 0.36465248465538025\n",
      "epoch: 3 step: 20, loss is 0.45207807421684265\n",
      "epoch: 3 step: 21, loss is 0.3807096481323242\n",
      "epoch: 3 step: 22, loss is 0.34743690490722656\n",
      "epoch: 3 step: 23, loss is 0.36935803294181824\n",
      "epoch: 3 step: 24, loss is 0.23295627534389496\n",
      "epoch: 3 step: 25, loss is 0.3809176981449127\n",
      "epoch: 3 step: 26, loss is 0.38750770688056946\n",
      "epoch: 3 step: 27, loss is 0.28804486989974976\n",
      "epoch: 3 step: 28, loss is 0.5826719403266907\n",
      "epoch: 3 step: 29, loss is 0.39976274967193604\n",
      "epoch: 3 step: 30, loss is 0.29062992334365845\n",
      "epoch: 3 step: 31, loss is 0.3468320965766907\n",
      "epoch: 3 step: 32, loss is 0.44314244389533997\n",
      "epoch: 3 step: 33, loss is 0.44748860597610474\n",
      "epoch: 3 step: 34, loss is 0.2438524067401886\n",
      "epoch: 3 step: 35, loss is 0.3553225100040436\n",
      "epoch: 3 step: 36, loss is 0.5063091516494751\n",
      "epoch: 3 step: 37, loss is 0.28073370456695557\n",
      "epoch: 3 step: 38, loss is 0.5510832667350769\n",
      "epoch: 3 step: 39, loss is 0.3131484389305115\n",
      "epoch: 3 step: 40, loss is 0.3855641484260559\n",
      "epoch: 3 step: 41, loss is 0.3712071180343628\n",
      "epoch: 3 step: 42, loss is 0.5372892618179321\n",
      "epoch: 3 step: 43, loss is 0.36826246976852417\n",
      "epoch: 3 step: 44, loss is 0.5226032733917236\n",
      "epoch: 3 step: 45, loss is 0.3288128674030304\n",
      "epoch: 3 step: 46, loss is 0.5370168089866638\n",
      "epoch: 3 step: 47, loss is 0.5774369239807129\n",
      "epoch: 3 step: 48, loss is 0.33507537841796875\n",
      "epoch: 3 step: 49, loss is 0.4721927046775818\n",
      "epoch: 3 step: 50, loss is 0.24597488343715668\n",
      "epoch: 3 step: 51, loss is 0.3971308767795563\n",
      "epoch: 3 step: 52, loss is 0.3875022232532501\n",
      "epoch: 3 step: 53, loss is 0.3899011015892029\n",
      "epoch: 3 step: 54, loss is 0.3172304630279541\n",
      "epoch: 3 step: 55, loss is 0.3017815947532654\n",
      "epoch: 3 step: 56, loss is 0.2944982051849365\n",
      "epoch: 3 step: 57, loss is 0.36928293108940125\n",
      "epoch: 3 step: 58, loss is 0.4092211425304413\n",
      "epoch: 3 step: 59, loss is 0.28367990255355835\n",
      "epoch: 3 step: 60, loss is 0.3507942259311676\n",
      "epoch: 3 step: 61, loss is 0.31580132246017456\n",
      "epoch: 3 step: 62, loss is 0.4940425455570221\n",
      "epoch: 3 step: 63, loss is 0.43864119052886963\n",
      "epoch: 3 step: 64, loss is 0.225507453083992\n",
      "epoch: 3 step: 65, loss is 0.42329737544059753\n",
      "epoch: 3 step: 66, loss is 0.4031532406806946\n",
      "epoch: 3 step: 67, loss is 0.3754804730415344\n",
      "epoch: 3 step: 68, loss is 0.3414188325405121\n",
      "epoch: 3 step: 69, loss is 0.5712741017341614\n",
      "epoch: 3 step: 70, loss is 0.30061736702919006\n",
      "epoch: 3 step: 71, loss is 0.32048580050468445\n",
      "epoch: 3 step: 72, loss is 0.4516463577747345\n",
      "epoch: 3 step: 73, loss is 0.5266300439834595\n",
      "epoch: 3 step: 74, loss is 0.3001338839530945\n",
      "epoch: 3 step: 75, loss is 0.28357115387916565\n",
      "epoch: 3 step: 76, loss is 0.48898106813430786\n",
      "epoch: 3 step: 77, loss is 0.3077925443649292\n",
      "epoch: 3 step: 78, loss is 0.40967857837677\n",
      "epoch: 3 step: 79, loss is 0.47797852754592896\n",
      "epoch: 3 step: 80, loss is 0.5732460021972656\n",
      "epoch: 3 step: 81, loss is 0.29626113176345825\n",
      "epoch: 3 step: 82, loss is 0.23952814936637878\n",
      "epoch: 3 step: 83, loss is 0.42327824234962463\n",
      "epoch: 3 step: 84, loss is 0.32758399844169617\n",
      "epoch: 3 step: 85, loss is 0.42416656017303467\n",
      "epoch: 3 step: 86, loss is 0.519209623336792\n",
      "epoch: 3 step: 87, loss is 0.522167980670929\n",
      "epoch: 3 step: 88, loss is 0.22745265066623688\n",
      "epoch: 3 step: 89, loss is 0.4535169303417206\n",
      "epoch: 3 step: 90, loss is 0.46724867820739746\n",
      "epoch: 3 step: 91, loss is 0.2937076985836029\n",
      "epoch: 3 step: 92, loss is 0.47357702255249023\n",
      "epoch: 3 step: 93, loss is 0.30042099952697754\n",
      "epoch: 3 step: 94, loss is 0.27053529024124146\n",
      "epoch: 3 step: 95, loss is 0.313630074262619\n",
      "epoch: 3 step: 96, loss is 0.3379133343696594\n",
      "epoch: 3 step: 97, loss is 0.3069891333580017\n",
      "epoch: 3 step: 98, loss is 0.2085944414138794\n",
      "epoch: 3 step: 99, loss is 0.32952478528022766\n",
      "epoch: 3 step: 100, loss is 0.3733398914337158\n",
      "epoch: 3 step: 101, loss is 0.25516220927238464\n",
      "epoch: 3 step: 102, loss is 0.3213314116001129\n",
      "epoch: 3 step: 103, loss is 0.4546250104904175\n",
      "epoch: 3 step: 104, loss is 0.31105417013168335\n",
      "epoch: 3 step: 105, loss is 0.33872467279434204\n",
      "epoch: 3 step: 106, loss is 0.39139193296432495\n",
      "epoch: 3 step: 107, loss is 0.349696546792984\n",
      "epoch: 3 step: 108, loss is 0.34957894682884216\n",
      "epoch: 3 step: 109, loss is 0.278909832239151\n",
      "epoch: 3 step: 110, loss is 0.20819589495658875\n",
      "epoch: 3 step: 111, loss is 0.3002762496471405\n",
      "epoch: 3 step: 112, loss is 0.36360445618629456\n",
      "epoch: 3 step: 113, loss is 0.2698094844818115\n",
      "epoch: 3 step: 114, loss is 0.2842939496040344\n",
      "epoch: 3 step: 115, loss is 0.24682161211967468\n",
      "epoch: 3 step: 116, loss is 0.46492838859558105\n",
      "epoch: 3 step: 117, loss is 0.31611546874046326\n",
      "epoch: 3 step: 118, loss is 0.4275034964084625\n",
      "epoch: 3 step: 119, loss is 0.31802690029144287\n",
      "epoch: 3 step: 120, loss is 0.4947997033596039\n",
      "epoch: 3 step: 121, loss is 0.26726216077804565\n",
      "epoch: 3 step: 122, loss is 0.2825344204902649\n",
      "epoch: 3 step: 123, loss is 0.2652435600757599\n",
      "epoch: 3 step: 124, loss is 0.3873259127140045\n",
      "epoch: 3 step: 125, loss is 0.3535920977592468\n",
      "epoch: 3 step: 126, loss is 0.7317917346954346\n",
      "epoch: 3 step: 127, loss is 0.5882871747016907\n",
      "epoch: 3 step: 128, loss is 0.5268710851669312\n",
      "epoch: 3 step: 129, loss is 0.33791303634643555\n",
      "epoch: 3 step: 130, loss is 0.32790082693099976\n",
      "epoch: 3 step: 131, loss is 0.5209001898765564\n",
      "epoch: 3 step: 132, loss is 0.3495981991291046\n",
      "epoch: 3 step: 133, loss is 0.3558223843574524\n",
      "epoch: 3 step: 134, loss is 0.26447367668151855\n",
      "epoch: 3 step: 135, loss is 0.3067306578159332\n",
      "epoch: 3 step: 136, loss is 0.3261955976486206\n",
      "epoch: 3 step: 137, loss is 0.42117294669151306\n",
      "epoch: 3 step: 138, loss is 0.3148515224456787\n",
      "epoch: 3 step: 139, loss is 0.2412741780281067\n",
      "epoch: 3 step: 140, loss is 0.3065618574619293\n",
      "epoch: 3 step: 141, loss is 0.29836782813072205\n",
      "epoch: 3 step: 142, loss is 0.2748429775238037\n",
      "epoch: 3 step: 143, loss is 0.29701605439186096\n",
      "epoch: 3 step: 144, loss is 0.28291386365890503\n",
      "epoch: 3 step: 145, loss is 0.32332611083984375\n",
      "epoch: 3 step: 146, loss is 0.28048691153526306\n",
      "epoch: 3 step: 147, loss is 0.3058319687843323\n",
      "epoch: 3 step: 148, loss is 0.2898026406764984\n",
      "epoch: 3 step: 149, loss is 0.1637692153453827\n",
      "epoch: 3 step: 150, loss is 0.4063555896282196\n",
      "epoch: 3 step: 151, loss is 0.21240653097629547\n",
      "epoch: 3 step: 152, loss is 0.5178424715995789\n",
      "epoch: 3 step: 153, loss is 0.32235023379325867\n",
      "epoch: 3 step: 154, loss is 0.39862701296806335\n",
      "epoch: 3 step: 155, loss is 0.43577486276626587\n",
      "epoch: 3 step: 156, loss is 0.45570486783981323\n",
      "epoch: 3 step: 157, loss is 0.38784879446029663\n",
      "epoch: 3 step: 158, loss is 0.3793431520462036\n",
      "epoch: 3 step: 159, loss is 0.46492427587509155\n",
      "epoch: 3 step: 160, loss is 0.22967052459716797\n",
      "epoch: 3 step: 161, loss is 0.3374817669391632\n",
      "epoch: 3 step: 162, loss is 0.32722193002700806\n",
      "epoch: 3 step: 163, loss is 0.32756298780441284\n",
      "epoch: 3 step: 164, loss is 0.4852713942527771\n",
      "epoch: 3 step: 165, loss is 0.48085319995880127\n",
      "epoch: 3 step: 166, loss is 0.37659579515457153\n",
      "epoch: 3 step: 167, loss is 0.5511037707328796\n",
      "epoch: 3 step: 168, loss is 0.2172183245420456\n",
      "epoch: 3 step: 169, loss is 0.19863295555114746\n",
      "epoch: 3 step: 170, loss is 0.4988952577114105\n",
      "epoch: 3 step: 171, loss is 0.6080353260040283\n",
      "epoch: 3 step: 172, loss is 0.20558466017246246\n",
      "epoch: 3 step: 173, loss is 0.2855944037437439\n",
      "epoch: 3 step: 174, loss is 0.33576300740242004\n",
      "epoch: 3 step: 175, loss is 0.39181220531463623\n",
      "epoch: 3 step: 176, loss is 0.23803535103797913\n",
      "epoch: 3 step: 177, loss is 0.2517167925834656\n",
      "epoch: 3 step: 178, loss is 0.3464175760746002\n",
      "epoch: 3 step: 179, loss is 0.6058306694030762\n",
      "epoch: 3 step: 180, loss is 0.4084605276584625\n",
      "epoch: 3 step: 181, loss is 0.3945501744747162\n",
      "epoch: 3 step: 182, loss is 0.2568608522415161\n",
      "epoch: 3 step: 183, loss is 0.3652438223361969\n",
      "epoch: 3 step: 184, loss is 0.4297470450401306\n",
      "epoch: 3 step: 185, loss is 0.23643597960472107\n",
      "epoch: 3 step: 186, loss is 0.41035133600234985\n",
      "epoch: 3 step: 187, loss is 0.5222646594047546\n",
      "epoch: 3 step: 188, loss is 0.3489508330821991\n",
      "epoch: 3 step: 189, loss is 0.3471899628639221\n",
      "epoch: 3 step: 190, loss is 0.31809696555137634\n",
      "epoch: 3 step: 191, loss is 0.26036420464515686\n",
      "epoch: 3 step: 192, loss is 0.5223446488380432\n",
      "epoch: 3 step: 193, loss is 0.34225496649742126\n",
      "epoch: 3 step: 194, loss is 0.24140271544456482\n",
      "epoch: 3 step: 195, loss is 0.5310725569725037\n",
      "epoch: 3 step: 196, loss is 0.3285607099533081\n",
      "epoch: 3 step: 197, loss is 0.3689377009868622\n",
      "epoch: 3 step: 198, loss is 0.23651760816574097\n",
      "epoch: 3 step: 199, loss is 0.4249360263347626\n",
      "epoch: 3 step: 200, loss is 0.2244681566953659\n",
      "epoch: 3 step: 201, loss is 0.38525596261024475\n",
      "epoch: 3 step: 202, loss is 0.2741836905479431\n",
      "epoch: 3 step: 203, loss is 0.4694053828716278\n",
      "epoch: 3 step: 204, loss is 0.5165024399757385\n",
      "epoch: 3 step: 205, loss is 0.38953521847724915\n",
      "epoch: 3 step: 206, loss is 0.2756499946117401\n",
      "epoch: 3 step: 207, loss is 0.14622695744037628\n",
      "epoch: 3 step: 208, loss is 0.3054530918598175\n",
      "epoch: 3 step: 209, loss is 0.2603147327899933\n",
      "epoch: 3 step: 210, loss is 0.49184274673461914\n",
      "epoch: 3 step: 211, loss is 0.43769967555999756\n",
      "epoch: 3 step: 212, loss is 0.388944149017334\n",
      "epoch: 3 step: 213, loss is 0.4004157781600952\n",
      "epoch: 3 step: 214, loss is 0.39645686745643616\n",
      "epoch: 3 step: 215, loss is 0.34744158387184143\n",
      "epoch: 3 step: 216, loss is 0.3264980614185333\n",
      "epoch: 3 step: 217, loss is 0.352434366941452\n",
      "epoch: 3 step: 218, loss is 0.30096158385276794\n",
      "epoch: 3 step: 219, loss is 0.2475598305463791\n",
      "epoch: 3 step: 220, loss is 0.31772610545158386\n",
      "epoch: 3 step: 221, loss is 0.2574281692504883\n",
      "epoch: 3 step: 222, loss is 0.23671531677246094\n",
      "epoch: 3 step: 223, loss is 0.635083794593811\n",
      "epoch: 3 step: 224, loss is 0.31657958030700684\n",
      "epoch: 3 step: 225, loss is 0.40309634804725647\n",
      "epoch: 3 step: 226, loss is 0.3770866394042969\n",
      "epoch: 3 step: 227, loss is 0.4825696647167206\n",
      "epoch: 3 step: 228, loss is 0.3640300929546356\n",
      "epoch: 3 step: 229, loss is 0.449066698551178\n",
      "epoch: 3 step: 230, loss is 0.34381526708602905\n",
      "epoch: 3 step: 231, loss is 0.2996319532394409\n",
      "epoch: 3 step: 232, loss is 0.3544716536998749\n",
      "epoch: 3 step: 233, loss is 0.33591872453689575\n",
      "epoch: 3 step: 234, loss is 0.23888850212097168\n",
      "epoch: 3 step: 235, loss is 0.41382038593292236\n",
      "epoch: 3 step: 236, loss is 0.21593478322029114\n",
      "epoch: 3 step: 237, loss is 0.43139925599098206\n",
      "epoch: 3 step: 238, loss is 0.5318435430526733\n",
      "epoch: 3 step: 239, loss is 0.4411833882331848\n",
      "epoch: 3 step: 240, loss is 0.36522069573402405\n",
      "epoch: 3 step: 241, loss is 0.28591278195381165\n",
      "epoch: 3 step: 242, loss is 0.29874610900878906\n",
      "epoch: 3 step: 243, loss is 0.36591601371765137\n",
      "epoch: 3 step: 244, loss is 0.28215280175209045\n",
      "epoch: 3 step: 245, loss is 0.20730647444725037\n",
      "epoch: 3 step: 246, loss is 0.4265984296798706\n",
      "epoch: 3 step: 247, loss is 0.31672537326812744\n",
      "epoch: 3 step: 248, loss is 0.2597905993461609\n",
      "epoch: 3 step: 249, loss is 0.2926464378833771\n",
      "epoch: 3 step: 250, loss is 0.31013354659080505\n",
      "epoch: 3 step: 251, loss is 0.6277730464935303\n",
      "epoch: 3 step: 252, loss is 0.36524757742881775\n",
      "epoch: 3 step: 253, loss is 0.361526221036911\n",
      "epoch: 3 step: 254, loss is 0.4015546143054962\n",
      "epoch: 3 step: 255, loss is 0.581082284450531\n",
      "epoch: 3 step: 256, loss is 0.399772584438324\n",
      "epoch: 3 step: 257, loss is 0.41490113735198975\n",
      "epoch: 3 step: 258, loss is 0.28770819306373596\n",
      "epoch: 3 step: 259, loss is 0.3867250978946686\n",
      "epoch: 3 step: 260, loss is 0.2533837556838989\n",
      "epoch: 3 step: 261, loss is 0.1537267416715622\n",
      "epoch: 3 step: 262, loss is 0.3207690417766571\n",
      "epoch: 3 step: 263, loss is 0.4012111723423004\n",
      "epoch: 3 step: 264, loss is 0.21704374253749847\n",
      "epoch: 3 step: 265, loss is 0.22398068010807037\n",
      "epoch: 3 step: 266, loss is 0.36506903171539307\n",
      "epoch: 3 step: 267, loss is 0.3393961489200592\n",
      "epoch: 3 step: 268, loss is 0.528450071811676\n",
      "epoch: 3 step: 269, loss is 0.3264513611793518\n",
      "epoch: 3 step: 270, loss is 0.3839396834373474\n",
      "epoch: 3 step: 271, loss is 0.58055579662323\n",
      "epoch: 3 step: 272, loss is 0.25283870100975037\n",
      "epoch: 3 step: 273, loss is 0.3368160128593445\n",
      "epoch: 3 step: 274, loss is 0.40911832451820374\n",
      "epoch: 3 step: 275, loss is 0.4234950840473175\n",
      "epoch: 3 step: 276, loss is 0.2524714171886444\n",
      "epoch: 3 step: 277, loss is 0.4500387907028198\n",
      "epoch: 3 step: 278, loss is 0.2856636643409729\n",
      "epoch: 3 step: 279, loss is 0.4270731508731842\n",
      "epoch: 3 step: 280, loss is 0.287208616733551\n",
      "epoch: 3 step: 281, loss is 0.26767027378082275\n",
      "epoch: 3 step: 282, loss is 0.22171667218208313\n",
      "epoch: 3 step: 283, loss is 0.45456090569496155\n",
      "epoch: 3 step: 284, loss is 0.4257294833660126\n",
      "epoch: 3 step: 285, loss is 0.2999040186405182\n",
      "epoch: 3 step: 286, loss is 0.28283435106277466\n",
      "epoch: 3 step: 287, loss is 0.36048591136932373\n",
      "epoch: 3 step: 288, loss is 0.39907100796699524\n",
      "epoch: 3 step: 289, loss is 0.456929087638855\n",
      "epoch: 3 step: 290, loss is 0.361478716135025\n",
      "epoch: 3 step: 291, loss is 0.5343217849731445\n",
      "epoch: 3 step: 292, loss is 0.40406322479248047\n",
      "epoch: 3 step: 293, loss is 0.3428034484386444\n",
      "epoch: 3 step: 294, loss is 0.32405731081962585\n",
      "epoch: 3 step: 295, loss is 0.24548721313476562\n",
      "epoch: 3 step: 296, loss is 0.5028703808784485\n",
      "epoch: 3 step: 297, loss is 0.37511494755744934\n",
      "epoch: 3 step: 298, loss is 0.26323944330215454\n",
      "epoch: 3 step: 299, loss is 0.14904840290546417\n",
      "epoch: 3 step: 300, loss is 0.3376310467720032\n",
      "epoch: 3 step: 301, loss is 0.3294280469417572\n",
      "epoch: 3 step: 302, loss is 0.48879703879356384\n",
      "epoch: 3 step: 303, loss is 0.37316760420799255\n",
      "epoch: 3 step: 304, loss is 0.3761855661869049\n",
      "epoch: 3 step: 305, loss is 0.18459081649780273\n",
      "epoch: 3 step: 306, loss is 0.2402132749557495\n",
      "epoch: 3 step: 307, loss is 0.3873245418071747\n",
      "epoch: 3 step: 308, loss is 0.45697513222694397\n",
      "epoch: 3 step: 309, loss is 0.24458958208560944\n",
      "epoch: 3 step: 310, loss is 0.345427542924881\n",
      "epoch: 3 step: 311, loss is 0.30440112948417664\n",
      "epoch: 3 step: 312, loss is 0.35036978125572205\n",
      "epoch: 3 step: 313, loss is 0.31590649485588074\n",
      "epoch: 3 step: 314, loss is 0.28289636969566345\n",
      "epoch: 3 step: 315, loss is 0.3212469220161438\n",
      "epoch: 3 step: 316, loss is 0.44114649295806885\n",
      "epoch: 3 step: 317, loss is 0.4590277671813965\n",
      "epoch: 3 step: 318, loss is 0.41832253336906433\n",
      "epoch: 3 step: 319, loss is 0.4106920659542084\n",
      "epoch: 3 step: 320, loss is 0.4531789720058441\n",
      "epoch: 3 step: 321, loss is 0.3064620792865753\n",
      "epoch: 3 step: 322, loss is 0.3414944112300873\n",
      "epoch: 3 step: 323, loss is 0.3804691731929779\n",
      "epoch: 3 step: 324, loss is 0.390974760055542\n",
      "epoch: 3 step: 325, loss is 0.22940337657928467\n",
      "epoch: 3 step: 326, loss is 0.5291455388069153\n",
      "epoch: 3 step: 327, loss is 0.48879170417785645\n",
      "epoch: 3 step: 328, loss is 0.44116270542144775\n",
      "epoch: 3 step: 329, loss is 0.3828120529651642\n",
      "epoch: 3 step: 330, loss is 0.36034098267555237\n",
      "epoch: 3 step: 331, loss is 0.2341061532497406\n",
      "epoch: 3 step: 332, loss is 0.46196281909942627\n",
      "epoch: 3 step: 333, loss is 0.38559389114379883\n",
      "epoch: 3 step: 334, loss is 0.2748832106590271\n",
      "epoch: 3 step: 335, loss is 0.3379956781864166\n",
      "epoch: 3 step: 336, loss is 0.3103594183921814\n",
      "epoch: 3 step: 337, loss is 0.306497186422348\n",
      "epoch: 3 step: 338, loss is 0.3044809103012085\n",
      "epoch: 3 step: 339, loss is 0.17602938413619995\n",
      "epoch: 3 step: 340, loss is 0.4292924702167511\n",
      "epoch: 3 step: 341, loss is 0.32354068756103516\n",
      "epoch: 3 step: 342, loss is 0.5987086296081543\n",
      "epoch: 3 step: 343, loss is 0.4249933063983917\n",
      "epoch: 3 step: 344, loss is 0.3085479140281677\n",
      "epoch: 3 step: 345, loss is 0.3114783763885498\n",
      "epoch: 3 step: 346, loss is 0.35491758584976196\n",
      "epoch: 3 step: 347, loss is 0.25090059638023376\n",
      "epoch: 3 step: 348, loss is 0.33119603991508484\n",
      "epoch: 3 step: 349, loss is 0.3720323145389557\n",
      "epoch: 3 step: 350, loss is 0.5351659059524536\n",
      "epoch: 3 step: 351, loss is 0.21079333126544952\n",
      "epoch: 3 step: 352, loss is 0.24425166845321655\n",
      "epoch: 3 step: 353, loss is 0.22610118985176086\n",
      "epoch: 3 step: 354, loss is 0.5343248844146729\n",
      "epoch: 3 step: 355, loss is 0.5252726078033447\n",
      "epoch: 3 step: 356, loss is 0.21664927899837494\n",
      "epoch: 3 step: 357, loss is 0.31321877241134644\n",
      "epoch: 3 step: 358, loss is 0.24442508816719055\n",
      "epoch: 3 step: 359, loss is 0.3815261721611023\n",
      "epoch: 3 step: 360, loss is 0.4464280605316162\n",
      "epoch: 3 step: 361, loss is 0.2914116680622101\n",
      "epoch: 3 step: 362, loss is 0.26053324341773987\n",
      "epoch: 3 step: 363, loss is 0.32681724429130554\n",
      "epoch: 3 step: 364, loss is 0.3055070638656616\n",
      "epoch: 3 step: 365, loss is 0.43672794103622437\n",
      "epoch: 3 step: 366, loss is 0.22128649055957794\n",
      "epoch: 3 step: 367, loss is 0.2423667460680008\n",
      "epoch: 3 step: 368, loss is 0.2619185447692871\n",
      "epoch: 3 step: 369, loss is 0.4152665436267853\n",
      "epoch: 3 step: 370, loss is 0.19616198539733887\n",
      "epoch: 3 step: 371, loss is 0.32134929299354553\n",
      "epoch: 3 step: 372, loss is 0.40505650639533997\n",
      "epoch: 3 step: 373, loss is 0.27832579612731934\n",
      "epoch: 3 step: 374, loss is 0.22414986789226532\n",
      "epoch: 3 step: 375, loss is 0.4195985794067383\n",
      "epoch: 3 step: 376, loss is 0.2261715531349182\n",
      "epoch: 3 step: 377, loss is 0.42478081583976746\n",
      "epoch: 3 step: 378, loss is 0.5769405364990234\n",
      "epoch: 3 step: 379, loss is 0.19665952026844025\n",
      "epoch: 3 step: 380, loss is 0.320051908493042\n",
      "epoch: 3 step: 381, loss is 0.46815845370292664\n",
      "epoch: 3 step: 382, loss is 0.47229382395744324\n",
      "epoch: 3 step: 383, loss is 0.20737767219543457\n",
      "epoch: 3 step: 384, loss is 0.3734259307384491\n",
      "epoch: 3 step: 385, loss is 0.39686423540115356\n",
      "epoch: 3 step: 386, loss is 0.29942700266838074\n",
      "epoch: 3 step: 387, loss is 0.3904796838760376\n",
      "epoch: 3 step: 388, loss is 0.34287229180336\n",
      "epoch: 3 step: 389, loss is 0.3445685803890228\n",
      "epoch: 3 step: 390, loss is 0.3613409996032715\n",
      "epoch: 3 step: 391, loss is 0.26610690355300903\n",
      "epoch: 3 step: 392, loss is 0.4456805884838104\n",
      "epoch: 3 step: 393, loss is 0.3347979485988617\n",
      "epoch: 3 step: 394, loss is 0.40847712755203247\n",
      "epoch: 3 step: 395, loss is 0.45157283544540405\n",
      "epoch: 3 step: 396, loss is 0.662183403968811\n",
      "epoch: 3 step: 397, loss is 0.2540523409843445\n",
      "epoch: 3 step: 398, loss is 0.27235013246536255\n",
      "epoch: 3 step: 399, loss is 0.34012532234191895\n",
      "epoch: 3 step: 400, loss is 0.2895244061946869\n",
      "epoch: 3 step: 401, loss is 0.21994951367378235\n",
      "epoch: 3 step: 402, loss is 0.3903089761734009\n",
      "epoch: 3 step: 403, loss is 0.5268592834472656\n",
      "epoch: 3 step: 404, loss is 0.3138171434402466\n",
      "epoch: 3 step: 405, loss is 0.3674607276916504\n",
      "epoch: 3 step: 406, loss is 0.41550758481025696\n",
      "epoch: 3 step: 407, loss is 0.3315492570400238\n",
      "epoch: 3 step: 408, loss is 0.3208390772342682\n",
      "epoch: 3 step: 409, loss is 0.2589981257915497\n",
      "epoch: 3 step: 410, loss is 0.38305047154426575\n",
      "epoch: 3 step: 411, loss is 0.4664246737957001\n",
      "epoch: 3 step: 412, loss is 0.5758537650108337\n",
      "epoch: 3 step: 413, loss is 0.22357985377311707\n",
      "epoch: 3 step: 414, loss is 0.27638062834739685\n",
      "epoch: 3 step: 415, loss is 0.3769136667251587\n",
      "epoch: 3 step: 416, loss is 0.31099921464920044\n",
      "epoch: 3 step: 417, loss is 0.4200092852115631\n",
      "epoch: 3 step: 418, loss is 0.35319992899894714\n",
      "epoch: 3 step: 419, loss is 0.4030551612377167\n",
      "epoch: 3 step: 420, loss is 0.19377501308918\n",
      "epoch: 3 step: 421, loss is 0.1930568665266037\n",
      "epoch: 3 step: 422, loss is 0.2369430512189865\n",
      "epoch: 3 step: 423, loss is 0.3201116919517517\n",
      "epoch: 3 step: 424, loss is 0.4684031903743744\n",
      "epoch: 3 step: 425, loss is 0.2744467556476593\n",
      "epoch: 3 step: 426, loss is 0.43068304657936096\n",
      "epoch: 3 step: 427, loss is 0.37036657333374023\n",
      "epoch: 3 step: 428, loss is 0.23080629110336304\n",
      "epoch: 3 step: 429, loss is 0.3199590742588043\n",
      "epoch: 3 step: 430, loss is 0.28505653142929077\n",
      "epoch: 3 step: 431, loss is 0.4710482656955719\n",
      "epoch: 3 step: 432, loss is 0.3281707167625427\n",
      "epoch: 3 step: 433, loss is 0.25376325845718384\n",
      "epoch: 3 step: 434, loss is 0.4119725525379181\n",
      "epoch: 3 step: 435, loss is 0.3475314676761627\n",
      "epoch: 3 step: 436, loss is 0.22325223684310913\n",
      "epoch: 3 step: 437, loss is 0.2662149965763092\n",
      "epoch: 3 step: 438, loss is 0.35732370615005493\n",
      "epoch: 3 step: 439, loss is 0.26344919204711914\n",
      "epoch: 3 step: 440, loss is 0.2741059362888336\n",
      "epoch: 3 step: 441, loss is 0.2128022164106369\n",
      "epoch: 3 step: 442, loss is 0.40443485975265503\n",
      "epoch: 3 step: 443, loss is 0.26845279335975647\n",
      "epoch: 3 step: 444, loss is 0.34156468510627747\n",
      "epoch: 3 step: 445, loss is 0.4878353476524353\n",
      "epoch: 3 step: 446, loss is 0.2977123558521271\n",
      "epoch: 3 step: 447, loss is 0.36993229389190674\n",
      "epoch: 3 step: 448, loss is 0.3698109984397888\n",
      "epoch: 3 step: 449, loss is 0.3284784257411957\n",
      "epoch: 3 step: 450, loss is 0.37406831979751587\n",
      "epoch: 3 step: 451, loss is 0.5169396996498108\n",
      "epoch: 3 step: 452, loss is 0.4769939184188843\n",
      "epoch: 3 step: 453, loss is 0.2563035190105438\n",
      "epoch: 3 step: 454, loss is 0.4055427014827728\n",
      "epoch: 3 step: 455, loss is 0.36034131050109863\n",
      "epoch: 3 step: 456, loss is 0.4104672372341156\n",
      "epoch: 3 step: 457, loss is 0.334457129240036\n",
      "epoch: 3 step: 458, loss is 0.1887122392654419\n",
      "epoch: 3 step: 459, loss is 0.5574378371238708\n",
      "epoch: 3 step: 460, loss is 0.17875328660011292\n",
      "epoch: 3 step: 461, loss is 0.3969387114048004\n",
      "epoch: 3 step: 462, loss is 0.35190626978874207\n",
      "epoch: 3 step: 463, loss is 0.4872027635574341\n",
      "epoch: 3 step: 464, loss is 0.5481765866279602\n",
      "epoch: 3 step: 465, loss is 0.28125444054603577\n",
      "epoch: 3 step: 466, loss is 0.27231839299201965\n",
      "epoch: 3 step: 467, loss is 0.48186999559402466\n",
      "epoch: 3 step: 468, loss is 0.29806551337242126\n",
      "epoch: 3 step: 469, loss is 0.2885070741176605\n",
      "epoch: 3 step: 470, loss is 0.364057719707489\n",
      "epoch: 3 step: 471, loss is 0.304673969745636\n",
      "epoch: 3 step: 472, loss is 0.3424234390258789\n",
      "epoch: 3 step: 473, loss is 0.33792048692703247\n",
      "epoch: 3 step: 474, loss is 0.27254700660705566\n",
      "epoch: 3 step: 475, loss is 0.42153576016426086\n",
      "epoch: 3 step: 476, loss is 0.3287551701068878\n",
      "epoch: 3 step: 477, loss is 0.31673482060432434\n",
      "epoch: 3 step: 478, loss is 0.3932563364505768\n",
      "epoch: 3 step: 479, loss is 0.4590260088443756\n",
      "epoch: 3 step: 480, loss is 0.4262659251689911\n",
      "epoch: 3 step: 481, loss is 0.2809129059314728\n",
      "epoch: 3 step: 482, loss is 0.47140783071517944\n",
      "epoch: 3 step: 483, loss is 0.2637854218482971\n",
      "epoch: 3 step: 484, loss is 0.36795610189437866\n",
      "epoch: 3 step: 485, loss is 0.4551573395729065\n",
      "epoch: 3 step: 486, loss is 0.33654680848121643\n",
      "epoch: 3 step: 487, loss is 0.6758692264556885\n",
      "epoch: 3 step: 488, loss is 0.29569971561431885\n",
      "epoch: 3 step: 489, loss is 0.37526750564575195\n",
      "epoch: 3 step: 490, loss is 0.3344530463218689\n",
      "epoch: 3 step: 491, loss is 0.274265319108963\n",
      "epoch: 3 step: 492, loss is 0.24331676959991455\n",
      "epoch: 3 step: 493, loss is 0.3512704074382782\n",
      "epoch: 3 step: 494, loss is 0.43868398666381836\n",
      "epoch: 3 step: 495, loss is 0.40246498584747314\n",
      "epoch: 3 step: 496, loss is 0.22534635663032532\n",
      "epoch: 3 step: 497, loss is 0.396291583776474\n",
      "epoch: 3 step: 498, loss is 0.5012533068656921\n",
      "epoch: 3 step: 499, loss is 0.38251355290412903\n",
      "epoch: 3 step: 500, loss is 0.2809368073940277\n",
      "epoch: 3 step: 501, loss is 0.539577841758728\n",
      "epoch: 3 step: 502, loss is 0.42605072259902954\n",
      "epoch: 3 step: 503, loss is 0.36856621503829956\n",
      "epoch: 3 step: 504, loss is 0.5167766809463501\n",
      "epoch: 3 step: 505, loss is 0.30106857419013977\n",
      "epoch: 3 step: 506, loss is 0.27905938029289246\n",
      "epoch: 3 step: 507, loss is 0.4087652266025543\n",
      "epoch: 3 step: 508, loss is 0.4462786316871643\n",
      "epoch: 3 step: 509, loss is 0.3653813600540161\n",
      "epoch: 3 step: 510, loss is 0.35210689902305603\n",
      "epoch: 3 step: 511, loss is 0.2551575005054474\n",
      "epoch: 3 step: 512, loss is 0.5344187617301941\n",
      "epoch: 3 step: 513, loss is 0.2593761682510376\n",
      "epoch: 3 step: 514, loss is 0.3599987328052521\n",
      "epoch: 3 step: 515, loss is 0.48640793561935425\n",
      "epoch: 3 step: 516, loss is 0.48992499709129333\n",
      "epoch: 3 step: 517, loss is 0.3259757459163666\n",
      "epoch: 3 step: 518, loss is 0.3415755331516266\n",
      "epoch: 3 step: 519, loss is 0.29967236518859863\n",
      "epoch: 3 step: 520, loss is 0.26932305097579956\n",
      "epoch: 3 step: 521, loss is 0.3865988254547119\n",
      "epoch: 3 step: 522, loss is 0.3852580189704895\n",
      "epoch: 3 step: 523, loss is 0.3876936435699463\n",
      "epoch: 3 step: 524, loss is 0.36969953775405884\n",
      "epoch: 3 step: 525, loss is 0.3785731792449951\n",
      "epoch: 3 step: 526, loss is 0.2500883936882019\n",
      "epoch: 3 step: 527, loss is 0.35425543785095215\n",
      "epoch: 3 step: 528, loss is 0.28180181980133057\n",
      "epoch: 3 step: 529, loss is 0.47576043009757996\n",
      "epoch: 3 step: 530, loss is 0.48384082317352295\n",
      "epoch: 3 step: 531, loss is 0.3084261119365692\n",
      "epoch: 3 step: 532, loss is 0.28023314476013184\n",
      "epoch: 3 step: 533, loss is 0.5134561657905579\n",
      "epoch: 3 step: 534, loss is 0.2276691347360611\n",
      "epoch: 3 step: 535, loss is 0.4521826207637787\n",
      "epoch: 3 step: 536, loss is 0.48320433497428894\n",
      "epoch: 3 step: 537, loss is 0.2880938947200775\n",
      "epoch: 3 step: 538, loss is 0.20811735093593597\n",
      "epoch: 3 step: 539, loss is 0.4621996581554413\n",
      "epoch: 3 step: 540, loss is 0.3198504149913788\n",
      "epoch: 3 step: 541, loss is 0.5082433819770813\n",
      "epoch: 3 step: 542, loss is 0.4990326464176178\n",
      "epoch: 3 step: 543, loss is 0.21302255988121033\n",
      "epoch: 3 step: 544, loss is 0.23022402822971344\n",
      "epoch: 3 step: 545, loss is 0.454102098941803\n",
      "epoch: 3 step: 546, loss is 0.407869428396225\n",
      "epoch: 3 step: 547, loss is 0.4545583426952362\n",
      "epoch: 3 step: 548, loss is 0.3641904890537262\n",
      "epoch: 3 step: 549, loss is 0.12243091315031052\n",
      "epoch: 3 step: 550, loss is 0.4580477476119995\n",
      "epoch: 3 step: 551, loss is 0.3368275761604309\n",
      "epoch: 3 step: 552, loss is 0.22505638003349304\n",
      "epoch: 3 step: 553, loss is 0.40114402770996094\n",
      "epoch: 3 step: 554, loss is 0.2900765836238861\n",
      "epoch: 3 step: 555, loss is 0.547908365726471\n",
      "epoch: 3 step: 556, loss is 0.38806191086769104\n",
      "epoch: 3 step: 557, loss is 0.28283825516700745\n",
      "epoch: 3 step: 558, loss is 0.2528150975704193\n",
      "epoch: 3 step: 559, loss is 0.34587302803993225\n",
      "epoch: 3 step: 560, loss is 0.27070188522338867\n",
      "epoch: 3 step: 561, loss is 0.3052985668182373\n",
      "epoch: 3 step: 562, loss is 0.4071301519870758\n",
      "epoch: 3 step: 563, loss is 0.520416796207428\n",
      "epoch: 3 step: 564, loss is 0.3056434392929077\n",
      "epoch: 3 step: 565, loss is 0.5510495901107788\n",
      "epoch: 3 step: 566, loss is 0.35634636878967285\n",
      "epoch: 3 step: 567, loss is 0.2352013736963272\n",
      "epoch: 3 step: 568, loss is 0.29398712515830994\n",
      "epoch: 3 step: 569, loss is 0.31317299604415894\n",
      "epoch: 3 step: 570, loss is 0.25600242614746094\n",
      "epoch: 3 step: 571, loss is 0.28164374828338623\n",
      "epoch: 3 step: 572, loss is 0.18827278912067413\n",
      "epoch: 3 step: 573, loss is 0.30900028347969055\n",
      "epoch: 3 step: 574, loss is 0.34757673740386963\n",
      "epoch: 3 step: 575, loss is 0.35776758193969727\n",
      "epoch: 3 step: 576, loss is 0.31907355785369873\n",
      "epoch: 3 step: 577, loss is 0.3881755769252777\n",
      "epoch: 3 step: 578, loss is 0.5791076421737671\n",
      "epoch: 3 step: 579, loss is 0.2416621297597885\n",
      "epoch: 3 step: 580, loss is 0.43728747963905334\n",
      "epoch: 3 step: 581, loss is 0.29224035143852234\n",
      "epoch: 3 step: 582, loss is 0.38012367486953735\n",
      "epoch: 3 step: 583, loss is 0.2791922688484192\n",
      "epoch: 3 step: 584, loss is 0.44244062900543213\n",
      "epoch: 3 step: 585, loss is 0.22230693697929382\n",
      "epoch: 3 step: 586, loss is 0.38226401805877686\n",
      "epoch: 3 step: 587, loss is 0.45806246995925903\n",
      "epoch: 3 step: 588, loss is 0.42936834692955017\n",
      "epoch: 3 step: 589, loss is 0.27219080924987793\n",
      "epoch: 3 step: 590, loss is 0.26612478494644165\n",
      "epoch: 3 step: 591, loss is 0.1856546700000763\n",
      "epoch: 3 step: 592, loss is 0.49490413069725037\n",
      "epoch: 3 step: 593, loss is 0.25703752040863037\n",
      "epoch: 3 step: 594, loss is 0.2382366806268692\n",
      "epoch: 3 step: 595, loss is 0.35115841031074524\n",
      "epoch: 3 step: 596, loss is 0.4238359034061432\n",
      "epoch: 3 step: 597, loss is 0.36704355478286743\n",
      "epoch: 3 step: 598, loss is 0.3026104271411896\n",
      "epoch: 3 step: 599, loss is 0.3098863959312439\n",
      "epoch: 3 step: 600, loss is 0.2600342631340027\n",
      "epoch: 3 step: 601, loss is 0.3506467342376709\n",
      "epoch: 3 step: 602, loss is 0.6374782919883728\n",
      "epoch: 3 step: 603, loss is 0.33235302567481995\n",
      "epoch: 3 step: 604, loss is 0.6611415147781372\n",
      "epoch: 3 step: 605, loss is 0.35149773955345154\n",
      "epoch: 3 step: 606, loss is 0.5083156824111938\n",
      "epoch: 3 step: 607, loss is 0.4992549419403076\n",
      "epoch: 3 step: 608, loss is 0.44782716035842896\n",
      "epoch: 3 step: 609, loss is 0.479528546333313\n",
      "epoch: 3 step: 610, loss is 0.14455729722976685\n",
      "epoch: 3 step: 611, loss is 0.27808433771133423\n",
      "epoch: 3 step: 612, loss is 0.3073137700557709\n",
      "epoch: 3 step: 613, loss is 0.24670375883579254\n",
      "epoch: 3 step: 614, loss is 0.35085731744766235\n",
      "epoch: 3 step: 615, loss is 0.37192368507385254\n",
      "epoch: 3 step: 616, loss is 0.3210570812225342\n",
      "epoch: 3 step: 617, loss is 0.38668060302734375\n",
      "epoch: 3 step: 618, loss is 0.19744297862052917\n",
      "epoch: 3 step: 619, loss is 0.3929165303707123\n",
      "epoch: 3 step: 620, loss is 0.339362770318985\n",
      "epoch: 3 step: 621, loss is 0.2404651939868927\n",
      "epoch: 3 step: 622, loss is 0.191670224070549\n",
      "epoch: 3 step: 623, loss is 0.2500366270542145\n",
      "epoch: 3 step: 624, loss is 0.20506559312343597\n",
      "epoch: 3 step: 625, loss is 0.468889057636261\n",
      "epoch: 3 step: 626, loss is 0.3191092908382416\n",
      "epoch: 3 step: 627, loss is 0.34947314858436584\n",
      "epoch: 3 step: 628, loss is 0.1701193004846573\n",
      "epoch: 3 step: 629, loss is 0.3294372260570526\n",
      "epoch: 3 step: 630, loss is 0.38487544655799866\n",
      "epoch: 3 step: 631, loss is 0.20803815126419067\n",
      "epoch: 3 step: 632, loss is 0.4951973259449005\n",
      "epoch: 3 step: 633, loss is 0.345956027507782\n",
      "epoch: 3 step: 634, loss is 0.21315576136112213\n",
      "epoch: 3 step: 635, loss is 0.36072880029678345\n",
      "epoch: 3 step: 636, loss is 0.303669273853302\n",
      "epoch: 3 step: 637, loss is 0.24917072057724\n",
      "epoch: 3 step: 638, loss is 0.298200786113739\n",
      "epoch: 3 step: 639, loss is 0.22850991785526276\n",
      "epoch: 3 step: 640, loss is 0.4719860851764679\n",
      "epoch: 3 step: 641, loss is 0.5383810997009277\n",
      "epoch: 3 step: 642, loss is 0.3474943935871124\n",
      "epoch: 3 step: 643, loss is 0.3909413516521454\n",
      "epoch: 3 step: 644, loss is 0.6183347702026367\n",
      "epoch: 3 step: 645, loss is 0.39553049206733704\n",
      "epoch: 3 step: 646, loss is 0.26599130034446716\n",
      "epoch: 3 step: 647, loss is 0.5912280082702637\n",
      "epoch: 3 step: 648, loss is 0.32328593730926514\n",
      "epoch: 3 step: 649, loss is 0.3932129144668579\n",
      "epoch: 3 step: 650, loss is 0.27349817752838135\n",
      "epoch: 3 step: 651, loss is 0.386568158864975\n",
      "epoch: 3 step: 652, loss is 0.2693506181240082\n",
      "epoch: 3 step: 653, loss is 0.43951481580734253\n",
      "epoch: 3 step: 654, loss is 0.3151833713054657\n",
      "epoch: 3 step: 655, loss is 0.37933680415153503\n",
      "epoch: 3 step: 656, loss is 0.4461323320865631\n",
      "epoch: 3 step: 657, loss is 0.4064883887767792\n",
      "epoch: 3 step: 658, loss is 0.34157124161720276\n",
      "epoch: 3 step: 659, loss is 0.38958677649497986\n",
      "epoch: 3 step: 660, loss is 0.19951371848583221\n",
      "epoch: 3 step: 661, loss is 0.33712226152420044\n",
      "epoch: 3 step: 662, loss is 0.3613641858100891\n",
      "epoch: 3 step: 663, loss is 0.5228406190872192\n",
      "epoch: 3 step: 664, loss is 0.18742872774600983\n",
      "epoch: 3 step: 665, loss is 0.4548875391483307\n",
      "epoch: 3 step: 666, loss is 0.38618549704551697\n",
      "epoch: 3 step: 667, loss is 0.2770378291606903\n",
      "epoch: 3 step: 668, loss is 0.24829930067062378\n",
      "epoch: 3 step: 669, loss is 0.2337973713874817\n",
      "epoch: 3 step: 670, loss is 0.38354671001434326\n",
      "epoch: 3 step: 671, loss is 0.5171130895614624\n",
      "epoch: 3 step: 672, loss is 0.46455293893814087\n",
      "epoch: 3 step: 673, loss is 0.36197611689567566\n",
      "epoch: 3 step: 674, loss is 0.36346864700317383\n",
      "epoch: 3 step: 675, loss is 0.2953846752643585\n",
      "epoch: 3 step: 676, loss is 0.4007808268070221\n",
      "epoch: 3 step: 677, loss is 0.35185760259628296\n",
      "epoch: 3 step: 678, loss is 0.3714197874069214\n",
      "epoch: 3 step: 679, loss is 0.46677547693252563\n",
      "epoch: 3 step: 680, loss is 0.3467318117618561\n",
      "epoch: 3 step: 681, loss is 0.3099270761013031\n",
      "epoch: 3 step: 682, loss is 0.2876358926296234\n",
      "epoch: 3 step: 683, loss is 0.30864280462265015\n",
      "epoch: 3 step: 684, loss is 0.3867633044719696\n",
      "epoch: 3 step: 685, loss is 0.2585570514202118\n",
      "epoch: 3 step: 686, loss is 0.2787758409976959\n",
      "epoch: 3 step: 687, loss is 0.4387776255607605\n",
      "epoch: 3 step: 688, loss is 0.2843148410320282\n",
      "epoch: 3 step: 689, loss is 0.16100123524665833\n",
      "epoch: 3 step: 690, loss is 0.2963933050632477\n",
      "epoch: 3 step: 691, loss is 0.29941534996032715\n",
      "epoch: 3 step: 692, loss is 0.4428083300590515\n",
      "epoch: 3 step: 693, loss is 0.33588987588882446\n",
      "epoch: 3 step: 694, loss is 0.59426349401474\n",
      "epoch: 3 step: 695, loss is 0.4069208800792694\n",
      "epoch: 3 step: 696, loss is 0.3909326195716858\n",
      "epoch: 3 step: 697, loss is 0.3481237292289734\n",
      "epoch: 3 step: 698, loss is 0.2816721200942993\n",
      "epoch: 3 step: 699, loss is 0.3722345530986786\n",
      "epoch: 3 step: 700, loss is 0.2662750780582428\n",
      "epoch: 3 step: 701, loss is 0.6056909561157227\n",
      "epoch: 3 step: 702, loss is 0.46194028854370117\n",
      "epoch: 3 step: 703, loss is 0.4125044345855713\n",
      "epoch: 3 step: 704, loss is 0.3631035387516022\n",
      "epoch: 3 step: 705, loss is 0.3749910295009613\n",
      "epoch: 3 step: 706, loss is 0.3588755130767822\n",
      "epoch: 3 step: 707, loss is 0.40607526898384094\n",
      "epoch: 3 step: 708, loss is 0.4401170313358307\n",
      "epoch: 3 step: 709, loss is 0.42183616757392883\n",
      "epoch: 3 step: 710, loss is 0.20675458014011383\n",
      "epoch: 3 step: 711, loss is 0.2448078840970993\n",
      "epoch: 3 step: 712, loss is 0.31748977303504944\n",
      "epoch: 3 step: 713, loss is 0.2953017055988312\n",
      "epoch: 3 step: 714, loss is 0.2774415612220764\n",
      "epoch: 3 step: 715, loss is 0.34246715903282166\n",
      "epoch: 3 step: 716, loss is 0.36311274766921997\n",
      "epoch: 3 step: 717, loss is 0.3960360884666443\n",
      "epoch: 3 step: 718, loss is 0.3901042938232422\n",
      "epoch: 3 step: 719, loss is 0.32869285345077515\n",
      "epoch: 3 step: 720, loss is 0.3329239785671234\n",
      "epoch: 3 step: 721, loss is 0.27385735511779785\n",
      "epoch: 3 step: 722, loss is 0.3445894420146942\n",
      "epoch: 3 step: 723, loss is 0.314533531665802\n",
      "epoch: 3 step: 724, loss is 0.2521386444568634\n",
      "epoch: 3 step: 725, loss is 0.3654787838459015\n",
      "epoch: 3 step: 726, loss is 0.4117361009120941\n",
      "epoch: 3 step: 727, loss is 0.3855321407318115\n",
      "epoch: 3 step: 728, loss is 0.23449571430683136\n",
      "epoch: 3 step: 729, loss is 0.31662264466285706\n",
      "epoch: 3 step: 730, loss is 0.367000937461853\n",
      "epoch: 3 step: 731, loss is 0.47535616159439087\n",
      "epoch: 3 step: 732, loss is 0.22218811511993408\n",
      "epoch: 3 step: 733, loss is 0.36843329668045044\n",
      "epoch: 3 step: 734, loss is 0.2762308120727539\n",
      "epoch: 3 step: 735, loss is 0.5058164596557617\n",
      "epoch: 3 step: 736, loss is 0.3793903589248657\n",
      "epoch: 3 step: 737, loss is 0.3244406580924988\n",
      "epoch: 3 step: 738, loss is 0.402288019657135\n",
      "epoch: 3 step: 739, loss is 0.2764577865600586\n",
      "epoch: 3 step: 740, loss is 0.2768896818161011\n",
      "epoch: 3 step: 741, loss is 0.37639349699020386\n",
      "epoch: 3 step: 742, loss is 0.4750286936759949\n",
      "epoch: 3 step: 743, loss is 0.2044600546360016\n",
      "epoch: 3 step: 744, loss is 0.47047746181488037\n",
      "epoch: 3 step: 745, loss is 0.3852654695510864\n",
      "epoch: 3 step: 746, loss is 0.24959203600883484\n",
      "epoch: 3 step: 747, loss is 0.24880445003509521\n",
      "epoch: 3 step: 748, loss is 0.41107267141342163\n",
      "epoch: 3 step: 749, loss is 0.2653450667858124\n",
      "epoch: 3 step: 750, loss is 0.39029449224472046\n",
      "epoch: 3 step: 751, loss is 0.44895491003990173\n",
      "epoch: 3 step: 752, loss is 0.4765322804450989\n",
      "epoch: 3 step: 753, loss is 0.46092459559440613\n",
      "epoch: 3 step: 754, loss is 0.2888224720954895\n",
      "epoch: 3 step: 755, loss is 0.6203654408454895\n",
      "epoch: 3 step: 756, loss is 0.3232787251472473\n",
      "epoch: 3 step: 757, loss is 0.23907597362995148\n",
      "epoch: 3 step: 758, loss is 0.3626919686794281\n",
      "epoch: 3 step: 759, loss is 0.21689562499523163\n",
      "epoch: 3 step: 760, loss is 0.25038257241249084\n",
      "epoch: 3 step: 761, loss is 0.4024553894996643\n",
      "epoch: 3 step: 762, loss is 0.32654091715812683\n",
      "epoch: 3 step: 763, loss is 0.24545511603355408\n",
      "epoch: 3 step: 764, loss is 0.3813129663467407\n",
      "epoch: 3 step: 765, loss is 0.20587149262428284\n",
      "epoch: 3 step: 766, loss is 0.3838788568973541\n",
      "epoch: 3 step: 767, loss is 0.4001820385456085\n",
      "epoch: 3 step: 768, loss is 0.2714668810367584\n",
      "epoch: 3 step: 769, loss is 0.3003973364830017\n",
      "epoch: 3 step: 770, loss is 0.2065286785364151\n",
      "epoch: 3 step: 771, loss is 0.3678991496562958\n",
      "epoch: 3 step: 772, loss is 0.3921799659729004\n",
      "epoch: 3 step: 773, loss is 0.34410470724105835\n",
      "epoch: 3 step: 774, loss is 0.23818732798099518\n",
      "epoch: 3 step: 775, loss is 0.3938795030117035\n",
      "epoch: 3 step: 776, loss is 0.3664490878582001\n",
      "epoch: 3 step: 777, loss is 0.1816863864660263\n",
      "epoch: 3 step: 778, loss is 0.4277343153953552\n",
      "epoch: 3 step: 779, loss is 0.22590069472789764\n",
      "epoch: 3 step: 780, loss is 0.1989067643880844\n",
      "epoch: 3 step: 781, loss is 0.35310181975364685\n",
      "epoch: 3 step: 782, loss is 0.6027624607086182\n",
      "epoch: 3 step: 783, loss is 0.3462323546409607\n",
      "epoch: 3 step: 784, loss is 0.34896641969680786\n",
      "epoch: 3 step: 785, loss is 0.2988688349723816\n",
      "epoch: 3 step: 786, loss is 0.32125356793403625\n",
      "epoch: 3 step: 787, loss is 0.32092493772506714\n",
      "epoch: 3 step: 788, loss is 0.571208119392395\n",
      "epoch: 3 step: 789, loss is 0.2651253640651703\n",
      "epoch: 3 step: 790, loss is 0.5348497629165649\n",
      "epoch: 3 step: 791, loss is 0.3837864100933075\n",
      "epoch: 3 step: 792, loss is 0.2168470323085785\n",
      "epoch: 3 step: 793, loss is 0.3068317174911499\n",
      "epoch: 3 step: 794, loss is 0.4226685166358948\n",
      "epoch: 3 step: 795, loss is 0.17034904658794403\n",
      "epoch: 3 step: 796, loss is 0.28356969356536865\n",
      "epoch: 3 step: 797, loss is 0.461647629737854\n",
      "epoch: 3 step: 798, loss is 0.23322442173957825\n",
      "epoch: 3 step: 799, loss is 0.456606388092041\n",
      "epoch: 3 step: 800, loss is 0.40727534890174866\n",
      "epoch: 3 step: 801, loss is 0.2811940014362335\n",
      "epoch: 3 step: 802, loss is 0.26120415329933167\n",
      "epoch: 3 step: 803, loss is 0.4957740306854248\n",
      "epoch: 3 step: 804, loss is 0.35771214962005615\n",
      "epoch: 3 step: 805, loss is 0.600987434387207\n",
      "epoch: 3 step: 806, loss is 0.39162081480026245\n",
      "epoch: 3 step: 807, loss is 0.43847963213920593\n",
      "epoch: 3 step: 808, loss is 0.3384974002838135\n",
      "epoch: 3 step: 809, loss is 0.35484808683395386\n",
      "epoch: 3 step: 810, loss is 0.30226075649261475\n",
      "epoch: 3 step: 811, loss is 0.2909451127052307\n",
      "epoch: 3 step: 812, loss is 0.3064175248146057\n",
      "epoch: 3 step: 813, loss is 0.5073198080062866\n",
      "epoch: 3 step: 814, loss is 0.24258364737033844\n",
      "epoch: 3 step: 815, loss is 0.31001996994018555\n",
      "epoch: 3 step: 816, loss is 0.23194889724254608\n",
      "epoch: 3 step: 817, loss is 0.5052427053451538\n",
      "epoch: 3 step: 818, loss is 0.34531843662261963\n",
      "epoch: 3 step: 819, loss is 0.4068055748939514\n",
      "epoch: 3 step: 820, loss is 0.33139899373054504\n",
      "epoch: 3 step: 821, loss is 0.2874341905117035\n",
      "epoch: 3 step: 822, loss is 0.3235433101654053\n",
      "epoch: 3 step: 823, loss is 0.3391193449497223\n",
      "epoch: 3 step: 824, loss is 0.21151937544345856\n",
      "epoch: 3 step: 825, loss is 0.2530565559864044\n",
      "epoch: 3 step: 826, loss is 0.24959838390350342\n",
      "epoch: 3 step: 827, loss is 0.21175557374954224\n",
      "epoch: 3 step: 828, loss is 0.46176859736442566\n",
      "epoch: 3 step: 829, loss is 0.38367268443107605\n",
      "epoch: 3 step: 830, loss is 0.2815439999103546\n",
      "epoch: 3 step: 831, loss is 0.2741996943950653\n",
      "epoch: 3 step: 832, loss is 0.4789629280567169\n",
      "epoch: 3 step: 833, loss is 0.3729492425918579\n",
      "epoch: 3 step: 834, loss is 0.36282292008399963\n",
      "epoch: 3 step: 835, loss is 0.628755509853363\n",
      "epoch: 3 step: 836, loss is 0.4573311507701874\n",
      "epoch: 3 step: 837, loss is 0.22619503736495972\n",
      "epoch: 3 step: 838, loss is 0.2918339967727661\n",
      "epoch: 3 step: 839, loss is 0.3999592363834381\n",
      "epoch: 3 step: 840, loss is 0.2779600918292999\n",
      "epoch: 3 step: 841, loss is 0.29561808705329895\n",
      "epoch: 3 step: 842, loss is 0.23693585395812988\n",
      "epoch: 3 step: 843, loss is 0.32281526923179626\n",
      "epoch: 3 step: 844, loss is 0.3835747241973877\n",
      "epoch: 3 step: 845, loss is 0.27990561723709106\n",
      "epoch: 3 step: 846, loss is 0.4553705155849457\n",
      "epoch: 3 step: 847, loss is 0.38161227107048035\n",
      "epoch: 3 step: 848, loss is 0.3282853364944458\n",
      "epoch: 3 step: 849, loss is 0.25711309909820557\n",
      "epoch: 3 step: 850, loss is 0.2202780693769455\n",
      "epoch: 3 step: 851, loss is 0.17951136827468872\n",
      "epoch: 3 step: 852, loss is 0.38908350467681885\n",
      "epoch: 3 step: 853, loss is 0.42499634623527527\n",
      "epoch: 3 step: 854, loss is 0.3591574728488922\n",
      "epoch: 3 step: 855, loss is 0.36917534470558167\n",
      "epoch: 3 step: 856, loss is 0.5293248295783997\n",
      "epoch: 3 step: 857, loss is 0.29858261346817017\n",
      "epoch: 3 step: 858, loss is 0.46553486585617065\n",
      "epoch: 3 step: 859, loss is 0.28827330470085144\n",
      "epoch: 3 step: 860, loss is 0.4078819453716278\n",
      "epoch: 3 step: 861, loss is 0.2730356752872467\n",
      "epoch: 3 step: 862, loss is 0.4657506048679352\n",
      "epoch: 3 step: 863, loss is 0.31702879071235657\n",
      "epoch: 3 step: 864, loss is 0.1873425394296646\n",
      "epoch: 3 step: 865, loss is 0.3290957510471344\n",
      "epoch: 3 step: 866, loss is 0.520433783531189\n",
      "epoch: 3 step: 867, loss is 0.3906441330909729\n",
      "epoch: 3 step: 868, loss is 0.336895227432251\n",
      "epoch: 3 step: 869, loss is 0.5263358950614929\n",
      "epoch: 3 step: 870, loss is 0.29377880692481995\n",
      "epoch: 3 step: 871, loss is 0.28457486629486084\n",
      "epoch: 3 step: 872, loss is 0.3386805057525635\n",
      "epoch: 3 step: 873, loss is 0.333339124917984\n",
      "epoch: 3 step: 874, loss is 0.26019665598869324\n",
      "epoch: 3 step: 875, loss is 0.21872003376483917\n",
      "epoch: 3 step: 876, loss is 0.2683087885379791\n",
      "epoch: 3 step: 877, loss is 0.29720818996429443\n",
      "epoch: 3 step: 878, loss is 0.33564528822898865\n",
      "epoch: 3 step: 879, loss is 0.3052467107772827\n",
      "epoch: 3 step: 880, loss is 0.18495257198810577\n",
      "epoch: 3 step: 881, loss is 0.492358922958374\n",
      "epoch: 3 step: 882, loss is 0.508832573890686\n",
      "epoch: 3 step: 883, loss is 0.36991968750953674\n",
      "epoch: 3 step: 884, loss is 0.6327255368232727\n",
      "epoch: 3 step: 885, loss is 0.3336023986339569\n",
      "epoch: 3 step: 886, loss is 0.16561274230480194\n",
      "epoch: 3 step: 887, loss is 0.27436521649360657\n",
      "epoch: 3 step: 888, loss is 0.21186789870262146\n",
      "epoch: 3 step: 889, loss is 0.3217984735965729\n",
      "epoch: 3 step: 890, loss is 0.36561909317970276\n",
      "epoch: 3 step: 891, loss is 0.4671325385570526\n",
      "epoch: 3 step: 892, loss is 0.21241982281208038\n",
      "epoch: 3 step: 893, loss is 0.3151758909225464\n",
      "epoch: 3 step: 894, loss is 0.4127030074596405\n",
      "epoch: 3 step: 895, loss is 0.24774734675884247\n",
      "epoch: 3 step: 896, loss is 0.3656350374221802\n",
      "epoch: 3 step: 897, loss is 0.20014217495918274\n",
      "epoch: 3 step: 898, loss is 0.31818926334381104\n",
      "epoch: 3 step: 899, loss is 0.5010548233985901\n",
      "epoch: 3 step: 900, loss is 0.2333965003490448\n",
      "epoch: 3 step: 901, loss is 0.21689948439598083\n",
      "epoch: 3 step: 902, loss is 0.2305765002965927\n",
      "epoch: 3 step: 903, loss is 0.203499436378479\n",
      "epoch: 3 step: 904, loss is 0.20588651299476624\n",
      "epoch: 3 step: 905, loss is 0.3011375367641449\n",
      "epoch: 3 step: 906, loss is 0.26723456382751465\n",
      "epoch: 3 step: 907, loss is 0.2950882911682129\n",
      "epoch: 3 step: 908, loss is 0.48336514830589294\n",
      "epoch: 3 step: 909, loss is 0.27538517117500305\n",
      "epoch: 3 step: 910, loss is 0.32566455006599426\n",
      "epoch: 3 step: 911, loss is 0.30518823862075806\n",
      "epoch: 3 step: 912, loss is 0.32981622219085693\n",
      "epoch: 3 step: 913, loss is 0.4408907890319824\n",
      "epoch: 3 step: 914, loss is 0.3071231245994568\n",
      "epoch: 3 step: 915, loss is 0.47716978192329407\n",
      "epoch: 3 step: 916, loss is 0.2614138424396515\n",
      "epoch: 3 step: 917, loss is 0.1981229931116104\n",
      "epoch: 3 step: 918, loss is 0.22691506147384644\n",
      "epoch: 3 step: 919, loss is 0.1462075263261795\n",
      "epoch: 3 step: 920, loss is 0.4448509216308594\n",
      "epoch: 3 step: 921, loss is 0.2518666386604309\n",
      "epoch: 3 step: 922, loss is 0.3730905055999756\n",
      "epoch: 3 step: 923, loss is 0.365050733089447\n",
      "epoch: 3 step: 924, loss is 0.3087773323059082\n",
      "epoch: 3 step: 925, loss is 0.35667234659194946\n",
      "epoch: 3 step: 926, loss is 0.3839673101902008\n",
      "epoch: 3 step: 927, loss is 0.3344407081604004\n",
      "epoch: 3 step: 928, loss is 0.36273255944252014\n",
      "epoch: 3 step: 929, loss is 0.24085509777069092\n",
      "epoch: 3 step: 930, loss is 0.4144376814365387\n",
      "epoch: 3 step: 931, loss is 0.44021451473236084\n",
      "epoch: 3 step: 932, loss is 0.28284141421318054\n",
      "epoch: 3 step: 933, loss is 0.3253268003463745\n",
      "epoch: 3 step: 934, loss is 0.6291496157646179\n",
      "epoch: 3 step: 935, loss is 0.44702112674713135\n",
      "epoch: 3 step: 936, loss is 0.6077296137809753\n",
      "epoch: 3 step: 937, loss is 0.31394511461257935\n",
      "epoch: 4 step: 1, loss is 0.33252331614494324\n",
      "epoch: 4 step: 2, loss is 0.4043106436729431\n",
      "epoch: 4 step: 3, loss is 0.1876886785030365\n",
      "epoch: 4 step: 4, loss is 0.5117971897125244\n",
      "epoch: 4 step: 5, loss is 0.3717430531978607\n",
      "epoch: 4 step: 6, loss is 0.2536366879940033\n",
      "epoch: 4 step: 7, loss is 0.32601654529571533\n",
      "epoch: 4 step: 8, loss is 0.2090228646993637\n",
      "epoch: 4 step: 9, loss is 0.237155944108963\n",
      "epoch: 4 step: 10, loss is 0.4725223183631897\n",
      "epoch: 4 step: 11, loss is 0.31628096103668213\n",
      "epoch: 4 step: 12, loss is 0.48553308844566345\n",
      "epoch: 4 step: 13, loss is 0.3045842945575714\n",
      "epoch: 4 step: 14, loss is 0.3672771453857422\n",
      "epoch: 4 step: 15, loss is 0.506519079208374\n",
      "epoch: 4 step: 16, loss is 0.30563482642173767\n",
      "epoch: 4 step: 17, loss is 0.3466936945915222\n",
      "epoch: 4 step: 18, loss is 0.4540383219718933\n",
      "epoch: 4 step: 19, loss is 0.36821991205215454\n",
      "epoch: 4 step: 20, loss is 0.3243478536605835\n",
      "epoch: 4 step: 21, loss is 0.34808605909347534\n",
      "epoch: 4 step: 22, loss is 0.3369126617908478\n",
      "epoch: 4 step: 23, loss is 0.280952125787735\n",
      "epoch: 4 step: 24, loss is 0.4386284053325653\n",
      "epoch: 4 step: 25, loss is 0.2581055760383606\n",
      "epoch: 4 step: 26, loss is 0.5506847500801086\n",
      "epoch: 4 step: 27, loss is 0.15731339156627655\n",
      "epoch: 4 step: 28, loss is 0.46720772981643677\n",
      "epoch: 4 step: 29, loss is 0.40037524700164795\n",
      "epoch: 4 step: 30, loss is 0.40029194951057434\n",
      "epoch: 4 step: 31, loss is 0.36023572087287903\n",
      "epoch: 4 step: 32, loss is 0.26941683888435364\n",
      "epoch: 4 step: 33, loss is 0.25961408019065857\n",
      "epoch: 4 step: 34, loss is 0.19838972389698029\n",
      "epoch: 4 step: 35, loss is 0.5504601001739502\n",
      "epoch: 4 step: 36, loss is 0.2806961238384247\n",
      "epoch: 4 step: 37, loss is 0.421528160572052\n",
      "epoch: 4 step: 38, loss is 0.32957354187965393\n",
      "epoch: 4 step: 39, loss is 0.4166097939014435\n",
      "epoch: 4 step: 40, loss is 0.33877646923065186\n",
      "epoch: 4 step: 41, loss is 0.297709196805954\n",
      "epoch: 4 step: 42, loss is 0.30252379179000854\n",
      "epoch: 4 step: 43, loss is 0.3902221918106079\n",
      "epoch: 4 step: 44, loss is 0.4007919430732727\n",
      "epoch: 4 step: 45, loss is 0.4140833914279938\n",
      "epoch: 4 step: 46, loss is 0.22939784824848175\n",
      "epoch: 4 step: 47, loss is 0.2607852518558502\n",
      "epoch: 4 step: 48, loss is 0.2832649052143097\n",
      "epoch: 4 step: 49, loss is 0.2524552345275879\n",
      "epoch: 4 step: 50, loss is 0.2517068088054657\n",
      "epoch: 4 step: 51, loss is 0.24048659205436707\n",
      "epoch: 4 step: 52, loss is 0.20699341595172882\n",
      "epoch: 4 step: 53, loss is 0.42798948287963867\n",
      "epoch: 4 step: 54, loss is 0.23762314021587372\n",
      "epoch: 4 step: 55, loss is 0.45935070514678955\n",
      "epoch: 4 step: 56, loss is 0.2345523089170456\n",
      "epoch: 4 step: 57, loss is 0.18861551582813263\n",
      "epoch: 4 step: 58, loss is 0.28907543420791626\n",
      "epoch: 4 step: 59, loss is 0.3641182482242584\n",
      "epoch: 4 step: 60, loss is 0.287882536649704\n",
      "epoch: 4 step: 61, loss is 0.41904112696647644\n",
      "epoch: 4 step: 62, loss is 0.39851096272468567\n",
      "epoch: 4 step: 63, loss is 0.5360274314880371\n",
      "epoch: 4 step: 64, loss is 0.27163395285606384\n",
      "epoch: 4 step: 65, loss is 0.2810311019420624\n",
      "epoch: 4 step: 66, loss is 0.31472453474998474\n",
      "epoch: 4 step: 67, loss is 0.39275193214416504\n",
      "epoch: 4 step: 68, loss is 0.5023321509361267\n",
      "epoch: 4 step: 69, loss is 0.2918972373008728\n",
      "epoch: 4 step: 70, loss is 0.3014184236526489\n",
      "epoch: 4 step: 71, loss is 0.2337433099746704\n",
      "epoch: 4 step: 72, loss is 0.3884246051311493\n",
      "epoch: 4 step: 73, loss is 0.2910909354686737\n",
      "epoch: 4 step: 74, loss is 0.45411595702171326\n",
      "epoch: 4 step: 75, loss is 0.2897147834300995\n",
      "epoch: 4 step: 76, loss is 0.2620255649089813\n",
      "epoch: 4 step: 77, loss is 0.26642945408821106\n",
      "epoch: 4 step: 78, loss is 0.5941401720046997\n",
      "epoch: 4 step: 79, loss is 0.3155427873134613\n",
      "epoch: 4 step: 80, loss is 0.3184662461280823\n",
      "epoch: 4 step: 81, loss is 0.37395694851875305\n",
      "epoch: 4 step: 82, loss is 0.2659575343132019\n",
      "epoch: 4 step: 83, loss is 0.3056516945362091\n",
      "epoch: 4 step: 84, loss is 0.41044342517852783\n",
      "epoch: 4 step: 85, loss is 0.4146729111671448\n",
      "epoch: 4 step: 86, loss is 0.33558857440948486\n",
      "epoch: 4 step: 87, loss is 0.3026586174964905\n",
      "epoch: 4 step: 88, loss is 0.33892622590065\n",
      "epoch: 4 step: 89, loss is 0.371859073638916\n",
      "epoch: 4 step: 90, loss is 0.3547641336917877\n",
      "epoch: 4 step: 91, loss is 0.2798008918762207\n",
      "epoch: 4 step: 92, loss is 0.30154067277908325\n",
      "epoch: 4 step: 93, loss is 0.2504151165485382\n",
      "epoch: 4 step: 94, loss is 0.5749579071998596\n",
      "epoch: 4 step: 95, loss is 0.4722769856452942\n",
      "epoch: 4 step: 96, loss is 0.32832637429237366\n",
      "epoch: 4 step: 97, loss is 0.2817923426628113\n",
      "epoch: 4 step: 98, loss is 0.3881392776966095\n",
      "epoch: 4 step: 99, loss is 0.3030284345149994\n",
      "epoch: 4 step: 100, loss is 0.3066518008708954\n",
      "epoch: 4 step: 101, loss is 0.3901304006576538\n",
      "epoch: 4 step: 102, loss is 0.39393988251686096\n",
      "epoch: 4 step: 103, loss is 0.24046637117862701\n",
      "epoch: 4 step: 104, loss is 0.5055361390113831\n",
      "epoch: 4 step: 105, loss is 0.27858853340148926\n",
      "epoch: 4 step: 106, loss is 0.4662991166114807\n",
      "epoch: 4 step: 107, loss is 0.3194052577018738\n",
      "epoch: 4 step: 108, loss is 0.33744698762893677\n",
      "epoch: 4 step: 109, loss is 0.4450509250164032\n",
      "epoch: 4 step: 110, loss is 0.29295140504837036\n",
      "epoch: 4 step: 111, loss is 0.2420436292886734\n",
      "epoch: 4 step: 112, loss is 0.3575221598148346\n",
      "epoch: 4 step: 113, loss is 0.2343640923500061\n",
      "epoch: 4 step: 114, loss is 0.35129642486572266\n",
      "epoch: 4 step: 115, loss is 0.3941817581653595\n",
      "epoch: 4 step: 116, loss is 0.2233700454235077\n",
      "epoch: 4 step: 117, loss is 0.40766263008117676\n",
      "epoch: 4 step: 118, loss is 0.3386736214160919\n",
      "epoch: 4 step: 119, loss is 0.4734035134315491\n",
      "epoch: 4 step: 120, loss is 0.3024185001850128\n",
      "epoch: 4 step: 121, loss is 0.38974428176879883\n",
      "epoch: 4 step: 122, loss is 0.3699306845664978\n",
      "epoch: 4 step: 123, loss is 0.3298187553882599\n",
      "epoch: 4 step: 124, loss is 0.2502916157245636\n",
      "epoch: 4 step: 125, loss is 0.3198860287666321\n",
      "epoch: 4 step: 126, loss is 0.42326539754867554\n",
      "epoch: 4 step: 127, loss is 0.42543721199035645\n",
      "epoch: 4 step: 128, loss is 0.600443959236145\n",
      "epoch: 4 step: 129, loss is 0.170087993144989\n",
      "epoch: 4 step: 130, loss is 0.409536212682724\n",
      "epoch: 4 step: 131, loss is 0.3646158277988434\n",
      "epoch: 4 step: 132, loss is 0.5076643824577332\n",
      "epoch: 4 step: 133, loss is 0.2292555570602417\n",
      "epoch: 4 step: 134, loss is 0.5575680136680603\n",
      "epoch: 4 step: 135, loss is 0.39157360792160034\n",
      "epoch: 4 step: 136, loss is 0.25484055280685425\n",
      "epoch: 4 step: 137, loss is 0.3021826446056366\n",
      "epoch: 4 step: 138, loss is 0.35815930366516113\n",
      "epoch: 4 step: 139, loss is 0.3277626037597656\n",
      "epoch: 4 step: 140, loss is 0.29301542043685913\n",
      "epoch: 4 step: 141, loss is 0.18460766971111298\n",
      "epoch: 4 step: 142, loss is 0.2754557430744171\n",
      "epoch: 4 step: 143, loss is 0.24327164888381958\n",
      "epoch: 4 step: 144, loss is 0.24632664024829865\n",
      "epoch: 4 step: 145, loss is 0.2663681209087372\n",
      "epoch: 4 step: 146, loss is 0.4129678010940552\n",
      "epoch: 4 step: 147, loss is 0.14800959825515747\n",
      "epoch: 4 step: 148, loss is 0.256086528301239\n",
      "epoch: 4 step: 149, loss is 0.3246169686317444\n",
      "epoch: 4 step: 150, loss is 0.43710488080978394\n",
      "epoch: 4 step: 151, loss is 0.29647988080978394\n",
      "epoch: 4 step: 152, loss is 0.25136369466781616\n",
      "epoch: 4 step: 153, loss is 0.30467620491981506\n",
      "epoch: 4 step: 154, loss is 0.281665563583374\n",
      "epoch: 4 step: 155, loss is 0.4780851900577545\n",
      "epoch: 4 step: 156, loss is 0.19010008871555328\n",
      "epoch: 4 step: 157, loss is 0.23045192658901215\n",
      "epoch: 4 step: 158, loss is 0.3484203815460205\n",
      "epoch: 4 step: 159, loss is 0.5180187821388245\n",
      "epoch: 4 step: 160, loss is 0.23889274895191193\n",
      "epoch: 4 step: 161, loss is 0.3712627589702606\n",
      "epoch: 4 step: 162, loss is 0.32305294275283813\n",
      "epoch: 4 step: 163, loss is 0.5153431296348572\n",
      "epoch: 4 step: 164, loss is 0.2971590757369995\n",
      "epoch: 4 step: 165, loss is 0.32563313841819763\n",
      "epoch: 4 step: 166, loss is 0.47509437799453735\n",
      "epoch: 4 step: 167, loss is 0.21628054976463318\n",
      "epoch: 4 step: 168, loss is 0.40054991841316223\n",
      "epoch: 4 step: 169, loss is 0.15463237464427948\n",
      "epoch: 4 step: 170, loss is 0.43888866901397705\n",
      "epoch: 4 step: 171, loss is 0.42975372076034546\n",
      "epoch: 4 step: 172, loss is 0.34127330780029297\n",
      "epoch: 4 step: 173, loss is 0.3298947811126709\n",
      "epoch: 4 step: 174, loss is 0.35065996646881104\n",
      "epoch: 4 step: 175, loss is 0.2537843585014343\n",
      "epoch: 4 step: 176, loss is 0.45780274271965027\n",
      "epoch: 4 step: 177, loss is 0.2808842957019806\n",
      "epoch: 4 step: 178, loss is 0.526528537273407\n",
      "epoch: 4 step: 179, loss is 0.3838142454624176\n",
      "epoch: 4 step: 180, loss is 0.3698366582393646\n",
      "epoch: 4 step: 181, loss is 0.2858007550239563\n",
      "epoch: 4 step: 182, loss is 0.285904198884964\n",
      "epoch: 4 step: 183, loss is 0.40315237641334534\n",
      "epoch: 4 step: 184, loss is 0.329960435628891\n",
      "epoch: 4 step: 185, loss is 0.19594664871692657\n",
      "epoch: 4 step: 186, loss is 0.2866505980491638\n",
      "epoch: 4 step: 187, loss is 0.41821160912513733\n",
      "epoch: 4 step: 188, loss is 0.23592185974121094\n",
      "epoch: 4 step: 189, loss is 0.25178998708724976\n",
      "epoch: 4 step: 190, loss is 0.21131104230880737\n",
      "epoch: 4 step: 191, loss is 0.35637813806533813\n",
      "epoch: 4 step: 192, loss is 0.22519661486148834\n",
      "epoch: 4 step: 193, loss is 0.327729195356369\n",
      "epoch: 4 step: 194, loss is 0.33016929030418396\n",
      "epoch: 4 step: 195, loss is 0.36597463488578796\n",
      "epoch: 4 step: 196, loss is 0.22424672544002533\n",
      "epoch: 4 step: 197, loss is 0.169078066945076\n",
      "epoch: 4 step: 198, loss is 0.2630424499511719\n",
      "epoch: 4 step: 199, loss is 0.18736273050308228\n",
      "epoch: 4 step: 200, loss is 0.2939867079257965\n",
      "epoch: 4 step: 201, loss is 0.34348568320274353\n",
      "epoch: 4 step: 202, loss is 0.5104276537895203\n",
      "epoch: 4 step: 203, loss is 0.3426204025745392\n",
      "epoch: 4 step: 204, loss is 0.4231377840042114\n",
      "epoch: 4 step: 205, loss is 0.23057933151721954\n",
      "epoch: 4 step: 206, loss is 0.3444189727306366\n",
      "epoch: 4 step: 207, loss is 0.1886310875415802\n",
      "epoch: 4 step: 208, loss is 0.2700638175010681\n",
      "epoch: 4 step: 209, loss is 0.2464103102684021\n",
      "epoch: 4 step: 210, loss is 0.3188364505767822\n",
      "epoch: 4 step: 211, loss is 0.21842633187770844\n",
      "epoch: 4 step: 212, loss is 0.2125381976366043\n",
      "epoch: 4 step: 213, loss is 0.3239079713821411\n",
      "epoch: 4 step: 214, loss is 0.3669348359107971\n",
      "epoch: 4 step: 215, loss is 0.324266642332077\n",
      "epoch: 4 step: 216, loss is 0.31473517417907715\n",
      "epoch: 4 step: 217, loss is 0.5613701343536377\n",
      "epoch: 4 step: 218, loss is 0.21759289503097534\n",
      "epoch: 4 step: 219, loss is 0.4034217298030853\n",
      "epoch: 4 step: 220, loss is 0.2524109482765198\n",
      "epoch: 4 step: 221, loss is 0.36741507053375244\n",
      "epoch: 4 step: 222, loss is 0.28815940022468567\n",
      "epoch: 4 step: 223, loss is 0.32724452018737793\n",
      "epoch: 4 step: 224, loss is 0.29688942432403564\n",
      "epoch: 4 step: 225, loss is 0.26428723335266113\n",
      "epoch: 4 step: 226, loss is 0.49930137395858765\n",
      "epoch: 4 step: 227, loss is 0.3859477937221527\n",
      "epoch: 4 step: 228, loss is 0.33587461709976196\n",
      "epoch: 4 step: 229, loss is 0.2478252798318863\n",
      "epoch: 4 step: 230, loss is 0.23825553059577942\n",
      "epoch: 4 step: 231, loss is 0.32447710633277893\n",
      "epoch: 4 step: 232, loss is 0.1744741052389145\n",
      "epoch: 4 step: 233, loss is 0.38927513360977173\n",
      "epoch: 4 step: 234, loss is 0.37505778670310974\n",
      "epoch: 4 step: 235, loss is 0.2566727101802826\n",
      "epoch: 4 step: 236, loss is 0.33006584644317627\n",
      "epoch: 4 step: 237, loss is 0.3492037355899811\n",
      "epoch: 4 step: 238, loss is 0.23228737711906433\n",
      "epoch: 4 step: 239, loss is 0.32287076115608215\n",
      "epoch: 4 step: 240, loss is 0.37351953983306885\n",
      "epoch: 4 step: 241, loss is 0.3127879500389099\n",
      "epoch: 4 step: 242, loss is 0.2976852059364319\n",
      "epoch: 4 step: 243, loss is 0.3574602007865906\n",
      "epoch: 4 step: 244, loss is 0.3812183439731598\n",
      "epoch: 4 step: 245, loss is 0.3336635231971741\n",
      "epoch: 4 step: 246, loss is 0.303008109331131\n",
      "epoch: 4 step: 247, loss is 0.30392685532569885\n",
      "epoch: 4 step: 248, loss is 0.32491081953048706\n",
      "epoch: 4 step: 249, loss is 0.26791390776634216\n",
      "epoch: 4 step: 250, loss is 0.356803297996521\n",
      "epoch: 4 step: 251, loss is 0.18657003343105316\n",
      "epoch: 4 step: 252, loss is 0.2119695544242859\n",
      "epoch: 4 step: 253, loss is 0.4070627987384796\n",
      "epoch: 4 step: 254, loss is 0.30819031596183777\n",
      "epoch: 4 step: 255, loss is 0.40107035636901855\n",
      "epoch: 4 step: 256, loss is 0.286840558052063\n",
      "epoch: 4 step: 257, loss is 0.31136247515678406\n",
      "epoch: 4 step: 258, loss is 0.1704190969467163\n",
      "epoch: 4 step: 259, loss is 0.4968295991420746\n",
      "epoch: 4 step: 260, loss is 0.2747015953063965\n",
      "epoch: 4 step: 261, loss is 0.28321656584739685\n",
      "epoch: 4 step: 262, loss is 0.40587279200553894\n",
      "epoch: 4 step: 263, loss is 0.4232499301433563\n",
      "epoch: 4 step: 264, loss is 0.27173203229904175\n",
      "epoch: 4 step: 265, loss is 0.21758854389190674\n",
      "epoch: 4 step: 266, loss is 0.4643971920013428\n",
      "epoch: 4 step: 267, loss is 0.3266678750514984\n",
      "epoch: 4 step: 268, loss is 0.3931025564670563\n",
      "epoch: 4 step: 269, loss is 0.29660171270370483\n",
      "epoch: 4 step: 270, loss is 0.3748120367527008\n",
      "epoch: 4 step: 271, loss is 0.4820239543914795\n",
      "epoch: 4 step: 272, loss is 0.32178282737731934\n",
      "epoch: 4 step: 273, loss is 0.36827322840690613\n",
      "epoch: 4 step: 274, loss is 0.35374483466148376\n",
      "epoch: 4 step: 275, loss is 0.3206535875797272\n",
      "epoch: 4 step: 276, loss is 0.29580751061439514\n",
      "epoch: 4 step: 277, loss is 0.4203135371208191\n",
      "epoch: 4 step: 278, loss is 0.22557765245437622\n",
      "epoch: 4 step: 279, loss is 0.413107693195343\n",
      "epoch: 4 step: 280, loss is 0.3689691126346588\n",
      "epoch: 4 step: 281, loss is 0.4287090003490448\n",
      "epoch: 4 step: 282, loss is 0.34860217571258545\n",
      "epoch: 4 step: 283, loss is 0.25224924087524414\n",
      "epoch: 4 step: 284, loss is 0.2169010192155838\n",
      "epoch: 4 step: 285, loss is 0.3368624448776245\n",
      "epoch: 4 step: 286, loss is 0.26140186190605164\n",
      "epoch: 4 step: 287, loss is 0.36081162095069885\n",
      "epoch: 4 step: 288, loss is 0.36730054020881653\n",
      "epoch: 4 step: 289, loss is 0.2087661325931549\n",
      "epoch: 4 step: 290, loss is 0.3122882544994354\n",
      "epoch: 4 step: 291, loss is 0.31262508034706116\n",
      "epoch: 4 step: 292, loss is 0.3784622550010681\n",
      "epoch: 4 step: 293, loss is 0.26209136843681335\n",
      "epoch: 4 step: 294, loss is 0.2342303842306137\n",
      "epoch: 4 step: 295, loss is 0.24822421371936798\n",
      "epoch: 4 step: 296, loss is 0.33378833532333374\n",
      "epoch: 4 step: 297, loss is 0.30631861090660095\n",
      "epoch: 4 step: 298, loss is 0.3929709494113922\n",
      "epoch: 4 step: 299, loss is 0.25951847434043884\n",
      "epoch: 4 step: 300, loss is 0.31584808230400085\n",
      "epoch: 4 step: 301, loss is 0.2637637257575989\n",
      "epoch: 4 step: 302, loss is 0.3004145920276642\n",
      "epoch: 4 step: 303, loss is 0.2396778017282486\n",
      "epoch: 4 step: 304, loss is 0.3706792891025543\n",
      "epoch: 4 step: 305, loss is 0.28592708706855774\n",
      "epoch: 4 step: 306, loss is 0.4433484673500061\n",
      "epoch: 4 step: 307, loss is 0.3682565689086914\n",
      "epoch: 4 step: 308, loss is 0.22492823004722595\n",
      "epoch: 4 step: 309, loss is 0.4460258185863495\n",
      "epoch: 4 step: 310, loss is 0.26025569438934326\n",
      "epoch: 4 step: 311, loss is 0.39977216720581055\n",
      "epoch: 4 step: 312, loss is 0.37700924277305603\n",
      "epoch: 4 step: 313, loss is 0.4309157431125641\n",
      "epoch: 4 step: 314, loss is 0.3736025393009186\n",
      "epoch: 4 step: 315, loss is 0.3164720833301544\n",
      "epoch: 4 step: 316, loss is 0.19466474652290344\n",
      "epoch: 4 step: 317, loss is 0.26009926199913025\n",
      "epoch: 4 step: 318, loss is 0.35338059067726135\n",
      "epoch: 4 step: 319, loss is 0.4416894018650055\n",
      "epoch: 4 step: 320, loss is 0.32117074728012085\n",
      "epoch: 4 step: 321, loss is 0.4031906723976135\n",
      "epoch: 4 step: 322, loss is 0.44364863634109497\n",
      "epoch: 4 step: 323, loss is 0.34693944454193115\n",
      "epoch: 4 step: 324, loss is 0.33652812242507935\n",
      "epoch: 4 step: 325, loss is 0.4681163728237152\n",
      "epoch: 4 step: 326, loss is 0.33315977454185486\n",
      "epoch: 4 step: 327, loss is 0.23818019032478333\n",
      "epoch: 4 step: 328, loss is 0.15997160971164703\n",
      "epoch: 4 step: 329, loss is 0.1588621437549591\n",
      "epoch: 4 step: 330, loss is 0.30582326650619507\n",
      "epoch: 4 step: 331, loss is 0.29081717133522034\n",
      "epoch: 4 step: 332, loss is 0.3166314959526062\n",
      "epoch: 4 step: 333, loss is 0.19150777161121368\n",
      "epoch: 4 step: 334, loss is 0.43520134687423706\n",
      "epoch: 4 step: 335, loss is 0.3612556755542755\n",
      "epoch: 4 step: 336, loss is 0.23637712001800537\n",
      "epoch: 4 step: 337, loss is 0.35308241844177246\n",
      "epoch: 4 step: 338, loss is 0.5434743165969849\n",
      "epoch: 4 step: 339, loss is 0.38756439089775085\n",
      "epoch: 4 step: 340, loss is 0.34527459740638733\n",
      "epoch: 4 step: 341, loss is 0.4881768524646759\n",
      "epoch: 4 step: 342, loss is 0.4046436846256256\n",
      "epoch: 4 step: 343, loss is 0.26118582487106323\n",
      "epoch: 4 step: 344, loss is 0.506396472454071\n",
      "epoch: 4 step: 345, loss is 0.3517092764377594\n",
      "epoch: 4 step: 346, loss is 0.58384770154953\n",
      "epoch: 4 step: 347, loss is 0.37982243299484253\n",
      "epoch: 4 step: 348, loss is 0.36246880888938904\n",
      "epoch: 4 step: 349, loss is 0.29163575172424316\n",
      "epoch: 4 step: 350, loss is 0.3799642324447632\n",
      "epoch: 4 step: 351, loss is 0.23956358432769775\n",
      "epoch: 4 step: 352, loss is 0.33248117566108704\n",
      "epoch: 4 step: 353, loss is 0.24983102083206177\n",
      "epoch: 4 step: 354, loss is 0.2416875958442688\n",
      "epoch: 4 step: 355, loss is 0.3086031675338745\n",
      "epoch: 4 step: 356, loss is 0.32485175132751465\n",
      "epoch: 4 step: 357, loss is 0.3396891951560974\n",
      "epoch: 4 step: 358, loss is 0.2936241328716278\n",
      "epoch: 4 step: 359, loss is 0.5618048906326294\n",
      "epoch: 4 step: 360, loss is 0.18875226378440857\n",
      "epoch: 4 step: 361, loss is 0.4005444347858429\n",
      "epoch: 4 step: 362, loss is 0.30745112895965576\n",
      "epoch: 4 step: 363, loss is 0.3089459240436554\n",
      "epoch: 4 step: 364, loss is 0.2411479353904724\n",
      "epoch: 4 step: 365, loss is 0.41382792592048645\n",
      "epoch: 4 step: 366, loss is 0.30651962757110596\n",
      "epoch: 4 step: 367, loss is 0.32623422145843506\n",
      "epoch: 4 step: 368, loss is 0.46377459168434143\n",
      "epoch: 4 step: 369, loss is 0.33246031403541565\n",
      "epoch: 4 step: 370, loss is 0.27841585874557495\n",
      "epoch: 4 step: 371, loss is 0.4511561095714569\n",
      "epoch: 4 step: 372, loss is 0.2324858009815216\n",
      "epoch: 4 step: 373, loss is 0.3159998059272766\n",
      "epoch: 4 step: 374, loss is 0.24003727734088898\n",
      "epoch: 4 step: 375, loss is 0.5707732439041138\n",
      "epoch: 4 step: 376, loss is 0.3604617118835449\n",
      "epoch: 4 step: 377, loss is 0.17408573627471924\n",
      "epoch: 4 step: 378, loss is 0.3652989864349365\n",
      "epoch: 4 step: 379, loss is 0.36394667625427246\n",
      "epoch: 4 step: 380, loss is 0.20758546888828278\n",
      "epoch: 4 step: 381, loss is 0.38304150104522705\n",
      "epoch: 4 step: 382, loss is 0.19883637130260468\n",
      "epoch: 4 step: 383, loss is 0.3274271786212921\n",
      "epoch: 4 step: 384, loss is 0.23670142889022827\n",
      "epoch: 4 step: 385, loss is 0.31043678522109985\n",
      "epoch: 4 step: 386, loss is 0.2673197090625763\n",
      "epoch: 4 step: 387, loss is 0.35394811630249023\n",
      "epoch: 4 step: 388, loss is 0.3125675916671753\n",
      "epoch: 4 step: 389, loss is 0.5442404747009277\n",
      "epoch: 4 step: 390, loss is 0.4184897840023041\n",
      "epoch: 4 step: 391, loss is 0.2882450520992279\n",
      "epoch: 4 step: 392, loss is 0.26869383454322815\n",
      "epoch: 4 step: 393, loss is 0.3772708773612976\n",
      "epoch: 4 step: 394, loss is 0.3487338125705719\n",
      "epoch: 4 step: 395, loss is 0.4664728343486786\n",
      "epoch: 4 step: 396, loss is 0.35774075984954834\n",
      "epoch: 4 step: 397, loss is 0.2489716112613678\n",
      "epoch: 4 step: 398, loss is 0.1351444572210312\n",
      "epoch: 4 step: 399, loss is 0.24132393300533295\n",
      "epoch: 4 step: 400, loss is 0.2541305422782898\n",
      "epoch: 4 step: 401, loss is 0.3755471706390381\n",
      "epoch: 4 step: 402, loss is 0.22630377113819122\n",
      "epoch: 4 step: 403, loss is 0.4557696282863617\n",
      "epoch: 4 step: 404, loss is 0.269137442111969\n",
      "epoch: 4 step: 405, loss is 0.36284542083740234\n",
      "epoch: 4 step: 406, loss is 0.461751252412796\n",
      "epoch: 4 step: 407, loss is 0.40539801120758057\n",
      "epoch: 4 step: 408, loss is 0.3202407658100128\n",
      "epoch: 4 step: 409, loss is 0.4361858069896698\n",
      "epoch: 4 step: 410, loss is 0.4637068510055542\n",
      "epoch: 4 step: 411, loss is 0.270068496465683\n",
      "epoch: 4 step: 412, loss is 0.305721640586853\n",
      "epoch: 4 step: 413, loss is 0.4772571623325348\n",
      "epoch: 4 step: 414, loss is 0.4377923011779785\n",
      "epoch: 4 step: 415, loss is 0.44129180908203125\n",
      "epoch: 4 step: 416, loss is 0.17639458179473877\n",
      "epoch: 4 step: 417, loss is 0.46097272634506226\n",
      "epoch: 4 step: 418, loss is 0.28292036056518555\n",
      "epoch: 4 step: 419, loss is 0.3870210647583008\n",
      "epoch: 4 step: 420, loss is 0.3087832033634186\n",
      "epoch: 4 step: 421, loss is 0.27721020579338074\n",
      "epoch: 4 step: 422, loss is 0.3455987870693207\n",
      "epoch: 4 step: 423, loss is 0.18697087466716766\n",
      "epoch: 4 step: 424, loss is 0.35051974654197693\n",
      "epoch: 4 step: 425, loss is 0.31933456659317017\n",
      "epoch: 4 step: 426, loss is 0.49763521552085876\n",
      "epoch: 4 step: 427, loss is 0.2410576194524765\n",
      "epoch: 4 step: 428, loss is 0.22206351161003113\n",
      "epoch: 4 step: 429, loss is 0.36093834042549133\n",
      "epoch: 4 step: 430, loss is 0.21877500414848328\n",
      "epoch: 4 step: 431, loss is 0.2509727478027344\n",
      "epoch: 4 step: 432, loss is 0.27143198251724243\n",
      "epoch: 4 step: 433, loss is 0.2688731849193573\n",
      "epoch: 4 step: 434, loss is 0.24067078530788422\n",
      "epoch: 4 step: 435, loss is 0.10599232465028763\n",
      "epoch: 4 step: 436, loss is 0.2758156359195709\n",
      "epoch: 4 step: 437, loss is 0.43525710701942444\n",
      "epoch: 4 step: 438, loss is 0.3010522127151489\n",
      "epoch: 4 step: 439, loss is 0.4265385866165161\n",
      "epoch: 4 step: 440, loss is 0.22465328872203827\n",
      "epoch: 4 step: 441, loss is 0.4219767451286316\n",
      "epoch: 4 step: 442, loss is 0.27265694737434387\n",
      "epoch: 4 step: 443, loss is 0.22643785178661346\n",
      "epoch: 4 step: 444, loss is 0.2831692397594452\n",
      "epoch: 4 step: 445, loss is 0.16273365914821625\n",
      "epoch: 4 step: 446, loss is 0.23521924018859863\n",
      "epoch: 4 step: 447, loss is 0.21837754547595978\n",
      "epoch: 4 step: 448, loss is 0.31980782747268677\n",
      "epoch: 4 step: 449, loss is 0.25658807158470154\n",
      "epoch: 4 step: 450, loss is 0.27294138073921204\n",
      "epoch: 4 step: 451, loss is 0.33677107095718384\n",
      "epoch: 4 step: 452, loss is 0.280300110578537\n",
      "epoch: 4 step: 453, loss is 0.2697846591472626\n",
      "epoch: 4 step: 454, loss is 0.284658282995224\n",
      "epoch: 4 step: 455, loss is 0.23503412306308746\n",
      "epoch: 4 step: 456, loss is 0.26957663893699646\n",
      "epoch: 4 step: 457, loss is 0.3130677044391632\n",
      "epoch: 4 step: 458, loss is 0.18334057927131653\n",
      "epoch: 4 step: 459, loss is 0.31758809089660645\n",
      "epoch: 4 step: 460, loss is 0.5288801789283752\n",
      "epoch: 4 step: 461, loss is 0.2532156705856323\n",
      "epoch: 4 step: 462, loss is 0.2853142321109772\n",
      "epoch: 4 step: 463, loss is 0.35074687004089355\n",
      "epoch: 4 step: 464, loss is 0.3804379105567932\n",
      "epoch: 4 step: 465, loss is 0.4046086072921753\n",
      "epoch: 4 step: 466, loss is 0.27367323637008667\n",
      "epoch: 4 step: 467, loss is 0.2308872640132904\n",
      "epoch: 4 step: 468, loss is 0.3310891389846802\n",
      "epoch: 4 step: 469, loss is 0.2743157148361206\n",
      "epoch: 4 step: 470, loss is 0.34136995673179626\n",
      "epoch: 4 step: 471, loss is 0.39662423729896545\n",
      "epoch: 4 step: 472, loss is 0.4243197441101074\n",
      "epoch: 4 step: 473, loss is 0.2772802412509918\n",
      "epoch: 4 step: 474, loss is 0.4930398464202881\n",
      "epoch: 4 step: 475, loss is 0.3721649646759033\n",
      "epoch: 4 step: 476, loss is 0.29465794563293457\n",
      "epoch: 4 step: 477, loss is 0.24404284358024597\n",
      "epoch: 4 step: 478, loss is 0.24702242016792297\n",
      "epoch: 4 step: 479, loss is 0.2332838475704193\n",
      "epoch: 4 step: 480, loss is 0.2715321481227875\n",
      "epoch: 4 step: 481, loss is 0.5339165329933167\n",
      "epoch: 4 step: 482, loss is 0.31923142075538635\n",
      "epoch: 4 step: 483, loss is 0.42175427079200745\n",
      "epoch: 4 step: 484, loss is 0.19592149555683136\n",
      "epoch: 4 step: 485, loss is 0.34622839093208313\n",
      "epoch: 4 step: 486, loss is 0.18676196038722992\n",
      "epoch: 4 step: 487, loss is 0.2853910028934479\n",
      "epoch: 4 step: 488, loss is 0.2636840343475342\n",
      "epoch: 4 step: 489, loss is 0.32641127705574036\n",
      "epoch: 4 step: 490, loss is 0.42994871735572815\n",
      "epoch: 4 step: 491, loss is 0.4116361737251282\n",
      "epoch: 4 step: 492, loss is 0.33329612016677856\n",
      "epoch: 4 step: 493, loss is 0.5114621520042419\n",
      "epoch: 4 step: 494, loss is 0.5753084421157837\n",
      "epoch: 4 step: 495, loss is 0.43746522068977356\n",
      "epoch: 4 step: 496, loss is 0.4136268198490143\n",
      "epoch: 4 step: 497, loss is 0.3434823155403137\n",
      "epoch: 4 step: 498, loss is 0.21395765244960785\n",
      "epoch: 4 step: 499, loss is 0.27687326073646545\n",
      "epoch: 4 step: 500, loss is 0.3377246856689453\n",
      "epoch: 4 step: 501, loss is 0.27862951159477234\n",
      "epoch: 4 step: 502, loss is 0.34501177072525024\n",
      "epoch: 4 step: 503, loss is 0.4820961654186249\n",
      "epoch: 4 step: 504, loss is 0.29453909397125244\n",
      "epoch: 4 step: 505, loss is 0.13719940185546875\n",
      "epoch: 4 step: 506, loss is 0.3084893822669983\n",
      "epoch: 4 step: 507, loss is 0.2293744832277298\n",
      "epoch: 4 step: 508, loss is 0.293355256319046\n",
      "epoch: 4 step: 509, loss is 0.1487780064344406\n",
      "epoch: 4 step: 510, loss is 0.19755515456199646\n",
      "epoch: 4 step: 511, loss is 0.2928250730037689\n",
      "epoch: 4 step: 512, loss is 0.3348524272441864\n",
      "epoch: 4 step: 513, loss is 0.2996671497821808\n",
      "epoch: 4 step: 514, loss is 0.2816833257675171\n",
      "epoch: 4 step: 515, loss is 0.4010303318500519\n",
      "epoch: 4 step: 516, loss is 0.39997777342796326\n",
      "epoch: 4 step: 517, loss is 0.3045356869697571\n",
      "epoch: 4 step: 518, loss is 0.3086850941181183\n",
      "epoch: 4 step: 519, loss is 0.29170504212379456\n",
      "epoch: 4 step: 520, loss is 0.3103939890861511\n",
      "epoch: 4 step: 521, loss is 0.6378039717674255\n",
      "epoch: 4 step: 522, loss is 0.441775918006897\n",
      "epoch: 4 step: 523, loss is 0.26012128591537476\n",
      "epoch: 4 step: 524, loss is 0.25293102860450745\n",
      "epoch: 4 step: 525, loss is 0.34051454067230225\n",
      "epoch: 4 step: 526, loss is 0.2671191692352295\n",
      "epoch: 4 step: 527, loss is 0.18456627428531647\n",
      "epoch: 4 step: 528, loss is 0.3539191782474518\n",
      "epoch: 4 step: 529, loss is 0.2249479442834854\n",
      "epoch: 4 step: 530, loss is 0.3610571324825287\n",
      "epoch: 4 step: 531, loss is 0.33652225136756897\n",
      "epoch: 4 step: 532, loss is 0.20946770906448364\n",
      "epoch: 4 step: 533, loss is 0.41326969861984253\n",
      "epoch: 4 step: 534, loss is 0.3225043714046478\n",
      "epoch: 4 step: 535, loss is 0.42367348074913025\n",
      "epoch: 4 step: 536, loss is 0.4186914265155792\n",
      "epoch: 4 step: 537, loss is 0.38521966338157654\n",
      "epoch: 4 step: 538, loss is 0.14716364443302155\n",
      "epoch: 4 step: 539, loss is 0.356449156999588\n",
      "epoch: 4 step: 540, loss is 0.27762943506240845\n",
      "epoch: 4 step: 541, loss is 0.5541531443595886\n",
      "epoch: 4 step: 542, loss is 0.17200742661952972\n",
      "epoch: 4 step: 543, loss is 0.3501731753349304\n",
      "epoch: 4 step: 544, loss is 0.24377629160881042\n",
      "epoch: 4 step: 545, loss is 0.3355124592781067\n",
      "epoch: 4 step: 546, loss is 0.35815703868865967\n",
      "epoch: 4 step: 547, loss is 0.4447091817855835\n",
      "epoch: 4 step: 548, loss is 0.3742547035217285\n",
      "epoch: 4 step: 549, loss is 0.3741944134235382\n",
      "epoch: 4 step: 550, loss is 0.673301637172699\n",
      "epoch: 4 step: 551, loss is 0.28295814990997314\n",
      "epoch: 4 step: 552, loss is 0.37645047903060913\n",
      "epoch: 4 step: 553, loss is 0.18034379184246063\n",
      "epoch: 4 step: 554, loss is 0.33550578355789185\n",
      "epoch: 4 step: 555, loss is 0.43884798884391785\n",
      "epoch: 4 step: 556, loss is 0.2647446095943451\n",
      "epoch: 4 step: 557, loss is 0.28471699357032776\n",
      "epoch: 4 step: 558, loss is 0.25526556372642517\n",
      "epoch: 4 step: 559, loss is 0.26836949586868286\n",
      "epoch: 4 step: 560, loss is 0.31073057651519775\n",
      "epoch: 4 step: 561, loss is 0.3235186040401459\n",
      "epoch: 4 step: 562, loss is 0.3328506648540497\n",
      "epoch: 4 step: 563, loss is 0.37591156363487244\n",
      "epoch: 4 step: 564, loss is 0.3920399248600006\n",
      "epoch: 4 step: 565, loss is 0.36214011907577515\n",
      "epoch: 4 step: 566, loss is 0.4403812289237976\n",
      "epoch: 4 step: 567, loss is 0.32836228609085083\n",
      "epoch: 4 step: 568, loss is 0.3878399729728699\n",
      "epoch: 4 step: 569, loss is 0.46582627296447754\n",
      "epoch: 4 step: 570, loss is 0.35347145795822144\n",
      "epoch: 4 step: 571, loss is 0.3767738938331604\n",
      "epoch: 4 step: 572, loss is 0.3412078619003296\n",
      "epoch: 4 step: 573, loss is 0.3069332540035248\n",
      "epoch: 4 step: 574, loss is 0.2023961991071701\n",
      "epoch: 4 step: 575, loss is 0.3002811670303345\n",
      "epoch: 4 step: 576, loss is 0.41918477416038513\n",
      "epoch: 4 step: 577, loss is 0.28775766491889954\n",
      "epoch: 4 step: 578, loss is 0.30259111523628235\n",
      "epoch: 4 step: 579, loss is 0.32263875007629395\n",
      "epoch: 4 step: 580, loss is 0.5660541653633118\n",
      "epoch: 4 step: 581, loss is 0.3814134895801544\n",
      "epoch: 4 step: 582, loss is 0.3457281291484833\n",
      "epoch: 4 step: 583, loss is 0.2415202260017395\n",
      "epoch: 4 step: 584, loss is 0.32873207330703735\n",
      "epoch: 4 step: 585, loss is 0.28685882687568665\n",
      "epoch: 4 step: 586, loss is 0.3079596161842346\n",
      "epoch: 4 step: 587, loss is 0.16877038776874542\n",
      "epoch: 4 step: 588, loss is 0.2865051031112671\n",
      "epoch: 4 step: 589, loss is 0.21036884188652039\n",
      "epoch: 4 step: 590, loss is 0.29986366629600525\n",
      "epoch: 4 step: 591, loss is 0.1782100796699524\n",
      "epoch: 4 step: 592, loss is 0.1460859775543213\n",
      "epoch: 4 step: 593, loss is 0.3741195499897003\n",
      "epoch: 4 step: 594, loss is 0.4939998686313629\n",
      "epoch: 4 step: 595, loss is 0.3974698483943939\n",
      "epoch: 4 step: 596, loss is 0.6124409437179565\n",
      "epoch: 4 step: 597, loss is 0.5407215356826782\n",
      "epoch: 4 step: 598, loss is 0.1895870566368103\n",
      "epoch: 4 step: 599, loss is 0.5082211494445801\n",
      "epoch: 4 step: 600, loss is 0.2644585967063904\n",
      "epoch: 4 step: 601, loss is 0.4271651804447174\n",
      "epoch: 4 step: 602, loss is 0.34614798426628113\n",
      "epoch: 4 step: 603, loss is 0.38231509923934937\n",
      "epoch: 4 step: 604, loss is 0.3230198919773102\n",
      "epoch: 4 step: 605, loss is 0.4074334502220154\n",
      "epoch: 4 step: 606, loss is 0.24274799227714539\n",
      "epoch: 4 step: 607, loss is 0.42103278636932373\n",
      "epoch: 4 step: 608, loss is 0.1685440093278885\n",
      "epoch: 4 step: 609, loss is 0.3110586404800415\n",
      "epoch: 4 step: 610, loss is 0.15578219294548035\n",
      "epoch: 4 step: 611, loss is 0.4145808815956116\n",
      "epoch: 4 step: 612, loss is 0.2634487450122833\n",
      "epoch: 4 step: 613, loss is 0.28356122970581055\n",
      "epoch: 4 step: 614, loss is 0.37601616978645325\n",
      "epoch: 4 step: 615, loss is 0.4184429943561554\n",
      "epoch: 4 step: 616, loss is 0.35552042722702026\n",
      "epoch: 4 step: 617, loss is 0.30214276909828186\n",
      "epoch: 4 step: 618, loss is 0.44609513878822327\n",
      "epoch: 4 step: 619, loss is 0.2315026968717575\n",
      "epoch: 4 step: 620, loss is 0.22706180810928345\n",
      "epoch: 4 step: 621, loss is 0.2100519835948944\n",
      "epoch: 4 step: 622, loss is 0.24681197106838226\n",
      "epoch: 4 step: 623, loss is 0.5195731520652771\n",
      "epoch: 4 step: 624, loss is 0.29774269461631775\n",
      "epoch: 4 step: 625, loss is 0.442789226770401\n",
      "epoch: 4 step: 626, loss is 0.5677472949028015\n",
      "epoch: 4 step: 627, loss is 0.2094794660806656\n",
      "epoch: 4 step: 628, loss is 0.13500739634037018\n",
      "epoch: 4 step: 629, loss is 0.4387514889240265\n",
      "epoch: 4 step: 630, loss is 0.3358306586742401\n",
      "epoch: 4 step: 631, loss is 0.25117772817611694\n",
      "epoch: 4 step: 632, loss is 0.28676995635032654\n",
      "epoch: 4 step: 633, loss is 0.35876116156578064\n",
      "epoch: 4 step: 634, loss is 0.4026975929737091\n",
      "epoch: 4 step: 635, loss is 0.19531457126140594\n",
      "epoch: 4 step: 636, loss is 0.27146753668785095\n",
      "epoch: 4 step: 637, loss is 0.37209126353263855\n",
      "epoch: 4 step: 638, loss is 0.4025292694568634\n",
      "epoch: 4 step: 639, loss is 0.32503241300582886\n",
      "epoch: 4 step: 640, loss is 0.44332414865493774\n",
      "epoch: 4 step: 641, loss is 0.27083316445350647\n",
      "epoch: 4 step: 642, loss is 0.16635675728321075\n",
      "epoch: 4 step: 643, loss is 0.2381437122821808\n",
      "epoch: 4 step: 644, loss is 0.3479619324207306\n",
      "epoch: 4 step: 645, loss is 0.3087596595287323\n",
      "epoch: 4 step: 646, loss is 0.3285141587257385\n",
      "epoch: 4 step: 647, loss is 0.27124956250190735\n",
      "epoch: 4 step: 648, loss is 0.3404087722301483\n",
      "epoch: 4 step: 649, loss is 0.33493149280548096\n",
      "epoch: 4 step: 650, loss is 0.4040316343307495\n",
      "epoch: 4 step: 651, loss is 0.3302728831768036\n",
      "epoch: 4 step: 652, loss is 0.3880232572555542\n",
      "epoch: 4 step: 653, loss is 0.4007735550403595\n",
      "epoch: 4 step: 654, loss is 0.17157573997974396\n",
      "epoch: 4 step: 655, loss is 0.3329392969608307\n",
      "epoch: 4 step: 656, loss is 0.3492303192615509\n",
      "epoch: 4 step: 657, loss is 0.7265638709068298\n",
      "epoch: 4 step: 658, loss is 0.2219865918159485\n",
      "epoch: 4 step: 659, loss is 0.20085939764976501\n",
      "epoch: 4 step: 660, loss is 0.26185348629951477\n",
      "epoch: 4 step: 661, loss is 0.2654377818107605\n",
      "epoch: 4 step: 662, loss is 0.25332537293434143\n",
      "epoch: 4 step: 663, loss is 0.3557736575603485\n",
      "epoch: 4 step: 664, loss is 0.38960158824920654\n",
      "epoch: 4 step: 665, loss is 0.5916417241096497\n",
      "epoch: 4 step: 666, loss is 0.46722155809402466\n",
      "epoch: 4 step: 667, loss is 0.20682203769683838\n",
      "epoch: 4 step: 668, loss is 0.3040410280227661\n",
      "epoch: 4 step: 669, loss is 0.41301068663597107\n",
      "epoch: 4 step: 670, loss is 0.4440634250640869\n",
      "epoch: 4 step: 671, loss is 0.24152499437332153\n",
      "epoch: 4 step: 672, loss is 0.4718990921974182\n",
      "epoch: 4 step: 673, loss is 0.4067320227622986\n",
      "epoch: 4 step: 674, loss is 0.5125012397766113\n",
      "epoch: 4 step: 675, loss is 0.33879461884498596\n",
      "epoch: 4 step: 676, loss is 0.2732362747192383\n",
      "epoch: 4 step: 677, loss is 0.3572132885456085\n",
      "epoch: 4 step: 678, loss is 0.32918494939804077\n",
      "epoch: 4 step: 679, loss is 0.3309483528137207\n",
      "epoch: 4 step: 680, loss is 0.2407255619764328\n",
      "epoch: 4 step: 681, loss is 0.423043429851532\n",
      "epoch: 4 step: 682, loss is 0.29996955394744873\n",
      "epoch: 4 step: 683, loss is 0.43503475189208984\n",
      "epoch: 4 step: 684, loss is 0.32888105511665344\n",
      "epoch: 4 step: 685, loss is 0.28933992981910706\n",
      "epoch: 4 step: 686, loss is 0.3893420696258545\n",
      "epoch: 4 step: 687, loss is 0.2900888919830322\n",
      "epoch: 4 step: 688, loss is 0.23541359603405\n",
      "epoch: 4 step: 689, loss is 0.28254464268684387\n",
      "epoch: 4 step: 690, loss is 0.2667858302593231\n",
      "epoch: 4 step: 691, loss is 0.250894695520401\n",
      "epoch: 4 step: 692, loss is 0.236776664853096\n",
      "epoch: 4 step: 693, loss is 0.4003762900829315\n",
      "epoch: 4 step: 694, loss is 0.2583011984825134\n",
      "epoch: 4 step: 695, loss is 0.30016452074050903\n",
      "epoch: 4 step: 696, loss is 0.13423877954483032\n",
      "epoch: 4 step: 697, loss is 0.37118637561798096\n",
      "epoch: 4 step: 698, loss is 0.5437830090522766\n",
      "epoch: 4 step: 699, loss is 0.597000002861023\n",
      "epoch: 4 step: 700, loss is 0.2520599365234375\n",
      "epoch: 4 step: 701, loss is 0.24910278618335724\n",
      "epoch: 4 step: 702, loss is 0.2467823624610901\n",
      "epoch: 4 step: 703, loss is 0.328756719827652\n",
      "epoch: 4 step: 704, loss is 0.29065150022506714\n",
      "epoch: 4 step: 705, loss is 0.42458128929138184\n",
      "epoch: 4 step: 706, loss is 0.45545604825019836\n",
      "epoch: 4 step: 707, loss is 0.3419174253940582\n",
      "epoch: 4 step: 708, loss is 0.1930311769247055\n",
      "epoch: 4 step: 709, loss is 0.30846941471099854\n",
      "epoch: 4 step: 710, loss is 0.24072250723838806\n",
      "epoch: 4 step: 711, loss is 0.39735469222068787\n",
      "epoch: 4 step: 712, loss is 0.16176043450832367\n",
      "epoch: 4 step: 713, loss is 0.5858402252197266\n",
      "epoch: 4 step: 714, loss is 0.4787704050540924\n",
      "epoch: 4 step: 715, loss is 0.29923298954963684\n",
      "epoch: 4 step: 716, loss is 0.3214997351169586\n",
      "epoch: 4 step: 717, loss is 0.36728864908218384\n",
      "epoch: 4 step: 718, loss is 0.5263078808784485\n",
      "epoch: 4 step: 719, loss is 0.47246161103248596\n",
      "epoch: 4 step: 720, loss is 0.3041756749153137\n",
      "epoch: 4 step: 721, loss is 0.602495014667511\n",
      "epoch: 4 step: 722, loss is 0.4751184582710266\n",
      "epoch: 4 step: 723, loss is 0.38169166445732117\n",
      "epoch: 4 step: 724, loss is 0.4737773537635803\n",
      "epoch: 4 step: 725, loss is 0.2920134663581848\n",
      "epoch: 4 step: 726, loss is 0.6325405836105347\n",
      "epoch: 4 step: 727, loss is 0.4197291135787964\n",
      "epoch: 4 step: 728, loss is 0.24389253556728363\n",
      "epoch: 4 step: 729, loss is 0.27154508233070374\n",
      "epoch: 4 step: 730, loss is 0.2368728369474411\n",
      "epoch: 4 step: 731, loss is 0.29910606145858765\n",
      "epoch: 4 step: 732, loss is 0.26612570881843567\n",
      "epoch: 4 step: 733, loss is 0.3394657373428345\n",
      "epoch: 4 step: 734, loss is 0.25998663902282715\n",
      "epoch: 4 step: 735, loss is 0.18970701098442078\n",
      "epoch: 4 step: 736, loss is 0.33404335379600525\n",
      "epoch: 4 step: 737, loss is 0.4385398328304291\n",
      "epoch: 4 step: 738, loss is 0.2620927691459656\n",
      "epoch: 4 step: 739, loss is 0.23082678020000458\n",
      "epoch: 4 step: 740, loss is 0.4077782928943634\n",
      "epoch: 4 step: 741, loss is 0.4357316195964813\n",
      "epoch: 4 step: 742, loss is 0.27338725328445435\n",
      "epoch: 4 step: 743, loss is 0.3263254463672638\n",
      "epoch: 4 step: 744, loss is 0.35070541501045227\n",
      "epoch: 4 step: 745, loss is 0.3701888918876648\n",
      "epoch: 4 step: 746, loss is 0.42418625950813293\n",
      "epoch: 4 step: 747, loss is 0.37089329957962036\n",
      "epoch: 4 step: 748, loss is 0.23831415176391602\n",
      "epoch: 4 step: 749, loss is 0.5193698406219482\n",
      "epoch: 4 step: 750, loss is 0.4142133891582489\n",
      "epoch: 4 step: 751, loss is 0.4172881543636322\n",
      "epoch: 4 step: 752, loss is 0.4305582642555237\n",
      "epoch: 4 step: 753, loss is 0.5607240796089172\n",
      "epoch: 4 step: 754, loss is 0.33347538113594055\n",
      "epoch: 4 step: 755, loss is 0.4543291926383972\n",
      "epoch: 4 step: 756, loss is 0.30071523785591125\n",
      "epoch: 4 step: 757, loss is 0.3619050085544586\n",
      "epoch: 4 step: 758, loss is 0.3377218246459961\n",
      "epoch: 4 step: 759, loss is 0.2431735247373581\n",
      "epoch: 4 step: 760, loss is 0.22535093128681183\n",
      "epoch: 4 step: 761, loss is 0.48503538966178894\n",
      "epoch: 4 step: 762, loss is 0.30456119775772095\n",
      "epoch: 4 step: 763, loss is 0.3530621826648712\n",
      "epoch: 4 step: 764, loss is 0.29247093200683594\n",
      "epoch: 4 step: 765, loss is 0.263708233833313\n",
      "epoch: 4 step: 766, loss is 0.4672030210494995\n",
      "epoch: 4 step: 767, loss is 0.29039350152015686\n",
      "epoch: 4 step: 768, loss is 0.2759960889816284\n",
      "epoch: 4 step: 769, loss is 0.4141691327095032\n",
      "epoch: 4 step: 770, loss is 0.3281814754009247\n",
      "epoch: 4 step: 771, loss is 0.4797869324684143\n",
      "epoch: 4 step: 772, loss is 0.4299383759498596\n",
      "epoch: 4 step: 773, loss is 0.5442229509353638\n",
      "epoch: 4 step: 774, loss is 0.3739790618419647\n",
      "epoch: 4 step: 775, loss is 0.2704165279865265\n",
      "epoch: 4 step: 776, loss is 0.3483004868030548\n",
      "epoch: 4 step: 777, loss is 0.4332629144191742\n",
      "epoch: 4 step: 778, loss is 0.42037132382392883\n",
      "epoch: 4 step: 779, loss is 0.22838155925273895\n",
      "epoch: 4 step: 780, loss is 0.2135452777147293\n",
      "epoch: 4 step: 781, loss is 0.3557846248149872\n",
      "epoch: 4 step: 782, loss is 0.33877497911453247\n",
      "epoch: 4 step: 783, loss is 0.2823573350906372\n",
      "epoch: 4 step: 784, loss is 0.31163719296455383\n",
      "epoch: 4 step: 785, loss is 0.329628050327301\n",
      "epoch: 4 step: 786, loss is 0.38573431968688965\n",
      "epoch: 4 step: 787, loss is 0.3313532769680023\n",
      "epoch: 4 step: 788, loss is 0.33903586864471436\n",
      "epoch: 4 step: 789, loss is 0.2183484435081482\n",
      "epoch: 4 step: 790, loss is 0.26878827810287476\n",
      "epoch: 4 step: 791, loss is 0.44739171862602234\n",
      "epoch: 4 step: 792, loss is 0.2825860381126404\n",
      "epoch: 4 step: 793, loss is 0.28071632981300354\n",
      "epoch: 4 step: 794, loss is 0.2594921886920929\n",
      "epoch: 4 step: 795, loss is 0.2794242799282074\n",
      "epoch: 4 step: 796, loss is 0.2854972183704376\n",
      "epoch: 4 step: 797, loss is 0.2709428668022156\n",
      "epoch: 4 step: 798, loss is 0.28444910049438477\n",
      "epoch: 4 step: 799, loss is 0.3592850863933563\n",
      "epoch: 4 step: 800, loss is 0.26703473925590515\n",
      "epoch: 4 step: 801, loss is 0.372780978679657\n",
      "epoch: 4 step: 802, loss is 0.365286260843277\n",
      "epoch: 4 step: 803, loss is 0.32365190982818604\n",
      "epoch: 4 step: 804, loss is 0.2934280037879944\n",
      "epoch: 4 step: 805, loss is 0.27895376086235046\n",
      "epoch: 4 step: 806, loss is 0.3415944278240204\n",
      "epoch: 4 step: 807, loss is 0.38602420687675476\n",
      "epoch: 4 step: 808, loss is 0.3454621732234955\n",
      "epoch: 4 step: 809, loss is 0.5619338750839233\n",
      "epoch: 4 step: 810, loss is 0.27011024951934814\n",
      "epoch: 4 step: 811, loss is 0.4552651643753052\n",
      "epoch: 4 step: 812, loss is 0.41063305735588074\n",
      "epoch: 4 step: 813, loss is 0.31426137685775757\n",
      "epoch: 4 step: 814, loss is 0.33895522356033325\n",
      "epoch: 4 step: 815, loss is 0.2656794488430023\n",
      "epoch: 4 step: 816, loss is 0.2861713469028473\n",
      "epoch: 4 step: 817, loss is 0.18964731693267822\n",
      "epoch: 4 step: 818, loss is 0.5270041823387146\n",
      "epoch: 4 step: 819, loss is 0.2884882390499115\n",
      "epoch: 4 step: 820, loss is 0.29080259799957275\n",
      "epoch: 4 step: 821, loss is 0.3019053339958191\n",
      "epoch: 4 step: 822, loss is 0.41323545575141907\n",
      "epoch: 4 step: 823, loss is 0.25025293231010437\n",
      "epoch: 4 step: 824, loss is 0.3100976347923279\n",
      "epoch: 4 step: 825, loss is 0.3071977198123932\n",
      "epoch: 4 step: 826, loss is 0.215684711933136\n",
      "epoch: 4 step: 827, loss is 0.3202858567237854\n",
      "epoch: 4 step: 828, loss is 0.20733748376369476\n",
      "epoch: 4 step: 829, loss is 0.31046196818351746\n",
      "epoch: 4 step: 830, loss is 0.28176599740982056\n",
      "epoch: 4 step: 831, loss is 0.38269367814064026\n",
      "epoch: 4 step: 832, loss is 0.23303785920143127\n",
      "epoch: 4 step: 833, loss is 0.28490933775901794\n",
      "epoch: 4 step: 834, loss is 0.30717119574546814\n",
      "epoch: 4 step: 835, loss is 0.4655231535434723\n",
      "epoch: 4 step: 836, loss is 0.4098872244358063\n",
      "epoch: 4 step: 837, loss is 0.17485150694847107\n",
      "epoch: 4 step: 838, loss is 0.4029361307621002\n",
      "epoch: 4 step: 839, loss is 0.5297475457191467\n",
      "epoch: 4 step: 840, loss is 0.3294122517108917\n",
      "epoch: 4 step: 841, loss is 0.38987359404563904\n",
      "epoch: 4 step: 842, loss is 0.3325495421886444\n",
      "epoch: 4 step: 843, loss is 0.2009095400571823\n",
      "epoch: 4 step: 844, loss is 0.32562434673309326\n",
      "epoch: 4 step: 845, loss is 0.37842023372650146\n",
      "epoch: 4 step: 846, loss is 0.21995651721954346\n",
      "epoch: 4 step: 847, loss is 0.33391398191452026\n",
      "epoch: 4 step: 848, loss is 0.27592942118644714\n",
      "epoch: 4 step: 849, loss is 0.2843931019306183\n",
      "epoch: 4 step: 850, loss is 0.39302101731300354\n",
      "epoch: 4 step: 851, loss is 0.24562425911426544\n",
      "epoch: 4 step: 852, loss is 0.3086494207382202\n",
      "epoch: 4 step: 853, loss is 0.27432554960250854\n",
      "epoch: 4 step: 854, loss is 0.19148842990398407\n",
      "epoch: 4 step: 855, loss is 0.31834137439727783\n",
      "epoch: 4 step: 856, loss is 0.3098047971725464\n",
      "epoch: 4 step: 857, loss is 0.36495402455329895\n",
      "epoch: 4 step: 858, loss is 0.3865291178226471\n",
      "epoch: 4 step: 859, loss is 0.2582882046699524\n",
      "epoch: 4 step: 860, loss is 0.49875929951667786\n",
      "epoch: 4 step: 861, loss is 0.17269305884838104\n",
      "epoch: 4 step: 862, loss is 0.13022536039352417\n",
      "epoch: 4 step: 863, loss is 0.13721342384815216\n",
      "epoch: 4 step: 864, loss is 0.15449360013008118\n",
      "epoch: 4 step: 865, loss is 0.38640114665031433\n",
      "epoch: 4 step: 866, loss is 0.29907935857772827\n",
      "epoch: 4 step: 867, loss is 0.28721487522125244\n",
      "epoch: 4 step: 868, loss is 0.3866707384586334\n",
      "epoch: 4 step: 869, loss is 0.42697131633758545\n",
      "epoch: 4 step: 870, loss is 0.38050633668899536\n",
      "epoch: 4 step: 871, loss is 0.20516686141490936\n",
      "epoch: 4 step: 872, loss is 0.2615915536880493\n",
      "epoch: 4 step: 873, loss is 0.21538035571575165\n",
      "epoch: 4 step: 874, loss is 0.24469658732414246\n",
      "epoch: 4 step: 875, loss is 0.30456292629241943\n",
      "epoch: 4 step: 876, loss is 0.4275515079498291\n",
      "epoch: 4 step: 877, loss is 0.2192976027727127\n",
      "epoch: 4 step: 878, loss is 0.3478187918663025\n",
      "epoch: 4 step: 879, loss is 0.2901376783847809\n",
      "epoch: 4 step: 880, loss is 0.2811281979084015\n",
      "epoch: 4 step: 881, loss is 0.22154515981674194\n",
      "epoch: 4 step: 882, loss is 0.41265931725502014\n",
      "epoch: 4 step: 883, loss is 0.3121308982372284\n",
      "epoch: 4 step: 884, loss is 0.4710099399089813\n",
      "epoch: 4 step: 885, loss is 0.22563625872135162\n",
      "epoch: 4 step: 886, loss is 0.22849693894386292\n",
      "epoch: 4 step: 887, loss is 0.4194612503051758\n",
      "epoch: 4 step: 888, loss is 0.3031993508338928\n",
      "epoch: 4 step: 889, loss is 0.1651761680841446\n",
      "epoch: 4 step: 890, loss is 0.36755090951919556\n",
      "epoch: 4 step: 891, loss is 0.2863110899925232\n",
      "epoch: 4 step: 892, loss is 0.3367616534233093\n",
      "epoch: 4 step: 893, loss is 0.37856489419937134\n",
      "epoch: 4 step: 894, loss is 0.48342573642730713\n",
      "epoch: 4 step: 895, loss is 0.41916871070861816\n",
      "epoch: 4 step: 896, loss is 0.18732823431491852\n",
      "epoch: 4 step: 897, loss is 0.28706109523773193\n",
      "epoch: 4 step: 898, loss is 0.25849005579948425\n",
      "epoch: 4 step: 899, loss is 0.39660537242889404\n",
      "epoch: 4 step: 900, loss is 0.23447167873382568\n",
      "epoch: 4 step: 901, loss is 0.2937277853488922\n",
      "epoch: 4 step: 902, loss is 0.23170065879821777\n",
      "epoch: 4 step: 903, loss is 0.2607342004776001\n",
      "epoch: 4 step: 904, loss is 0.5338947772979736\n",
      "epoch: 4 step: 905, loss is 0.380802720785141\n",
      "epoch: 4 step: 906, loss is 0.2178294062614441\n",
      "epoch: 4 step: 907, loss is 0.1905568391084671\n",
      "epoch: 4 step: 908, loss is 0.3628973662853241\n",
      "epoch: 4 step: 909, loss is 0.17301414906978607\n",
      "epoch: 4 step: 910, loss is 0.28679609298706055\n",
      "epoch: 4 step: 911, loss is 0.3668542504310608\n",
      "epoch: 4 step: 912, loss is 0.21799856424331665\n",
      "epoch: 4 step: 913, loss is 0.16667206585407257\n",
      "epoch: 4 step: 914, loss is 0.6178343892097473\n",
      "epoch: 4 step: 915, loss is 0.3953423798084259\n",
      "epoch: 4 step: 916, loss is 0.5819419026374817\n",
      "epoch: 4 step: 917, loss is 0.4186575710773468\n",
      "epoch: 4 step: 918, loss is 0.3507402837276459\n",
      "epoch: 4 step: 919, loss is 0.2977183163166046\n",
      "epoch: 4 step: 920, loss is 0.32689157128334045\n",
      "epoch: 4 step: 921, loss is 0.2475728839635849\n",
      "epoch: 4 step: 922, loss is 0.2905660569667816\n",
      "epoch: 4 step: 923, loss is 0.2913861572742462\n",
      "epoch: 4 step: 924, loss is 0.32771703600883484\n",
      "epoch: 4 step: 925, loss is 0.21950797736644745\n",
      "epoch: 4 step: 926, loss is 0.3592967092990875\n",
      "epoch: 4 step: 927, loss is 0.4710034430027008\n",
      "epoch: 4 step: 928, loss is 0.3343665599822998\n",
      "epoch: 4 step: 929, loss is 0.3399662375450134\n",
      "epoch: 4 step: 930, loss is 0.2566963732242584\n",
      "epoch: 4 step: 931, loss is 0.41322699189186096\n",
      "epoch: 4 step: 932, loss is 0.403409868478775\n",
      "epoch: 4 step: 933, loss is 0.574065089225769\n",
      "epoch: 4 step: 934, loss is 0.36087530851364136\n",
      "epoch: 4 step: 935, loss is 0.14551344513893127\n",
      "epoch: 4 step: 936, loss is 0.2165839523077011\n",
      "epoch: 4 step: 937, loss is 0.314796507358551\n",
      "epoch: 5 step: 1, loss is 0.37010830640792847\n",
      "epoch: 5 step: 2, loss is 0.48573198914527893\n",
      "epoch: 5 step: 3, loss is 0.44415903091430664\n",
      "epoch: 5 step: 4, loss is 0.20108966529369354\n",
      "epoch: 5 step: 5, loss is 0.46439236402511597\n",
      "epoch: 5 step: 6, loss is 0.2381952852010727\n",
      "epoch: 5 step: 7, loss is 0.3220333456993103\n",
      "epoch: 5 step: 8, loss is 0.13504019379615784\n",
      "epoch: 5 step: 9, loss is 0.459158718585968\n",
      "epoch: 5 step: 10, loss is 0.29781174659729004\n",
      "epoch: 5 step: 11, loss is 0.18344365060329437\n",
      "epoch: 5 step: 12, loss is 0.33417239785194397\n",
      "epoch: 5 step: 13, loss is 0.5193652510643005\n",
      "epoch: 5 step: 14, loss is 0.4419231116771698\n",
      "epoch: 5 step: 15, loss is 0.29026854038238525\n",
      "epoch: 5 step: 16, loss is 0.21255967020988464\n",
      "epoch: 5 step: 17, loss is 0.29134538769721985\n",
      "epoch: 5 step: 18, loss is 0.22324983775615692\n",
      "epoch: 5 step: 19, loss is 0.3356446325778961\n",
      "epoch: 5 step: 20, loss is 0.2881617546081543\n",
      "epoch: 5 step: 21, loss is 0.3313436210155487\n",
      "epoch: 5 step: 22, loss is 0.5013306736946106\n",
      "epoch: 5 step: 23, loss is 0.3132217824459076\n",
      "epoch: 5 step: 24, loss is 0.38408800959587097\n",
      "epoch: 5 step: 25, loss is 0.2599445879459381\n",
      "epoch: 5 step: 26, loss is 0.28719669580459595\n",
      "epoch: 5 step: 27, loss is 0.4398149251937866\n",
      "epoch: 5 step: 28, loss is 0.2851197421550751\n",
      "epoch: 5 step: 29, loss is 0.16728954017162323\n",
      "epoch: 5 step: 30, loss is 0.2854502201080322\n",
      "epoch: 5 step: 31, loss is 0.4087640643119812\n",
      "epoch: 5 step: 32, loss is 0.2650517523288727\n",
      "epoch: 5 step: 33, loss is 0.35580411553382874\n",
      "epoch: 5 step: 34, loss is 0.2970423996448517\n",
      "epoch: 5 step: 35, loss is 0.44795891642570496\n",
      "epoch: 5 step: 36, loss is 0.35534533858299255\n",
      "epoch: 5 step: 37, loss is 0.39111000299453735\n",
      "epoch: 5 step: 38, loss is 0.30493345856666565\n",
      "epoch: 5 step: 39, loss is 0.24185378849506378\n",
      "epoch: 5 step: 40, loss is 0.2696264684200287\n",
      "epoch: 5 step: 41, loss is 0.5308036208152771\n",
      "epoch: 5 step: 42, loss is 0.3592003285884857\n",
      "epoch: 5 step: 43, loss is 0.38829028606414795\n",
      "epoch: 5 step: 44, loss is 0.15020066499710083\n",
      "epoch: 5 step: 45, loss is 0.22205443680286407\n",
      "epoch: 5 step: 46, loss is 0.16135089099407196\n",
      "epoch: 5 step: 47, loss is 0.2969629466533661\n",
      "epoch: 5 step: 48, loss is 0.48496147990226746\n",
      "epoch: 5 step: 49, loss is 0.29286491870880127\n",
      "epoch: 5 step: 50, loss is 0.21691562235355377\n",
      "epoch: 5 step: 51, loss is 0.26952695846557617\n",
      "epoch: 5 step: 52, loss is 0.4305707514286041\n",
      "epoch: 5 step: 53, loss is 0.4351710081100464\n",
      "epoch: 5 step: 54, loss is 0.25174427032470703\n",
      "epoch: 5 step: 55, loss is 0.5691410303115845\n",
      "epoch: 5 step: 56, loss is 0.25282666087150574\n",
      "epoch: 5 step: 57, loss is 0.5687590837478638\n",
      "epoch: 5 step: 58, loss is 0.29489588737487793\n",
      "epoch: 5 step: 59, loss is 0.3338099718093872\n",
      "epoch: 5 step: 60, loss is 0.3265005350112915\n",
      "epoch: 5 step: 61, loss is 0.32238876819610596\n",
      "epoch: 5 step: 62, loss is 0.34827229380607605\n",
      "epoch: 5 step: 63, loss is 0.33990713953971863\n",
      "epoch: 5 step: 64, loss is 0.41637441515922546\n",
      "epoch: 5 step: 65, loss is 0.36701610684394836\n",
      "epoch: 5 step: 66, loss is 0.25010162591934204\n",
      "epoch: 5 step: 67, loss is 0.1431456208229065\n",
      "epoch: 5 step: 68, loss is 0.37463584542274475\n",
      "epoch: 5 step: 69, loss is 0.42884331941604614\n",
      "epoch: 5 step: 70, loss is 0.5940133333206177\n",
      "epoch: 5 step: 71, loss is 0.3263949751853943\n",
      "epoch: 5 step: 72, loss is 0.1628529131412506\n",
      "epoch: 5 step: 73, loss is 0.2776172161102295\n",
      "epoch: 5 step: 74, loss is 0.41341736912727356\n",
      "epoch: 5 step: 75, loss is 0.2703474462032318\n",
      "epoch: 5 step: 76, loss is 0.2806188464164734\n",
      "epoch: 5 step: 77, loss is 0.2518446743488312\n",
      "epoch: 5 step: 78, loss is 0.44286665320396423\n",
      "epoch: 5 step: 79, loss is 0.4683746099472046\n",
      "epoch: 5 step: 80, loss is 0.42214253544807434\n",
      "epoch: 5 step: 81, loss is 0.2951147258281708\n",
      "epoch: 5 step: 82, loss is 0.2795883119106293\n",
      "epoch: 5 step: 83, loss is 0.4181555509567261\n",
      "epoch: 5 step: 84, loss is 0.13521835207939148\n",
      "epoch: 5 step: 85, loss is 0.32407906651496887\n",
      "epoch: 5 step: 86, loss is 0.3521367907524109\n",
      "epoch: 5 step: 87, loss is 0.2222064584493637\n",
      "epoch: 5 step: 88, loss is 0.4303971230983734\n",
      "epoch: 5 step: 89, loss is 0.24565501511096954\n",
      "epoch: 5 step: 90, loss is 0.2767145037651062\n",
      "epoch: 5 step: 91, loss is 0.3579404056072235\n",
      "epoch: 5 step: 92, loss is 0.2533307671546936\n",
      "epoch: 5 step: 93, loss is 0.30682849884033203\n",
      "epoch: 5 step: 94, loss is 0.3994969427585602\n",
      "epoch: 5 step: 95, loss is 0.5164432525634766\n",
      "epoch: 5 step: 96, loss is 0.2687399685382843\n",
      "epoch: 5 step: 97, loss is 0.17517417669296265\n",
      "epoch: 5 step: 98, loss is 0.22907154262065887\n",
      "epoch: 5 step: 99, loss is 0.23202486336231232\n",
      "epoch: 5 step: 100, loss is 0.2816935181617737\n",
      "epoch: 5 step: 101, loss is 0.45145878195762634\n",
      "epoch: 5 step: 102, loss is 0.2735271453857422\n",
      "epoch: 5 step: 103, loss is 0.2873912453651428\n",
      "epoch: 5 step: 104, loss is 0.27324846386909485\n",
      "epoch: 5 step: 105, loss is 0.2856508195400238\n",
      "epoch: 5 step: 106, loss is 0.5793451070785522\n",
      "epoch: 5 step: 107, loss is 0.2732575237751007\n",
      "epoch: 5 step: 108, loss is 0.2680512070655823\n",
      "epoch: 5 step: 109, loss is 0.27925756573677063\n",
      "epoch: 5 step: 110, loss is 0.2084859013557434\n",
      "epoch: 5 step: 111, loss is 0.1633402556180954\n",
      "epoch: 5 step: 112, loss is 0.36139145493507385\n",
      "epoch: 5 step: 113, loss is 0.31481003761291504\n",
      "epoch: 5 step: 114, loss is 0.21497665345668793\n",
      "epoch: 5 step: 115, loss is 0.18047547340393066\n",
      "epoch: 5 step: 116, loss is 0.1961085945367813\n",
      "epoch: 5 step: 117, loss is 0.3359743356704712\n",
      "epoch: 5 step: 118, loss is 0.2880130410194397\n",
      "epoch: 5 step: 119, loss is 0.3401111662387848\n",
      "epoch: 5 step: 120, loss is 0.4618513584136963\n",
      "epoch: 5 step: 121, loss is 0.3199003338813782\n",
      "epoch: 5 step: 122, loss is 0.23008427023887634\n",
      "epoch: 5 step: 123, loss is 0.15852200984954834\n",
      "epoch: 5 step: 124, loss is 0.3040699064731598\n",
      "epoch: 5 step: 125, loss is 0.3060469329357147\n",
      "epoch: 5 step: 126, loss is 0.2923385798931122\n",
      "epoch: 5 step: 127, loss is 0.2912746071815491\n",
      "epoch: 5 step: 128, loss is 0.4801749289035797\n",
      "epoch: 5 step: 129, loss is 0.40939801931381226\n",
      "epoch: 5 step: 130, loss is 0.3274727463722229\n",
      "epoch: 5 step: 131, loss is 0.31025972962379456\n",
      "epoch: 5 step: 132, loss is 0.15120834112167358\n",
      "epoch: 5 step: 133, loss is 0.18359528481960297\n",
      "epoch: 5 step: 134, loss is 0.33438119292259216\n",
      "epoch: 5 step: 135, loss is 0.29639238119125366\n",
      "epoch: 5 step: 136, loss is 0.44421643018722534\n",
      "epoch: 5 step: 137, loss is 0.2802307605743408\n",
      "epoch: 5 step: 138, loss is 0.43310490250587463\n",
      "epoch: 5 step: 139, loss is 0.332248330116272\n",
      "epoch: 5 step: 140, loss is 0.17440715432167053\n",
      "epoch: 5 step: 141, loss is 0.218986377120018\n",
      "epoch: 5 step: 142, loss is 0.13252122700214386\n",
      "epoch: 5 step: 143, loss is 0.2929593622684479\n",
      "epoch: 5 step: 144, loss is 0.2230350375175476\n",
      "epoch: 5 step: 145, loss is 0.2662069797515869\n",
      "epoch: 5 step: 146, loss is 0.4132952094078064\n",
      "epoch: 5 step: 147, loss is 0.3280383050441742\n",
      "epoch: 5 step: 148, loss is 0.3205663561820984\n",
      "epoch: 5 step: 149, loss is 0.19869685173034668\n",
      "epoch: 5 step: 150, loss is 0.2546534240245819\n",
      "epoch: 5 step: 151, loss is 0.2876526415348053\n",
      "epoch: 5 step: 152, loss is 0.30333834886550903\n",
      "epoch: 5 step: 153, loss is 0.36629718542099\n",
      "epoch: 5 step: 154, loss is 0.20525473356246948\n",
      "epoch: 5 step: 155, loss is 0.2410743534564972\n",
      "epoch: 5 step: 156, loss is 0.23901903629302979\n",
      "epoch: 5 step: 157, loss is 0.25384098291397095\n",
      "epoch: 5 step: 158, loss is 0.30244508385658264\n",
      "epoch: 5 step: 159, loss is 0.4919207692146301\n",
      "epoch: 5 step: 160, loss is 0.47859734296798706\n",
      "epoch: 5 step: 161, loss is 0.275390088558197\n",
      "epoch: 5 step: 162, loss is 0.39914050698280334\n",
      "epoch: 5 step: 163, loss is 0.3689161241054535\n",
      "epoch: 5 step: 164, loss is 0.28574445843696594\n",
      "epoch: 5 step: 165, loss is 0.22022582590579987\n",
      "epoch: 5 step: 166, loss is 0.3060061037540436\n",
      "epoch: 5 step: 167, loss is 0.3513672649860382\n",
      "epoch: 5 step: 168, loss is 0.319448858499527\n",
      "epoch: 5 step: 169, loss is 0.3899720013141632\n",
      "epoch: 5 step: 170, loss is 0.3411014676094055\n",
      "epoch: 5 step: 171, loss is 0.587819516658783\n",
      "epoch: 5 step: 172, loss is 0.3130671977996826\n",
      "epoch: 5 step: 173, loss is 0.3725203275680542\n",
      "epoch: 5 step: 174, loss is 0.48630717396736145\n",
      "epoch: 5 step: 175, loss is 0.3097061216831207\n",
      "epoch: 5 step: 176, loss is 0.34109511971473694\n",
      "epoch: 5 step: 177, loss is 0.4509739279747009\n",
      "epoch: 5 step: 178, loss is 0.23737618327140808\n",
      "epoch: 5 step: 179, loss is 0.25735366344451904\n",
      "epoch: 5 step: 180, loss is 0.27016687393188477\n",
      "epoch: 5 step: 181, loss is 0.33056825399398804\n",
      "epoch: 5 step: 182, loss is 0.35646915435791016\n",
      "epoch: 5 step: 183, loss is 0.15254093706607819\n",
      "epoch: 5 step: 184, loss is 0.27193719148635864\n",
      "epoch: 5 step: 185, loss is 0.17397795617580414\n",
      "epoch: 5 step: 186, loss is 0.3844176232814789\n",
      "epoch: 5 step: 187, loss is 0.31622764468193054\n",
      "epoch: 5 step: 188, loss is 0.3451937735080719\n",
      "epoch: 5 step: 189, loss is 0.396155446767807\n",
      "epoch: 5 step: 190, loss is 0.28333842754364014\n",
      "epoch: 5 step: 191, loss is 0.2698737382888794\n",
      "epoch: 5 step: 192, loss is 0.30766984820365906\n",
      "epoch: 5 step: 193, loss is 0.3180261552333832\n",
      "epoch: 5 step: 194, loss is 0.42164289951324463\n",
      "epoch: 5 step: 195, loss is 0.32990700006484985\n",
      "epoch: 5 step: 196, loss is 0.40720334649086\n",
      "epoch: 5 step: 197, loss is 0.2710435688495636\n",
      "epoch: 5 step: 198, loss is 0.2506103813648224\n",
      "epoch: 5 step: 199, loss is 0.24494785070419312\n",
      "epoch: 5 step: 200, loss is 0.23829162120819092\n",
      "epoch: 5 step: 201, loss is 0.34004583954811096\n",
      "epoch: 5 step: 202, loss is 0.23805104196071625\n",
      "epoch: 5 step: 203, loss is 0.29494935274124146\n",
      "epoch: 5 step: 204, loss is 0.25451597571372986\n",
      "epoch: 5 step: 205, loss is 0.30393487215042114\n",
      "epoch: 5 step: 206, loss is 0.3221369981765747\n",
      "epoch: 5 step: 207, loss is 0.3968903422355652\n",
      "epoch: 5 step: 208, loss is 0.169183611869812\n",
      "epoch: 5 step: 209, loss is 0.30302339792251587\n",
      "epoch: 5 step: 210, loss is 0.4352080821990967\n",
      "epoch: 5 step: 211, loss is 0.28079259395599365\n",
      "epoch: 5 step: 212, loss is 0.4137720465660095\n",
      "epoch: 5 step: 213, loss is 0.2339361608028412\n",
      "epoch: 5 step: 214, loss is 0.2627517879009247\n",
      "epoch: 5 step: 215, loss is 0.3832593560218811\n",
      "epoch: 5 step: 216, loss is 0.2796652913093567\n",
      "epoch: 5 step: 217, loss is 0.36478540301322937\n",
      "epoch: 5 step: 218, loss is 0.24284471571445465\n",
      "epoch: 5 step: 219, loss is 0.20990902185440063\n",
      "epoch: 5 step: 220, loss is 0.36337584257125854\n",
      "epoch: 5 step: 221, loss is 0.4191368520259857\n",
      "epoch: 5 step: 222, loss is 0.43870607018470764\n",
      "epoch: 5 step: 223, loss is 0.1882234513759613\n",
      "epoch: 5 step: 224, loss is 0.2843905985355377\n",
      "epoch: 5 step: 225, loss is 0.357756108045578\n",
      "epoch: 5 step: 226, loss is 0.3225627839565277\n",
      "epoch: 5 step: 227, loss is 0.2911090552806854\n",
      "epoch: 5 step: 228, loss is 0.1758640557527542\n",
      "epoch: 5 step: 229, loss is 0.2762020528316498\n",
      "epoch: 5 step: 230, loss is 0.2731999456882477\n",
      "epoch: 5 step: 231, loss is 0.402912437915802\n",
      "epoch: 5 step: 232, loss is 0.27261659502983093\n",
      "epoch: 5 step: 233, loss is 0.33831319212913513\n",
      "epoch: 5 step: 234, loss is 0.3329179584980011\n",
      "epoch: 5 step: 235, loss is 0.2834402024745941\n",
      "epoch: 5 step: 236, loss is 0.3794803023338318\n",
      "epoch: 5 step: 237, loss is 0.428196519613266\n",
      "epoch: 5 step: 238, loss is 0.2915193438529968\n",
      "epoch: 5 step: 239, loss is 0.38191306591033936\n",
      "epoch: 5 step: 240, loss is 0.31654804944992065\n",
      "epoch: 5 step: 241, loss is 0.4910067021846771\n",
      "epoch: 5 step: 242, loss is 0.4317888021469116\n",
      "epoch: 5 step: 243, loss is 0.43742573261260986\n",
      "epoch: 5 step: 244, loss is 0.27310606837272644\n",
      "epoch: 5 step: 245, loss is 0.28537678718566895\n",
      "epoch: 5 step: 246, loss is 0.4368349313735962\n",
      "epoch: 5 step: 247, loss is 0.4060875475406647\n",
      "epoch: 5 step: 248, loss is 0.35617387294769287\n",
      "epoch: 5 step: 249, loss is 0.3458431363105774\n",
      "epoch: 5 step: 250, loss is 0.26737111806869507\n",
      "epoch: 5 step: 251, loss is 0.26935267448425293\n",
      "epoch: 5 step: 252, loss is 0.2622431814670563\n",
      "epoch: 5 step: 253, loss is 0.22004163265228271\n",
      "epoch: 5 step: 254, loss is 0.17819300293922424\n",
      "epoch: 5 step: 255, loss is 0.31154754757881165\n",
      "epoch: 5 step: 256, loss is 0.32858359813690186\n",
      "epoch: 5 step: 257, loss is 0.4125254154205322\n",
      "epoch: 5 step: 258, loss is 0.3325265645980835\n",
      "epoch: 5 step: 259, loss is 0.25204867124557495\n",
      "epoch: 5 step: 260, loss is 0.2746773064136505\n",
      "epoch: 5 step: 261, loss is 0.2231379598379135\n",
      "epoch: 5 step: 262, loss is 0.26344355940818787\n",
      "epoch: 5 step: 263, loss is 0.3334231972694397\n",
      "epoch: 5 step: 264, loss is 0.43501225113868713\n",
      "epoch: 5 step: 265, loss is 0.2519463300704956\n",
      "epoch: 5 step: 266, loss is 0.39233508706092834\n",
      "epoch: 5 step: 267, loss is 0.20412695407867432\n",
      "epoch: 5 step: 268, loss is 0.29790109395980835\n",
      "epoch: 5 step: 269, loss is 0.5405689477920532\n",
      "epoch: 5 step: 270, loss is 0.3894237279891968\n",
      "epoch: 5 step: 271, loss is 0.24032758176326752\n",
      "epoch: 5 step: 272, loss is 0.49597153067588806\n",
      "epoch: 5 step: 273, loss is 0.43362462520599365\n",
      "epoch: 5 step: 274, loss is 0.30691689252853394\n",
      "epoch: 5 step: 275, loss is 0.3658391237258911\n",
      "epoch: 5 step: 276, loss is 0.35369691252708435\n",
      "epoch: 5 step: 277, loss is 0.2995447814464569\n",
      "epoch: 5 step: 278, loss is 0.3185677230358124\n",
      "epoch: 5 step: 279, loss is 0.25793537497520447\n",
      "epoch: 5 step: 280, loss is 0.31850430369377136\n",
      "epoch: 5 step: 281, loss is 0.5138346552848816\n",
      "epoch: 5 step: 282, loss is 0.2908194363117218\n",
      "epoch: 5 step: 283, loss is 0.1845356523990631\n",
      "epoch: 5 step: 284, loss is 0.12273770570755005\n",
      "epoch: 5 step: 285, loss is 0.3415149450302124\n",
      "epoch: 5 step: 286, loss is 0.2242150902748108\n",
      "epoch: 5 step: 287, loss is 0.2813381254673004\n",
      "epoch: 5 step: 288, loss is 0.18954013288021088\n",
      "epoch: 5 step: 289, loss is 0.30422094464302063\n",
      "epoch: 5 step: 290, loss is 0.25427210330963135\n",
      "epoch: 5 step: 291, loss is 0.35918495059013367\n",
      "epoch: 5 step: 292, loss is 0.48173487186431885\n",
      "epoch: 5 step: 293, loss is 0.4053401052951813\n",
      "epoch: 5 step: 294, loss is 0.3754051625728607\n",
      "epoch: 5 step: 295, loss is 0.2679186165332794\n",
      "epoch: 5 step: 296, loss is 0.2392977923154831\n",
      "epoch: 5 step: 297, loss is 0.40167662501335144\n",
      "epoch: 5 step: 298, loss is 0.23980259895324707\n",
      "epoch: 5 step: 299, loss is 0.2752179205417633\n",
      "epoch: 5 step: 300, loss is 0.3136780560016632\n",
      "epoch: 5 step: 301, loss is 0.2614007592201233\n",
      "epoch: 5 step: 302, loss is 0.2964363992214203\n",
      "epoch: 5 step: 303, loss is 0.2097260057926178\n",
      "epoch: 5 step: 304, loss is 0.3320003151893616\n",
      "epoch: 5 step: 305, loss is 0.5413870811462402\n",
      "epoch: 5 step: 306, loss is 0.16227811574935913\n",
      "epoch: 5 step: 307, loss is 0.2059718668460846\n",
      "epoch: 5 step: 308, loss is 0.2674929201602936\n",
      "epoch: 5 step: 309, loss is 0.2657339572906494\n",
      "epoch: 5 step: 310, loss is 0.48635730147361755\n",
      "epoch: 5 step: 311, loss is 0.42089784145355225\n",
      "epoch: 5 step: 312, loss is 0.2569064497947693\n",
      "epoch: 5 step: 313, loss is 0.2717050015926361\n",
      "epoch: 5 step: 314, loss is 0.2981172800064087\n",
      "epoch: 5 step: 315, loss is 0.29663339257240295\n",
      "epoch: 5 step: 316, loss is 0.38038212060928345\n",
      "epoch: 5 step: 317, loss is 0.3993018865585327\n",
      "epoch: 5 step: 318, loss is 0.30120953917503357\n",
      "epoch: 5 step: 319, loss is 0.3840593993663788\n",
      "epoch: 5 step: 320, loss is 0.3712228536605835\n",
      "epoch: 5 step: 321, loss is 0.2966245114803314\n",
      "epoch: 5 step: 322, loss is 0.3767588138580322\n",
      "epoch: 5 step: 323, loss is 0.2739574909210205\n",
      "epoch: 5 step: 324, loss is 0.3470104932785034\n",
      "epoch: 5 step: 325, loss is 0.21061193943023682\n",
      "epoch: 5 step: 326, loss is 0.28138628602027893\n",
      "epoch: 5 step: 327, loss is 0.169412761926651\n",
      "epoch: 5 step: 328, loss is 0.3731553852558136\n",
      "epoch: 5 step: 329, loss is 0.36570844054222107\n",
      "epoch: 5 step: 330, loss is 0.4082679748535156\n",
      "epoch: 5 step: 331, loss is 0.38488033413887024\n",
      "epoch: 5 step: 332, loss is 0.24013693630695343\n",
      "epoch: 5 step: 333, loss is 0.293578565120697\n",
      "epoch: 5 step: 334, loss is 0.3183320462703705\n",
      "epoch: 5 step: 335, loss is 0.424625426530838\n",
      "epoch: 5 step: 336, loss is 0.3383527398109436\n",
      "epoch: 5 step: 337, loss is 0.36024531722068787\n",
      "epoch: 5 step: 338, loss is 0.27352261543273926\n",
      "epoch: 5 step: 339, loss is 0.4143466353416443\n",
      "epoch: 5 step: 340, loss is 0.30349382758140564\n",
      "epoch: 5 step: 341, loss is 0.1927911788225174\n",
      "epoch: 5 step: 342, loss is 0.3300868272781372\n",
      "epoch: 5 step: 343, loss is 0.2016720473766327\n",
      "epoch: 5 step: 344, loss is 0.22224204242229462\n",
      "epoch: 5 step: 345, loss is 0.5150730013847351\n",
      "epoch: 5 step: 346, loss is 0.19130824506282806\n",
      "epoch: 5 step: 347, loss is 0.2652459144592285\n",
      "epoch: 5 step: 348, loss is 0.3253747522830963\n",
      "epoch: 5 step: 349, loss is 0.4654698669910431\n",
      "epoch: 5 step: 350, loss is 0.3535444140434265\n",
      "epoch: 5 step: 351, loss is 0.3383514881134033\n",
      "epoch: 5 step: 352, loss is 0.3770306706428528\n",
      "epoch: 5 step: 353, loss is 0.2627074718475342\n",
      "epoch: 5 step: 354, loss is 0.382309228181839\n",
      "epoch: 5 step: 355, loss is 0.3549901247024536\n",
      "epoch: 5 step: 356, loss is 0.42970597743988037\n",
      "epoch: 5 step: 357, loss is 0.35221412777900696\n",
      "epoch: 5 step: 358, loss is 0.33696094155311584\n",
      "epoch: 5 step: 359, loss is 0.26769164204597473\n",
      "epoch: 5 step: 360, loss is 0.34935176372528076\n",
      "epoch: 5 step: 361, loss is 0.4489097595214844\n",
      "epoch: 5 step: 362, loss is 0.2602863311767578\n",
      "epoch: 5 step: 363, loss is 0.23734651505947113\n",
      "epoch: 5 step: 364, loss is 0.2805938720703125\n",
      "epoch: 5 step: 365, loss is 0.4461022615432739\n",
      "epoch: 5 step: 366, loss is 0.34290310740470886\n",
      "epoch: 5 step: 367, loss is 0.18981848657131195\n",
      "epoch: 5 step: 368, loss is 0.30633416771888733\n",
      "epoch: 5 step: 369, loss is 0.3566787540912628\n",
      "epoch: 5 step: 370, loss is 0.2610437273979187\n",
      "epoch: 5 step: 371, loss is 0.36753973364830017\n",
      "epoch: 5 step: 372, loss is 0.3202197253704071\n",
      "epoch: 5 step: 373, loss is 0.3520234525203705\n",
      "epoch: 5 step: 374, loss is 0.40829798579216003\n",
      "epoch: 5 step: 375, loss is 0.2174639105796814\n",
      "epoch: 5 step: 376, loss is 0.25452879071235657\n",
      "epoch: 5 step: 377, loss is 0.19692851603031158\n",
      "epoch: 5 step: 378, loss is 0.3680320680141449\n",
      "epoch: 5 step: 379, loss is 0.3823380768299103\n",
      "epoch: 5 step: 380, loss is 0.4013252258300781\n",
      "epoch: 5 step: 381, loss is 0.3934638798236847\n",
      "epoch: 5 step: 382, loss is 0.20408830046653748\n",
      "epoch: 5 step: 383, loss is 0.5247225761413574\n",
      "epoch: 5 step: 384, loss is 0.3791125416755676\n",
      "epoch: 5 step: 385, loss is 0.39548537135124207\n",
      "epoch: 5 step: 386, loss is 0.34598439931869507\n",
      "epoch: 5 step: 387, loss is 0.30142033100128174\n",
      "epoch: 5 step: 388, loss is 0.31327927112579346\n",
      "epoch: 5 step: 389, loss is 0.24316087365150452\n",
      "epoch: 5 step: 390, loss is 0.283039391040802\n",
      "epoch: 5 step: 391, loss is 0.20029804110527039\n",
      "epoch: 5 step: 392, loss is 0.33962950110435486\n",
      "epoch: 5 step: 393, loss is 0.29724469780921936\n",
      "epoch: 5 step: 394, loss is 0.3115385174751282\n",
      "epoch: 5 step: 395, loss is 0.26787716150283813\n",
      "epoch: 5 step: 396, loss is 0.3346242606639862\n",
      "epoch: 5 step: 397, loss is 0.24199232459068298\n",
      "epoch: 5 step: 398, loss is 0.29624316096305847\n",
      "epoch: 5 step: 399, loss is 0.2941240072250366\n",
      "epoch: 5 step: 400, loss is 0.37291452288627625\n",
      "epoch: 5 step: 401, loss is 0.3099254369735718\n",
      "epoch: 5 step: 402, loss is 0.2997139096260071\n",
      "epoch: 5 step: 403, loss is 0.26265576481819153\n",
      "epoch: 5 step: 404, loss is 0.21961496770381927\n",
      "epoch: 5 step: 405, loss is 0.16112585365772247\n",
      "epoch: 5 step: 406, loss is 0.19476166367530823\n",
      "epoch: 5 step: 407, loss is 0.24231579899787903\n",
      "epoch: 5 step: 408, loss is 0.3182414472103119\n",
      "epoch: 5 step: 409, loss is 0.3887391686439514\n",
      "epoch: 5 step: 410, loss is 0.28803420066833496\n",
      "epoch: 5 step: 411, loss is 0.22876957058906555\n",
      "epoch: 5 step: 412, loss is 0.18657223880290985\n",
      "epoch: 5 step: 413, loss is 0.19177262485027313\n",
      "epoch: 5 step: 414, loss is 0.3626285791397095\n",
      "epoch: 5 step: 415, loss is 0.22900645434856415\n",
      "epoch: 5 step: 416, loss is 0.48417428135871887\n",
      "epoch: 5 step: 417, loss is 0.3383430540561676\n",
      "epoch: 5 step: 418, loss is 0.3732885420322418\n",
      "epoch: 5 step: 419, loss is 0.4625573754310608\n",
      "epoch: 5 step: 420, loss is 0.22385309636592865\n",
      "epoch: 5 step: 421, loss is 0.2602051794528961\n",
      "epoch: 5 step: 422, loss is 0.36906206607818604\n",
      "epoch: 5 step: 423, loss is 0.43125566840171814\n",
      "epoch: 5 step: 424, loss is 0.5516263842582703\n",
      "epoch: 5 step: 425, loss is 0.2923683226108551\n",
      "epoch: 5 step: 426, loss is 0.3116222321987152\n",
      "epoch: 5 step: 427, loss is 0.2794090509414673\n",
      "epoch: 5 step: 428, loss is 0.29364991188049316\n",
      "epoch: 5 step: 429, loss is 0.2854541540145874\n",
      "epoch: 5 step: 430, loss is 0.23540261387825012\n",
      "epoch: 5 step: 431, loss is 0.32256272435188293\n",
      "epoch: 5 step: 432, loss is 0.21987763047218323\n",
      "epoch: 5 step: 433, loss is 0.2564447522163391\n",
      "epoch: 5 step: 434, loss is 0.5380852222442627\n",
      "epoch: 5 step: 435, loss is 0.3050806522369385\n",
      "epoch: 5 step: 436, loss is 0.39422744512557983\n",
      "epoch: 5 step: 437, loss is 0.4098533093929291\n",
      "epoch: 5 step: 438, loss is 0.1655692756175995\n",
      "epoch: 5 step: 439, loss is 0.33549800515174866\n",
      "epoch: 5 step: 440, loss is 0.3032997250556946\n",
      "epoch: 5 step: 441, loss is 0.34742471575737\n",
      "epoch: 5 step: 442, loss is 0.36472034454345703\n",
      "epoch: 5 step: 443, loss is 0.18411819636821747\n",
      "epoch: 5 step: 444, loss is 0.37400391697883606\n",
      "epoch: 5 step: 445, loss is 0.33734744787216187\n",
      "epoch: 5 step: 446, loss is 0.25352776050567627\n",
      "epoch: 5 step: 447, loss is 0.497596800327301\n",
      "epoch: 5 step: 448, loss is 0.37416261434555054\n",
      "epoch: 5 step: 449, loss is 0.2720136344432831\n",
      "epoch: 5 step: 450, loss is 0.5261778235435486\n",
      "epoch: 5 step: 451, loss is 0.5512786507606506\n",
      "epoch: 5 step: 452, loss is 0.31950804591178894\n",
      "epoch: 5 step: 453, loss is 0.32043880224227905\n",
      "epoch: 5 step: 454, loss is 0.44204479455947876\n",
      "epoch: 5 step: 455, loss is 0.2074347734451294\n",
      "epoch: 5 step: 456, loss is 0.2974867522716522\n",
      "epoch: 5 step: 457, loss is 0.4299282431602478\n",
      "epoch: 5 step: 458, loss is 0.229119211435318\n",
      "epoch: 5 step: 459, loss is 0.2973499894142151\n",
      "epoch: 5 step: 460, loss is 0.2834673523902893\n",
      "epoch: 5 step: 461, loss is 0.3341373801231384\n",
      "epoch: 5 step: 462, loss is 0.2919291853904724\n",
      "epoch: 5 step: 463, loss is 0.2820965349674225\n",
      "epoch: 5 step: 464, loss is 0.3668932616710663\n",
      "epoch: 5 step: 465, loss is 0.46438315510749817\n",
      "epoch: 5 step: 466, loss is 0.4433835446834564\n",
      "epoch: 5 step: 467, loss is 0.46955686807632446\n",
      "epoch: 5 step: 468, loss is 0.36484837532043457\n",
      "epoch: 5 step: 469, loss is 0.5344666242599487\n",
      "epoch: 5 step: 470, loss is 0.331543892621994\n",
      "epoch: 5 step: 471, loss is 0.41607406735420227\n",
      "epoch: 5 step: 472, loss is 0.25031372904777527\n",
      "epoch: 5 step: 473, loss is 0.30137020349502563\n",
      "epoch: 5 step: 474, loss is 0.2057298868894577\n",
      "epoch: 5 step: 475, loss is 0.27983224391937256\n",
      "epoch: 5 step: 476, loss is 0.3226880133152008\n",
      "epoch: 5 step: 477, loss is 0.34930214285850525\n",
      "epoch: 5 step: 478, loss is 0.13111074268817902\n",
      "epoch: 5 step: 479, loss is 0.2198544293642044\n",
      "epoch: 5 step: 480, loss is 0.30649691820144653\n",
      "epoch: 5 step: 481, loss is 0.24257922172546387\n",
      "epoch: 5 step: 482, loss is 0.3330065608024597\n",
      "epoch: 5 step: 483, loss is 0.3040006160736084\n",
      "epoch: 5 step: 484, loss is 0.15350666642189026\n",
      "epoch: 5 step: 485, loss is 0.2724224925041199\n",
      "epoch: 5 step: 486, loss is 0.26211023330688477\n",
      "epoch: 5 step: 487, loss is 0.28087589144706726\n",
      "epoch: 5 step: 488, loss is 0.3505868911743164\n",
      "epoch: 5 step: 489, loss is 0.24846170842647552\n",
      "epoch: 5 step: 490, loss is 0.2352709174156189\n",
      "epoch: 5 step: 491, loss is 0.2700037956237793\n",
      "epoch: 5 step: 492, loss is 0.2654494047164917\n",
      "epoch: 5 step: 493, loss is 0.2406568080186844\n",
      "epoch: 5 step: 494, loss is 0.42254966497421265\n",
      "epoch: 5 step: 495, loss is 0.3236914277076721\n",
      "epoch: 5 step: 496, loss is 0.22437143325805664\n",
      "epoch: 5 step: 497, loss is 0.2615480422973633\n",
      "epoch: 5 step: 498, loss is 0.2699414789676666\n",
      "epoch: 5 step: 499, loss is 0.33888885378837585\n",
      "epoch: 5 step: 500, loss is 0.3314482271671295\n",
      "epoch: 5 step: 501, loss is 0.3126254975795746\n",
      "epoch: 5 step: 502, loss is 0.5145075917243958\n",
      "epoch: 5 step: 503, loss is 0.4262583255767822\n",
      "epoch: 5 step: 504, loss is 0.2610982358455658\n",
      "epoch: 5 step: 505, loss is 0.5733676552772522\n",
      "epoch: 5 step: 506, loss is 0.3280680179595947\n",
      "epoch: 5 step: 507, loss is 0.5141783952713013\n",
      "epoch: 5 step: 508, loss is 0.23171070218086243\n",
      "epoch: 5 step: 509, loss is 0.28905802965164185\n",
      "epoch: 5 step: 510, loss is 0.3667711317539215\n",
      "epoch: 5 step: 511, loss is 0.2418976128101349\n",
      "epoch: 5 step: 512, loss is 0.2658626437187195\n",
      "epoch: 5 step: 513, loss is 0.32327160239219666\n",
      "epoch: 5 step: 514, loss is 0.44100114703178406\n",
      "epoch: 5 step: 515, loss is 0.3948328495025635\n",
      "epoch: 5 step: 516, loss is 0.2707953155040741\n",
      "epoch: 5 step: 517, loss is 0.3541400134563446\n",
      "epoch: 5 step: 518, loss is 0.2592906951904297\n",
      "epoch: 5 step: 519, loss is 0.2892836332321167\n",
      "epoch: 5 step: 520, loss is 0.3654891550540924\n",
      "epoch: 5 step: 521, loss is 0.3542487323284149\n",
      "epoch: 5 step: 522, loss is 0.31109434366226196\n",
      "epoch: 5 step: 523, loss is 0.42130351066589355\n",
      "epoch: 5 step: 524, loss is 0.3401455283164978\n",
      "epoch: 5 step: 525, loss is 0.13765662908554077\n",
      "epoch: 5 step: 526, loss is 0.31477099657058716\n",
      "epoch: 5 step: 527, loss is 0.2482137531042099\n",
      "epoch: 5 step: 528, loss is 0.34579604864120483\n",
      "epoch: 5 step: 529, loss is 0.2422175407409668\n",
      "epoch: 5 step: 530, loss is 0.19647854566574097\n",
      "epoch: 5 step: 531, loss is 0.33928459882736206\n",
      "epoch: 5 step: 532, loss is 0.21594621241092682\n",
      "epoch: 5 step: 533, loss is 0.39636266231536865\n",
      "epoch: 5 step: 534, loss is 0.29825520515441895\n",
      "epoch: 5 step: 535, loss is 0.435435026884079\n",
      "epoch: 5 step: 536, loss is 0.39757272601127625\n",
      "epoch: 5 step: 537, loss is 0.49153876304626465\n",
      "epoch: 5 step: 538, loss is 0.2856043875217438\n",
      "epoch: 5 step: 539, loss is 0.5540902614593506\n",
      "epoch: 5 step: 540, loss is 0.21602432429790497\n",
      "epoch: 5 step: 541, loss is 0.31498005986213684\n",
      "epoch: 5 step: 542, loss is 0.26185905933380127\n",
      "epoch: 5 step: 543, loss is 0.3586045205593109\n",
      "epoch: 5 step: 544, loss is 0.2081649899482727\n",
      "epoch: 5 step: 545, loss is 0.3854966163635254\n",
      "epoch: 5 step: 546, loss is 0.4478974938392639\n",
      "epoch: 5 step: 547, loss is 0.4955304265022278\n",
      "epoch: 5 step: 548, loss is 0.34622061252593994\n",
      "epoch: 5 step: 549, loss is 0.41257163882255554\n",
      "epoch: 5 step: 550, loss is 0.31265348196029663\n",
      "epoch: 5 step: 551, loss is 0.26859286427497864\n",
      "epoch: 5 step: 552, loss is 0.26619622111320496\n",
      "epoch: 5 step: 553, loss is 0.28472304344177246\n",
      "epoch: 5 step: 554, loss is 0.2452080249786377\n",
      "epoch: 5 step: 555, loss is 0.2578407824039459\n",
      "epoch: 5 step: 556, loss is 0.47540178894996643\n",
      "epoch: 5 step: 557, loss is 0.18712972104549408\n",
      "epoch: 5 step: 558, loss is 0.5508819222450256\n",
      "epoch: 5 step: 559, loss is 0.38701075315475464\n",
      "epoch: 5 step: 560, loss is 0.3766462802886963\n",
      "epoch: 5 step: 561, loss is 0.3471088707447052\n",
      "epoch: 5 step: 562, loss is 0.34749504923820496\n",
      "epoch: 5 step: 563, loss is 0.20176663994789124\n",
      "epoch: 5 step: 564, loss is 0.37769612669944763\n",
      "epoch: 5 step: 565, loss is 0.2571782171726227\n",
      "epoch: 5 step: 566, loss is 0.28210049867630005\n",
      "epoch: 5 step: 567, loss is 0.20782144367694855\n",
      "epoch: 5 step: 568, loss is 0.35618355870246887\n",
      "epoch: 5 step: 569, loss is 0.33756259083747864\n",
      "epoch: 5 step: 570, loss is 0.3188580274581909\n",
      "epoch: 5 step: 571, loss is 0.37635138630867004\n",
      "epoch: 5 step: 572, loss is 0.32382288575172424\n",
      "epoch: 5 step: 573, loss is 0.33521562814712524\n",
      "epoch: 5 step: 574, loss is 0.35030850768089294\n",
      "epoch: 5 step: 575, loss is 0.2076900601387024\n",
      "epoch: 5 step: 576, loss is 0.3647046387195587\n",
      "epoch: 5 step: 577, loss is 0.32498699426651\n",
      "epoch: 5 step: 578, loss is 0.2772918939590454\n",
      "epoch: 5 step: 579, loss is 0.4993263781070709\n",
      "epoch: 5 step: 580, loss is 0.3238998055458069\n",
      "epoch: 5 step: 581, loss is 0.4756849706172943\n",
      "epoch: 5 step: 582, loss is 0.3753085434436798\n",
      "epoch: 5 step: 583, loss is 0.3029719293117523\n",
      "epoch: 5 step: 584, loss is 0.2949294447898865\n",
      "epoch: 5 step: 585, loss is 0.24937517940998077\n",
      "epoch: 5 step: 586, loss is 0.20455847680568695\n",
      "epoch: 5 step: 587, loss is 0.32883086800575256\n",
      "epoch: 5 step: 588, loss is 0.2758829593658447\n",
      "epoch: 5 step: 589, loss is 0.6200757622718811\n",
      "epoch: 5 step: 590, loss is 0.44543588161468506\n",
      "epoch: 5 step: 591, loss is 0.27594712376594543\n",
      "epoch: 5 step: 592, loss is 0.38342002034187317\n",
      "epoch: 5 step: 593, loss is 0.30640193819999695\n",
      "epoch: 5 step: 594, loss is 0.32013121247291565\n",
      "epoch: 5 step: 595, loss is 0.5639133453369141\n",
      "epoch: 5 step: 596, loss is 0.3785192668437958\n",
      "epoch: 5 step: 597, loss is 0.4188401401042938\n",
      "epoch: 5 step: 598, loss is 0.4903678596019745\n",
      "epoch: 5 step: 599, loss is 0.1305864006280899\n",
      "epoch: 5 step: 600, loss is 0.4845272898674011\n",
      "epoch: 5 step: 601, loss is 0.5063014030456543\n",
      "epoch: 5 step: 602, loss is 0.3283446133136749\n",
      "epoch: 5 step: 603, loss is 0.2319965660572052\n",
      "epoch: 5 step: 604, loss is 0.33481696248054504\n",
      "epoch: 5 step: 605, loss is 0.2401452362537384\n",
      "epoch: 5 step: 606, loss is 0.29179900884628296\n",
      "epoch: 5 step: 607, loss is 0.29351314902305603\n",
      "epoch: 5 step: 608, loss is 0.2497451901435852\n",
      "epoch: 5 step: 609, loss is 0.25925561785697937\n",
      "epoch: 5 step: 610, loss is 0.31944209337234497\n",
      "epoch: 5 step: 611, loss is 0.271515429019928\n",
      "epoch: 5 step: 612, loss is 0.3000606596469879\n",
      "epoch: 5 step: 613, loss is 0.17007480561733246\n",
      "epoch: 5 step: 614, loss is 0.26898524165153503\n",
      "epoch: 5 step: 615, loss is 0.3457995653152466\n",
      "epoch: 5 step: 616, loss is 0.27909377217292786\n",
      "epoch: 5 step: 617, loss is 0.25186237692832947\n",
      "epoch: 5 step: 618, loss is 0.4953196346759796\n",
      "epoch: 5 step: 619, loss is 0.2642989158630371\n",
      "epoch: 5 step: 620, loss is 0.40385085344314575\n",
      "epoch: 5 step: 621, loss is 0.29145294427871704\n",
      "epoch: 5 step: 622, loss is 0.27864909172058105\n",
      "epoch: 5 step: 623, loss is 0.24416403472423553\n",
      "epoch: 5 step: 624, loss is 0.28470364212989807\n",
      "epoch: 5 step: 625, loss is 0.2584051787853241\n",
      "epoch: 5 step: 626, loss is 0.20312601327896118\n",
      "epoch: 5 step: 627, loss is 0.23712868988513947\n",
      "epoch: 5 step: 628, loss is 0.3407856523990631\n",
      "epoch: 5 step: 629, loss is 0.3452286124229431\n",
      "epoch: 5 step: 630, loss is 0.3170969784259796\n",
      "epoch: 5 step: 631, loss is 0.3239545226097107\n",
      "epoch: 5 step: 632, loss is 0.38512974977493286\n",
      "epoch: 5 step: 633, loss is 0.15515443682670593\n",
      "epoch: 5 step: 634, loss is 0.4501301348209381\n",
      "epoch: 5 step: 635, loss is 0.2908778190612793\n",
      "epoch: 5 step: 636, loss is 0.3149467706680298\n",
      "epoch: 5 step: 637, loss is 0.2619641125202179\n",
      "epoch: 5 step: 638, loss is 0.20929935574531555\n",
      "epoch: 5 step: 639, loss is 0.22523556649684906\n",
      "epoch: 5 step: 640, loss is 0.2116636037826538\n",
      "epoch: 5 step: 641, loss is 0.47000905871391296\n",
      "epoch: 5 step: 642, loss is 0.17909367382526398\n",
      "epoch: 5 step: 643, loss is 0.16326144337654114\n",
      "epoch: 5 step: 644, loss is 0.2688089609146118\n",
      "epoch: 5 step: 645, loss is 0.2850782573223114\n",
      "epoch: 5 step: 646, loss is 0.4336530268192291\n",
      "epoch: 5 step: 647, loss is 0.18704238533973694\n",
      "epoch: 5 step: 648, loss is 0.29395782947540283\n",
      "epoch: 5 step: 649, loss is 0.09698087722063065\n",
      "epoch: 5 step: 650, loss is 0.29399558901786804\n",
      "epoch: 5 step: 651, loss is 0.2898278534412384\n",
      "epoch: 5 step: 652, loss is 0.18166635930538177\n",
      "epoch: 5 step: 653, loss is 0.2647497355937958\n",
      "epoch: 5 step: 654, loss is 0.1939103901386261\n",
      "epoch: 5 step: 655, loss is 0.254670649766922\n",
      "epoch: 5 step: 656, loss is 0.49305060505867004\n",
      "epoch: 5 step: 657, loss is 0.3341856598854065\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 训练有正则化的网络\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m train(ForwardFashionRegularization)\n",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(Net)\u001b[0m\n\u001b[0;32m     10\u001b[0m loss_cb \u001b[39m=\u001b[39m LossMonitor()\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m============== Starting Training ==============\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m model\u001b[39m.\u001b[39;49mtrain(\u001b[39m30\u001b[39;49m, ds_train, callbacks\u001b[39m=\u001b[39;49m[loss_cb], dataset_sink_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     13\u001b[0m \u001b[39m# 验证\u001b[39;00m\n\u001b[0;32m     14\u001b[0m metric \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval(ds_test)\n",
      "File \u001b[1;32mc:\\Users\\15781\\anaconda3\\envs\\mindspore_py38\\lib\\site-packages\\mindspore\\train\\model.py:1056\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, epoch, train_dataset, callbacks, dataset_sink_mode, sink_size, initial_epoch)\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[39mif\u001b[39;00m callbacks:\n\u001b[0;32m   1054\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_methods_for_custom_callbacks(callbacks, \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1056\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(epoch,\n\u001b[0;32m   1057\u001b[0m             train_dataset,\n\u001b[0;32m   1058\u001b[0m             callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1059\u001b[0m             dataset_sink_mode\u001b[39m=\u001b[39;49mdataset_sink_mode,\n\u001b[0;32m   1060\u001b[0m             sink_size\u001b[39m=\u001b[39;49msink_size,\n\u001b[0;32m   1061\u001b[0m             initial_epoch\u001b[39m=\u001b[39;49minitial_epoch)\n\u001b[0;32m   1063\u001b[0m \u001b[39m# When it's distributed training and using MindRT,\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[39m# the node id should be reset to start from 0.\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[39m# This is to avoid the timeout when finding the actor route tables in 'train' and 'eval' case(or 'fit').\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[39mif\u001b[39;00m _enable_distributed_mindrt():\n",
      "File \u001b[1;32mc:\\Users\\15781\\anaconda3\\envs\\mindspore_py38\\lib\\site-packages\\mindspore\\train\\model.py:100\u001b[0m, in \u001b[0;36m_save_final_ckpt.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 100\u001b[0m     func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\15781\\anaconda3\\envs\\mindspore_py38\\lib\\site-packages\\mindspore\\train\\model.py:614\u001b[0m, in \u001b[0;36mModel._train\u001b[1;34m(self, epoch, train_dataset, callbacks, dataset_sink_mode, sink_size, initial_epoch, valid_dataset, valid_frequency, valid_dataset_sink_mode)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[39melif\u001b[39;00m context\u001b[39m.\u001b[39mget_context(\u001b[39m\"\u001b[39m\u001b[39mdevice_target\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCPU\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    612\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mThe CPU cannot support dataset sink mode currently.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mSo the training process will be performed with dataset not sink.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 614\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_process(epoch, train_dataset, list_callback, cb_params, initial_epoch, valid_infos)\n\u001b[0;32m    615\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    616\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_dataset_sink_process(epoch, train_dataset, list_callback,\n\u001b[0;32m    617\u001b[0m                                      cb_params, sink_size, initial_epoch, valid_infos)\n",
      "File \u001b[1;32mc:\\Users\\15781\\anaconda3\\envs\\mindspore_py38\\lib\\site-packages\\mindspore\\train\\model.py:909\u001b[0m, in \u001b[0;36mModel._train_process\u001b[1;34m(self, epoch, train_dataset, list_callback, cb_params, initial_epoch, valid_infos)\u001b[0m\n\u001b[0;32m    907\u001b[0m list_callback\u001b[39m.\u001b[39mon_train_step_begin(run_context)\n\u001b[0;32m    908\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_network_mode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_network, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 909\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_network(\u001b[39m*\u001b[39;49mnext_element)\n\u001b[0;32m    910\u001b[0m cb_params\u001b[39m.\u001b[39mnet_outputs \u001b[39m=\u001b[39m outputs\n\u001b[0;32m    911\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loss_scale_manager \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loss_scale_manager\u001b[39m.\u001b[39mget_drop_overflow_update():\n",
      "File \u001b[1;32mc:\\Users\\15781\\anaconda3\\envs\\mindspore_py38\\lib\\site-packages\\mindspore\\nn\\cell.py:657\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mnew_graph(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 657\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_construct(args, kwargs)\n\u001b[0;32m    658\u001b[0m     _pynative_executor\u001b[39m.\u001b[39mend_graph(\u001b[39mself\u001b[39m, output, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    659\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\15781\\anaconda3\\envs\\mindspore_py38\\lib\\site-packages\\mindspore\\nn\\cell.py:445\u001b[0m, in \u001b[0;36mCell._run_construct\u001b[1;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[0;32m    443\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shard_fn(\u001b[39m*\u001b[39mcast_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    444\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 445\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(\u001b[39m*\u001b[39;49mcast_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_forward_hook:\n\u001b[0;32m    447\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward_hook(cast_inputs, output)\n",
      "File \u001b[1;32mc:\\Users\\15781\\anaconda3\\envs\\mindspore_py38\\lib\\site-packages\\mindspore\\nn\\wrap\\cell_wrapper.py:388\u001b[0m, in \u001b[0;36mTrainOneStepCell.construct\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    386\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork(\u001b[39m*\u001b[39minputs)\n\u001b[0;32m    387\u001b[0m sens \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mfill(loss\u001b[39m.\u001b[39mdtype, loss\u001b[39m.\u001b[39mshape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msens)\n\u001b[1;32m--> 388\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights)(\u001b[39m*\u001b[39;49minputs, sens)\n\u001b[0;32m    389\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_reducer(grads)\n\u001b[0;32m    390\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdepend(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer(grads))\n",
      "File \u001b[1;32mc:\\Users\\15781\\anaconda3\\envs\\mindspore_py38\\lib\\site-packages\\mindspore\\ops\\composite\\base.py:375\u001b[0m, in \u001b[0;36mGradOperation.__call__.<locals>.after_grad\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mafter_grad\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 375\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_(fn, weights)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\15781\\anaconda3\\envs\\mindspore_py38\\lib\\site-packages\\mindspore\\common\\api.py:102\u001b[0m, in \u001b[0;36m_wrap_func.<locals>.wrapper\u001b[1;34m(*arg, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[0;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39marg, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 102\u001b[0m     results \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49marg, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m _convert_python_data(results)\n",
      "File \u001b[1;32mc:\\Users\\15781\\anaconda3\\envs\\mindspore_py38\\lib\\site-packages\\mindspore\\ops\\composite\\base.py:367\u001b[0m, in \u001b[0;36mGradOperation.__call__.<locals>.after_grad\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pynative_forward_run(fn, grad_, weights, args, kwargs)\n\u001b[0;32m    366\u001b[0m _pynative_executor\u001b[39m.\u001b[39mgrad(fn, grad_, weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_position, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 367\u001b[0m out \u001b[39m=\u001b[39m _pynative_executor()\n\u001b[0;32m    368\u001b[0m out \u001b[39m=\u001b[39m _grads_divided_by_device_num_if_recomputation(out)\n\u001b[0;32m    369\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\15781\\anaconda3\\envs\\mindspore_py38\\lib\\site-packages\\mindspore\\common\\api.py:1015\u001b[0m, in \u001b[0;36m_PyNativeExecutor.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1008\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1009\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[39m    PyNative executor run grad graph.\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m \n\u001b[0;32m   1012\u001b[0m \u001b[39m    Return:\u001b[39;00m\n\u001b[0;32m   1013\u001b[0m \u001b[39m        The return object after running grad graph.\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1015\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executor()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练有正则化的网络\n",
    "# model = train(ForwardFashionRegularization)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个sample预测结果： 4    真实结果： 4\n",
      "第1个sample预测结果： 1    真实结果： 1\n",
      "第2个sample预测结果： 7    真实结果： 7\n",
      "第3个sample预测结果： 5    真实结果： 5\n",
      "第4个sample预测结果： 4    真实结果： 4\n",
      "第5个sample预测结果： 5    真实结果： 5\n",
      "第6个sample预测结果： 2    真实结果： 2\n",
      "第7个sample预测结果： 7    真实结果： 7\n",
      "第8个sample预测结果： 4    真实结果： 4\n",
      "第9个sample预测结果： 7    真实结果： 7\n",
      "第10个sample预测结果： 7    真实结果： 7\n",
      "第11个sample预测结果： 8    真实结果： 8\n",
      "第12个sample预测结果： 9    真实结果： 9\n",
      "第13个sample预测结果： 7    真实结果： 7\n",
      "第14个sample预测结果： 8    真实结果： 8\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "ds_test, _ = create_dataset()\n",
    "test_ = ds_test.create_dict_iterator(output_numpy=True).__next__()\n",
    "predictions = model.predict(Tensor(test_['x']))\n",
    "predictions = predictions.asnumpy()\n",
    "for i in range(15):\n",
    "    p_np = predictions[i, :]\n",
    "    p_list = p_np.tolist()\n",
    "    print('第' + str(i) + '个sample预测结果：', p_list.index(max(p_list)), '   真实结果：', test_['y'][i])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可视化结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'this_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m     predicted_label \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(predictions_array)\n\u001b[0;32m     28\u001b[0m     this_plot[predicted_label]\u001b[39m.\u001b[39mset_color(\u001b[39m'\u001b[39m\u001b[39mred\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m this_plot[true_label]\u001b[39m.\u001b[39mset_color(\u001b[39m'\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax_np\u001b[39m(x):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'this_plot' is not defined"
     ]
    }
   ],
   "source": [
    "# -------------------定义可视化函数--------------------------------\n",
    "# 输入预测结果序列，真实标签序列，以及图片序列\n",
    "# 目标是根据预测值对错，让其标签显示为红色或者蓝色。对：标签为红色；错：标签为蓝色\n",
    "def plot_image(predictions_array, true_label, img):\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    # 显示对应图片\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    # 显示预测结果的颜色，如果对上了是蓝色，否则为红色\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    # 显示对应标签的格式，样式\n",
    "    plt.xlabel('{},{:2.0f}% ({})'.format(class_names[predicted_label],\n",
    "                                         100 * np.max(predictions_array),\n",
    "                                         class_names[true_label]), color=color)\n",
    "# 将预测的结果以柱状图形状显示蓝对红错\n",
    "def plot_value_array(predictions_array, true_label):\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    this_plot = plt.bar(range(10), predictions_array, color='#777777')\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    this_plot[predicted_label].set_color('red')\n",
    "this_plot[true_label].set_color('blue')\n",
    "\n",
    "import numpy as np\n",
    "def softmax_np(x):\n",
    "    x = x - np.max(x)\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x/np.sum(exp_x)\n",
    "    return softmax_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测15个图像与标签，并展现出来\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows * num_cols\n",
    "plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)\n",
    "    pred_np_ = predictions[i, :]\n",
    "    pred_np_ = softmax_np(pred_np_)\n",
    "    plot_image(pred_np_, test_['y'][i], test_['x'][i, 0, ...])\n",
    "    plt.subplot(num_rows, 2 * num_cols, 2 * i + 2)\n",
    "    plot_value_array(pred_np_, test_['y'][i])\n",
    "plt.show() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
