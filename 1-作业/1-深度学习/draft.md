**参考**
VIT，MAE

[第一次组会汇报：MAE及其部分发展（ViT-SSL） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/586314552)

[何凯明最新一作论文：ViT最新预训练目标MAE，性能超越以往所有ViT变体 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/435566994)

[如何评价何恺明等关于Masked Autoencoder的新作? - 知乎 (zhihu.com)](https://www.zhihu.com/question/498423346)

# 论文简介



# 选读原因

本篇论文由Kai He等人所作，Follow了ViT所做的工作，其中ViT将广泛用于nlp领域的Transformer迁移到cv中，并取得了良好效果。本篇论文提出了xxxx，解决了xxxx问题，而思路也十分新颖，同时本文也开辟了xxxx，提出了一些值得关注的问题（如安全性），使得本篇论文具有一定的价值。

# 阅读方法

本人首先关注本篇论文的摘要，引言和结论，从整体上把握本篇文章所提出的方法，应用与缺点。然后仔细阅读本文的方法部分，从细节上把握本篇文章所提出的模型架构，最后阅读本篇文章所做的实验，进一步了解本篇文章所提出架构的优势。

# 论文背景



# 论文所用方法（需分析采用该方法的原因）

论文在第三小节详细介绍了Masked Autoencoders（MAE）是如何工作的：

- 第一步是进行掩码操作。这一步是将原始输入划分为多个patch，然后随机挑选出一部分patch用于MAE的后续输入部分。采样所服从的分布为均匀分布。总的来说，掩码操作使得输入变得高度稀疏（即被标成掩码的patch不需要作为输入送入后续的模型），大幅度降低了后续构建模型的复杂度。其次，服从均匀分布的随机采样防止了可能存在的中心偏差（即图像中心存在更多的被遮掩的patch）。
- 第二步则是开始构建MAE的编码器。作者介绍编码器是一个ViT。这里先介绍一下ViT：
  （介绍ViT）
  最后，Decoder的输入不包括被遮掩的Patch。这一操作大大降低了模型复杂度，这意味着需要使用的内存大大减少，使得模型更轻量。
- 第三步是构建MAE的解码器。解码器的输入是一系列的token，包括patch送入编码器后产生的输出，以及一系列的掩码token。每个掩码token是一个向量，用来表示等待预测的缺失的patch。这些掩码token还需要加上一系列的位置嵌入，来表示这个掩码token在原来的图片中代表哪个位置的patch。之后，解码器又是另外一系列Transformer块，用来在预训练期间执行图像重建任务。另外，解码器可以独立于编码器而灵活地进行设计，因此，解码器可以使用比编码器规模更小的架构，采用不对称的设计，使得预训练时间大幅度缩短。
- 最后，就可以使用整个模型对整张图片进行重建了。解码器的输出的每一个元素相当于原有图片对应位置的patch的像素值向量。输出通道数量与patch数量相等。损失函数为所有被遮掩的patch复原后与原有patch的像素值的均方误差。这里不考虑未被遮掩的patch的原因是输出和输入都是相同的，对未被遮掩的patch计算均方误差损失没有意义。此外，作者表明如果对像素进行归一化的预处理，就能提高图片的生成质量。

总体来说，这篇文章提出的算法比较简单，简单归纳一下，这篇文章所要解决的问题是对一张具有遮掩的图像进行恢复。其提出了非对称的编码器-解码器架构，使得解码器架构的设计有轻量化的可能；同时送入编码器的图像不包括被遮掩的patch。这两处关键设计使得该模型的训练速度得到一定程度的提升，模型大小能够得到缩小。实际上，MAE对图片分多个patch，设置mask的设计类似于NLP中BERT模型对输入token的mask操作，因此MAE也具有自监督性质。回想一下之前在编码器部分中所提到的的ViT模型，其可以认为是视觉任务上的Transformer（**V**is**I**on **T**ransformer），而BERT采用了Transformer的同时，也具有mask操作。可以说，MAE对多个patch的掩码操作与BERT的思想很类似，可以认为作者从这里得到了灵感启发。

# 论文实验结果



# 总结与个人感悟